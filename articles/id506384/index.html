<!doctype html>
<html class="no-js" lang="id">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🍏 👩‍🔬 🔌 Jaringan saraf - pelatihan tanpa guru. Metode Gradien Kebijakan 🛌🏻 👴 🐊</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Selamat siang, Habr
 Artikel ini membuka serangkaian artikel tentang cara melatih jaringan saraf tanpa guru. 
 (Penguatan Pembelajaran untuk Jaringan ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Jaringan saraf - pelatihan tanpa guru. Metode Gradien Kebijakan</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/506384/"><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Selamat siang, Habr</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Artikel ini membuka serangkaian artikel tentang cara melatih jaringan saraf tanpa guru. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 (Penguatan Pembelajaran untuk Jaringan Neuron) </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dalam siklus saya berencana untuk membuat tiga artikel tentang teori dan implementasi dalam kode tiga algoritma pelatihan untuk jaringan saraf tanpa guru. </font><font style="vertical-align: inherit;">Artikel pertama adalah tentang Gradien Kebijakan, artikel kedua tentang Q-learning, artikel ketiga akan final sesuai dengan metode Aktor-Kritik. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Selamat membaca.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Artikel Satu - Gradien Kebijakan </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 (Pembelajaran </font><font style="vertical-align: inherit;">Non-Guru </font><font style="vertical-align: inherit;">)</font></font></h4><br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">pengantar</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Di antara algoritma pembelajaran mesin, tempat khusus ditempati oleh algoritma pembelajaran mesin di mana algoritma belajar untuk menyelesaikan masalah sendiri tanpa campur tangan manusia, berinteraksi langsung dengan lingkungan di mana ia belajar. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Algoritme seperti itu telah menerima nama umum - algoritma pembelajaran tanpa guru, untuk algoritme seperti itu, Anda tidak perlu mengumpulkan basis data, Anda tidak perlu mengklasifikasikan atau menandai mereka. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Sudah cukup bagi siswa tanpa guru untuk hanya memberikan tanggapan mundur terhadap tindakan atau keputusannya - apakah itu baik atau tidak.</font></font><br>
<a name="habracut"></a><br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bab 1. Pelatihan Guru</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Jadi apa itu - Belajar dengan atau tanpa guru. Kami akan memeriksa ini lebih detail dengan contoh-contoh dari pembelajaran mesin modern dan tugas-tugas yang diselesaikannya. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Kebanyakan algoritma pembelajaran mesin modern untuk masalah klasifikasi, regresi, segmentasi pada dasarnya adalah algoritma pelatihan dengan seorang guru di mana orang itu sendiri adalah guru. Karena orang yang menandai data yang memberi tahu algoritma apa jawaban yang benar seharusnya dan dengan demikian algoritma berusaha mencari solusi sehingga jawaban yang diberikan algoritma ketika memecahkan masalah cocok dengan jawaban yang ditunjukkan orang untuk tugas yang diberikan sebagai jawaban yang benar. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dengan menggunakan contoh masalah klasifikasi untuk dataset Mnist, jawaban yang benar yang diberikan orang tersebut pada algoritma adalah label kelas digit dalam set pelatihan.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/za/kq/b6/zakqb6ob1_xce07caywnurtsl7k.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dalam dataset Mnist, untuk setiap gambar yang harus dipelajari oleh algoritma mesin untuk diklasifikasi, orang-orang telah mengatur label yang benar untuk kelas tersebut. Dalam proses pembelajaran, algoritma yang memprediksi kelas gambar membandingkan kelas yang diperolehnya untuk gambar tertentu dengan kelas sebenarnya untuk gambar yang sama dan secara bertahap menyesuaikan parameternya dalam proses pembelajaran sehingga kelas yang diprediksi oleh algoritma cenderung sesuai dengan kelas yang ditentukan oleh orang tersebut. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dengan demikian, ide berikut dapat diringkas - algoritma pembelajaran dengan guru adalah algoritma pembelajaran mesin, di mana kami memberikan algoritma bagaimana hal itu perlu dilakukan dengan benar dari sudut pandang kami.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dan tidak masalah bagaimana melanjutkan - menunjukkan kelas mana gambar ini harus ditugaskan jika itu adalah tugas klasifikasi, atau menggambar kontur objek jika itu adalah tugas segmentasi atau cara memutar roda kemudi mobil jika algoritme itu autopilot, penting bahwa untuk setiap situasi tertentu kita Kami secara eksplisit menunjukkan ke algoritma di mana jawaban yang benar adalah, bagaimana melakukannya dengan benar. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ini adalah kunci untuk memahami bagaimana secara mendasar algoritma pembelajaran dengan guru berbeda dari algoritma pembelajaran tanpa guru.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bab 2. Belajar tanpa guru</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Setelah mengetahui apa itu - mengajar dengan seorang guru, sekarang kita akan mengerti apa itu - mengajar tanpa guru. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Seperti yang kami temukan di bab terakhir, ketika mengajar dengan seorang guru, untuk setiap situasi pengajaran, kami memberikan algoritma pemahaman tentang jawaban mana yang benar dari sudut pandang kami, kemudian bergerak dari arah yang berlawanan - dalam belajar tanpa guru, untuk setiap situasi tertentu kami tidak memberikan jawaban seperti itu pada algoritma akan. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Tetapi kemudian muncul pertanyaan, jika kita tidak memberikan algoritma indikasi yang jelas tentang apa yang harus dilakukan dengan benar, lalu apa yang akan dipelajari algoritma? Bagaimana algoritma akan dilatih tanpa mengetahui di mana harus menyesuaikan parameter internal untuk melakukan hal yang benar dan akhirnya menyelesaikan masalah seperti yang kita inginkan.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Mari kita pikirkan topik ini. Penting bagi kami bahwa algoritme menyelesaikan masalah secara keseluruhan, dan bagaimana tepatnya ia akan bertindak dalam proses penyelesaian masalah ini dan bagaimana penyelesaiannya tidak menjadi masalah bagi kami, biarkan kami menyerah pada algoritme itu sendiri, kami hanya mengharapkan hasil akhirnya dari itu . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Karena itu, kami akan membiarkan algoritma memahami hasil akhir apakah itu memecahkan masalah kami dengan baik atau tidak. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Jadi, meringkas semua hal di atas, kita sampai pada kesimpulan bahwa kita menyebut algoritma pembelajaran seperti itu tanpa guru di mana tidak ada indikasi eksplisit untuk algoritma bagaimana melakukannya, tetapi hanya ada penilaian umum atas semua tindakannya dalam proses penyelesaian masalah.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/qi/ci/nn/qicinnqzow8o5b7lacir_mrs4aa.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dalam contoh permainan di mana raket mencoba menangkap kubus yang jatuh dari atas, kami tidak memberi tahu algoritma untuk mengontrol raket pada titik waktu tertentu di mana harus memindahkan raket. </font><font style="vertical-align: inherit;">Kami akan memberi tahu algoritma hanya hasil dari tindakannya - apakah ia menangkap kubus dengan raket atau tidak. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/-f/dp/df/-fdpdf5nbjpawt1mutegmxzw9iu.gif"><br>
<br>
<img src="https://habrastorage.org/webt/ve/jk/wz/vejkwz-fezjvbjcjydzmx7q4hk8.gif"><br>
<br>
<img src="https://habrastorage.org/webt/ws/gv/nb/wsgvnb34jwzwk--6iehpg3y3lqw.gif"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Inilah esensi belajar tanpa seorang guru. </font><font style="vertical-align: inherit;">Algoritme itu sendiri harus belajar untuk memutuskan apa yang harus dilakukan dalam setiap kasus berdasarkan pada penilaian akhir dari totalitas semua tindakannya.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bab 3. Agen, Lingkungan dan Penghargaan</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Setelah mengetahui apa pelatihan tanpa guru, kita akan mempelajari algoritma yang dapat belajar bagaimana memecahkan masalah tanpa tips kita tentang bagaimana melakukannya dengan benar. </font></font><br>
 <br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Saatnya memperkenalkan kita pada terminologi yang akan kita gunakan di masa depan. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Kami akan memanggil agen algoritme kami yang dapat menganalisis keadaan lingkungan dan melakukan beberapa tindakan di dalamnya. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Lingkungan adalah dunia virtual tempat Agen kami ada dan melalui tindakannya dapat mengubah statusnya ... </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Hadiah - umpan balik dari Lingkungan kepada Agen sebagai respons atas tindakannya.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/23/dp/gw/23dpgwboqw80sv_1uj5_ubifms8.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Lingkungan di mana agen kami tinggal mungkin kompleks sewenang-wenang, agen bahkan mungkin tidak tahu bagaimana itu disusun untuk membuat keputusan dan melakukan tindakan. </font><font style="vertical-align: inherit;">Untuk Agen, hanya umpan balik dalam bentuk hadiah yang ia terima dari lingkungan yang penting. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Jika kita mempertimbangkan secara lebih terperinci proses interaksi antara agen dan lingkungan, maka hal itu dapat dinyatakan dengan skema berikut </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ml/f5/eq/mlf5eqvivcvcwodu66mfyddfpdo.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
: Kondisi lingkungan dalam langkah t </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
aksi agen dalam langkah t </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
- hadiah dalam langkah t</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pada setiap saat waktu t, agen kami mengamati keadaan medium - St, melakukan aksi - at, di mana ia menerima hadiah - rt dari medium, bidang yang mediumnya menuju status St + 1, yang diamati agen kami, melakukan aksi - pada + 1, untuk yang menerima hadiah dari medium - rt + 1 dan keadaan seperti itu t, kita dapat memiliki set yang tak terbatas - n.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bab 4. Parameterisasi tugas belajar tanpa guru</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Untuk melatih agen, kita perlu mengukur parameter tugas belajar tanpa guru, dengan kata lain, untuk memahami fungsi apa yang akan kita optimalkan. </font><font style="vertical-align: inherit;">Dalam pembelajaran penguatan - dalam apa yang akan kita sebut pelatihan tanpa guru, ada tiga fungsi dasar seperti: </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
1) p (a | s) - fungsi kebijakan Fungsi </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
probabilitas optimalitas suatu tindakan adalah a, tergantung pada keadaan lingkungan -s. </font><font style="vertical-align: inherit;">Ini menunjukkan kepada kita bagaimana aksi a optimal dalam kondisi medium s. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
2) v (s) - nilai fungsi Fungsi nilai </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
negara adalah s. </font><font style="vertical-align: inherit;">Ini menunjukkan kepada kita seberapa besar keadaan secara umum berharga bagi kita dalam hal imbalan </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
3) Q (s, a) - Q-function</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Q adalah fungsi strategi yang optimal. Hal ini memungkinkan kita, menurut strategi optimal ini, di negara bagian - untuk memilih tindakan optimal untuk keadaan ini - a </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pertama kita menganggap fungsi - fungsi kebijakan, sebagai yang paling sederhana dan intuitif untuk memahami fungsi pembelajaran penguatan. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Karena kita akan menyelesaikan masalah penguatan pembelajaran melalui jaringan saraf. Kemudian, secara skematis, kita dapat membuat parameter fungsi kebijakan melalui jaringan saraf sebagai berikut.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/oa/fv/1x/oafv1xdki7zeejvxvy-tsgd5oeu.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Kami akan memasok status ke input dari jaringan saraf, dan membangun output dari jaringan saraf sedemikian rupa sehingga lapisan output dari jaringan saraf adalah lapisan SoftMax, dengan jumlah output sama dengan jumlah tindakan yang mungkin untuk agen di lingkungan kita. </font><font style="vertical-align: inherit;">Dengan demikian, melewati keadaan s pada keluaran melalui lapisan jaringan saraf, kita memperoleh distribusi probabilitas untuk tindakan agen dalam keadaan s. </font><font style="vertical-align: inherit;">Sebenarnya, apa yang diperlukan bagi kita untuk mulai melatih jaringan saraf kita dan secara iteratif meningkatkan fungsi kebijakan, yang sekarang pada dasarnya adalah jaringan saraf kita, melalui algoritma kesalahan back-propagation.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bab 5. Meningkatkan fungsi kebijakan melalui pelatihan jaringan saraf</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Untuk melatih jaringan saraf, kami menggunakan metode gradient descent. Karena lapisan terakhir dari jaringan saraf kita adalah lapisan SoftMax, fungsi Kerugiannya adalah: di </font></font><br>
<br>
<img src="https://habrastorage.org/webt/29/y9/u2/29y9u2wizl5w2yf5yeyw08nk1ic.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
mana: </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
1 - label sebenarnya * log (label prediksi) </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
2 - jumlah semua contoh </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Namun, bagaimana kita bisa melatih jaringan saraf jika kita belum memiliki label yang benar untuk tindakan agen di negara bagian S0-Sj? Dan kami tidak membutuhkannya, alih-alih label yang benar, kami akan menggunakan hadiah yang diterima agen dari media dengan melakukan tindakan yang diprediksi jaringan saraf untuknya.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/46/jp/or/46jporp-h9pj-bujcmytwhnfmze.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Kami memiliki hak untuk melakukan ini karena, untuk Kehilangan Cross Entropy, yj adalah label yang benar dan untuk kelas yang benar mereka sama dengan satu, dan untuk Kehilangan Fungsi Kebijakan, rj adalah hadiah yang diberikan oleh lingkungan kepada agen atas tindakan yang dia lakukan. </font><font style="vertical-align: inherit;">Yaitu, rj berfungsi sebagai bobot untuk gradien dalam propagasi kembali kesalahan ketika kita melatih jaringan saraf. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Menerima hadiah positif - itu artinya Anda perlu menambah bobot jaringan saraf tempat gradien diarahkan. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/2e/ge/bq/2egebqgdxjutx0tzxvwawunwffq.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Jika hadiah negatif, maka bobot yang sesuai dalam jaringan saraf, sesuai dengan arah gradien di mana kesalahan diarahkan, kita mengurangi.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bab 6. Membangun Set Data untuk Pelatihan</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Untuk melatih Agen kami - jaringan saraf menggunakan Machine Learning klasik - melalui metode back propagation of errors, kita perlu mengumpulkan dataset. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dari pernyataan masalah itu jelas bahwa pada input dari jaringan saraf kita ingin menyerahkan keadaan medium S - gambar sumur dengan kubus jatuh dan raket yang menangkapnya. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/4h/pv/hr/4hpvhry6ju1kcckfjj4vxu_7eps.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
dan Y - yang akan kami kumpulkan sesuai dengan negara S, ini akan menjadi tindakan yang diprediksi dari jaringan saraf - dan hadiah yang diterima lingkungan oleh agen untuk tindakan ini - r</font></font><br>
<br>
<img src="https://habrastorage.org/webt/an/gh/wo/anghwor5xj9mqdbgnnjrjgvzbta.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Namun bagaimanapun, Rabu, selama permainan, agen tidak dapat memberikan hadiah untuk setiap tindakan, misalnya, dalam kasus kami, agen akan menerima hadiah positif hanya ketika raket menangkap dadu yang jatuh. Jika raket tidak menangkap kubus dan jatuh ke dasar, maka hari Rabu akan menagih agen hadiah negatif. Sisa waktu, terlepas dari bagaimana agen menggerakkan raket, sampai kubus mengenai raket atau jatuh ke bawah, Rabu akan membebankan agen hadiah sebesar sama dengan nol.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/7f/rh/5u/7frh5uda0syl7dbcutr8ez8fcq8.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Seperti yang dapat dilihat dari deskripsi proses permainan kami, hadiah positif atau negatif dalam agen sangat jarang, tetapi pada dasarnya hadiah, terlepas dari tindakannya, adalah nol. Cara melatih Agen Kode sebagian besar waktu ia tidak menerima respons dari lingkungan atas tindakannya. Dari sudut pandang bahwa agen kami adalah jaringan saraf dan hadiah dari lingkungan adalah nol, maka gradien untuk propagasi balik kesalahan melalui jaringan saraf dalam banyak kasus dalam dataset akan menjadi nol, bobot jaringan saraf tidak akan memiliki tempat untuk berubah, yang berarti bahwa agen kami tidak akan belajar apa pun. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bagaimana mengatasi masalah tanpa imbalan di sebagian besar dataset yang akan melatih agen? </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ada dua cara untuk keluar dari situasi ini:</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
yang pertama adalah menetapkan semua tindakan agen yang dia ambil selama kubus jatuh satu dan hadiah akhir yang sama dari episode +1 atau -1, tergantung pada apakah agen menangkap kubus atau tidak menangkapnya. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dengan demikian, kami akan mempertimbangkan semua tindakan Agen jika dia menangkap kubus dengan benar dan akan mengkonsolidasikan perilaku agen tersebut selama pelatihan, menugaskan mereka hadiah positif. Jika Agen tidak menangkap kubus, maka kami akan memberikan hadiah negatif untuk semua tindakan Agen dalam episode dan akan melatihnya untuk menghindari urutan tindakan seperti itu di masa depan. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
yang kedua - penghargaan terakhir dengan langkah diskon tertentu dalam urutan menurun untuk meluas ke semua tindakan agen di episode ini. Dengan kata lain, semakin dekat tindakan agen ke final, semakin dekat ke +1 atau -1 hadiah untuk tindakan ini.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dengan memperkenalkan hadiah diskon untuk tindakan dalam urutan menurun saat aksi menjauh dari episode terakhir, kami membiarkan Agen memahami bahwa tindakan terakhir yang dia lakukan lebih penting untuk hasil dari episode Game daripada tindakan yang dia lakukan di awal. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Biasanya, hadiah diskon dihitung dengan rumus - Hadiah akhir episode dikalikan dengan koefisien diskon dalam kekuatan nomor langkah dikurangi satu untuk semua tindakan agen dalam episode (untuk saat kubus jatuh). </font></font><br>
<br>
<img src="https://habrastorage.org/webt/4r/cf/w6/4rcfw67kmlniyfhczs_p2v2kb-c.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Gamma - koefisien diskon (penurunan imbalan). Selalu berada dalam kisaran 0 hingga 1. Biasanya, gamut diambil di wilayah 0,95. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Setelah kami memutuskan data apa yang kami kumpulkan di DataSet, jalankan simulator lingkungan dan mainkan game dengan beberapa episode beberapa kali berturut-turut, kumpulkan data tentang:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">keadaan lingkungan, </font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> tindakan yang diambil oleh Agen </font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">hadiah yang diterima Agen.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Untuk memudahkan pemahaman, mari kita sebut jatuhnya satu kubus melalui sumur - episode permainan, kami juga menganggap bahwa permainan itu sendiri akan terdiri dari beberapa episode. </font><font style="vertical-align: inherit;">Ini berarti bahwa dalam satu permainan kita akan menjatuhkan beberapa dadu secara bergantian ke dalam sumur, dan raket akan mencoba untuk menangkap mereka. </font><font style="vertical-align: inherit;">Untuk setiap dadu yang tertangkap, agen akan diberikan +1 poin, untuk setiap dadu yang jatuh ke bawah dan raket tidak menangkapnya, agen akan diberikan -1 poin.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bab 7. Struktur internal agen dan lingkungan</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Lingkungan - karena kita memiliki lingkungan di mana agen itu ada, pada dasarnya terdiri dari matriks di dalam sumur yang kubusnya jatuh dengan kecepatan satu baris per jam, dan agen berjalan dalam satu arah juga satu sel. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Kami akan menulis simulator lingkungan yang dapat menjatuhkan kubus dari baris teratas pada kolom acak pada saat acak, tahu cara menerima perintah dari agen untuk memindahkan raket satu sel di salah satu arah, setelah itu memeriksa apakah kubus yang jatuh ditangkap oleh raket atau jika jatuh bagian bawah sumur. Bergantung pada ini, simulator mengembalikan kepada agen hadiah yang ia terima atas tindakannya.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Agen adalah elemen utama yang merupakan jaringan saraf kami yang mampu mengembalikan probabilitas semua tindakan untuk keadaan lingkungan tertentu karena lingkungan setia kepadanya pada input. </font><font style="vertical-align: inherit;">Dari kemungkinan tindakan yang diterima dari jaringan saraf, agen memilih yang terbaik, mengirimkannya ke lingkungan, menerima umpan balik dari lingkungan dalam bentuk hadiah dari lingkungan. </font><font style="vertical-align: inherit;">Juga, Agen harus memiliki algoritme internal berdasarkan yang ia akan dapat belajar untuk memaksimalkan hadiah yang diterima dari lingkungan.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bab 8. Pelatihan Agen</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Untuk melatih Agen, kita perlu mengumpulkan statistik sesuai dengan data dari simulator kami dan tindakan yang diambil oleh agen. </font><font style="vertical-align: inherit;">Kami akan mengumpulkan data statistik untuk pelatihan dalam tiga kali lipat nilai - keadaan lingkungan, tindakan agen dan hadiah untuk tindakan ini.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/yo/qq/od/yoqqodgrfzatgri5dljps6uqnvw.png"><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sedikit kode tentang cara mengumpulkan statistik</font></font></h4><br>
<img src="https://habrastorage.org/webt/ph/hr/6x/phhr6xb35udm3hyc3mr9rwv5yno.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Kami menempatkan masing-masing tiga nilai tersebut dalam buffer memori khusus, tempat kami menyimpannya sepanjang waktu saat kami mensimulasikan game dan mengumpulkan statistiknya.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Kode untuk mengatur buffer memori:</font></font></h4><br>
<img src="https://habrastorage.org/webt/u2/55/ty/u255ty2pzaoeagitk_osyxzxirg.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Setelah melakukan serangkaian permainan Agen kami pada hari Rabu dan mengumpulkan statistik, kami dapat melanjutkan ke pelatihan Agen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Untuk melakukan ini, kami mendapatkan kumpulan data kami dalam bentuk tiga kali lipat nilai dari buffer memori dengan statistik yang terakumulasi, membuka paketnya dan mengubahnya menjadi tensor Pytorch. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/jo/se/6t/jose6tkxmmtzsttgrnc-q0xkrf0.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 Kumpulan keadaan lingkungan dalam bentuk tensor Pytorch diterapkan pada input pada jaringan saraf agen, kami memperoleh distribusi probabilitas untuk setiap perpindahan agen untuk setiap kondisi lingkungan dalam pertandingan, mencatat probabilitas ini, melipatgandakan hadiah yang diterima oleh agen per gerakan dengan logaritma probabilitas ini, kemudian mengambil rata-rata atas produk dan jadikan ini negatif: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/hx/om/3i/hxom3ikgq9n13pb6brfzsfsbmkw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Tahap 1 </font></font><br>
<br>
<img src="https://habrastorage.org/webt/4j/gh/8l/4jgh8layvkcwg0dhrylxuwul2ri.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Tahap 2</font></font><br>
<br>
<img src="https://habrastorage.org/webt/pn/ac/pj/pnacpjuzovqe4-zql7lxlarerly.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Setelah kami mendapatkan nilai dari fungsi Rugi, kami membuat reverse pass melalui jaringan saraf untuk mendapatkan gradien dan membuat langkah pengoptimal untuk menyesuaikan bobot. </font><font style="vertical-align: inherit;">Tentang ini, siklus pelatihan agen kami sedang diunduh. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Karena setelah pengoptimal mengubah bobot jaringan saraf, data kami dalam statistik yang dikumpulkan tidak lagi relevan, karena jaringan saraf dengan bobot bergeser akan memberikan probabilitas yang sama sekali berbeda untuk tindakan pada kondisi lingkungan yang sama dan pelatihan agen akan salah lagi. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Karena itu, kami menghapus buffer kami dengan memori, kehilangan lagi sejumlah game untuk mengumpulkan statistik, dan memulai kembali proses pelatihan agen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ini adalah lingkaran pembelajaran ketika belajar tanpa guru menggunakan metode Policy Gradient.</font></font><br>
<br>
<ol>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Akumulasi statistik</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pelatihan Agen</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Atur Ulang Statistik</font></font></li>
</ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 Kami mengulangi proses pembelajaran berkali-kali sampai Agen kami belajar menerima dari sistem hadiah yang sesuai dengan kami.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bab 9. Eksperimen dengan Agen dan Lingkungan</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Mari kita mulai serangkaian percobaan untuk melatih Agen kami. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Untuk percobaan, kami memilih parameter lingkungan berikut: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/xi/ym/2_/xiym2_kitmamvf52hhuff6tkpqs.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Untuk Agen, kami memilih - kami membangun jaringan saraf - itu akan convolutional (saat kami bekerja dengan gambar), itu akan memiliki 9 output (1-kanan, 2-kiri, 3-atas, 4-bawah, pada output) 5-kanan-atas, 6-kiri-atas, 7-kanan-bawah, 8-kiri-bawah, 9-do nothing) dan SoftMax untuk mendapatkan probabilitas untuk setiap tindakan.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Arsitektur jaringan saraf</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Conv2d layer 32 neuron ukuran gambar pertama 1 * 32 * 16 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
MaxPool2d layer gambar ukuran 32 * 16 * 8 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Conv2d layer 32 neuron ukuran gambar 32 * 16 * 8 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
MaxPool2d gambar layer ukuran 32 * 8 * 4 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ratakan - luruskan gambar dengan ukuran 1024 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Linear layer untuk 1024 neuron </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dropout (0,25) layer </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Linear layer untuk 512 neuron </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Linear layer untuk 256 neuron </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Linear layer untuk 9 neuron dan SoftMax</font></font><br>
<br>
<img src="https://habrastorage.org/webt/2d/tz/ne/2dtzneqr2lsxdkz-yvu9xonh0uw.png"><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Kode penciptaan jaringan saraf Pytorch</font></font></h4><br>
<img src="https://habrastorage.org/webt/wq/mx/yw/wqmxyw67mbjtmnpnbj86zry8y4a.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Satu per satu, kami akan meluncurkan tiga siklus pelatihan Agen di Lingkungan, parameter yang dibahas di atas:</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Eksperimen No. 1 - Agen belajar untuk memecahkan masalah dalam 13.600 siklus game</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Keadaan awal Agen, </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ws/gv/nb/wsgvnb34jwzwk--6iehpg3y3lqw.gif"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
jadwal pelatihan </font><font style="vertical-align: inherit;">Agen, </font></font><br>
<br>
<img src="https://habrastorage.org/webt/jt/kd/3q/jtkd3qmdl7xixi-bwpkdp-tk0uy.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Keadaan terlatih Agen.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/4a/v5/9l/4av59lvev4oget2gotgg-xsfy2w.gif"><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Eksperimen No. 2 - Agen belajar untuk memecahkan masalah dalam 8250 siklus permainan</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Keadaan awal Agen, </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ve/jk/wz/vejkwz-fezjvbjcjydzmx7q4hk8.gif"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
jadwal pelatihan </font><font style="vertical-align: inherit;">Agen, </font></font><br>
<br>
<img src="https://habrastorage.org/webt/9e/zc/1x/9ezc1x4gfv7bilgb8sx-r3iemgy.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Keadaan terlatih Agen.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/l9/r7/bn/l9r7bnstfbaqszje4npotjrze4o.gif"><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Eksperimen No. 3 - Agen belajar untuk memecahkan masalah dalam siklus permainan 19800</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Keadaan awal Agen, </font></font><br>
<br>
<img src="https://habrastorage.org/webt/hz/wm/ss/hzwmssjogsxwzv0wrik-1nb8loc.gif"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
jadwal pelatihan </font><font style="vertical-align: inherit;">Agen, </font></font><br>
<br>
<img src="https://habrastorage.org/webt/sh/xv/zd/shxvzdpumy7fnoctkb6bqnhie2o.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Keadaan terlatih Agen.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/2h/ga/yh/2hgayhvtgypaq1fvu-i54d4l4ok.gif"><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bab 10. Kesimpulan</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Melihat grafik, dapat dikatakan bahwa pelatihan Agen ini tentu saja lambat. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Agen awalnya telah mencari setidaknya beberapa kebijakan yang masuk akal untuk tindakannya sejak lama untuk mulai menerima hadiah positif. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pada saat ini, pada tahap pertama dari jadwal, hadiah untuk permainan perlahan-lahan tumbuh, kemudian tiba-tiba Agen menemukan opsi yang baik untuk gerakannya, dan hadiah yang diterima olehnya untuk permainan meningkat tajam dan naik, dan kemudian mendekati hadiah maksimum, agen berjalan lagi peningkatan efisiensi yang lambat ketika dia memperbaiki kebijakan gerakan yang telah dia pelajari, tetapi berusaha, seperti algoritma serakah lainnya, untuk menerima hadiahnya sepenuhnya.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Saya juga ingin mencatat perlunya perhitungan untuk melatih Agen menggunakan metode Gradient Kebijakan, karena Waktu utama algoritme bekerja adalah pengumpulan statistik tentang pergerakan Agen dan bukan pelatihannya. Setelah mengumpulkan statistik tentang perpindahan dari seluruh susunan, kami hanya menggunakan satu kumpulan data untuk melatih Agen, dan kami membuang semua data lain yang sudah tidak sesuai untuk pelatihan. Dan lagi kami mengumpulkan data baru. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Anda masih bisa banyak bereksperimen dengan algoritma dan lingkungan ini - mengubah kedalaman dan lebar sumur, menambah atau mengurangi jumlah dadu yang jatuh selama permainan, membuat dadu ini dengan warna yang berbeda. Untuk mengamati apa efek ini pada efektivitas dan kecepatan pelatihan Agen.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Selain itu, bidang yang luas untuk eksperimen adalah parameter jaringan saraf, yang intinya kami latih Agen kami, Anda dapat mengubah lapisan, kernel konvolusi, mengaktifkan dan meregulasi pengaturan. </font><font style="vertical-align: inherit;">Ya, dan banyak lagi yang bisa Anda coba untuk meningkatkan efektivitas pelatihan Agen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dengan demikian, setelah meluncurkan percobaan praktis dengan mengajar tanpa guru menggunakan metode Gradient Kebijakan, kami yakin bahwa mengajar tanpa guru memiliki tempatnya, dan itu benar-benar berfungsi. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Agen itu dilatih secara mandiri untuk memaksimalkan ganjarannya dalam permainan. </font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tautan ke GitHub dengan kode yang disesuaikan untuk bekerja di laptop Google Colab</font></font></a></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id506358/index.html">3 opsi untuk perilaku pengadilan ketika mereka menerima kasus kunci penuntutan</a></li>
<li><a href="../id506362/index.html">Bagaimana kami melakukan bola dansa online</a></li>
<li><a href="../id506370/index.html">Bandingkan kinerja Periksa Kendala dan Kunci Asing di SQL Server</a></li>
<li><a href="../id506372/index.html">Naluri reproduksi ditemukan dalam kecerdasan buatan</a></li>
<li><a href="../id506380/index.html">Perangkat lunak bebas atau domestik. Uang sekolah standar atau gratis</a></li>
<li><a href="../id506386/index.html">Cara keluar dari sekolah dan memindahkan anak ke tempat yang jauh</a></li>
<li><a href="../id506392/index.html">Pemberitahuan Roskomnadzor tentang pemrosesan data pribadi pada tahun 2020</a></li>
<li><a href="../id506394/index.html">Anycubic Mega X 3D Printer: Printer Hebat dengan Harga Sederhana</a></li>
<li><a href="../id506396/index.html">Elon Musk: “Lidar adalah buang-buang waktu. Semua yang bergantung pada Lidar akan hancur ”</a></li>
<li><a href="../id506398/index.html">Joel Spolsky: "Bukan Kegunaan Sendiri"</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>