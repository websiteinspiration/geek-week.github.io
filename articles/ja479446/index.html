<!doctype html>
<html class="no-js" lang="ja">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🥇 🚠 🕴🏾 車はすでにテストを読んでいる人よりも先です。しかし、彼らは彼らが読んだものを理解していますか？ 👨🏼‍🏫 🧑🏻 👒</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="BERTと呼ばれるツールは、読書および理解テストで人々を追い抜くことができます。ただし、それでもAIが進む必要のある経路が示されます。
 
 
 2017年の秋、ニューヨーク大学の計算言語学者であるSam Bowmanは、コンピューターがまだテキストをよく理解していないと判断しました。もちろん、彼ら...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>車はすでにテストを読んでいる人よりも先です。しかし、彼らは彼らが読んだものを理解していますか？</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/479446/"><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">BERTと呼ばれるツールは、読書および理解テストで人々を追い抜くことができます。</font><font style="vertical-align: inherit;">ただし、それでもAIが進む必要のある経路が示されます。</font></font></h3><br>
<img src="https://habrastorage.org/getpro/habr/post_images/1e6/359/58b/1e635958bf5af44bcc9b6256e1a0101f.jpg"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
2017年の秋、</font><font style="vertical-align: inherit;">ニューヨーク大学の計算言語学者である</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sam Bowman</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">は、コンピューターがまだテキストをよく理解していないと判断しました。</font><font style="vertical-align: inherit;">もちろん、彼らは、自動翻訳や感情の分析など、特定の狭い領域でこの理解をシミュレートするのに十分なほどよく学びました（たとえば、彼が言ったように、文が「失礼か甘いか」を判断するために）。</font><font style="vertical-align: inherit;">しかし、ボーマンは測定可能な証言、つまり人間の言語で概説された、書かれたものの真の理解を望んでいました。</font><font style="vertical-align: inherit;">そして彼はテストを思いついた。</font></font><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
では</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">仕事</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2018年4月、ワシントン大学および人工知能に携わるGoogleが所有する会社であるDeepMindの同僚と共同で執筆されたボーマンは、一般的な名前GLUE（一般言語理解評価）[一般化された言語を理解する評価]。このテストは、「研究コミュニティが興味深いタスクを検討することのかなり実例となる例」として設計されましたが、「人にとって簡単」であるとボーマン氏は言いました。たとえば、あるタスクでは、前の文の情報に基づいて推定する必要がある文の真実について質問します。 「トランプ大統領がイラクに上陸し、7日間の訪問を始めた」というメッセージは、「トランプ大統領が海外を訪問している」ことを意味していますあなたはテストに合格します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
車は彼に失敗した。高度なニューラルネットワークでさえ、すべてのテストで合計100ポイントのうち69ポイントしか得られませんでした。ボーマンと同僚たちは驚かなかった。ニューラルネットワーク-哺乳類の脳のニューロンの働きにほぼ似た計算接続を備えた多層構造-「自然言語処理」の分野で良い結果を示していますが、研究者たちはこれらのシステムが真剣に何かを教えられていると確信していませんでした言語。そしてグルーはそれを証明します。 「初期の結果は、GLUEテストに合格すると、既存のモデルとメソッドの機能を超えていることを示しています」とBowmanら。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
しかし、彼らの評価は長くは続かなかった。 2018年10月、Googleは新しいメソッドBERT（トランスフォーマーからの双方向エンコーダー表現）[トランスフォーマーの双方向エンコーダープレゼンテーション]を導入しました。彼はGLUEで80.5のスコアを得ました。わずか6か月で、この新しいテストでは、車はマイナス3からマイナス4に急上昇しました。これは、機械による自然言語の実際の理解を測定します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
「それは「いまいましい」のようなものでした」とボウマンはよりカラフルな言葉を使って回想します。 -このメッセージはコミュニティから不信感を持って受信されました。 BERTは、私たちが可能な限り最大と見なしたものに近い多くのテストグレードで受信されました。実際、BERTがGLUEテストに登場する前は、比較できる人間の達成スコアすらありませんでした。 2019年2月にボウマンと彼の大学院生の1人がそれらをGLUEに追加したとき、彼らはほんの数ヶ月続き、その後MicrosoftのBERTベースのモデルも</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">それらを打ち負かしました</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
これを書いている時点で</font><font style="vertical-align: inherit;">は、GLUEテストの</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">最初の場所の</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ほとんどすべてが</font><font style="vertical-align: inherit;">、BERTモデルを含む、拡張する、または最適化するシステムによって占められています。そのうちの5つは人間の能力に優れています。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
しかし、これはAIが私たちの言語を理解し始めていることを意味するのでしょうか、それともそれは単に私たちのシステムを打つことを学んでいるだけなのでしょうか？</font><font style="vertical-align: inherit;">BERTベースのニューラルネットワークがGLUEタイプのテストを席巻した後、これらのNLPシステムを「</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">スマートハンス</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">」の</font><font style="vertical-align: inherit;">コンピュータバージョンであると見なす新しい評価方法が登場し</font><font style="vertical-align: inherit;">ました。心の中で算術計算を行うが、実際にその所有者によってそれに与えられた無意識の兆候を読んでください。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
「私たちは、非常に退屈で狭い意味で言語を理解してからAIを作成するまでの間にグレーゾーンにいることを知っています」とボウマン氏は語った。</font><font style="vertical-align: inherit;">-一般に、専門家の反応は次のように説明できます。これはどのようにして起こりましたか？</font><font style="vertical-align: inherit;">どういう意味ですか？</font><font style="vertical-align: inherit;">さて何をしようか？"</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">独自のルールを書く</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
有名な「</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">中国の部屋</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">」の</font><font style="vertical-align: inherit;">思考実験では</font><font style="vertical-align: inherit;">、中国語を知らない人が、ルールのある本がたくさん詰まった部屋に座っています。本の中では、部屋に入る一連の漢字を受け入れる方法と、適切な答えを与えるための正確な指示を見つけることができます。部屋のドアの下で、中国語の書かれた質問を手のひらの外の人が掌握しています。中の人はルールのある本を読み、完全に合理的な答えを中国語で作成します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
この実験は、外の印象にもかかわらず、部屋の人が中国語を理解しているとは言えないことを証明するために使用されました。しかし、理解のシミュレーションでさえ、NLPの許容できる目標でした。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
唯一の問題は、完全なルールブックがないことです。これは、自然言語が複雑で体系的ではないため、一連の堅固な仕様に限定できないためです。たとえば、構文を考えてみましょう。意味のある文への単語のグループ化を決定する規則（経験を含む）です。 「</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">無色の緑のアイデアを激しく眠る</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">」という文に</font><font style="vertical-align: inherit;">は構文がありますが、その言語を知っている人なら誰でもその無意味さを理解できます。特別に設計されたルールブックには、自然言語に関連するこの書かれていない事実を含めることができます-他の無数の事実は言うまでもありませんか？</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
NLPの研究者はこの</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">四角い円</font></a><font style="vertical-align: inherit;">を見つけようとしました</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、いわゆるニューラルネットワークのプロセスで独自の職人によるルールブックを作成することを強制する「事前トレーニング」または事前トレーニング。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
2018年まで、主要なトレーニングツールの1つは辞書のようなものでした。この辞書</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">は、単語のベクトル表現</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> [単語の埋め込み]を</font><font style="vertical-align: inherit;">使用して</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">、単語</font></a><font style="vertical-align: inherit;">間のつながりを数字の形式で記述し、ニューラルネットワークがこの情報を入力として認識できるようにしました-中国の部屋にいる人の大まかな用語集のようなものです。しかし、事前に訓練されたベクトル辞書ニューラルネットワークは、文レベルでの単語の意味をまだ知らないままでした。</font><font style="vertical-align: inherit;">ジョンズホプキンス大学の計算言語学者である</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tel Linsen</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">氏は、「彼女の視点から見ると、「犬が犬を噛んだ」と「犬が犬を噛んだ」という文は同じです」と語った</font><font style="vertical-align: inherit;">。</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/55a/245/f48/55a245f4866cf5ffcf2689d2161f8536.jpg" width="50%"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ジョンズホプキンス大学のコンピューティング言語学者、テルリンセン。</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
改善された方法では、事前トレーニングを使用して、特定のNLPタスクの実行を教える前に、ニューラルネットワークに、辞書だけでなくコンテキストの構文も含む豊富なルールブックを提供します。 2018年の初めに、OpenAI、サンフランシスコ大学、アレン人工知能研究所、ワシントン大学の研究者たちは、同時にこれに近づくためのトリッキーな方法を考え出しました。単語のベクトル表現を使用してネットワークの最初の層である1つだけを事前トレーニングする代わりに、研究者は言語モデリングと呼ばれるより一般的なタスクのためにネットワーク全体をトレーニングし始めました。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
「言語モデリングの最も単純なバージョンは次のとおりです。私は一連の単語を読み、以下を予測しようとします」と説明しました</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Facebookの研究者である</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">Mile Ott</font></a><font style="vertical-align: inherit;">。 「 『ジョージW.ブッシュが生まれた』と言えば、モデルはこの文の次の単語を予測する必要があります。」</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
このような深い事前トレーニングを備えた言語モデルは、非常に効率的に作成できます。研究者は、ウィキペディアなどの無料のリソース（文法的に正しい文章に配置された数十億の単語）から膨大な量のテキストをニューラルネットワークに供給し、ネットワークに次の単語を独自に予測させる機能を提供します。実際、これは、中国の部屋にいる人を招待して、参照用の中国のメッセージを使用して独自のルールのセットを作成するという事実と同等です。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
「このアプローチの優れた点は、モデルが大量の構文知識を得ることです」とOtt氏は述べています。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
さらに、そのような事前訓練されたニューラルネットワークは、その言語表現を適用して、単語予測に関連しないより狭いタスクを微調整プロセスに教えることができます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
「モデルを事前トレーニングフェーズから取得して、必要な実際のタスクに適応させることができます」とOtt氏は説明します。 「その後、問題を最初から解決しようとした場合よりもはるかに良い結果が得られます。」</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
OpenAIが</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">GPTニューラルネットワークを</font></a><font style="vertical-align: inherit;">導入した2018年6月</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、10億語（デジタルブック11,038から取得）で1か月間トレーニングされた言語モデルが含まれているため、GLUEテストでの結果は72.8ポイントで、すぐに最高になりました。</font><font style="vertical-align: inherit;">それでも、サムボウマンは、システムが少なくとも人間のレベルに近づく前に、この領域が非常に長い間発展することを示唆しました。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
そしてBERTが登場。</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">有望なレシピ</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
では、BERTとは何でしょうか。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
まず、これは完全に訓練されたニューラルネットワークではなく、人間レベルですぐに結果を提供することができます。ボーマン氏は、これは「ニューラルネットワークをトレーニングするための非常に正確なレシピ」であると語っています。パン屋ができるように、レシピに従って、おいしいケーキケーキを提供することを保証します。これは、ブルーベリーからほうれん草のキッシュまで、さまざまなケーキに使用できます。Googleの研究者は、ニューラルネットワークの「ベーキング」の理想的な基盤として機能するBERTレシピを作成しました（つまり、それらの微調整）、自然言語の処理におけるさまざまなタスクにうまく対応できるようにします。 GoogleはBERTコードをオープンにしました。つまり、他の研究者はこのレシピを最初から繰り返す必要がなくなり、ダウンロードするだけで済みます。まるでお店でケーキの焼き菓子を買うようなものです。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
BERTがレシピの場合、その成分のリストは何ですか？ 「これは、システムが機能し始めるように3つの異なるものが相互に接続された結果です」と</font><font style="vertical-align: inherit;">、BERTデバイス</font><font style="vertical-align: inherit;">を</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">分析し</font></a><font style="vertical-align: inherit;">たFacebookの研究者である</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Omer Levai</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">は</font><font style="vertical-align: inherit;">述べて</font><font style="vertical-align: inherit;">い</font><font style="vertical-align: inherit;">ます。</font><i><font style="vertical-align: inherit;">Facebook</font></i><font style="vertical-align: inherit;"> 
の</font><i><font style="vertical-align: inherit;">研究者であるOmer Livai</font></i><font style="vertical-align: inherit;"> 1つ目は、事前にトレーニングされた言語モデルです。つまり、中国語ルームの同じディレクトリです。 2番目は、提案のどの機能が最も重要であるかを決定する機会です。</font><font style="vertical-align: inherit;">
2017年、</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">ジェイコブナローレート</font></a></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/1ee/f23/473/1eef234733ede2389825e047170009e0.jpg" width="50%"><br>
<i><font style="vertical-align: inherit;"></font></i><br>
<br><font style="vertical-align: inherit;"></font><br>
<br><font style="vertical-align: inherit;"></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、Google Brainのエンジニアは、会社の言語理解への取り組みを加速する方法に取り組みました。彼は、すべての高度なニューラルネットワークには固有の制限があることに注意しました。そのような「シーケンス」は、人々がテキストをどのように読むかという考えと一致するように見えました。しかし、Uzkoreitは興味を示し、「線形の順次モードで言語を理解することが最適ではないのではないか」と考えました。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
同僚との比率が低いため、ニューラルネットワークの新しいアーキテクチャが開発され、ニューラルネットワークの各レイヤーが他のレイヤーと比較して入力データの特定の機能に大きな重みを割り当てることができるメカニズムである「注意」に焦点が当てられました。この新しいアーキテクチャであるトランスフォーマーは、「犬が男を噛む」などの文を入力として受け取り、各単語をさまざまな方法で並列にエンコードできます。たとえば、「a」を無視して、トランスフォーマーは「bites」と「person」を動詞と主語-目的語としてバインドできます。同時に、彼女は記事「the」を無視して、「かみ傷」と「犬」を動詞と主語-主語として関連付けることができます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
トランスの一貫性のない性質は、より表現力豊かに、またはUzkoreitが言うようにツリー状に文章を提示します。ニューラルネットワークの各層は、特定の単語の間に多くの並列接続を確立し、残りを無視します。つまり、小学校の生徒が文を部分に分解する方法についてです。これらの接続は、近くにない可能性のある単語間で行われることがよくあります。 「そのような構造は、いくつかの木のオーバーレイのように見えます」とUzkoreitは説明しました。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
このようなツリーのような文の表現は、トランスフォーマーにコンテキストの意味をモデル化する機会を与え、複雑な文で遠く離れている単語間の関係を効果的に研究します。 「これは幾分直観に反します」とウズコライトは言いました、「しかし、ツリーのような言語モデルに長い間関わってきた言語学から来ています。」</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/21e/0a1/fe2/21e0a1fe2331c110f5cb2a966b2b173d.jpg" width="50%"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ベルリンチームGoogle AI Brainの責任者であるJacob Uzkoreyt</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
最後に、レシピの3番目の成分であるBERTは非線形リーディングをさらに拡張します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
テラバイトのテキストをニューラルネットワークで左から右に処理することによって作成された他の事前トレーニング済み言語モデルとは異なり、BERTモデルは右から左に、同時に左から右に読み取り、文章からランダムに除外された単語を予測することを学習します。たとえば、BERTは「George W. Bush [...] in Connecticut in 1946」という形式の文を受け入れ、両方向にテキストを処理して、文の中央に隠されている単語（この場合は「born」）を予測できます。 「この双方向性により、ニューラルネットワークは単語のサブセットから可能な限り多くの情報を抽出する必要があります」とUzkoreit氏は述べています。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
単語ゲームのように使用されるBERTベースのふり（マスキングを使用した言語モデリング）は新しいものではありません。</font><font style="vertical-align: inherit;">言語の人々の理解を測定するために何十年もの間使用されてきました。</font><font style="vertical-align: inherit;">グーグルのために、彼は以前にこの領域を支配していた一方向の事前訓練方法の代わりにニューラルネットワークで双方向性を使用する実用的な方法を提供しました。</font><font style="vertical-align: inherit;">「BERT以前は、単方向言語モデリングが標準でしたが、これはオプションの制限です」と</font><font style="vertical-align: inherit;">、Googleの研究者である</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ケントンリー</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">は述べ</font><font style="vertical-align: inherit;">ています。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
これら3つの各要素（事前トレーニング、注意、および双方向性を備えたディープ言語モデル）は、BERTの前に個別に存在していました。</font><font style="vertical-align: inherit;">しかし、Googleが2018年の終わりにレシピをリリースするまで、誰もそのような成功した方法でそれらを組み合わせませんでした。</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">洗練されたレシピ</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
他の優れたレシピと同様に、BRETはすぐにさまざまなシェフによって好みに合わせられました。 2019年の春には、「MicrosoftとAlibabaがお互いに追随し、週ごとに順位を変え、モデルを調整した」時期がありました、とBowman氏は振り返ります。 BERTの改良版が8月に初めてRoBERTaという名前でリリースされたとき、</font><font style="vertical-align: inherit;">DeepMindの</font><font style="vertical-align: inherit;">研究者</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sebastian Ruder</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">は彼の人気のある</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">NLPニュースレターで</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">「新しい月、および事前トレーニングを備えた新しい高度な言語モデル」</font><font style="vertical-align: inherit;">と乾いた</font><font style="vertical-align: inherit;">コメントをしました</font><font style="vertical-align: inherit;">。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
「ケーキ」と同様に、BERTにはその作業の品質に影響を与えるいくつかの設計上の決定があります。これには、ベイク処理されたニューラルネットワークのサイズ、事前トレーニングに使用されるデータの量、単語をマスクする方法、およびニューラルネットワークがこのデータを使用していた期間が含まれます。そして、RoBERTaなどの後続のレシピでは、研究者はこれらの決定を微調整します-レシピを指定するシェフのように。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
RoBERTaの場合、Facebookとワシントン大学の研究者はいくつかの成分（事前トレーニングデータ、入力シーケンスの長さ、トレーニング時間）を増やし、1つの成分を削除しました（「次の文を予測する」というタスクはもともとBERTにあり、結果に悪影響を及ぼしました） ）、およびその他の変更（個々の単語をマスクするタスクが複雑になりました）。その結果、簡単にGLUEランキングで1位になりました。 6週間後、マイクロソフトとメリーランド大学の研究者</font><font style="vertical-align: inherit;">たちは、RoBERTaに改良を</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">加え</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、次の勝利を引き出しました。現時点では、別のモデルがGLUE、ALBERT（「lite BERT」の省略形、つまり「lite BERT」）で最初に登場し、BERTの基本構造をわずかに変更しました。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
「私たちはまだどのレシピが機能し、どのレシピが機能しないかを考え出している」とRoBERTaに取り組んだFacebookのOttは言った。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
しかし、プリベークケーキの技術を改善しても化学の基礎は学べないため、BERTを段階的に改善しても、NLPの開発に関する理論的な知識はあまり得られません。</font><font style="vertical-align: inherit;">ジョンズ・ホプキンス大学の計算言語学者であるリンセン氏は、「私はあなたに非常に正直になります。私はこれらの研究には従いません。私にとっては、それらは非常に退屈です」と述べました。</font><font style="vertical-align: inherit;">「ここには特定の科学的な謎があります」と彼は認めますが、BERTとそのすべての子孫をより賢くする方法や、なぜそれらがそれほど賢いのかを理解することすらできません。</font><font style="vertical-align: inherit;">その代わり、「我々はこれらのモデルが実際に言語を理解する程度を理解しようとしている」と彼は言った、「私たちが通常これらのモデルを評価するデータセットで何らかの方法で動作する奇妙なトリックを学ぶのではありません。」</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
言い換えれば、BERTは正しいことを行っています。</font><font style="vertical-align: inherit;">しかし、彼が間違った理由でそれをしたとしたら？</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">トリッキーだがスマートではない</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
2019年7月に、台湾州立大学のCheng Kunの2人の研究者がBERTを使用して、「引数理解タスク」と呼ばれる比較的知られていないパフォーマンステストで印象的な結果を得ました。</font><font style="vertical-align: inherit;">タスクを完了するには、任意のステートメントを支持して引数をサポートする暗黙の初期条件（「基礎」）を選択する必要があります。</font><font style="vertical-align: inherit;">たとえば、「科学的研究で喫煙とがんの関連性が示されているため」（議論）、「喫煙ががんを引き起こす」（ステートメント）ことを証明するには、「科学的研究は信頼できる」（「基礎」）という引数を選択する必要があります。 「科学的研究は高価です」（ただし、それでも、この文脈では関係ありません）。</font><font style="vertical-align: inherit;">すべてクリア？</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
すべてではない場合でも、心配しないでください。人々さえ、練習なしでこの仕事があまり得意ではありません。トレーニングを受けていない人の平均ベースラインは100のうち80です。BERTは77に達しました—著者によると、これは「予想外」でした。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
しかし、BERTがニューラルネットワークにアリストテレスよりも悪い推論を行う能力を与えることができると決定する代わりに、彼らはすべてが実際には単純であると疑っています。実際、彼らの訓練データを分析した後、著者らはそのようないわゆる「誤った手がかり」たとえば、「ない」粒子を含むすべてのベースを選択した場合、61％のケースで質問に正しく答えることができます。そのような規則性をすべてデータから取り除いた結果、科学者たちはBERTの結果が77から53に低下したことを発見しました。これはランダムな選択とほぼ同等です。機械学習雑誌「スタンフォード人工知能研究所」の「The Gradient」の記事が</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">比較されました</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">演算能力が高いと思われる馬、スマートなハンスとのBERT。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
別の記事「</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Rights for</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Wrong Reasons」では、Linsenらは、特定のGLUEテストでのBERTの高い結果がトレーニングデータ内の誤った手掛かりの存在にも起因する可能性があるという証拠を公開しました。この方法で機能する能力をBERTから奪うように設計された代替データセットが開発されました。データセットはHans（自然言語推論システムのヒューリスティック分析、HANS）[自然言語に基づいて結論を下すシステムのヒューリスティック分析]と呼ばれていました。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
それで、BERTと彼の親戚全員がハイスコアテーブルを襲撃しているのは単なるデマだけですか？ボウマンはレンズンに同意し、いくつかのGLUEデータがずさんであることを認めています。それらは、それを作成した人々に固有の認識の歪みで満たされ、これは強力なBERTベースのネットワークによって悪用される可能性があります。 「GLUEのすべての問題を解決する万能のトリックはありませんが、「手抜き」を行う可能性はたくさんあります」とボウマン氏は語り、「モデルはそれらを見つけることができます。」しかし、BERTが価値のあるものに基づいているとは考えていません。 「言語について本当に興味深いことを学んだモデルがあるようです」と彼は言った。 「しかし、彼女は確かに一般的な意味で人間の言語を理解していません。」</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ワシントン大学とアレン研究所のコンピューターサイエンティストであるYojin Choi氏によると、言語の共通理解に向けての進歩を促す方法の1つは、BERTバージョンの改善だけでなく、発生の可能性を減らすより良い品質のテストとトレーニングデータの開発にも集中することです。 「スマートハンス」のスタイルで偽の技術。彼女の研究では、アルゴリズムを使用してNLPのトレーニングデータを検証し、過度に繰り返された例やニューラルネットワークに暗黙の手掛かりを残した例を削除する、敵対的なフィルタリングアプローチを探求しています。そのような競争的なフィルタリングの後、「BERTの有効性は大幅に低下する可能性があります」と彼女は言い、「人間の有効性はそれほど低下していません。」</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
それにもかかわらず、一部のNLP研究者は、言語モデルの指導手順が改善されたとしても、言語を実際に理解することには依然として実際の障害があると信じています。強力なトレーニングを行っても、BERTは一般的なケースで言語を完全にモデル化できません。調整後、彼は「特定のNLPタスク、またはそのタスクの特定のデータセット」をモデル化</font><font style="vertical-align: inherit;">したと、マサチューセッツ大学のMachine Text Laboratoryの計算言語学者である</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Anna Rogers</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">は</font><font style="vertical-align: inherit;">述べています</font><font style="vertical-align: inherit;">。慎重に準備またはフィルタリングされたトレーニングデータのセットには、自然言語を使用する人々が簡単に処理できる極端なケースや予測できない入力データをすべて含めることができない可能性があります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ボウマン氏は、ニューラルネットワークが言語を実際に理解したことを私たちに納得させることができるものを理解することさえ難しいと指摘しています。標準テストは、テストされたものの知識に関して社交的な何かを明らかにする必要があります。しかし、どの学生もテストがだまされやすいことを知っています。 「十分に重く、欺瞞から十分に保護されたテストを考案することは非常に困難です。その結果、それらのソリューションは、AIの言語テクノロジのある側面で問題を本当に解決したと私たちに納得させます。」と彼は言った。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ボーマンと同僚は最近</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">SuperGLUE</font></a><font style="vertical-align: inherit;">と呼ばれるテストを発表しました</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">BERTベースのシステム用に複雑になるように特別に設計されています。これまでのところ、人を追い越すことができるネットワークはありません。しかし、これが（またはいつ）発生したとしても、マシンが以前よりも言語を理解することを学ぶことができるということですか？それとも、科学が機械にこのテストに合格する方法を教えるのにもっと良くなるだけなのでしょうか？</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
「いい例えです」とボーマンは言った。</font><font style="vertical-align: inherit;">「LSATおよびMCATテストに合格する方法を見つけましたが、医師または弁護士になる資格がない可能性があります。」</font><font style="vertical-align: inherit;">それでも、すべてを見ると、これはまさにAIの分野の研究が進んでいる方法です。</font><font style="vertical-align: inherit;">「チェスは、ゲーム用のプログラムの書き方を理解するまで、知性の深刻なテストのように見えた」と彼は言った。</font><font style="vertical-align: inherit;">「言語の理解を表す、ますます複雑になるタスクを発明し、それらを解決する方法を考案することが目標であった時代に、私たちは間違いなく時代を迎えました。」</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../ja479428/index.html">12月9日から15日までのモスクワでのデジタルイベント</a></li>
<li><a href="../ja479430/index.html">12月9〜15日のサンクトペテルブルクでのデジタルイベント</a></li>
<li><a href="../ja479432/index.html">Yandex.Maps：私はカードコントローラに行きました-私はすぐにユーザーの位置を取得しました（大丈夫、今は真剣に）</a></li>
<li><a href="../ja479438/index.html">Postgres-Tuesday＃5：「PostgreSQLとKubernetes。CI / CD。テスト自動化»</a></li>
<li><a href="../ja479442/index.html">Alexey Savvateev：社会分割のゲーム理論モデル（+ nginx調査）</a></li>
<li><a href="../ja479450/index.html">AppCode 2019.3：より速く動作し、Swiftをよりよく理解し、Mac Catalystについて理解し、アセンブリメッセージを便利に表示します</a></li>
<li><a href="../ja479452/index.html">ドメインネームシステムの開発方法：ARPANET時代</a></li>
<li><a href="../ja479458/index.html">サーバールームの美しさまたは実用性</a></li>
<li><a href="../ja479460/index.html">空飛ぶ車のガイド</a></li>
<li><a href="../ja479462/index.html">C ++でのシリアル化</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>