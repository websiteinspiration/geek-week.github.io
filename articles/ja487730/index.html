<!doctype html>
<html class="no-js" lang="ja">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🤶🏾 ⛲️ 🤴🏻 自然言語処理。2019年の結果と2020年の傾向 🛸 🤰🏽 🧑🏿‍🤝‍🧑🏻</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="みなさん、こんにちは。少し遅れて、この記事を公開することにしました。毎年、私は自然言語処理の分野で何が起こったかを要約しようとしています。今年も例外ではありませんでした。
 
 BERT、BERTはいたるところにあります
 順番に始めましょう。過去1年半、遠隔地のシベリアのタイガやゴアでの休暇に出か...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>自然言語処理。2019年の結果と2020年の傾向</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/huawei/blog/487730/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">みなさん、こんにちは。</font><font style="vertical-align: inherit;">少し遅れて、この記事を公開することにしました。</font><font style="vertical-align: inherit;">毎年、私は自然言語処理の分野で何が起こったかを要約しようとしています。</font><font style="vertical-align: inherit;">今年も例外ではありませんでした。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">BERT、BERTはいたるところにあります</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
順番に始めましょう。</font><font style="vertical-align: inherit;">過去1年半、遠隔地のシベリアのタイガやゴアでの休暇に出かけなかった場合は、BERTという言葉を聞いたことがあるはずです。</font><font style="vertical-align: inherit;">2018年の終わりに登場するこのモデルは、過去数年にわたって非常に人気を博しており、まさにそのような写真が適切です。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/cu/vm/_i/cuvm_irxzrscw8rctmtyoqywxss.png"><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
BERTは、NLPに入力できるすべてのものを本当に魅了しました。</font><font style="vertical-align: inherit;">それらは、分類、名前付きエンティティの認識、さらには機械翻訳にも使用され始めました。</font><font style="vertical-align: inherit;">簡単に言えば、それらをバイパスすることはできず、それが何であるかを伝える必要があります。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/5j/mo/sq/5jmosqk9vhjts6ai88v8hcrdhci.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
写真は、機会のヒーロー（左）と2つのモデルの比較を示しています。</font><font style="vertical-align: inherit;">右側は、BERTの前身である</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ELMo</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">モデル</font><font style="vertical-align: inherit;">です。</font></font><br>
<br>
<div class="spoiler"><b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">叙情的な余談。</font></font></b><div class="spoiler_text"><img src="https://habrastorage.org/getpro/habr/post_images/8a1/bb1/e07/8a1bb1e076e3b3b1b2637343e28359d4.jpg" alt="image"><br>
         « »:           ,        ,   Elmo,  Bert —   ;    ,   ,   , —    .         .  ,    ,   .<br>
</div></div><br><font style="vertical-align: inherit;"></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Allen AI</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
ELMoモデル</font><font style="vertical-align: inherit;">は、過去の地域の開発全体に対する一種の後継者です。つまり、双方向のリカレントニューラルネットワークと、起動するためのいくつかの新しいトリックです。</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">OpenAIの</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">同僚は、</font><font style="vertical-align: inherit;">何をより良くできるか</font><font style="vertical-align: inherit;">を</font><font style="vertical-align: inherit;">決定しました。そのためには、Googleの1年前に発表された</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Transformer</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">アーキテクチャ</font><font style="vertical-align: inherit;">をこのタスク</font><font style="vertical-align: inherit;">に適用するだけです</font><font style="vertical-align: inherit;">。過去2.5年間で、誰もがこのアーキテクチャにすでに慣れ親しんでいると思いますので、詳しくは説明しません。聖体拝領をご希望の方</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">は、2017年からのレビュー</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">を参考にさせて</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">いただき</font></a><font style="vertical-align: inherit;">ます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
彼ら（OpenAIの従業員）は</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GPT-2</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">モデルと呼んでいました</font><font style="vertical-align: inherit;">。そして、このモデルでは、彼ら</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">はかなり良い仕事をしました</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。しかし、それを彼らの良心に任せて、私たちの羊、つまりモデルに戻りましょう。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ELMoの最も重要なトリックの1つは、未割り当ての大規模なケースの事前トレーニングでした。それは非常にうまくいき、Googleの同僚は私たちがさらに上手にできると判断しました。トランスフォーマーアーキテクチャ（既にGPT-2に含まれていました）の適用に加えて、トランスフォーマーからの双方向エンコーダー表現、つまりトランスフォーマーアーキテクチャーに基づく双方向エンコーダーからのベクトル表現を表すBERTには、さらに重要なものがいくつか含まれていました。特に、最も重要なのは、大きなケースでトレーニングする方法でした。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/lb/hw/yw/lbhwywgm70j3shvnrtzrnx6clyy.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
図は、未割り当てデータをマークアップする方法を示しています。 2つのレイアウト方法が同時に具体的に示されています。最初に、トークン（単語）のシーケンス（例：文）が取得され、このシーケンスで任意のトークン（[MASK]）がマスクされます。そして、学習プロセスのモデルは、どのようなトークンが偽装されたかを推測する必要があります。 2番目の方法-2つの文が順番に、またはテキストの任意の場所から取得されます。そして、モデルはこれらの文が連続しているかどうかを推測する必要があります（[CLS]と[SEP]）。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
そのようなトレーニングのアイデアは非常に効果的でした。 Facebookの誓いの友人からの回答は</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">RoBERTa</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">モデルでした。</font><font style="vertical-align: inherit;">このモデルに関する記事は「Sustainably Optimized BERT Training」と呼ばれています。さらにもっと。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Transfomerアーキテクチャーに基づく大規模言語モデルのトレーニングを改善する方法をすべて列挙するわけではありません。これは、それが単純に退屈であるためです。</font><font style="vertical-align: inherit;">たぶん、香港からの私の同僚の</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ERNIE</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">の仕事だけに言及し</font><font style="vertical-align: inherit;">ます。</font><font style="vertical-align: inherit;">彼らの仕事では、同僚は知識グラフを使用してトレーニングを充実させています。</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">次に進む</font></a></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
前に、いくつかの役立つリンクを示します</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。BERT</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">に関する記事</font><font style="vertical-align: inherit;">です。</font><font style="vertical-align: inherit;">さらに</font><font style="vertical-align: inherit;">、ロシア語用のトレーニング済みのBERTおよびELMoモデルの</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">セット</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">小さなモデル</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
しかし、BERTについては十分です。さらに重要な傾向がいくつかあります。まず、これはモデルのサイズを小さくする傾向です。同じBERTはリソースを非常に要求し、多くの人がモデルを機能させるために必要なリソースを減らし、品質を維持する（または実際に失わない）方法について考え始めました。 Googleの同僚は少しBERTを思いついた、冗談ではない</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">-ALBERT：少しのBERT</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。小さなBERTはほとんどのタスクで兄よりも優れており、パラメータが1桁少ないことがわかります。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/y5/su/h3/y5suh3uzlmgy16l8stcoahmio4w.png"> <br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
同じバーに対する別のアプローチが、香港の同僚によって再び行われました。彼らは小さなBERT- </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">TinyBERTを思いついた</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。 （この時点で、名前が繰り返され始めたと思った場合、私はあなたに同意する傾向があります。）</font></font><br>
<br>
<img src="https://habrastorage.org/webt/7_/bb/el/7_bbelco1m2dewd3gj6cjqq2upw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
上記の2つのモデルの基本的な違いは、ALBERTがトリッキートリックを使用して元のBERTモデルを削減する場合、たとえば、パラメーターの共有や行列分解による内部ベクトル表現の次元の削減では、TinyBERTは根本的に異なるアプローチ、つまり知識の抽出を使用するということです。学習過程で姉の後に繰り返すことを学ぶ小さなモデル。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">小さなケース</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
近年（1990年ごろからインターネットが登場して以来）、利用可能な建物が増えています。次に、このような大規模なエンクロージャを処理できるようになったアルゴリズムが登場しました（これが「ディープラーニング革命」と呼ばれるものです。これはすでに2013年からの年です）。そしてその結果、あるタスクで高品質を得るには、マークアップされたデータの膨大な配列が必要であると通常は認識され始めました-私たちの場合、テキストのコーパス。たとえば、今日の機械翻訳タスクを学習する典型的なケースは、数百万の文のペアで測定されます。多くのタスクで、そのようなケースを妥当な時間と妥当な金額で組み立てることが不可能であることは長い間明らかでした。長い間、それについて何をすべきかはあまり明確ではありませんでした。しかし、昨年（誰だと思いますか？）BERTが登場しました。このモデルは、大量の未割り当てテキストを事前にトレーニングすることができ、完成したモデルは、小さなケースでタスクに簡単に適応できました。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/7_/bb/el/7_bbelco1m2dewd3gj6cjqq2upw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
この表にリストされているすべてのタスクには、数</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">千</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ユニットの</font><font style="vertical-align: inherit;">サイズの訓練隊があります</font><font style="vertical-align: inherit;">。</font><font style="vertical-align: inherit;">つまり、2〜3桁小さくなります。</font><font style="vertical-align: inherit;">これが、BERT（およびその子孫と親戚）が非常に人気を博しているもう1つの理由です。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">新しいトレンド</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
さて、結局、私が見たように、いくつかの新しいトレンドがあります。まず、これはテキストに対する態度の根本的な変化です。以前のほとんどのタスクでは、テキストは入力資料としてのみ認識され、出力はクラスラベルなどの有用なものでした。これでコミュニティは、テキストが主にコミュニケーションの手段であることを思い出す機会を得ました。つまり、モデルに「話しかける」ことができます。質問をして、人間が読めるテキストの形式で回答を受け取ることができます。これがGoogle </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">T5の</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">新しい記事の内容です</font><font style="vertical-align: inherit;">（この名前は「5倍トランスフォーマー」と翻訳できます）。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/ba/vz/mj/bavzmjwryypmza-ywo18njxfbjy.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
別の重要な傾向は、地域が長いテキストで作業するために再学習していることです。 70年代以降、コミュニティには任意の長さのテキストを扱う方法があります。同じTF-IDFを使用してください。ただし、これらのモデルには独自の品質制限があります。しかし、新しい深層学習モデルは長いテキストを処理できませんでした（同じBERTには、入力テキストの長さの512トークンの制限があります）。しかし最近では、少なくとも2つの作品が登場し、異なる側面から長いテキストの問題に取り組んでいます。 Ruslan Salakhutdinovのグループによる最初の作品は、Transformer-XLと呼ばれていました。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/ci/op/gj/ciopgjs1htbc2gmucz7dwkiwqtk.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
この作業では、再帰的なネットワークが非常に人気になったというアイデアが復活しました。以前の状態を保存して、次の状態を構築するために使用できます（勾配を時間的にロールバックしなくても（BPTT））。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
二つ目</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">この作業</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">はルジャンドル多項式で機能し、その助けを借りて、再帰型ニューラルネットワークで数万のトークンのシーケンスを処理できます。</font></font><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
これについて、起こった変化と新たなトレンドのレビューを終えたいと思います。</font><font style="vertical-align: inherit;">今年は何が起こるか見てみましょう。きっと面白いことがたくさんあると思います。</font><font style="vertical-align: inherit;">データツリーの同じトピックに関する私のスピーチのビデオ：</font></font><br>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/cdlAUcaOCDY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
PS私たちはもう少し興味深い発表があります、切り替えないでください！</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../ja487706/index.html">Consulの例を使用した分散システムでのサービス検出。アレクサンドル・シガチェフ</a></li>
<li><a href="../ja487716/index.html">完璧なSAST。パーサー</a></li>
<li><a href="../ja487720/index.html">競争的コーティニズムについて（例としてリアクティブプログラミングを使用）</a></li>
<li><a href="../ja487724/index.html">BlazingPizza：最初から最後までBlazorアプリ。パート2.コンポーネントを追加する</a></li>
<li><a href="../ja487728/index.html">@Pythonetcコンパイル、2020年1月</a></li>
<li><a href="../ja487734/index.html">Entity Framework Coreの高速化</a></li>
<li><a href="../ja487738/index.html">SCADAのスキーマアニメーション</a></li>
<li><a href="../ja487740/index.html">ポータブル磁力計の組み立て</a></li>
<li><a href="../ja487742/index.html">アービトラージ取引（Bellman-Fordアルゴリズム）</a></li>
<li><a href="../ja487744/index.html">FAROがFOCUS S 70レーザー3Dスキャナーを発表</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>