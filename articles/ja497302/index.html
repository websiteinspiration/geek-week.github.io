<!doctype html>
<html class="no-js" lang="ja">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🕓 🤵🏻 💒 移動ロボットの自律ナビゲーション 🙇 🙇🏽 👨‍👩‍👦</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="ロボットが相互作用するためにロボットが外界から情報を受け取ることができる方法は非常にたくさんあります。また、割り当てられたタスクによって、この情報の処理方法が異なります。この記事では、学校プロジェクトの一環として行われる作業の主要な段階について説明します。その目的は、自律ロボットナビゲーションのさま...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>移動ロボットの自律ナビゲーション</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/497302/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ロボットが相互作用するためにロボットが外界から情報を受け取ることができる方法は非常にたくさんあります。</font><font style="vertical-align: inherit;">また、割り当てられたタスクによって、この情報の処理方法が異なります。</font><font style="vertical-align: inherit;">この記事では、学校プロジェクトの一環として行われる作業の主要な段階について説明します。その目的は、自律ロボットナビゲーションのさまざまな方法に関する情報を体系化し、「RTKカップ」コンテストのロボットを作成するプロセスで得られた知識を適用することです。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/qb/xw/dw/qbxwdwc_cwrypc3c8dthahudzmk.jpeg"><br>
<a name="habracut"></a><br>
<h2><font color="#4d7f95"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">前書き</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
競技会「RTKカップ」では、オペレーターの介入なしに完了する必要のあるタスクのブロックがあります。</font><font style="vertical-align: inherit;">ロボット設計の作成とプログラムの作成の複雑さのように見えるため、他の競合する分野から大きく単純化されたタスクが1つのトレーニンググラウンドにまとめられるため、多くの参加者がこれらのタスクを不当に回避していると思います。</font><font style="vertical-align: inherit;">私のプロジェクトでは、一例として次のように考えて、そのような問題の可能な解決策を示したいと思います。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
プロジェクトの目標を達成するために、次の中間タスクが策定されました。</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">大会「RTKカップ」の</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ルール</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">分析</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">移動ロボットの自律的方向付けのための既存のアルゴリズムの分析</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ソフトウェアの作成</font></font></li>
</ul><br>
<h2><font color="#4d7f95"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">大会「RTKカップ」のルール分析</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
「RTKカップ」コンペティションでは、参加者には、さまざまな複雑さのセクションがモデル化されるトレーニングの場が提示されます。</font><font style="vertical-align: inherit;">このコンテストの目的は、若いロボットを刺激して、極端な条件で動作し、障害物を乗り越え、人間の制御下で、または自律的に動作できるデバイスを作成することです。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/6a/sr/6w/6asr6wzvlhnqgkmz0b8dfyf2h9o.jpeg"><br>
<br>
<div class="spoiler"><b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ポリゴンを構成する要素について簡単に</font></font></b><div class="spoiler_text"> «»          ,    .    ,       ,     (), ,      (),   ..<br>
<br>
:<br>
<br>
<img src="https://habrastorage.org/webt/mk/ks/d3/mkksd313rprmlsilxcrq5xdytg4.png" width="300"><br>
<br>
:<br>
<br>
<img src="https://habrastorage.org/webt/rv/fp/cu/rvfpcu-6qtvdfrclsqjzlr2xok4.jpeg" width="300"><br>
<br>
  –  ,     «»  ( )  ,        . ,         ,    ,      ,         .<br>
<br>
     .    ,       ,     ,   ,     ,    ,    .<br>
</div></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
コンテストは、お互いに根本的に異なる2つのノミネーション「シーカー」と「エクストリーム」に分けられます。これは、年齢とロボットシステムの開発経験の差が最小の参加者間で競争が行われるようにするためです。若いレベルのSeeker、および14歳以上の参加者のExtreme。 Seekerノミネーションでは、オペレーターは範囲内を自由に動き、機械に直接アイコンタクトできます。一方、Extremeノミネーションは、オペレーターが迷路内を移動して迷路内を移動する必要があるため、ロボットにビデオ通信システムまたはコンピュータービジョンがあることを前提としています。ロボットに組み込まれたカメラとセンサー。特別なスクリーンの後ろにあります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
競技に出場するには、ロボットはマニピュレーターのリモートコントロールのタスクに合格するか、自律性の要素の1つを実行する必要があります。</font><font style="vertical-align: inherit;">プロジェクトのフレームワークでは、タスクはオペレーターから最低のコストで最も多くのポイントを与えるため、自律タスクを実行するように設定されました。</font><font style="vertical-align: inherit;">自律性の要素は次のとおりです。</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">アンビエントライトセンサーまたは視線システムを使用してラインに沿って運転</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">距離センサーまたはビジョンシステムを使用したスタンドアロンビーコンのキャプチャ</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">コンパス、ジャイロスコープ、加速度計、ビジョンシステム、または組み合わせた方法を使用した、線に沿った複雑な軌道（階段の上り/下りなど）に沿った動き</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
また、ロボットが自律的に障害物を通過すると、障害物を乗り越えるためのポイントが2倍になります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
このプロジェクトの枠組みの中で、最初のタスクの解決策が検討されます-線に沿った動き。ラインに沿って移動するときに使用される最も一般的な方法は、光センサーとカメラです。センサーのプラスには、プログラムを作成する簡単さが含まれます-それらの多くは調整抵抗器を備えているため、センサーを背景照明に設定することで、ライン上かどうかに応じて0または1を出力します。同じ理由で、光センサーは使用するコントローラーの処理能力を要求しません。また、このため、光センサーの助けを借りて問題を解決することは、最もコストがかかりません。最も単純なセンサーのコストは35ルーブルであり、ラインに沿って比較的安定して走行するには、3つのセンサーで十分です（ラインに1つ、側面に2つ）。しかしながら、このようなセンサーの主な欠点の1つは、インストールの制限です。理想的には、センサーは床から少し離れた中央に正確に設置する必要があります。そうしないと、値が正しくありません。これは、ロボットがトラックに沿って可能な限り高速で走行する必要がある特殊な競技では問題ではありませんが、「RTKカップ」競技の条件では、上記のセンサーの欠陥のすべてが重大になる可能性があります。それらの設置では、ロボットに追加の機械部品が存在する必要があります。センサーを下げると、ロボットに追加のスペースが必要になり、センサーを動かす別のエンジンが必要になり、損傷の可能性があり、ロボットの質量が増加します。それ以外の場合は、誤った値が表示されます。これは、ロボットがトラックに沿って可能な限り高速で走行する必要がある特殊な競技では問題ではありませんが、「RTKカップ」競技の条件では、上記のセンサーの欠陥のすべてが重大になる可能性があります。それらの設置では、ロボットに追加の機械部品が存在する必要があります。センサーを下げると、ロボットに追加のスペースが必要になり、別のエンジンがセンサーを動かします。また、損傷の可能性があり、ロボットの質量が増加します。それ以外の場合は、誤った値を示します。これは、ロボットがトラックに沿って可能な限り高速で走行する必要がある特殊な競技では問題ではありませんが、「RTKカップ」競技の条件では、上記のセンサーの欠陥のすべてが重大になる可能性があります。それらの設置では、ロボットに追加の機械部品が存在する必要があります。センサーを下げると、ロボットに追加のスペースが必要になり、別のエンジンがセンサーを動かします。また、損傷の可能性があり、ロボットの質量が増加します。上記のすべてのセンサーの欠陥は重大である可能性があります-それらの設置には、主にロボットにセンサーを上下させる追加の機械部品の存在が必要であり、これにはロボットに追加のスペース、センサーを動かす別個のモーターが必要であり、潜在的な損傷の場所であり、ロボットの質量を増加させます。上記のすべてのセンサーの欠陥は重大である可能性があります-それらの設置には、主にロボットにセンサーを上下させる追加の機械部品の存在が必要であり、これにはロボットに追加のスペース、センサーを動かす別個のモーターが必要であり、潜在的な損傷の場所であり、ロボットの質量を増加させます。</font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"><img src="https://habrastorage.org/webt/_g/pt/zz/_gptzzip0rdk6ocgg67ttumux2k.jpeg" width="300" align="right"></a><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
次に、カメラには次の利点があります：（センサーと比較して）実質的に無制限の測定半径があります。ロボットの真下とラインの両方から同時にラインを確認できるカメラモジュールは1つだけです。これにより、たとえば、その曲率を評価して比例制御アクションを選択できます。同時に、カメラは床から離れた位置に固定されているため、自律性を必要としない埋め立て地の他の部分でのロボットの前進を妨げません。カメラの主な欠点は、ビデオ処理にはロボットに搭載された強力なコンピューティングコンプレックスが必要であり、カメラとコンピューターは3つの光センサーよりも外界から1桁多い情報を受け取るため、開発中のソフトウェアにはさらに微調整が必​​要なことです。受信した情報を処理できるのは、3つのセンサーと「アードゥイン」の何倍にもなります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
私にとって個人的には答えは明白です。「極値」の指名では、ロボットは、オペレーターがナビゲートする指向性カメラを備えている必要があります。</font><font style="vertical-align: inherit;">既製のFPVソリューションを使用する場合、「センサー」の総コストはさらに高くなる可能性がありますが、追加のデバイスをインストールする必要があります。</font><font style="vertical-align: inherit;">さらに、ラズベリーpiとカメラを備えたロボットは、カメラが幅広いタスクを解決でき、ラインの移動だけでなく、デザインを大幅に複雑化することなく使用できるため、自律移動の開発の可能性が大きくなります。</font></font><br>
<br>
<h2><font color="#4d7f95"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">既存のコンピュータービジョンアルゴリズムの分析</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
コンピュータビジョンは、実世界のオブジェクトの画像を受け取り、得られたデータを処理および使用して、人間の介入なしにさまざまな種類の適用された問題を解決できるデバイスを作成する理論です。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
コンピュータビジョンシステムは次のもので構成されます。</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1台以上のカメラ </font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">コンピューター複合体 </font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">画像処理ツールを提供するソフトウェア</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ターゲットおよびテレメトリ情報を送信するための通信チャネル。 </font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
以前に書かれたように、私たちの興味のあるオブジェクトを識別するための多くの方法があります。ラインに沿って走行する場合、ライン自体を対照的な背景から分離する必要があります（白い背景に黒いライン、または反転ラインの黒い背景に白いライン）。コンピュータビジョンシステムを使用するアルゴリズムは、元の画像を処理するためのいくつかの「ステップ」に分割できます。</font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">画像取得</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">：デジタル画像は、カメラから直接、デバイスに送信されるビデオストリームから、または個別の画像として</font><b><font style="vertical-align: inherit;">取得</font></b><font style="vertical-align: inherit;">されます。ピクセル値は通常、光の強度（カラーまたはグレースケール画像）に対応しますが、たとえば、熱画像カメラからの温度など、さまざまな物理測定値に関連付けることができます。</font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">予備処理</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">：コンピュータビジョン手法をビデオデータに適用する前に、使用する手法に応じて、特定の条件を導入するための前処理が必要です。</font><font style="vertical-align: inherit;">次に例を示します。</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用済みセンサーによるノイズや歪みの除去</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">カメラの操作中に発生する小さなアーティファクト、解凍要素、ノイズなどを取り除くために使用される画像のぼかし。</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">コントラストを改善して、正しい情報をより確実に検出できるようにする</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">露出を変更して、シャドウまたはハイライトをトリミングします</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">画像の構造をよりよく区別するためのスケーリングまたはトリミング。</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">システムパフォーマンスを向上させるために、イメージをモノクロ形式に変換するか、解像度を変更する</font></font></li>
</ul><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">強調表示の詳細</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">：さまざまな難易度の画像の詳細がビデオデータから抽出されます。</font><font style="vertical-align: inherit;">そのような詳細の典型的な例は、ライン、境界、エッジ、個々のポイント、任意のフィーチャに特徴的なエリアです。</font></font><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">検出</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">：プログラムの作業の特定の段階で、プログラムに関連する情報が残りの画像から分離されます。</font><font style="vertical-align: inherit;">次に例を示します。</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">色、ある点で類似している孤立したピクセルの数（図の曲率、色、明るさなど）の特定のポイントのセットを選択します。</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">特徴的なオブジェクトを含む1つ以上の画像セクションのセグメンテーション。</font></font></li>
</ul><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">高レベルの処理</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">：このステップでは、画像からの豊富な情報が、簡単に処理できるサイズに削減されます。たとえば、特定のピクセルのセットや、対象のオブジェクトが配置されていると思われる画像の部分の座標などです。</font><font style="vertical-align: inherit;">次に例を示します。</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">任意の基準による値のフィルタリング</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">オブジェクトの物理的な寸法、形状、フレーム内の位置、または他の特徴的なオブジェクトとの相対的なパラメータなどの評価</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">分類</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
次に、プログラムの作成に使用するライブラリを選択する必要がありました。</font><font style="vertical-align: inherit;">私が選んだ主な要因は次のとおりです。</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">初心者によるこの言語の学習が比較的容易なため、ライブラリのPythonインターフェースのサポートは単純な構文であり、プログラムの可読性に有利な影響を与えます。</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">移植性、つまり </font><font style="vertical-align: inherit;">ラズベリーpi3でこのライブラリを使用してプログラムを実行する機能。</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ライブラリの普及。これは、作業中に発生する可能性のある問題にすでに遭遇した可能性があるプログラマの十分に開発されたコミュニティを保証します。</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
私が検討したオプションの中で、OpenCVオープンコンピュータービジョンライブラリは、Pythonをサポートしているため、広範なオンラインドキュメントを備えていることを強調しました。</font><font style="vertical-align: inherit;">このライブラリでの作業のすべての機微を説明するインターネット上の多くの記事と指示があります。</font><font style="vertical-align: inherit;">開発者による公式フォーラムがあり、誰でも質問できます。</font><font style="vertical-align: inherit;">また、このライブラリはシステムのパフォーマンスを保証するC / C ++言語で実装されており、その構造は、パフォーマンスを向上させるために無効にできるさまざまなモジュールをサポートしています。</font></font><br>
<br>
<h2><font color="#4d7f95"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ソフトウェア開発</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
OSとRaspberry piの初期構成をインストールした後、プログラムの作成を開始する前に、プログラムに必要なすべてのパッケージをインストールする必要があります。</font><font style="vertical-align: inherit;">これらのパッケージのほとんどは、pipパッケージマネージャー（Python 3の場合、pip3）を使用してインストールされます。</font></font><br>
<br>
<pre><code class="bash hljs">$ sudo apt install python3-pip</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
次のライブラリがインストールされています。</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">picamera-ラズベリーpiカメラを操作するためのライブラリ</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">numpy-画像として多次元データ配列を操作するためのライブラリ</font></font></li>
</ul><br>
<pre><code class="bash hljs">$ sudo pip3 install picamera<font></font>
$ sudo pip3 install numpy<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
cmake-ソースコードからプログラムを自動的に構築するためのユーティリティ</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
cmake-curses-gui-cmakeのGUIパッケージ（グラフィカルインターフェイス）</font></font><br>
<br>
<pre><code class="bash hljs">$ sudo apt-get install cmake cmake-curses-gui libgtk2.0-dev<font></font>
$ sudo apt-get install cmake cmake-curses-gui libgtk2.0-dev<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
さまざまな画像やビデオ形式などを操作するためのライブラリ</font></font><br>
<br>
<pre><code class="bash hljs">$ sudo apt-get install libavcodec-dev libavformat-dev libswscale-dev libv4l-dev libx264-dev libxvidcore-dev<font></font>
$ sudo apt-get install libjpeg-dev libpng12-dev libtiff5-dev libjasper-dev<font></font>
$ sudo apt-get install gfortran libatlas-base-dev<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ロボットからコンピューターにビデオデータを送信するには、GStreamerを使用します。これは、マルチメディアデータを受信、処理、送信するために設計されたフレームワークです。</font></font><br>
<br>
<pre><code class="bash hljs">$ sudo apt install libgstreamer1.0-0 gstreamer1.0-plugins-base gstreamer1.0-plugins-good gstreamer1.0-plugins-bad gstreamer1.0-plugins-ugly gstreamer1.0-libav gstreamer1.0-doc gstreamer1.0-tools gstreamer1.0-x gstreamer1.0-alsa gstreamer1.0-gl gstreamer1.0-gtk3 gstreamer1.0-qt5 gstreamer1.0-pulseaudio
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
次のステップは、openCVライブラリ自体をソースからインストールし、構成してビルドすることです。</font><font style="vertical-align: inherit;">これを行うには、opencv作業フォルダーを作成します。</font></font><br>
<br>
<pre><code class="bash hljs">$ mkdir opencv<font></font>
$ <span class="hljs-built_in">cd</span> opencv
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ライブラリの最新バージョンをダウンロードするには、ネットワークからファイルをダウンロードするためのコンソールプログラムであるwgetを使用します。</font><font style="vertical-align: inherit;">プログラムの作成時点では、openCVの最新の安定バージョンは4.1.0なので、ソースをダウンロードして解凍します。</font></font><br>
<br>
<pre><code class="bash hljs">$ wget https://github.com/opencv/opencv/archive/4.1.0.zip -O opencv_source.zip<font></font>
$ unzip opencv_source.zip<font></font>
$ wget https://github.com/opencv/opencv_contrib/archive/4.1.0.zip -O opencv_contrib.zip<font></font>
$ unzip opencv_contrib.zip<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
解凍プロセスが完了したら、ソースアーカイブを削除できます。</font></font><br>
<br>
<pre><code class="bash hljs">$ rm opencv_source.zip<font></font>
$ rm opencv_contrib.zip<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
アセンブリと構成用のディレクトリが作成されます。</font></font><br>
<br>
<pre><code class="bash hljs">$ <span class="hljs-built_in">cd</span> /home/pi/opencv/opencv-4.1.0<font></font>
$ mkdir build<font></font>
$ <span class="hljs-built_in">cd</span> build
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ビルドパラメータは、cmakeユーティリティを使用して構成されます。</font><font style="vertical-align: inherit;">これを行うには、すべての重要なパラメーターを、割り当てられた値とともにユーティリティ変数として渡します。</font></font><br>
<br>
<pre><code class="cmake hljs">cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local -D INSTALL_PYTHON_EXAMPLES=<span class="hljs-keyword">ON</span> -D INSTALL_C_EXAMPLES=<span class="hljs-keyword">OFF</span> -D BUILD_opencv_python2=<span class="hljs-keyword">OFF</span> -D WITH_GSTREAMER=<span class="hljs-keyword">ON</span> -D BUILD_EXAMPLES=<span class="hljs-keyword">ON</span> -DENABLE_VFPV3=<span class="hljs-keyword">ON</span> -DENABLE_NEON=<span class="hljs-keyword">ON</span> -DCPU_BASELINE=NEON ..
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
設定をセットアップした後、ユーティリティはすべてのパラメータを表示します。次に、ライブラリをコンパイルする必要があります。これを行うには、コンソールコマンドmake –jNを使用します。Nはコンパイルプロセスに関与するコアの数です。ラズベリーpi 3の場合、コアの数は4ですが、コンソールでnprocコマンドを書くことによって、この数を確実に見つけることができます。</font></font><br>
<br>
<pre><code class="bash hljs">$ make –j4</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ラズベリーのリソースは限られているため、コンパイルにはかなり時間がかかる場合があります。</font><font style="vertical-align: inherit;">場合によっては、ラズベリーがフリーズすることもありますが、後でビルドフォルダーに移動してmakeを再登録すると、作業は続行されます。</font><font style="vertical-align: inherit;">これが発生した場合、関連するコアの数を減らす価値がありますが、私のコンパイルは問題なく行われました。</font><font style="vertical-align: inherit;">また、この段階では、プロセッサの温度が約75度に達したため、ラズベリーのアクティブ冷却について検討する価値があります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
コンパイルが成功したら、ライブラリをインストールする必要があります。</font><font style="vertical-align: inherit;">これもmakeユーティリティを使用して行われます。</font><font style="vertical-align: inherit;">次に、ldconfigユーティリティを使用して必要な接続をすべて形成します。</font></font><br>
<br>
<pre><code class="bash hljs">$ sudo make install<font></font>
$ sudo ldconfig<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pythonインタラクティブモードで次のコマンドを記述して、インストールを確認します。</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">import</span> cv2<font></font>
print(cv2.getBuildInformation())<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
プログラムの次の結論は、正しいインストールの証拠になります。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/np/vi/ng/npving2rmncveg11-8qxvhvco1q.jpeg"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
上記のライブラリのコンパイル手順は、ロボットと、ロボットの制御が計画され、ロボットからのブロードキャストが受信されるPCの両方で実行する必要があることに注意してください。</font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ビデオ配信スキームの作成</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
コードを書き始める前に、アルゴリズムが機能するスキームを開発する必要があります。エクストリームノミネートのRTKカップ大会に参加するために作成されたロボットのソフトウェア開発を検討する場合、プログラム全体が2つの部分に分けられます。ロボットとリモコンで、Linuxがインストールされたコンピューターで再生されます。ここで最も重要なタスクの1つは、アルゴリズムのさまざまな部分間でビデオデータがどのように送信されるかを概算することです。 Wi-Fiは、2つのデバイス間の通信チャネルとして使用されます。ロボット制御とフィードバックデータを提供するデータパケットは、ソケットライブラリを使用して実装されたUDPプロトコルを使用して、あるデバイスから別のデバイスに転送されます。ビデオデータUDPパケットのサイズに制限があるため、GStreamerを使用して送信されます。デバッグの便宜上、2つのビデオストリームが実装されます。</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">メインビデオストリーム-ロボットのカメラからコンピューターに直接ビデオデータを転送して、制御遅延を最小限に抑えます。</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">補助ビデオストリーム-ロボットによって処理されたビデオデータを転送します。コンピュータービジョンを実装するプログラムのセットアップとデバッグに必要です。</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ロボットで2つのビデオストリームが同時にアクティブになり、有効になっているドライブモードに応じて、コンピューターに目的の画像が表示されます。</font><font style="vertical-align: inherit;">ロボットは、自律モードがオンかオフかに応じて、コンピュータから受信した、またはイメージプロセッサによって生成された制御データを使用します。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/zw/0l/uw/zw0luwrmjjesbm1ygsra6gxtkm4.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ロボットとコンピューターの2つの平行な流れの働きにより、ロボットのリモート制御が実行されます。</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">サイクルの「コンソール」は、使用可能なすべての入力デバイスをポーリングし、データ自体とチェックサムで構成される制御データパケットを形成します（記事に最終的な変更を加える時点で、遅延を減らすためにチェックサムの作成を拒否しましたが、ソースでは、コードのこのセクションは残ります）-送信中にデータの整合性を判断するために使用されるいくつかのアルゴリズムの操作によってデータセットから計算された値</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ロボット-コンピュータからのデータアクセスを待ちます。</font><font style="vertical-align: inherit;">データを解凍し、チェックサムを再計算して、送信されたものと比較し、コンピューター側で計算します。</font><font style="vertical-align: inherit;">チェックサムが一致する場合、データはメインプログラムに転送されます。</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ライン検出アルゴリズムを解析する前に、ロボットの設計機能について理解しておくことをお勧めします。</font></font><br>
<br>
<div class="spoiler"><b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ロボットについて</font></font></b><div class="spoiler_text">          .<br>
<br>
<img src="https://habrastorage.org/webt/vw/ao/ex/vwaoexwehb49titxcgdis_ryonc.jpeg" width="200" align="right"><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"></a> —       .      (3  )        .                 ,      .      6 ,        .           .      .          .     ,    -  .     «»   rasberry pi 3 b —      .<br>
<br>
<img src="https://habrastorage.org/webt/ho/zw/bi/hozwbiptp_fihoiqncxq2zsbpim.png" width="200" align="left"> ,       ,   ,   ,   Solidworks    petg .    ,     raspberry        .<br>
<br>
<img src="https://habrastorage.org/webt/mh/po/bd/mhpobduedmyoxzrdbhhac2ewdpq.png" width="200" align="left">          ubiquiti bullet M5 hp.     (   )      ,          .   ,   «»  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"> </a> . <br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/ti/h_/7l/tih_7l74vjx8leso89cwynfpb3o.jpeg" width="400"></div><br>
:     «»     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow">  thingiverse</a>.    ,  ,   ,      ,          .<br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/wd/df/q_/wddfq_nyi7-xcqmdiil5glkybqe.gif" width="300"></div><br>
   ,     ,   .       ,     .              ,  ,        ,     ,    .     ,       ,        ,     .<br>
<br>
<img src="https://habrastorage.org/webt/sl/ju/f9/sljuf9jwaqm2kdadelgsgubyf5o.gif" width="450"><br>
<br>
<img src="https://habrastorage.org/webt/sg/xk/_k/sgxk_kt1f0xdxkg4igwgzmbudk0.png" width="250"><br>
<br>
-     (   -  200 )    ,       ,     90       70   (     ),          ,     « ». ,            VL53L0X        ,      .<br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/jq/wh/hc/jqwhhc7et4crpin64qkaw6txgk0.png" width="250"></div><br>
 «»     ,     ,    (<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow">rds3115</a>).    — ,     ,  ,     ,     .<br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/4p/ot/ic/4poticuqt_itsiasls1of3927ma.jpeg" width="250"></div><br>
      ,      ,    ,   :<br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/qc/w5/ol/qcw5olk3klaxq75typuz41hdt18.png" width="250"></div><br>
- ,       ,          ,      .           . <img src="https://habrastorage.org/webt/he/4o/kp/he4okpaqyd5pof9x1cjwc1aalwi.jpeg" width="200" align="left">        raspberry,      ,     .       ,      .<br>
<br>
     ,   USB.            ,            ,     .<br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/rq/wv/yr/rqwvyr8kv5dtvpgz6x7rahoyfho.gif" width="200"></div><br>
<i>        </i><br>
</div></div><br>
<h3><font color="#4d7f95"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">OpenCVライブラリメソッドを使用したライン検出アルゴリズムの作成</font></font></font></h3><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I.データの受信</font></font></b> <br>
<br>
<img src="https://habrastorage.org/webt/ua/q7/zo/uaq7zojtflqtezqkiq2meem5mam.png" width="300" align="right"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">イメージプロセッサがカメラから直接ビデオデータを受信するのではなく、メインストリームからビデオデータを受信するため、変換に使用される形式から画像処理に使用される形式、つまり赤の値で構成されるnumpy配列にデータを転送する必要があります。 、各ピクセルの緑と青。</font><font style="vertical-align: inherit;">これを行うには、初期データ（ラズベリーpiカメラモジュールから受信したフレーム）が必要です。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
さらに処理するためにカメラcからフレームを取得する最も簡単な方法は、picameraライブラリを使用することです。</font><font style="vertical-align: inherit;">始める前に、raspi-config-&gt;インターフェースオプションカメラ-&gt; [はい]を選択して、カメラへのアクセスを許可する必要があります。</font></font><br>
<br>
<pre><code class="bash hljs">sudo raspi-config</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
コードの次のセクションはラズベリーカメラに接続され、所定の周波数のサイクルで、opencvライブラリで使用できる配列の形式でフレームを受信します。</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> picamera.array <span class="hljs-keyword">import</span> PiRGBArray
<span class="hljs-keyword">from</span> picamera <span class="hljs-keyword">import</span> PiCamera
<span class="hljs-keyword">import</span> cv2
<span class="hljs-comment">#   </span><font></font>
camera = PiCamera()<font></font>
camera.resolution = (<span class="hljs-number">640</span>, <span class="hljs-number">480</span>) <font></font>
camera.framerate = <span class="hljs-number">30</span>
cap = PiRGBArray(camera, size=(<span class="hljs-number">640</span>, <span class="hljs-number">480</span>))<font></font>
<font></font>
<span class="hljs-keyword">for</span> frame <span class="hljs-keyword">in</span> camera.capture_continuous(cap , format=<span class="hljs-string">"bgr"</span>, use_video_port=<span class="hljs-literal">True</span>):<font></font>
	new_frame = frame.array<font></font>
	cap.truncate(<span class="hljs-number">0</span>)
	<span class="hljs-keyword">if</span> <span class="hljs-literal">False</span>: <span class="hljs-comment">#   -   </span>
		<span class="hljs-keyword">break</span>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
このフレームをキャプチャする方法は、最も簡単ですが、重大な欠点があります。GStreamerを介してフレームをブロードキャストする必要がある場合、ビデオを再エンコードするために数回必要になるため、プログラムの速度が低下するため、あまり効果的ではありません。</font><font style="vertical-align: inherit;">画像を取得するはるかに高速な方法は、画像プロセッサの要求に応じてビデオストリームからフレームを出力することですが、画像処理の以降の段階は、使用する方法に依存しません。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
処理なしのロボットの方位カメラからの画像の例：</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/bo/yi/ez/boyiezf6vfa1nqrlcdllhwbmsgg.png" width="350"></div><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">II。前処理</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
ライン上を走行する場合、背景色と最も対照的なポイントの領域を分離するのが最も簡単です。この方法は、白い背景に黒い線を使用するため（逆断面の黒い背景に白い線を使用）、RTKカップの競技に最適です。処理する必要のある情報量を減らすために、2値化アルゴリズムを適用することができます。つまり、イメージをモノクロ形式に変換します。ここでは、ピクセルのタイプが2種類しかない-ダークとライトです。その前に、写真をグレースケールに変換し、カメラの操作中に必然的に現れる小さな欠陥やノイズをカットするためにぼかす必要があります。画像をぼかすために、ガウスフィルターが使用されます。</font></font><br>
<br>
<pre><code class="python hljs">gray = cv2.cvtColor(self._frame, cv2.COLOR_RGB2GRAY)<font></font>
blur = cv2.GaussianBlur(gray, (ksize, ksize), <span class="hljs-number">0</span>)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ここで、ksizeはガウスコアのサイズで、これを大きくすると、ぼかしの度合いを上げることができます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
グレースケールとぼかしで変換した後の画像の例：</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/ye/5h/_d/ye5h_d7dqttbxo_af3hhkxnllce.png" width="350"></div><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">III。</font><font style="vertical-align: inherit;">詳細の選択</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
画像がグレースケールに変換された後、所定のしきい値で画像を2値化する必要があります。</font><font style="vertical-align: inherit;">このアクションにより、データ量をさらに削減できます。このしきい値は、新しい場所でロボットが出発する前、または照明条件が変化したときに調整されます。</font><font style="vertical-align: inherit;">理想的には、キャリブレーションのタスクは、線の輪郭が画像上で定義されていることを確認することですが、同時に、線ではない画像に他の詳細があってはなりません。</font></font><br>
<br>
<pre><code class="python hljs">thresh = cv2.threshold(blur, self._limit, <span class="hljs-number">255</span>, cv2.THRESH_BINARY_INV)[<span class="hljs-number">1</span>]</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ここでは、しきい値（self._limit）より暗いすべてのピクセルが0（黒）、明るい-255（白）に置き換えられます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
処理後の画像は次のようになります。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/ne/rq/tl/nerqtlzt7p-k0q-4quw6nhsbfau.png" width="350"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ご覧のとおり、プログラムは画像の最も暗い部分のいくつかを識別しています。</font><font style="vertical-align: inherit;">ただし、ヘッドフォンを完全に「キャッチ」するようにしきい値を調整すると、画面に他の白い要素が表示されます。</font><font style="vertical-align: inherit;">もちろん、しきい値を微調整することもできます。競争の激しいトレーニング場では、カメラが下を向いてフレームに不要な要素を入れないようにしますが、私はラインを他のものから分離する必要があると考えています。</font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">IV。検出</font></font></b> <br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
2値化された画像では、ボーダーサーチアルゴリズムを適用しました。</font><font style="vertical-align: inherit;">自立スポットを決定し、それらを境界を構成するポイントの座標値の便利な配列に変換するために必要です。</font><font style="vertical-align: inherit;">ドキュメントに記載されているopencvの場合、ループを見つけるための標準アルゴリズムはSuzuki85アルゴリズムを使用します（opencvドキュメント以外ではこの名前のアルゴリズムへの参照は見つかりませんでしたが、これは</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Suzuki-Abe</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">アルゴリズムであると想定します</font><font style="vertical-align: inherit;">）。</font></font><br>
<br>
<pre><code class="python hljs">contours = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)[<span class="hljs-number">0</span>]</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
そして、これはこの段階で得られたフレームです：</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/ex/3r/gx/ex3rgxc7bmefqdwhetn5wfxrtko.png" width="350"></div><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">V.高レベル処理</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
フレーム内のすべての輪郭が見つかったので、最大の面積を持つ輪郭が選択され、線の輪郭として採用されます。この輪郭のすべての点の座標がわかれば、その中心の座標がわかります。これには、いわゆる「イメージモーメント」が使用されます。モーメントは、輪郭のすべてのピクセルの座標を合計することによって計算される、輪郭の全体的な特性です。モーメントにはいくつかのタイプがあります-3次までです。この問題では、ゼロ次モーメント（m00）のみが必要です-輪郭を構成するすべてのポイントの数（輪郭の周囲）、すべてのポイントのX座標の合計である1次モーメント（m10）、およびすべてのポイントのY座標の合計であるm01。いずれかの軸に沿ったポイントの座標の合計をそれらの数で割ることにより、算術平均が得られます—輪郭の中心のおおよその座標。次に、コースからのロボットの偏差が計算されます。コースは、「直接」、フレーム幅に近いXに沿った中心点の座標を2で除算した値に対応します。ラインの中心の座標がフレームの中心に近い場合、制御動作は最小限であり、したがってロボットは現在のコースを維持します。ロボットが片側から逸脱すると、戻るまで、その偏差に比例した制御動作が導入されます。</font></font><br>
<br>
<pre><code class="python hljs">mainContour = max(contours, key = cv2.contourArea)<font></font>
M = cv2.moments(mainContour)<font></font>
<span class="hljs-keyword">if</span> M[<span class="hljs-string">'m00'</span>] != <span class="hljs-number">0</span>:<span class="hljs-comment">#     (..   -  )</span>
    cx = int(M[<span class="hljs-string">'m10'</span>]/M[<span class="hljs-string">'m00'</span>])<font></font>
    cy = int(M[<span class="hljs-string">'m01'</span>]/M[<span class="hljs-string">'m00'</span>])
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
以下は、ラインとフレームに対するロボットの位置の概略図であり、プログラムの結果がそれらに重ねられています。「メイン」コンター、コンターの中心を通るライン、および偏差を推定するために中心にあるポイント。</font><font style="vertical-align: inherit;">これらの要素は、次のコードを使用して追加されます。</font></font><br>
<br>
<pre><code class="python hljs">cv2.line(frame, (cx, <span class="hljs-number">0</span>), (cx, self.height), (<span class="hljs-number">255</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>), <span class="hljs-number">1</span>)    <span class="hljs-comment">#    </span>
cv2.line(frame, (<span class="hljs-number">0</span>, cy), (self.width, cy), (<span class="hljs-number">255</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>), <span class="hljs-number">1</span>)                  <font></font>
cv2.circle(frame, (self.width//<span class="hljs-number">2</span>, self.height//<span class="hljs-number">2</span>), <span class="hljs-number">3</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">255</span>), <span class="hljs-number">-1</span>) <span class="hljs-comment">#  </span>
cv2.drawContours(frame, mainContour, <span class="hljs-number">-1</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">255</span>, <span class="hljs-number">0</span>), <span class="hljs-number">2</span>, cv2.FILLED) <span class="hljs-comment">#   </span>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
デバッグの便宜上、前述のすべての要素が未加工のフレームに追加されます。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/f4/fl/os/f4flos522ouvlu-vi2b9rr_17lg.png" width="350"><br>
<br>
<img src="https://habrastorage.org/webt/uc/r1/vx/ucr1vxcecjqdv5qdswcbjsltw9w.png" width="350"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
したがって、処理アルゴリズムを通じてフレームを駆動すると、対象のオブジェクトの中心とデバッグイメージのX座標とY座標が得られます。</font><font style="vertical-align: inherit;">次に、ラインに対するロボットの位置と、処理アルゴリズムを通過した画像を模式的に示します。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/4r/co/fy/4rcofyknawjnesluhuvoyp9zomu.jpeg" width="500"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
プログラムの次のステップは、前のステップで取得した情報を2つのモーターの電力値に変換することです。 </font></font><br>
<br>
<img src="https://habrastorage.org/webt/io/s3/gn/ios3gnw-mt2xsmvh_hsfrpsdkdu.jpeg" width="250" align="right"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">フレームの中心に対するカラースポットの中心のずれの差を変換する最も簡単な方法は、比例レギュレーターです（リレーレギュレーターもありますが、その動作の特徴のため、ラインに沿った駆動にはあまり適していません）。このようなアルゴリズムの動作原理は、コントローラーがエラーの大きさに比例してオブジェクトに対して制御アクションを生成するというものです。比例コントローラーに加えて、積分コンポーネントもあり、時間の経過とともに積分コンポーネントがエラーと微分エラーを「累積」します。その原理は、制御変数の十分な変化のみを伴う規制の影響の適用に基づいています。実際には、これらの最も単純なP、I、Dコントローラーは、PI、PD、PIDタイプのコントローラーに結合されます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
私のロボットでPIDコントローラーを「起動」しようとしたが、その使用は通常の比例コントローラーに比べて深刻な利点をもたらさなかったことに言及する価値がある。レギュレータを適切に調整できなかったことは認めますが、物理的に高速を発生させることができない重いロボットの場合、その利点があまり明確に見えない可能性もあります。執筆時のプログラムの最新バージョンでは、単純な比例レギュレーターが使用されていますが、カメラからのより多くの情報を使用できる小さな機能があります。エラー値を生成するとき、スポットの中点の水平位置だけでなく垂直方向も考慮され、さまざまな方法が可能になりました線要素に応答する「距離内」にあり、ロボットの正面または真下にあります（ロボットのヘッディングカメラには大きな視野角があるため、45度下に向けると、ロボットの下のフィールドのかなりの部分がすでに見えます）。</font></font><br>
<br>
<pre><code class="python hljs">error= cx / (self.width/<span class="hljs-number">2</span>) - <span class="hljs-number">1</span>  
<span class="hljs-comment">#  ( 0   )  [-1; 1]</span>
error*= cy / self.height + self.gain <span class="hljs-comment">#</span>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ほとんどの場合、「RTKカップ」コンテストの条件では、参加者はいわゆる「タンクサーキット」を使用します。1つ以上のエンジンがロボットの片側を制御し、トラックとホイールの両方で動作します。このスキームを使用すると、破損の可能性を高める複雑な伝達要素（ディファレンシャルまたはカルダンシャフト）を排除し、最小の回転半径を取得できます。これにより、閉じ込められたポリゴンで利点が得られます。このスキームには、複雑な経路に沿って移動するための2つの「側面」の並行制御が含まれます。これを行うために、プログラムは2つの変数を使用します-右と左のモーターの出力です。この電力は、基本速度（BASE_SPEED）に依存し、0〜100の範囲で変化します。エラー（エラー）-フレームの中心とラインの中央の座標の差、およびオペレーターによって調整される比例効果の係数（self._koof）。その絶対値は、ロボットが自分自身をラインに揃えようとする速度に影響します。一方のエンジンではコントロールアクションがベーススピードから差し引かれ、もう一方のエンジンでは追加されるため、コースから逸脱するとターンが実行されます。反転が実行される方向は、self._koof変数の符号を変更することで調整できます。また、次のコードセクションの結果として、100を超えるパワー値が表示される場合がありますが、私のプログラムでは、このような場合は後でさらに処理されます。その絶対値は、ロボットが自分自身をラインに揃えようとする速度に影響します。一方のエンジンではコントロールアクションがベーススピードから差し引かれ、もう一方のエンジンでは追加されるため、コースから逸脱するとターンが実行されます。反転が実行される方向は、self._koof変数の符号を変更することで調整できます。また、次のコードセクションの結果として、100を超えるパワー値が表示される場合がありますが、私のプログラムでは、このような場合は後でさらに処理されます。その絶対値は、ロボットが自分自身をラインに揃えようとする速度に影響します。一方のエンジンではコントロールアクションがベーススピードから差し引かれ、もう一方のエンジンでは追加されるため、コースから逸脱するとターンが実行されます。反転が実行される方向は、self._koof変数の符号を変更することで調整できます。また、次のコードセクションの結果として、100を超えるパワー値が表示される場合がありますが、私のプログラムでは、このような場合は後でさらに処理されます。反転が行われる場所では、self._koof変数の符号を変更することで調整できます。また、次のコードセクションの結果として、100を超えるパワー値が表示される場合がありますが、私のプログラムでは、このような場合は後でさらに処理されます。反転が行われる場所では、self._koof変数の符号を変更することで調整できます。また、次のコードセクションの結果として、100を超えるパワー値が表示される場合がありますが、私のプログラムでは、このような場合は後でさらに処理されます。</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-comment">#if lineFound:</span><font></font>
leftSpeed = round(self.base_speed + error*self.koof)<font></font>
rightSpeed = round(self.base_speed - error*self.koof)<font></font>
</code></pre><br>
<h2><font color="#4d7f95"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">結論</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
結果として得られたプログラムをテストした結果、プログラムをセットアップする際の主な困難な瞬間は、照明機能に対するアルゴリズムのキャリブレーションであると言えるでしょう。</font><font style="vertical-align: inherit;">記事を作成する段階が宣言された自己分離と一致したため、小さな部屋での作業のデモンストレーションを含むビデオを作成する必要がありました。</font><font style="vertical-align: inherit;">これは私に次の困難をもたらしました：</font></font><br>
<br>
<ul>
<li> -,    ,    (   ,     ),        .        ,    ,         ,      .      ,     , ,            ,              </li>
<li> -,       —    ,   ,         </li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
実際のコンペティションの状況ではこれらの問題は両方ともないという事実にもかかわらず、プログラムの作業が外部要因に最小限に依存するように、私は対策を講じます。</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
また、将来的には、コンピュータビジョン手法を使用したアルゴリズムの実装に関する作業を継続し、記事の最初の部分で説明した自律性の残りの要素（自律ビーコンキャプチャ、複雑なパスに沿った移動）を通過できるソフトウェアを作成する予定です。センサーを追加してロボットの機能を拡張する予定です。距離計、ジャイロスコープ、加速度計、コンパスです。この記事の公開により、義務教育科目としてのプロジェクトへの取り組みが終了するという事実にもかかわらず、今後の開発の段階について、引き続き説明する予定です。ですから、この作品についてのコメントをお願いします。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
プロジェクトの問題を解決することを目的としたすべてのステップを実行した後、プログラミングとデバッグにおけるすべての比較的複雑なコンピュータビジョンアルゴリズムの使用は、競争自体の段階で最大の利益をもたらすと言っても安全です。カメラの寸法が小さいと、カメラは複数の「従来の」センサーを一度に交換できると同時に、外界から信じられないほど多くの情報を受け取ることができるため、ソフトウェア開発の面で大きな可能性を秘めています。プロジェクトの目標を実現することは可能でした。コンピュータビジョンを使用して「RTKカップ」競技の状況におけるロボットの自律ナビゲーションの問題を解決するプログラムを作成し、プログラムの作成プロセスと画像処理の主要な段階を説明することです。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
先に述べたように、家のラインの複雑な軌跡を再現することはできませんでした。この例は、アルゴリズムがターンをどのように実行するかを示しています。</font><font style="vertical-align: inherit;">ここでの線の太さは、規制による太さに対応しており、ターンのほとんどのカーブは、ポリゴンの90度の回転曲率をほぼ反映しています。</font></font><br>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/YmHk3f-qQ5E" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
プログラムコードを表示したり、プロジェクトの今後の作業を監視したりすることができます（</font><font style="vertical-align: inherit;">続行する場合は</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、github</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">またはここ）。</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../ja497286/index.html">Ludum Dare：開始の1週間前のチェックリスト</a></li>
<li><a href="../ja497288/index.html">装飾シーリングライトFeron AL5000</a></li>
<li><a href="../ja497290/index.html">Sandy Bridge +でuopキャッシュを使用してパフォーマンスを向上させる</a></li>
<li><a href="../ja497292/index.html">テクノロジースタックシロゲーム</a></li>
<li><a href="../ja497296/index.html">ITプロフェッショナルの間で人気のある英語のエラー。パート2：発音</a></li>
<li><a href="../ja497304/index.html">Android用にリリースされたIntercepter-NG 2.5</a></li>
<li><a href="../ja497306/index.html">DLLスプーフィング（DLLハイジャック）</a></li>
<li><a href="../ja497308/index.html">人工知能は芸術を行うことができますか？</a></li>
<li><a href="../ja497310/index.html">双極形態学的ネットワーク：増殖のないニューロン</a></li>
<li><a href="../ja497312/index.html">CAN FDに関する質問</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>