<!doctype html>
<html class="no-js" lang="pt">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®‚Äçüëß üóíÔ∏è üë• Processamento de linguagem natural. Resultados 2019 e tend√™ncias para 2020 üçô ü§Æ üë®‚Äçüè≠</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ol√° a todos. Com algum atraso, decidi publicar este artigo. Todo ano eu tento resumir o que aconteceu no campo do processamento de linguagem natural. ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Processamento de linguagem natural. Resultados 2019 e tend√™ncias para 2020</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/huawei/blog/487730/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ol√° a todos. </font><font style="vertical-align: inherit;">Com algum atraso, decidi publicar este artigo. </font><font style="vertical-align: inherit;">Todo ano eu tento resumir o que aconteceu no campo do processamento de linguagem natural. </font><font style="vertical-align: inherit;">Esse ano n√£o foi exce√ß√£o.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">BERTs, BERTs est√£o em toda parte</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Vamos come√ßar em ordem. </font><font style="vertical-align: inherit;">Se voc√™ n√£o foi para a taiga siberiana remota ou passou f√©rias em Goa no √∫ltimo ano e meio, deve ter ouvido a palavra BERT. </font><font style="vertical-align: inherit;">Aparecendo no final de 2018, nos √∫ltimos tempos, este modelo ganhou tanta popularidade que apenas uma imagem desse tipo ser√° perfeita:</font></font><br>
<br>
<img src="https://habrastorage.org/webt/cu/vm/_i/cuvm_irxzrscw8rctmtyoqywxss.png"><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Os BERTs realmente cativaram tudo o que poderia ser preenchido na PNL. </font><font style="vertical-align: inherit;">Eles come√ßaram a ser usados ‚Äã‚Äãpara classifica√ß√£o, reconhecimento de entidades nomeadas e at√© para tradu√ß√£o autom√°tica. </font><font style="vertical-align: inherit;">Simplificando, voc√™ n√£o pode ignor√°-los e ainda precisa dizer o que √©. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/5j/mo/sq/5jmosqk9vhjts6ai88v8hcrdhci.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A imagem mostra uma compara√ß√£o do her√≥i da ocasi√£o (esquerda) com dois modelos que tamb√©m soaram. </font><font style="vertical-align: inherit;">√Ä direita est√° o antecessor imediato do BERT - o modelo </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ELMo</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<div class="spoiler"><b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Digress√£o l√≠rica.</font></font></b><div class="spoiler_text"><img src="https://habrastorage.org/getpro/habr/post_images/8a1/bb1/e07/8a1bb1e076e3b3b1b2637343e28359d4.jpg" alt="image"><br>
         ¬´ ¬ª:           ,        ,   Elmo,  Bert ‚Äî   ;    ,   ,   , ‚Äî    .         .  ,    ,   .<br>
</div></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O modelo </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Allen AI</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ELMo </font><font style="vertical-align: inherit;">√© um tipo de sucessor de todo o desenvolvimento da regi√£o nos anos anteriores - a saber, uma rede neural recorrente bidirecional, al√©m de v√°rios novos truques para inicializar. </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Os</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> colegas da </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;">OpenAI</font></a><font style="vertical-align: inherit;"> decidiram o que pode ser feito melhor. E para isso, basta aplicar a arquitetura </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Transformer</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> apresentada no ano anterior </font><font style="vertical-align: inherit;">ao </font><font style="vertical-align: inherit;">Google </font><font style="vertical-align: inherit;">a esta tarefa. Acredito que, nos √∫ltimos 2,5 anos, todo mundo j√° conseguiu se familiarizar com essa arquitetura, por isso n√£o vou me aprofundar nisso em detalhes. Para aqueles que desejam receber a comunh√£o, refiro-me √† minha </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">revis√£o a partir do ano 2017</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Eles (funcion√°rios da OpenAI) chamaram seu modelo </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GPT-2</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . E ent√£o, nesse modelo, eles </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">fizeram um bom trabalho</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Mas vamos deix√°-lo em consci√™ncia e retornar √†s nossas ovelhas, ou seja, os modelos. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Um dos truques mais importantes do ELMo foi o pr√©-treinamento em um caso grande e n√£o alocado. Acabou muito bem, e colegas do Google decidiram que podemos fazer ainda melhor. Al√©m de aplicar a arquitetura Transformer (que j√° estava na GPT-2), o BERT, que significa Representa√ß√µes de codificadores bidirecionais dos transformadores, ou seja, representa√ß√µes vetoriais de um codificador bidirecional baseado na arquitetura do Transformer, continha v√°rias coisas mais importantes. Especificamente, o mais importante era a maneira de treinar em um caso grande.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/lb/hw/yw/lbhwywgm70j3shvnrtzrnx6clyy.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A imagem mostra um m√©todo para marcar dados n√£o alocados. Dois m√©todos de layout s√£o mostrados especificamente ao mesmo tempo. Primeiro, uma sequ√™ncia de tokens (palavras) √© usada, por exemplo, uma senten√ßa e nessa sequ√™ncia um token arbitr√°rio ([MASK]) √© mascarado. E o modelo no processo de aprendizado deve adivinhar que tipo de token foi disfar√ßado. A segunda maneira - duas frases s√£o tomadas seq√ºencialmente ou de lugares arbitr√°rios no texto. E o modelo deve adivinhar se essas senten√ßas eram seq√ºenciais ([CLS] e [SEP]). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A ideia desse treinamento foi extremamente eficaz. A resposta dos amigos juramentados do Facebook foi o modelo </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">RoBERTa</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , um artigo sobre esse modelo √© chamado ‚ÄúTreinamento BERT otimizado de forma sustent√°vel‚Äù. Al√©m disso.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
N√£o listarei todas as maneiras de melhorar o treinamento de um modelo de linguagem grande com base na arquitetura do Transfomer devido ao fato de ser simplesmente chato. </font><font style="vertical-align: inherit;">Menciono, talvez, apenas o trabalho dos meus colegas de Hong Kong - </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ERNIE</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Em seu trabalho, os colegas enriquecem o treinamento atrav√©s do uso de gr√°ficos de conhecimento. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Antes de prosseguir, aqui est√£o alguns links √∫teis: um artigo sobre o </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">BERT</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Bem como um </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">conjunto de</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> modelos BERT e ELMo treinados para o idioma russo.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Modelos pequenos</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Mas chega de BERTs. Existem v√°rias tend√™ncias mais importantes. Primeiro de tudo, esta √© uma tend√™ncia para reduzir o tamanho do modelo. O mesmo BERT √© muito exigente em recursos, e muitos come√ßaram a pensar em como manter (ou realmente n√£o perder) a qualidade, reduzir os recursos necess√°rios para o funcionamento dos modelos. Os colegas do Google criaram um pouco de BERT, n√£o estou brincando - </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ALBERT: Um pouco de BERT</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Voc√™ pode ver que o pequeno BERT supera seu irm√£o mais velho na maioria das tarefas, mantendo uma ordem de magnitude menos par√¢metros. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/y5/su/h3/y5suh3uzlmgy16l8stcoahmio4w.png"> <br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Outra abordagem para o mesmo bar foi feita novamente pelos meus colegas de Hong Kong. Eles criaram um pequeno BERT - </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">TinyBERT</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . (Se nesse momento voc√™ pensou que os nomes come√ßaram a ser repetidos, estou inclinado a concordar com voc√™.)</font></font><br>
<br>
<img src="https://habrastorage.org/webt/7_/bb/el/7_bbelco1m2dewd3gj6cjqq2upw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A diferen√ßa fundamental entre os dois modelos acima √© que, se a ALBERT usar truques complicados para reduzir o modelo BERT original, por exemplo, compartilhamento de par√¢metros e redu√ß√£o da dimens√£o das representa√ß√µes vetoriais internas por meio da decomposi√ß√£o da matriz, o TinyBERT usar√° uma abordagem fundamentalmente diferente, ou seja, a destila√ß√£o do conhecimento, ou seja, haver√° um pequeno modelo que aprende a repetir ap√≥s a irm√£ mais velha no processo de aprendizado.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Casos pequenos</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nos √∫ltimos anos (desde cerca de 1990, quando a Internet apareceu), houve um aumento nos edif√≠cios dispon√≠veis. Depois vieram os algoritmos que se tornaram capazes de processar gabinetes t√£o grandes (isso √© o que chamamos de "revolu√ß√£o do aprendizado profundo", este j√° √© o ano desde 2013). E, como resultado, come√ßou a ser percebido normalmente que, para obter boa qualidade em alguma tarefa, s√£o necess√°rias grandes matrizes de dados marcados - corpus de textos no nosso caso. Por exemplo, casos t√≠picos para aprender tarefas de tradu√ß√£o autom√°tica hoje s√£o medidos em milh√µes de pares de frases. H√° muito que √© √≥bvio que, para muitas tarefas, √© imposs√≠vel reunir esses casos em um per√≠odo de tempo razo√°vel e com uma quantia razo√°vel de dinheiro. Durante muito tempo, n√£o ficou muito claro o que fazer sobre isso. Mas no ano passado (quem voc√™ pensaria?), O BERT entrou em cena.Esse modelo foi capaz de pr√©-treinar em grandes volumes de textos n√£o alocados, e o modelo finalizado foi f√°cil de adaptar √† tarefa com um estojo pequeno.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/7_/bb/el/7_bbelco1m2dewd3gj6cjqq2upw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Todas as tarefas listadas nesta tabela possuem corpo de treinamento no tamanho de v√°rios </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">milhares de</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> unidades. </font><font style="vertical-align: inherit;">Ou seja, duas a tr√™s ordens de magnitude a menos. </font><font style="vertical-align: inherit;">E essa √© outra raz√£o pela qual o BERT (e seus descendentes e parentes) se tornaram t√£o populares.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Novas tend√™ncias</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bem, no final, algumas novas tend√™ncias, como eu as vi. Antes de tudo, √© uma mudan√ßa fundamental de atitude em rela√ß√£o ao texto. Se todo o tempo anterior na maioria das tarefas, o texto era percebido apenas como material de entrada, e a sa√≠da era algo √∫til, por exemplo, um r√≥tulo de classe. Agora a comunidade tem a oportunidade de lembrar que o texto √© principalmente um meio de comunica√ß√£o, ou seja, voc√™ pode "conversar" com o modelo - fazer perguntas e receber respostas na forma de um texto leg√≠vel por humanos. √â o que diz o novo artigo do Google </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">T5</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (o nome pode ser traduzido como "cinco vezes transformador").</font></font><br>
<br>
<img src="https://habrastorage.org/webt/ba/vz/mj/bavzmjwryypmza-ywo18njxfbjy.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Outra tend√™ncia importante √© que a regi√£o est√° reaprendendo a trabalhar com textos longos. Desde os anos 70, a comunidade tem maneiras de trabalhar com textos de comprimentos arbitr√°rios - use o mesmo TF-IDF. Mas esses modelos t√™m seu pr√≥prio limite de qualidade. Mas os novos modelos de aprendizado profundo n√£o foram capazes de trabalhar com textos longos (o mesmo BERT tem um limite de 512 tokens do comprimento do texto de entrada). Ultimamente, por√©m, pelo menos duas obras surgiram que de lados diferentes abordam o problema do texto longo. O primeiro trabalho do grupo de Ruslan Salakhutdinov chamado Transformer-XL. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ci/op/gj/ciopgjs1htbc2gmucz7dwkiwqtk.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Neste trabalho, √© revivida a id√©ia que tornou as redes recursivas t√£o populares - voc√™ pode salvar o estado anterior e us√°-lo para criar o pr√≥ximo, mesmo se n√£o reverter o gradiente no tempo (BPTT). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O segundo</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">o trabalho</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> trabalha com os polin√¥mios de Legendre e, com a ajuda deles, permite processar seq√º√™ncias de dezenas de milhares de tokens com redes neurais recorrentes. </font></font><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Sobre isso, gostaria de terminar a revis√£o das mudan√ßas que ocorreram e das tend√™ncias emergentes. </font><font style="vertical-align: inherit;">Vamos ver o que vai acontecer este ano, tenho certeza de que muitas coisas interessantes. </font><font style="vertical-align: inherit;">V√≠deo do meu discurso sobre o mesmo t√≥pico na √Årvore de Dados:</font></font><br>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/cdlAUcaOCDY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
PS Em breve teremos alguns an√∫ncios mais interessantes, n√£o mude!</font></font></div>
      
    </div><p class="reference-to-source js-reference-to-source">Source: https://habr.com/ru/post/undefined/</p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt487706/index.html">Service Discovery em sistemas distribu√≠dos usando o exemplo Consul. Alexander Sigachev</a></li>
<li><a href="../pt487716/index.html">Perfeito SAST. Analisador</a></li>
<li><a href="../pt487720/index.html">Sobre corutinismo competitivo (usando programa√ß√£o reativa como exemplo)</a></li>
<li><a href="../pt487724/index.html">BlazingPizza: aplicativo Blazor do in√≠cio ao fim. Parte 2. Adicione um componente</a></li>
<li><a href="../pt487728/index.html">@Pythonetc compilation, janeiro 2020</a></li>
<li><a href="../pt487734/index.html">Acelerando o Entity Framework Core</a></li>
<li><a href="../pt487738/index.html">Anima√ß√£o de esquema no SCADA</a></li>
<li><a href="../pt487740/index.html">Montagem de um magnet√¥metro port√°til</a></li>
<li><a href="../pt487742/index.html">Negocia√ß√£o de arbitragem (algoritmo de Bellman-Ford)</a></li>
<li><a href="../pt487744/index.html">FARO apresenta o scanner 3D a laser FOCUS S 70</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>