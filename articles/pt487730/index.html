<!doctype html>
<html class="no-js" lang="pt">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👨‍👧 🗒️ 👥 Processamento de linguagem natural. Resultados 2019 e tendências para 2020 🍙 🤮 👨‍🏭</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Olá a todos. Com algum atraso, decidi publicar este artigo. Todo ano eu tento resumir o que aconteceu no campo do processamento de linguagem natural. ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Processamento de linguagem natural. Resultados 2019 e tendências para 2020</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/huawei/blog/487730/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Olá a todos. </font><font style="vertical-align: inherit;">Com algum atraso, decidi publicar este artigo. </font><font style="vertical-align: inherit;">Todo ano eu tento resumir o que aconteceu no campo do processamento de linguagem natural. </font><font style="vertical-align: inherit;">Esse ano não foi exceção.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">BERTs, BERTs estão em toda parte</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Vamos começar em ordem. </font><font style="vertical-align: inherit;">Se você não foi para a taiga siberiana remota ou passou férias em Goa no último ano e meio, deve ter ouvido a palavra BERT. </font><font style="vertical-align: inherit;">Aparecendo no final de 2018, nos últimos tempos, este modelo ganhou tanta popularidade que apenas uma imagem desse tipo será perfeita:</font></font><br>
<br>
<img src="https://habrastorage.org/webt/cu/vm/_i/cuvm_irxzrscw8rctmtyoqywxss.png"><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Os BERTs realmente cativaram tudo o que poderia ser preenchido na PNL. </font><font style="vertical-align: inherit;">Eles começaram a ser usados ​​para classificação, reconhecimento de entidades nomeadas e até para tradução automática. </font><font style="vertical-align: inherit;">Simplificando, você não pode ignorá-los e ainda precisa dizer o que é. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/5j/mo/sq/5jmosqk9vhjts6ai88v8hcrdhci.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A imagem mostra uma comparação do herói da ocasião (esquerda) com dois modelos que também soaram. </font><font style="vertical-align: inherit;">À direita está o antecessor imediato do BERT - o modelo </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ELMo</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<div class="spoiler"><b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Digressão lírica.</font></font></b><div class="spoiler_text"><img src="https://habrastorage.org/getpro/habr/post_images/8a1/bb1/e07/8a1bb1e076e3b3b1b2637343e28359d4.jpg" alt="image"><br>
         « »:           ,        ,   Elmo,  Bert —   ;    ,   ,   , —    .         .  ,    ,   .<br>
</div></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O modelo </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Allen AI</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ELMo </font><font style="vertical-align: inherit;">é um tipo de sucessor de todo o desenvolvimento da região nos anos anteriores - a saber, uma rede neural recorrente bidirecional, além de vários novos truques para inicializar. </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Os</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> colegas da </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;">OpenAI</font></a><font style="vertical-align: inherit;"> decidiram o que pode ser feito melhor. E para isso, basta aplicar a arquitetura </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Transformer</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> apresentada no ano anterior </font><font style="vertical-align: inherit;">ao </font><font style="vertical-align: inherit;">Google </font><font style="vertical-align: inherit;">a esta tarefa. Acredito que, nos últimos 2,5 anos, todo mundo já conseguiu se familiarizar com essa arquitetura, por isso não vou me aprofundar nisso em detalhes. Para aqueles que desejam receber a comunhão, refiro-me à minha </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">revisão a partir do ano 2017</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Eles (funcionários da OpenAI) chamaram seu modelo </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GPT-2</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . E então, nesse modelo, eles </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">fizeram um bom trabalho</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Mas vamos deixá-lo em consciência e retornar às nossas ovelhas, ou seja, os modelos. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Um dos truques mais importantes do ELMo foi o pré-treinamento em um caso grande e não alocado. Acabou muito bem, e colegas do Google decidiram que podemos fazer ainda melhor. Além de aplicar a arquitetura Transformer (que já estava na GPT-2), o BERT, que significa Representações de codificadores bidirecionais dos transformadores, ou seja, representações vetoriais de um codificador bidirecional baseado na arquitetura do Transformer, continha várias coisas mais importantes. Especificamente, o mais importante era a maneira de treinar em um caso grande.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/lb/hw/yw/lbhwywgm70j3shvnrtzrnx6clyy.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A imagem mostra um método para marcar dados não alocados. Dois métodos de layout são mostrados especificamente ao mesmo tempo. Primeiro, uma sequência de tokens (palavras) é usada, por exemplo, uma sentença e nessa sequência um token arbitrário ([MASK]) é mascarado. E o modelo no processo de aprendizado deve adivinhar que tipo de token foi disfarçado. A segunda maneira - duas frases são tomadas seqüencialmente ou de lugares arbitrários no texto. E o modelo deve adivinhar se essas sentenças eram seqüenciais ([CLS] e [SEP]). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A ideia desse treinamento foi extremamente eficaz. A resposta dos amigos juramentados do Facebook foi o modelo </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">RoBERTa</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , um artigo sobre esse modelo é chamado “Treinamento BERT otimizado de forma sustentável”. Além disso.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Não listarei todas as maneiras de melhorar o treinamento de um modelo de linguagem grande com base na arquitetura do Transfomer devido ao fato de ser simplesmente chato. </font><font style="vertical-align: inherit;">Menciono, talvez, apenas o trabalho dos meus colegas de Hong Kong - </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ERNIE</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Em seu trabalho, os colegas enriquecem o treinamento através do uso de gráficos de conhecimento. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Antes de prosseguir, aqui estão alguns links úteis: um artigo sobre o </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">BERT</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Bem como um </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">conjunto de</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> modelos BERT e ELMo treinados para o idioma russo.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Modelos pequenos</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Mas chega de BERTs. Existem várias tendências mais importantes. Primeiro de tudo, esta é uma tendência para reduzir o tamanho do modelo. O mesmo BERT é muito exigente em recursos, e muitos começaram a pensar em como manter (ou realmente não perder) a qualidade, reduzir os recursos necessários para o funcionamento dos modelos. Os colegas do Google criaram um pouco de BERT, não estou brincando - </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ALBERT: Um pouco de BERT</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Você pode ver que o pequeno BERT supera seu irmão mais velho na maioria das tarefas, mantendo uma ordem de magnitude menos parâmetros. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/y5/su/h3/y5suh3uzlmgy16l8stcoahmio4w.png"> <br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Outra abordagem para o mesmo bar foi feita novamente pelos meus colegas de Hong Kong. Eles criaram um pequeno BERT - </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">TinyBERT</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . (Se nesse momento você pensou que os nomes começaram a ser repetidos, estou inclinado a concordar com você.)</font></font><br>
<br>
<img src="https://habrastorage.org/webt/7_/bb/el/7_bbelco1m2dewd3gj6cjqq2upw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A diferença fundamental entre os dois modelos acima é que, se a ALBERT usar truques complicados para reduzir o modelo BERT original, por exemplo, compartilhamento de parâmetros e redução da dimensão das representações vetoriais internas por meio da decomposição da matriz, o TinyBERT usará uma abordagem fundamentalmente diferente, ou seja, a destilação do conhecimento, ou seja, haverá um pequeno modelo que aprende a repetir após a irmã mais velha no processo de aprendizado.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Casos pequenos</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nos últimos anos (desde cerca de 1990, quando a Internet apareceu), houve um aumento nos edifícios disponíveis. Depois vieram os algoritmos que se tornaram capazes de processar gabinetes tão grandes (isso é o que chamamos de "revolução do aprendizado profundo", este já é o ano desde 2013). E, como resultado, começou a ser percebido normalmente que, para obter boa qualidade em alguma tarefa, são necessárias grandes matrizes de dados marcados - corpus de textos no nosso caso. Por exemplo, casos típicos para aprender tarefas de tradução automática hoje são medidos em milhões de pares de frases. Há muito que é óbvio que, para muitas tarefas, é impossível reunir esses casos em um período de tempo razoável e com uma quantia razoável de dinheiro. Durante muito tempo, não ficou muito claro o que fazer sobre isso. Mas no ano passado (quem você pensaria?), O BERT entrou em cena.Esse modelo foi capaz de pré-treinar em grandes volumes de textos não alocados, e o modelo finalizado foi fácil de adaptar à tarefa com um estojo pequeno.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/7_/bb/el/7_bbelco1m2dewd3gj6cjqq2upw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Todas as tarefas listadas nesta tabela possuem corpo de treinamento no tamanho de vários </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">milhares de</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> unidades. </font><font style="vertical-align: inherit;">Ou seja, duas a três ordens de magnitude a menos. </font><font style="vertical-align: inherit;">E essa é outra razão pela qual o BERT (e seus descendentes e parentes) se tornaram tão populares.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Novas tendências</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bem, no final, algumas novas tendências, como eu as vi. Antes de tudo, é uma mudança fundamental de atitude em relação ao texto. Se todo o tempo anterior na maioria das tarefas, o texto era percebido apenas como material de entrada, e a saída era algo útil, por exemplo, um rótulo de classe. Agora a comunidade tem a oportunidade de lembrar que o texto é principalmente um meio de comunicação, ou seja, você pode "conversar" com o modelo - fazer perguntas e receber respostas na forma de um texto legível por humanos. É o que diz o novo artigo do Google </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">T5</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (o nome pode ser traduzido como "cinco vezes transformador").</font></font><br>
<br>
<img src="https://habrastorage.org/webt/ba/vz/mj/bavzmjwryypmza-ywo18njxfbjy.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Outra tendência importante é que a região está reaprendendo a trabalhar com textos longos. Desde os anos 70, a comunidade tem maneiras de trabalhar com textos de comprimentos arbitrários - use o mesmo TF-IDF. Mas esses modelos têm seu próprio limite de qualidade. Mas os novos modelos de aprendizado profundo não foram capazes de trabalhar com textos longos (o mesmo BERT tem um limite de 512 tokens do comprimento do texto de entrada). Ultimamente, porém, pelo menos duas obras surgiram que de lados diferentes abordam o problema do texto longo. O primeiro trabalho do grupo de Ruslan Salakhutdinov chamado Transformer-XL. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ci/op/gj/ciopgjs1htbc2gmucz7dwkiwqtk.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Neste trabalho, é revivida a idéia que tornou as redes recursivas tão populares - você pode salvar o estado anterior e usá-lo para criar o próximo, mesmo se não reverter o gradiente no tempo (BPTT). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O segundo</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">o trabalho</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> trabalha com os polinômios de Legendre e, com a ajuda deles, permite processar seqüências de dezenas de milhares de tokens com redes neurais recorrentes. </font></font><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Sobre isso, gostaria de terminar a revisão das mudanças que ocorreram e das tendências emergentes. </font><font style="vertical-align: inherit;">Vamos ver o que vai acontecer este ano, tenho certeza de que muitas coisas interessantes. </font><font style="vertical-align: inherit;">Vídeo do meu discurso sobre o mesmo tópico na Árvore de Dados:</font></font><br>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/cdlAUcaOCDY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
PS Em breve teremos alguns anúncios mais interessantes, não mude!</font></font></div>
      
    </div><p class="reference-to-source js-reference-to-source">Source: https://habr.com/ru/post/undefined/</p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt487706/index.html">Service Discovery em sistemas distribuídos usando o exemplo Consul. Alexander Sigachev</a></li>
<li><a href="../pt487716/index.html">Perfeito SAST. Analisador</a></li>
<li><a href="../pt487720/index.html">Sobre corutinismo competitivo (usando programação reativa como exemplo)</a></li>
<li><a href="../pt487724/index.html">BlazingPizza: aplicativo Blazor do início ao fim. Parte 2. Adicione um componente</a></li>
<li><a href="../pt487728/index.html">@Pythonetc compilation, janeiro 2020</a></li>
<li><a href="../pt487734/index.html">Acelerando o Entity Framework Core</a></li>
<li><a href="../pt487738/index.html">Animação de esquema no SCADA</a></li>
<li><a href="../pt487740/index.html">Montagem de um magnetômetro portátil</a></li>
<li><a href="../pt487742/index.html">Negociação de arbitragem (algoritmo de Bellman-Ford)</a></li>
<li><a href="../pt487744/index.html">FARO apresenta o scanner 3D a laser FOCUS S 70</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>