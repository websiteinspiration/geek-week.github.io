<!doctype html>
<html class="no-js" lang="pt">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë© ü§±üèª üêÅ No√ß√µes b√°sicas do ZFS: armazenamento e desempenho ‚ùáÔ∏è ‚≠êÔ∏è ‚õ™Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Nesta primavera, j√° discutimos alguns t√≥picos introdut√≥rios, como verificar a velocidade de suas unidades e o que √© RAID . No segundo deles, prometemo...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>No√ß√µes b√°sicas do ZFS: armazenamento e desempenho</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/504692/"><img src="https://habrastorage.org/getpro/habr/post_images/abf/883/e96/abf883e96b01dbc78420e0dc1a158460.jpg"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nesta primavera, j√° discutimos alguns t√≥picos introdut√≥rios, </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">como verificar a velocidade de suas unidades</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> e o </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">que √© RAID</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">No segundo deles, prometemos continuar estudando o desempenho de v√°rias topologias de v√°rios discos no ZFS. </font><font style="vertical-align: inherit;">Este √© o sistema de arquivos da pr√≥xima gera√ß√£o que est√° sendo implementado em qualquer lugar: da </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Apple</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ao </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ubuntu</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bem, hoje √© o melhor dia para conhecer o ZFS, leitores curiosos. </font><font style="vertical-align: inherit;">Lembre-se de que, de acordo com uma avalia√ß√£o conservadora do desenvolvedor do OpenZFS, Matt Arens, "√© realmente complicado". </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Mas antes de chegarmos aos n√∫meros - e eles ser√£o, prometo - para todas as variantes de configura√ß√£o do ZFS vosmidiskovoy, √© necess√°rio falar sobre </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">como</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> fazer o ZFS armazena dados no disco.</font></font><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zpool, vdev e dispositivo</font></font></h1><br>
<img src="https://habrastorage.org/getpro/habr/post_images/674/1c6/ab3/6741c6ab310f4e0edf2adf7e2ca4c6bb.png"><br>
<i><font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Esse diagrama completo do pool inclui tr√™s vdevs auxiliares, um para cada classe e quatro para o RAIDz2.Normalmente, </font></font></font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/b9f/82c/887/b9f82c88748c44d1f86cc412a053bf94.png"><br>
<i><font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">n√£o h√° raz√£o para criar um pool de tipos e tamanhos inadequados de </font></font></font></i><font style="vertical-align: inherit;"><i><font color="gray"><font style="vertical-align: inherit;">vdev </font></font></i><i><font color="gray"><font style="vertical-align: inherit;">- mas, se voc√™ quiser, nada o impede de fazer isso.</font></font></i></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Para realmente entender o sistema de arquivos ZFS , voc√™ precisa examinar atentamente sua estrutura real. </font><font style="vertical-align: inherit;">Primeiro, o ZFS combina os n√≠veis tradicionais de gerenciamento de volume e o sistema de arquivos. </font><font style="vertical-align: inherit;">Em segundo lugar, ele usa um mecanismo de c√≥pia transacional ao escrever. </font><font style="vertical-align: inherit;">Esses recursos significam que o sistema √© estruturalmente muito diferente dos sistemas de arquivos comuns e matrizes RAID. </font><font style="vertical-align: inherit;">O primeiro conjunto de componentes b√°sicos a serem entendidos: um pool de armazenamento (zpool), um dispositivo virtual (vdev) e um dispositivo real (dispositivo).</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">zpool</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O pool de armazenamento zpool √© a estrutura ZFS superior. Cada pool cont√©m um ou mais dispositivos virtuais. Por sua vez, cada um deles cont√©m um ou mais dispositivos reais (dispositivo). Pools virtuais s√£o blocos aut√¥nomos. Um computador f√≠sico pode conter dois ou mais conjuntos separados, mas cada um √© completamente independente dos outros. Os pools n√£o podem compartilhar dispositivos virtuais. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A redund√¢ncia do ZFS est√° no n√≠vel de dispositivos virtuais, mas n√£o no n√≠vel de conjuntos. No n√≠vel do pool, n√£o h√° absolutamente redund√¢ncia - se alguma unidade vdev ou vdev especial for perdida, o pool inteiro ser√° perdido junto com ele.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Os pools de armazenamento modernos podem sobreviver √† perda de um cache ou log de dispositivo virtual - embora possam perder uma pequena quantidade de dados sujos se perderem o log do vdev durante uma queda de energia ou falha no sistema. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Existe um equ√≠voco comum de que ‚Äúfaixas de dados‚Äù (tiras) do ZFS s√£o registradas em todo o pool. Isso n√£o √© verdade. O Zpool n√£o √© um RAID0 divertido, √© um </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">JBOD</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> divertido </font><font style="vertical-align: inherit;">com um mecanismo de distribui√ß√£o vari√°vel complexo.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Na maioria das vezes, as entradas s√£o distribu√≠das entre os dispositivos virtuais dispon√≠veis de acordo com o espa√ßo dispon√≠vel; portanto, teoricamente, todos ser√£o preenchidos ao mesmo tempo. Nas vers√µes posteriores do ZFS, o uso atual (utiliza√ß√£o) do vdev √© levado em considera√ß√£o - se um dispositivo virtual for significativamente mais carregado que o outro (por exemplo, devido √† carga de leitura), ele ser√° temporariamente ignorado para grava√ß√£o, apesar da presen√ßa do maior coeficiente de espa√ßo livre. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Um mecanismo de detec√ß√£o de reciclagem incorporado aos m√©todos modernos de distribui√ß√£o de registros ZFS pode reduzir a lat√™ncia e aumentar o rendimento durante per√≠odos de carga extraordinariamente alta - mas isso n√£o √© uma </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">carta branca</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">involuntariamente misturando HDDs lentos e SSDs r√°pidos em um pool. </font><font style="vertical-align: inherit;">Um pool t√£o desigual ainda funcionar√° na velocidade do dispositivo mais lento, ou seja, como se fosse inteiramente composto por esses dispositivos.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">vdev</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Cada conjunto de armazenamento consiste em um ou mais dispositivos virtuais (dispositivo virtual, vdev). </font><font style="vertical-align: inherit;">Por sua vez, cada vdev inclui um ou mais dispositivos reais. </font><font style="vertical-align: inherit;">A maioria dos dispositivos virtuais √© usada para armazenar dados com facilidade, mas existem v√°rias classes auxiliares vdev, incluindo CACHE, LOG e SPECIAL. </font><font style="vertical-align: inherit;">Cada um desses tipos de vdev pode ter uma das cinco topologias: dispositivo √∫nico, RAIDz1, RAIDz2, RAIDz3 ou espelho.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
RAIDz1, RAIDz2 e RAIDz3 s√£o varia√ß√µes especiais do que os antigos chamam de paridade dupla (diagonal) RAID. 1, 2 e 3 se referem a quantos blocos de paridade est√£o alocados para cada banda de dados. Em vez de discos separados para paridade, os dispositivos RAIDz virtuais distribuem uniformemente essa paridade entre discos. Uma matriz RAIDz pode perder quantos discos tiver blocos de paridade; se ele perder outro, ele falhar√° e levar√° o pool de armazenamento com ele.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nos dispositivos virtuais espelhados (mirror vdev), cada bloco √© armazenado em cada dispositivo no vdev. Embora os espelhos de duas larguras mais comuns, possa haver qualquer n√∫mero arbitr√°rio de dispositivos no espelho - em grandes instala√ß√µes, os triplos s√£o frequentemente usados ‚Äã‚Äãpara aumentar o desempenho de leitura e a toler√¢ncia a falhas. O espelho vdev pode sobreviver a qualquer falha enquanto pelo menos um dispositivo no vdev continua funcionando. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Os vdevs √∫nicos s√£o inerentemente perigosos. Esse dispositivo virtual n√£o sobreviver√° a uma √∫nica falha - e se for usado como armazenamento ou um vdev especial, sua falha levar√° √† destrui√ß√£o de todo o pool. Tenha muito, muito cuidado aqui.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Os dispositivos virtuais CACHE, LOG e SPECIAL podem ser criados usando qualquer uma das topologias acima - mas lembre-se de que perder um dispositivo virtual SPECIAL significa perder um pool; portanto, √© altamente recomend√°vel uma topologia excessiva.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">dispositivo</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Este √© provavelmente o termo mais f√°cil de entender no ZFS - √© literalmente um dispositivo de acesso aleat√≥rio em bloco. Lembre-se de que os dispositivos virtuais s√£o compostos de dispositivos individuais e o pool √© composto de dispositivos virtuais. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Discos - magn√©ticos ou de estado s√≥lido - s√£o os dispositivos de bloco mais comuns usados ‚Äã‚Äãcomo blocos de constru√ß√£o do vdev. No entanto, qualquer dispositivo com um identificador em / dev √© adequado - para que voc√™ possa usar matrizes RAID de hardware inteiras como dispositivos separados. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Um arquivo simples √© um dos dispositivos de bloco alternativos mais importantes dos quais o vdev pode ser constru√≠do. Conjuntos de teste de </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">arquivos esparsos</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">&nbsp;- Uma maneira muito conveniente de verificar os comandos do pool e ver quanto espa√ßo est√° dispon√≠vel no pool ou no dispositivo virtual dessa topologia. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/5cf/aa5/62c/5cfaa562cb208b654af113f7535b8f57.png"><br>
<i><font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Voc√™ pode criar um pool de teste a partir de arquivos esparsos em apenas alguns segundos - mas n√£o se esque√ßa de excluir o pool inteiro e seus componentes posteriormente.</font></font></font></i> <br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Suponha que voc√™ queira colocar um servidor em oito discos e planeje usar discos de 10 TB (~ 9300 GiB) - mas n√£o tem certeza de qual A topologia atende melhor √†s suas necessidades. No exemplo acima, em quest√£o de segundos, criamos um pool de testes a partir de arquivos esparsos - e agora sabemos que o RAIDz2 vdev de oito unidades de 10 TB fornece 50 TiB de capacidade √∫til.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Outra classe especial de dispositivos √© SOBRESSALENTE (sobressalente). Os dispositivos com troca a quente, diferentemente dos dispositivos convencionais, pertencem a todo o pool, n√£o apenas a um dispositivo virtual. Se algum vdev no pool falhar e o dispositivo sobressalente estiver conectado e dispon√≠vel, ele entrar√° automaticamente no vdev afetado. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ap√≥s conectar-se ao vdev afetado, o dispositivo sobressalente come√ßa a receber c√≥pias ou reconstru√ß√£o de dados que devem estar no dispositivo ausente. No RAID tradicional, isso √© chamado de reconstru√ß√£o, enquanto no ZFS √© chamado de "resilvering".</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
√â importante observar que os dispositivos de substitui√ß√£o n√£o substituem permanentemente os dispositivos com falha. </font><font style="vertical-align: inherit;">Esta √© apenas uma substitui√ß√£o tempor√°ria para reduzir o tempo durante o qual a degrada√ß√£o do vdev √© observada. </font><font style="vertical-align: inherit;">Depois que o administrador substituiu o dispositivo vdev com falha, a redund√¢ncia √© restaurada para esse dispositivo permanente e o SPARE se desconecta do vdev e volta a funcionar como um sobressalente para todo o pool.</font></font><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Conjuntos de dados, blocos e setores</font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O pr√≥ximo conjunto de blocos de constru√ß√£o que voc√™ precisa entender em nossa jornada pelo ZFS n√£o √© tanto hardware, mas como os dados s√£o organizados e armazenados. </font><font style="vertical-align: inherit;">Ignoramos v√°rios n√≠veis aqui - como o metaslab - para n√£o acumular os detalhes, mantendo um entendimento da estrutura geral.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Conjunto de dados</font></font></h3><br>
<img src="https://habrastorage.org/getpro/habr/post_images/bd3/c48/d9d/bd3c48d9dff6e0f493a5d90d1dca6d1d.png"><br>
<i><font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Quando criamos um conjunto de dados, ele mostra todo o espa√ßo dispon√≠vel no pool. Em seguida, definimos a cota - e alteramos o ponto de montagem. Magia! </font></font></font></i> <br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/a18/3de/210/a183de210cdc57cd1421652201cbf2c3.png"><br>
<i><font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O Zvol √© na maioria das vezes apenas um conjunto de dados, desprovido de sua camada de sistema de arquivos, que substitu√≠mos aqui por um</font></font></font></i> <br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
sistema de arquivos</font><i><font color="gray"><font style="vertical-align: inherit;"> ext4 completamente normal.O</font></font></i><font style="vertical-align: inherit;"> conjunto de dados ZFS √© aproximadamente o mesmo que um sistema de arquivos montado padr√£o. Como um sistema de arquivos comum, √† primeira vista, parece ser "apenas outra pasta". Mas tamb√©m, como sistemas de arquivos montados convencionais, cada conjunto de dados do ZFS possui seu pr√≥prio conjunto de propriedades b√°sicas.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Primeiro de tudo, um conjunto de dados pode ter uma cota atribu√≠da. Se instalado</font></font><code>zfs set quota=100G poolname/datasetname</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, voc√™ n√£o poder√° gravar na pasta montada</font></font><code>/poolname/datasetname</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">mais de 100 GiB.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Percebe a presen√ßa - e a aus√™ncia - de barras no in√≠cio de cada linha? Cada conjunto de dados tem seu pr√≥prio lugar na hierarquia do ZFS e na hierarquia de montagem do sistema. N√£o h√° barra invertida na hierarquia do ZFS - voc√™ come√ßa com o nome do pool e depois o caminho de um conjunto de dados para o pr√≥ximo. Por exemplo, </font></font><code>pool/parent/child</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">para um conjunto de dados nomeado </font></font><code>child</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">no conjunto de dados pai </font></font><code>parent</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">em um pool com um nome de criativo </font></font><code>pool</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Por padr√£o, o ponto de montagem do conjunto de dados ser√° equivalente ao seu nome na hierarquia ZFS, com uma barra no in√≠cio - a piscina com o nome √© </font></font><code>pool</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">montado como </font></font><code>/pool</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, o conjunto de dados √© </font></font><code>parent</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">montado em </font></font><code>/pool/parent</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, ea crian√ßa </font><font style="vertical-align: inherit;">conjunto de dados </font><font style="vertical-align: inherit;">√© montado </font></font><code>child</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">em </font></font><code>/pool/parent/child</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. No entanto, o ponto de montagem do sistema para o conjunto de dados pode ser alterado. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Se indicarmos</font></font><code>zfs set mountpoint=/lol pool/parent/child</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, o conjunto de dados √© </font></font><code>pool/parent/child</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">montado no sistema como </font></font><code>/lol</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Al√©m dos conjuntos de dados, devemos mencionar volumes (zvols). </font><font style="vertical-align: inherit;">Um volume √© aproximadamente semelhante a um conjunto de dados, exceto pelo fato de n√£o possuir um sistema de arquivos - √© apenas um dispositivo de bloco. </font><font style="vertical-align: inherit;">Voc√™ pode, por exemplo, criar </font></font><code>zvol</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">com um nome </font></font><code>mypool/myzvol</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, format√°-lo com o sistema de arquivos ext4 e montar esse sistema de arquivos - agora voc√™ possui o sistema de arquivos ext4, mas com suporte para todos os recursos de seguran√ßa do ZFS! </font><font style="vertical-align: inherit;">Isso pode parecer bobagem em um computador, mas faz muito mais sentido como back-end ao exportar um dispositivo iSCSI.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Blocos</font></font></h3><br>
<img src="https://habrastorage.org/getpro/habr/post_images/74b/4dd/d00/74b4ddd009e67db1b1b6c4467bcf6fa3.png"><br>
<i><font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Um arquivo √© representado por um ou mais blocos. Cada bloco √© armazenado em um dispositivo virtual. O tamanho do bloco geralmente √© igual ao par√¢metro </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">recordsize</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , mas pode ser reduzido para </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2 ^ ashift</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> se contiver metadados ou um arquivo pequeno. </font></font></font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/8d1/7fd/ad2/8d17fdad2eda641c801e5e6a302f6e38.png"><br>
<i><font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Realmente, </font><font style="vertical-align: inherit;">n√£o </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">estamos</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> brincando sobre o enorme dano no desempenho se voc√™ instalar um desvio muito pequeno</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
No pool do ZFS, todos os dados, incluindo metadados, s√£o armazenados em blocos. O tamanho m√°ximo do bloco para cada conjunto de dados √© definido na propriedade</font></font><code>recordsize</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(tamanho do registro). O tamanho do registro pode variar, mas isso n√£o altera o tamanho ou o local de nenhum bloco que j√° tenha sido gravado no conjunto de dados - ele √© v√°lido apenas para novos blocos √† medida que s√£o gravados.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Salvo indica√ß√£o em contr√°rio, o tamanho atual da grava√ß√£o √© 128 KiB por padr√£o. Esse √© um tipo de compromisso dif√≠cil em que o desempenho n√£o ser√° ideal, mas n√£o √© terr√≠vel na maioria dos casos. </font></font><code>Recordsize</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">pode ser definido com qualquer valor de 4K a 1M (com configura√ß√µes adicionais, </font></font><code>recordsize</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">voc√™ pode definir ainda mais, mas isso raramente √© uma boa ideia). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Qualquer bloco refere-se aos dados de apenas um arquivo - voc√™ n√£o pode espremer dois arquivos diferentes em um bloco. Cada arquivo consiste em um ou mais blocos, dependendo do tamanho. Se o tamanho do arquivo for menor que o tamanho do registro, ele ser√° salvo em um bloco menor - por exemplo, um bloco com um arquivo de 2 KiB ocupar√° apenas um setor de 4 KiB no disco. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Se o arquivo for grande o suficiente e exigir v√°rios blocos, todos os registros com esse arquivo ter√£o um tamanho</font></font><code>recordsize</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">&nbsp;- incluindo o √∫ltimo registro, cuja parte principal pode se tornar um </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">espa√ßo n√£o utilizado</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Os volumes Zvol n√£o t√™m uma propriedade </font></font><code>recordsize</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">&nbsp;- eles t√™m uma propriedade equivalente </font></font><code>volblocksize</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Setores</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O √∫ltimo bloco de constru√ß√£o mais b√°sico √© o setor. Essa √© a menor unidade f√≠sica que pode ser gravada ou lida na unidade base. Por v√°rias d√©cadas, a maioria dos discos usou setores de 512 bytes. Recentemente, a maioria das unidades est√° configurada para 4 setores KiB e, em alguns setores - especialmente SSDs - 8 KiB ou mais. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O ZFS possui uma propriedade que permite definir manualmente o tamanho do setor. Esta √© uma propriedade </font></font><code>ashift</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. √â um pouco confuso que o desvio de dinheiro seja uma pot√™ncia de dois. Por exemplo, </font></font><code>ashift=9</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">significa um tamanho de setor de 2 ^ 9 ou 512 bytes.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O ZFS solicita ao sistema operacional informa√ß√µes detalhadas sobre cada dispositivo de bloco quando ele √© adicionado ao novo vdev e, teoricamente, define automaticamente o desvio corretamente com base nessas informa√ß√µes. Infelizmente, muitos discos mentem sobre o tamanho do setor para manter a compatibilidade com o Windows XP (que n√£o conseguia entender discos com outros tamanhos de setor). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Isso significa que √© recomend√°vel que o administrador do ZFS conhe√ßa o tamanho real do setor de seus dispositivos e instale manualmente</font></font><code>ashift</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Se um ashift muito pequeno for definido, o n√∫mero de opera√ß√µes de leitura / grava√ß√£o aumentar√° astronomicamente. Portanto, escrever "setores" de 512 bytes no setor real de 4 KiB significa escrever o primeiro "setor", depois ler o setor de 4 KiB, alter√°-lo com o segundo "setor" de 512 bytes, grav√°-lo no novo setor de 4 KiB e assim por diante para cada entrada. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
No mundo real, essa penalidade supera os </font></font><code>ashift=13</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SSDs </font><font style="vertical-align: inherit;">Samsung EVO, pelos quais deve agir </font><font style="vertical-align: inherit;">, mas esses SSDs se referem ao tamanho do setor e, portanto, s√£o definidos por padr√£o </font></font><code>ashift=9</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Se um administrador de sistema experiente n√£o alterar essa configura√ß√£o, esse SSD ser√° </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">mais lento que um</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> HDD magn√©tico comum. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para compara√ß√£o, para um tamanho muito grande</font></font><code>ashift</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">praticamente n√£o h√° penalidade. </font><font style="vertical-align: inherit;">N√£o h√° redu√ß√£o real da produtividade e o aumento do espa√ßo n√£o utilizado √© infinitamente pequeno (ou igual a zero com a compacta√ß√£o ativada). </font><font style="vertical-align: inherit;">Portanto, √© altamente recomend√°vel que mesmo as unidades que realmente usam setores de 512 bytes sejam instaladas </font></font><code>ashift=12</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ou </font></font><code>ashift=13</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">que olhem com confian√ßa para o futuro. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A propriedade √© </font></font><code>ashift</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">configurada para cada dispositivo virtual vdev, e </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">n√£o para o pool</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , como muitos pensam erroneamente - e n√£o muda ap√≥s a instala√ß√£o. </font><font style="vertical-align: inherit;">Se voc√™ acidentalmente derrubou </font></font><code>ashift</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ao adicionar um novo vdev √† piscina, contaminou-a irrevogavelmente com um dispositivo de baixo desempenho e, como regra, n√£o h√° outra maneira sen√£o destruir a piscina e come√ßar tudo de novo. </font><font style="vertical-align: inherit;">Mesmo remover o vdev n√£o salvar√° voc√™ de uma instala√ß√£o corrompida</font></font><code>ashift</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">!</font></font><br>
<br>
<h3>   </h3><br>
<img src="https://habrastorage.org/getpro/habr/post_images/38b/a1e/4a8/38ba1e4a8fa0e255081ed8db259a302f.gif"><br>
<i><font color="gray">      &nbsp;‚Äî     ,   </font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/90d/4cb/a35/90d4cba35ffa5a3e44a7ca5f61d4491b.gif"><br>
<i><font color="gray">         ,     </font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/c8b/2af/ffb/c8b2afffbc46f63f6a7fe1167edf5dcb.gif"><br>
<i><font color="gray">  ,      ,   ¬´ ¬ª   ¬´ ¬ª,        </font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/4c1/e2b/818/4c1e2b818077cb07d651527e214363fe.gif"><br>
<i><font color="gray">     ,       ‚Äî      ,     ,       </font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O Copy on Write (CoW) √© a base fundamental do que torna o ZFS t√£o impressionante. O conceito b√°sico √© simples - se voc√™ pedir ao sistema de arquivos tradicional para modificar o arquivo, ele far√° exatamente o que voc√™ solicitou. Se voc√™ pedir ao sistema de arquivos com c√≥pia durante a grava√ß√£o que fa√ßa o mesmo, ser√° exibido "bom" - mas ser√° uma mentira para voc√™. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Em vez disso, o sistema de arquivos de c√≥pia / grava√ß√£o grava a nova vers√£o do bloco modificado e atualiza os metadados do arquivo para quebrar o v√≠nculo com o bloco antigo e associar o novo bloco que voc√™ acabou de escrever.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Desconectar a unidade antiga e vincular a nova √© feita em uma opera√ß√£o, para que n√£o possa ser interrompida - se voc√™ reiniciar a energia ap√≥s isso acontecer, voc√™ ter√° uma nova vers√£o do arquivo e, se reiniciar a energia mais cedo, ter√° a vers√£o antiga. </font><font style="vertical-align: inherit;">De qualquer forma, n√£o haver√° conflito no sistema de arquivos. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A c√≥pia ao gravar no ZFS ocorre n√£o apenas no n√≠vel do sistema de arquivos, mas tamb√©m no n√≠vel de gerenciamento de disco. </font><font style="vertical-align: inherit;">Isso significa que o ZFS n√£o est√° sujeito a um espa√ßo no registro (um </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">buraco no RAID</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ) - um fen√¥meno em que a faixa conseguiu gravar parcialmente antes de o sistema travar, com a matriz danificada ap√≥s uma reinicializa√ß√£o. </font><font style="vertical-align: inherit;">Aqui a tira √© at√¥mica, vdev √© sempre consistente e </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bob √© seu tio</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ZIL: log de inten√ß√£o do ZFS</font></font></h3><br>
<img src="https://habrastorage.org/getpro/habr/post_images/567/71c/73f/56771c73f9a28ebaed161e02313deadb.png"><br>
<i><font color="gray"> ZFS     &nbsp;‚Äî  ,      ZIL,            </font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/cec/7c5/437/cec7c5437087f6816f9cdea5f6829820.png"><br>
<i><font color="gray"> ,   ZIL,    .      </font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/075/65a/d6b/07565ad6b2f431db3e6bc20cd24a653b.png"><br>
<i><font color="gray">SLOG,   LOG-, ‚Äî   &nbsp;‚Äî , ,  &nbsp;‚Äî&nbsp;vdev,  ZIL      </font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/927/0f7/539/9270f7539b759aa37896d41e04c4ec47.png"><br>
<i><font color="gray">      ZIL &nbsp;‚Äî    ZIL   SLOG,      </font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Existem duas categorias principais de opera√ß√µes de grava√ß√£o - s√≠ncrona (sincroniza√ß√£o) e ass√≠ncrona (ass√≠ncrona). Para a maioria das cargas de trabalho, a grande maioria das opera√ß√µes de grava√ß√£o √© ass√≠ncrona - o sistema de arquivos permite agreg√°-las e entreg√°-las em lotes, reduzindo a fragmenta√ß√£o e aumentando significativamente a taxa de transfer√™ncia. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Grava√ß√µes s√≠ncronas s√£o uma quest√£o completamente diferente. Quando um aplicativo solicita uma grava√ß√£o s√≠ncrona, ele diz ao sistema de arquivos: "Voc√™ precisa confirmar isso na mem√≥ria n√£o vol√°til </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">no momento</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> e at√© ent√£o n√£o posso fazer mais nada". Portanto, grava√ß√µes s√≠ncronas devem ser imediatamente confirmadas no disco - e se isso aumenta a fragmenta√ß√£o ou reduz a largura de banda, o mesmo acontece.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O ZFS processa registros s√≠ncronos de maneira diferente dos sistemas de arquivos comuns - em vez de envi√°-los imediatamente para o armazenamento regular, o ZFS os grava em uma √°rea de armazenamento especial chamada log de inten√ß√£o do ZFS - ZFS Intent Log ou ZIL. O truque √© que esses registros </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">tamb√©m</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> permanecem na mem√≥ria, sendo agregados juntamente com solicita√ß√µes regulares de grava√ß√£o ass√≠ncrona, para posteriormente serem despejados no armazenamento como TXGs perfeitamente normais (grupos de transa√ß√µes, grupos de transa√ß√µes). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Em opera√ß√£o normal, o ZIL √© gravado e nunca mais √© lido. Quando, ap√≥s alguns instantes, as grava√ß√µes do ZIL s√£o fixadas no armazenamento principal no TXG comum da RAM, elas s√£o desconectadas do ZIL. A √∫nica coisa quando algo √© lido do ZIL √© ao importar o pool.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Se o ZFS travar - o sistema operacional travar ou a falta de energia - quando houver dados no ZIL, esses dados ser√£o lidos durante a pr√≥xima importa√ß√£o de pool (por exemplo, quando o sistema de emerg√™ncia reiniciar). Tudo o que est√° no ZIL ser√° lido, combinado em grupos TXG, confirmado no armazenamento principal e, em seguida, desconectado do ZIL durante o processo de importa√ß√£o. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Uma das classes auxiliares do vdev √© chamada LOG ou SLOG, o dispositivo LOG secund√°rio. Ele tem uma tarefa - fornecer ao pool um dispositivo vdev separado e, de prefer√™ncia, muito mais r√°pido, com resist√™ncia de grava√ß√£o muito alta para armazenar o ZIL, em vez de armazenar o ZIL no armazenamento principal do vdev. O pr√≥prio ZIL se comporta da mesma maneira, independentemente do local de armazenamento, mas se o vdev com LOG tiver um desempenho de grava√ß√£o muito alto, as grava√ß√µes s√≠ncronas ser√£o mais r√°pidas.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Adicionar vdev com LOG ao pool </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">n√£o pode</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> melhorar o desempenho de grava√ß√£o ass√≠ncrona - mesmo se voc√™ for√ßar todas as grava√ß√µes no ZIL usando </font></font><code>zfs set sync=always</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, elas ainda estar√£o vinculadas ao reposit√≥rio principal no TXG da mesma maneira e no mesmo ritmo que sem um log. </font><font style="vertical-align: inherit;">A √∫nica melhoria direta no desempenho √© o atraso na grava√ß√£o s√≠ncrona (uma vez que uma velocidade mais alta do log acelera as opera√ß√µes </font></font><code>sync</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
No entanto, em um ambiente que j√° exige um grande n√∫mero de grava√ß√µes s√≠ncronas, o vdev LOG pode indiretamente acelerar grava√ß√µes ass√≠ncronas e leituras n√£o armazenadas em cache. </font><font style="vertical-align: inherit;">Carregar registros ZIL em um vdev LOG separado significa menos concorr√™ncia por IOPS no armazenamento prim√°rio, o que, em certa medida, melhora o desempenho de todas as opera√ß√µes de leitura e grava√ß√£o.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Instant√¢neos</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O mecanismo de c√≥pia de grava√ß√£o tamb√©m √© uma base essencial para snapshots at√¥micos do ZFS e replica√ß√£o ass√≠ncrona incremental. </font><font style="vertical-align: inherit;">O sistema de arquivos ativo possui uma √°rvore de ponteiros que marca todos os registros com dados atuais - quando voc√™ tira um instant√¢neo, basta fazer uma c√≥pia dessa √°rvore de ponteiros. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Quando um registro √© substitu√≠do no sistema de arquivos ativo, o ZFS primeiro grava a nova vers√£o do bloco no espa√ßo n√£o utilizado. </font><font style="vertical-align: inherit;">Em seguida, desanexa a vers√£o antiga do bloco do sistema de arquivos atual. </font><font style="vertical-align: inherit;">Mas se algum instant√¢neo se referir ao bloco antigo, ele permanecer√° inalterado. </font><font style="vertical-align: inherit;">O bloco antigo n√£o ser√° realmente restaurado como espa√ßo livre at√© que todas as capturas instant√¢neas vinculadas a esse bloco sejam destru√≠das!</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Replica√ß√£o</font></font></h3><br>
<img src="https://habrastorage.org/getpro/habr/post_images/e69/167/01d/e6916701d4aa3ff27bb42efc43be60da.png"><br>
<i><font color="gray">  Steam  2015   158&nbsp;   126&nbsp;927 .        rsync&nbsp;‚Äî  ZFS    ¬´ ¬ª  750% .</font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/25f/376/0ab/25f3760ab64d6647571b9c02804b39f0.png"><br>
<i><font color="gray">      40-     Windows 7&nbsp;‚Äî   .  ZFS   289  ,  rsync&nbsp;‚Äî  ¬´¬ª  161  ,    ,   rsync   --inplace.</font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/776/46b/a3a/77646ba3ac20eeb0933d7dc7d644296c.png"><br>
<i><font color="gray">    ,  rsync    .  1,9         &nbsp;‚Äî    ,   ZFS   1148  ,  rsync,    rsync --inplace</font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Depois de entender como os instant√¢neos funcionam, √© f√°cil entender a ess√™ncia da replica√ß√£o. Como um instant√¢neo √© apenas uma √°rvore de ponteiros para registros, segue-se que, se fizermos um </font></font><code>zfs send</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">instant√¢neo, enviaremos essa √°rvore e todos os registros associados a ela. Quando passamos este </font></font><code>zfs send</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">em </font></font><code>zfs receive</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">que o objeto alvo, ele escreve ambos o conte√∫do real do bloco e da √°rvore de ponteiros que fazem refer√™ncia os blocos para o conjunto de dados de destino. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Tudo se torna ainda mais interessante no segundo </font></font><code>zfs send</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Agora, temos dois sistemas, cada um dos quais cont√©m </font></font><code>poolname/datasetname@1</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, e voc√™ tira um novo instant√¢neo </font></font><code>poolname/datasetname@2</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Portanto, no pool de origem voc√™ possui </font></font><code>datasetname@1</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">e </font></font><code>datasetname@2</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, e no pool de destino at√© agora, apenas o primeiro instant√¢neo </font></font><code>datasetname@1</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Como temos um instant√¢neo comum entre a origem e o destino</font></font><code>datasetname@1</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, podemos fazer </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">incremental</font></font></i> <code>zfs send</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> em cima disso. Quando dizemos ao sistema </font></font><code>zfs send -i poolname/datasetname@1 poolname/datasetname@2</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, ele compara duas √°rvores indicadoras. Qualquer ponteiro que exista apenas em </font></font><code>@2</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, obviamente, se refere a novos blocos - portanto, precisamos do conte√∫do desses blocos. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Em um sistema remoto, o processamento incremental √© </font></font><code>send</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">igualmente simples. Primeiro, registramos todas as novas entradas inclu√≠das no fluxo </font></font><code>send</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">e adicionamos ponteiros a esses blocos. Voila, em nosso </font></font><code>@2</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">novo sistema! </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A replica√ß√£o incremental ass√≠ncrona do ZFS √© uma grande melhoria em rela√ß√£o aos m√©todos anteriores n√£o instant√¢neos, como o rsync. Nos dois casos, apenas os dados alterados s√£o transmitidos - mas o rsync deve primeiro </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ler</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">do disco todos os dados de ambos os lados para verificar a quantidade e compar√°-la. </font><font style="vertical-align: inherit;">Por outro lado, a replica√ß√£o do ZFS l√™ apenas √°rvores de ponteiro - e todos os blocos que n√£o s√£o representados no instant√¢neo geral.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Compacta√ß√£o em linha</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O mecanismo de c√≥pia na grava√ß√£o tamb√©m simplifica o sistema de compacta√ß√£o embutido. Em um sistema de arquivos tradicional, a compacta√ß√£o √© problem√°tica - a vers√£o antiga e a nova vers√£o dos dados alterados est√£o no mesmo espa√ßo. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Se voc√™ considerar um dado no meio de um arquivo que inicia sua vida √∫til como um megabyte de zeros de 0x00000000 e assim por diante - √© muito f√°cil compact√°-lo para um setor do disco. Mas o que acontece se substituirmos esse megabyte de zeros por um megabyte de dados incompress√≠veis, como JPEG ou ru√≠do pseudo-aleat√≥rio? De repente, esse megabyte de dados exigir√° n√£o um, mas 256 setores de 4 KiB e, nesse local do disco, apenas um setor ser√° reservado.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O ZFS n√£o tem esse problema, pois os registros alterados s√£o sempre gravados no espa√ßo n√£o utilizado - o bloco original ocupa apenas um setor de 4 KiB e um novo registro leva 256, mas isso n√£o √© um problema - um fragmento alterado recentemente do "meio" do arquivo seria gravado no espa√ßo n√£o utilizado independentemente de seu tamanho ter sido alterado ou n√£o, portanto, para o ZFS, essa √© uma situa√ß√£o normal. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A compacta√ß√£o ZFS integrada √© desativada por padr√£o e o sistema oferece algoritmos de plug-in - agora entre eles est√£o LZ4, gzip (1-9), LZJB e ZLE.</font></font><br>
<br>
<ul>
<li><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O LZ4</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> √© um algoritmo de streaming que oferece ganhos de compacta√ß√£o e descompacta√ß√£o e desempenho extremamente r√°pidos para a maioria dos casos de uso - mesmo em CPUs bastante lentas.</font></font><br>
</li>
<li><b>GZIP</b> ‚Äî  ,       Unix-.        1-9,       CPU      9.       (   )  ,    &nbsp;   c CPU&nbsp;‚Äî    ,     .<br>
</li>
<li><b>LZJB</b> ‚Äî    ZFS.       , LZ4     .<br>
</li>
<li><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ZLE</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - codifica√ß√£o de n√≠vel zero, codifica√ß√£o de n√≠vel zero. </font><font style="vertical-align: inherit;">Ele n√£o toca nos dados normais, mas comprime grandes seq√º√™ncias de zeros. </font><font style="vertical-align: inherit;">√ötil para conjuntos de dados completamente incompress√≠veis (por exemplo, JPEG, MP4 ou outros formatos j√° compactados), pois ignora dados incompress√≠veis, mas compacta o espa√ßo n√£o utilizado nos registros resultantes.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Recomendamos a compacta√ß√£o LZ4 para quase todos os casos de uso; </font><font style="vertical-align: inherit;">A penalidade de desempenho por encontrar dados incompress√≠veis √© muito pequena e o </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ganho de</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> desempenho para dados t√≠picos √© significativo. </font><font style="vertical-align: inherit;">Copiar uma imagem de m√°quina virtual para uma nova instala√ß√£o do sistema operacional Windows (SO rec√©m-instalado, sem dados ainda) </font></font><code>compression=lz4</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">passou 27% mais r√°pido do que com </font></font><code>compression=none</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">neste teste de 2015</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ARC - cache de substitui√ß√£o adapt√°vel</font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O ZFS √© o √∫nico sistema de arquivos moderno conhecido por n√≥s que usa seu pr√≥prio mecanismo de cache de leitura e n√£o depende do cache da p√°gina do sistema operacional para armazenar c√≥pias de blocos de leitura recentes na RAM. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Embora seu pr√≥prio cache n√£o tenha problemas, o ZFS n√£o pode responder a novas solicita√ß√µes de aloca√ß√£o de mem√≥ria t√£o r√°pido quanto o kernel, portanto, uma nova chamada de </font></font><code>malloc()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">aloca√ß√£o de mem√≥ria poder√° falhar se precisar de RAM atualmente ocupada pelo ARC. </font><font style="vertical-align: inherit;">Mas h√° boas raz√µes para usar seu pr√≥prio cache, pelo menos por enquanto.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Todos os sistemas operacionais modernos bem conhecidos, incluindo MacOS, Windows, Linux e BSD, usam o algoritmo LRU (Menos Utilizados Recentemente) para implementar o cache da p√°gina. Este √© um algoritmo primitivo que eleva o bloco em cache "at√© a fila" ap√≥s cada leitura e empurra os blocos "em espera" conforme necess√°rio para adicionar novas falhas de cache (blocos que deveriam ter sido lidos a partir do disco, n√£o do cache). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Normalmente, o algoritmo funciona bem, mas em sistemas com grandes conjuntos de dados em funcionamento, o LRU leva facilmente a surtos - eliminando os blocos frequentemente necess√°rios para abrir espa√ßo para blocos que nunca ser√£o lidos no cache novamente. </font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ARCO</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">&nbsp;- um algoritmo muito menos ing√™nuo, que pode ser considerado como um cache "ponderado". Ap√≥s cada leitura do bloco em cache, ele se torna um pouco "mais pesado" e fica mais dif√≠cil sair do grupo - e mesmo depois de sair do bloco, o bloco √© </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">rastreado</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> por um determinado per√≠odo de tempo. Um bloco que foi extra√≠do, mas precisa ser lido novamente no cache, tamb√©m se tornar√° "mais pesado".</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O resultado final de tudo isso √© um cache com uma taxa de acerto muito maior - a propor√ß√£o entre acertos no cache (lidos no cache) e erros (lidos no disco). </font><font style="vertical-align: inherit;">Essa √© uma estat√≠stica extremamente importante - n√£o apenas o cache atinge ordens de servi√ßo de magnitude mais rapidamente, mas tamb√©m pode ser atendido mais rapidamente, porque quanto mais cliques em cache, menos solicita√ß√µes de disco simult√¢neas e menos atraso para as falhas restantes que devem ser atendidas. dirigir.</font></font><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Conclus√£o</font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Depois de estudar a sem√¢ntica b√°sica do ZFS - como a c√≥pia funciona durante a grava√ß√£o, bem como os relacionamentos entre pools de armazenamento, dispositivos virtuais, blocos, setores e arquivos - estamos prontos para discutir o desempenho real com n√∫meros reais. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Na pr√≥xima parte, veremos o desempenho real de pools com vdev e RAIDz espelhados, em compara√ß√£o entre si e em compara√ß√£o com as topologias tradicionais de RAID do kernel Linux, que examinamos </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">anteriormente</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Inicialmente, quer√≠amos considerar apenas o b√°sico - as topologias do ZFS - mas depois </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">disso</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> estaremos prontos para falar sobre o ajuste e ajuste mais avan√ßados do ZFS, incluindo o uso de tipos de vdev auxiliares como L2ARC, SLOG e aloca√ß√£o especial.</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt504680/index.html">Vis√£o geral da biblioteca de PNL da SpaL</a></li>
<li><a href="../pt504682/index.html">Nostalgia Post: j2me, Gravity Defied, 64kb</a></li>
<li><a href="../pt504686/index.html">Como desenhar um gato</a></li>
<li><a href="../pt504688/index.html">M√°scaras s√£o in√∫teis: cr√≠tica cient√≠fica da pol√≠tica social no KOVID-19</a></li>
<li><a href="../pt504690/index.html">O conto de como eu configurei o Azure AD B2C em React e React Native Part 3 (Tutorial)</a></li>
<li><a href="../pt504694/index.html">Como compilar um decorador - C ++, Python e sua pr√≥pria implementa√ß√£o. Parte 1</a></li>
<li><a href="../pt504696/index.html">Not√≠cias do mundo do OpenStreetMap n¬∫ 513 (12.05.2020-18.05.2020)</a></li>
<li><a href="../pt504698/index.html">Integra√ß√£o em um site remoto</a></li>
<li><a href="../pt504700/index.html">Tablet gr√°fico sovi√©tico "esbo√ßo"</a></li>
<li><a href="../pt504702/index.html">As pessoas n√£o querem saber ingl√™s</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>