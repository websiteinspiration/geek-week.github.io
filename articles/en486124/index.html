<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚úåüèª üè¨ üéÖ Impala vs Hive vs Spark SQL: Choosing the right SQL engine to work properly in the Cloudera Data Warehouse üî§ üë∂üèæ üöß</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="We always lack data. And we don‚Äôt just want more data ... we want new types of data that will allow us to better understand our products, customers an...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Impala vs Hive vs Spark SQL: Choosing the right SQL engine to work properly in the Cloudera Data Warehouse</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/486124/"><img src="https://habrastorage.org/webt/bu/1w/sb/bu1wsbowqiicmki0x1nqsraewjw.jpeg"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We always lack data. And we don‚Äôt just want more data ... we want new types of data that will allow us to better understand our products, customers and markets. We are always in search of new data, data of all shapes and sizes, structured and not very. We want to open our doors to a new generation of business and technical specialists who will enthusiastically open new databases and technologies with us, which will subsequently change the nature of how we interact with data and what impact they have on our lives.</font></font><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
I will give a life example so that you better understand what I mean. About two years ago, the data saved the life of my friend‚Äôs daughter. When she was born she was diagnosed with seven heart defects. Thanks to new technologies, such as interactive 3D graphics, virtual modeling, more intelligent ECG analysis, modern solutions for monitoring patients undergoing bed rest, and thanks to other advanced medical procedures based on data, she managed to survive two open heart surgeries and now lives a healthy life . The data saved her life. This is what pushes me every day to search for new innovative solutions and ways to transfer data faster to those who need them more than others.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
I am proud to be part of the Cloudera Data Warehouse (CDW) team, powered by the Cloudera Data Platform (CDP). CDP was created from scratch as an enterprise data cloud or Enterprise Data Cloud (EDC). EDC is a multifunctional tool for implementing many tasks on one platform. Thanks to the use of hybrid and multi-cloud systems, CDP can work anywhere - both on a platform without an operating system, and in a private and public cloud. As more cloud solutions are introduced as part of our digital development plan, we see hybrid and multi-cloud solutions becoming the new norm. However, these combined solutions create problems in managing them, which in turn creates new security risks,the likelihood of tracking the user and subsequently breaking the law. To solve these problems, CDP has advanced security and control capabilities that will make it possible to open access to data without risking violating anyone's security policy or even the law.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
CDW on CDP is a new service that allows you to create a self-service data warehouse for BI analytics teams. You can quickly create new data warehouses and use them yourself, or provide access to them to a group of people and use a single database with them. Do you remember the times when you could manage your data warehouse on your own? Manage it without the participation of platforms and the infrastructure necessary for its operation? This has never happened before. CDW made this possible. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Thanks to CDW, various SQL engines have become available, but confusion comes with great choices. Let's look at the SQL engines available in CDW on CDP, and discuss which SQL option is more suitable for a specific task.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Such a great choice! </font><font style="vertical-align: inherit;">Impala? </font><font style="vertical-align: inherit;">Hive LLAP? </font><font style="vertical-align: inherit;">Spark? </font><font style="vertical-align: inherit;">What to use and when? </font><font style="vertical-align: inherit;">Let's figure it out.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Impala sql engine</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Impala is a popular open source MPP engine with a wide range of features in Cloudera Distribution Hadoop (CDH) and CDP. Impala has earned market confidence with its low-latency highly interactive SQL queries. Impala's capabilities are very wide, Impala not only supports the Hadoop Distributed File System (HDFS - Hadoop Distributed File System) with Parquet, Optimized Row Columnar (ORC - Optimized Storage Node), JavaScript Object Notation (JSON), Avro, and text formats, but also has built-in support for Kudu, Microsoft Azure Data Lake Storage (ADLS), and Amazon Simple Storage Service (S3). Impala has a high level of security with either sentry or ranger and, as you know, can support thousands of users with clusters of hundreds of nodes on multi-petabyte datasets.Let's look at the overall Impala architecture.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/on/f3/uk/onf3ukvs8cr_4lorbm6tkholx9c.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Impala uses the StateStore to verify the health of the cluster. If for some reason the Impala node goes offline, the StateStore will send a message about this to all nodes and skip the inaccessible node. The Impala Directory Service manages metadata for all SQL statements for all nodes in the cluster. The StateStore and the directory service exchange data with the Hive MetaStore to store blocks and files, and then transfer metadata to work nodes. When a request arrives, it is transferred to one of the many matching programs where compilation is performed and planning is initiated. Fragments of the plan are returned, and the coordination program organizes its implementation. Intermediate results are passed between Impala services and then returned.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
This architecture is ideal for cases where we need data marts for business intelligence to receive answers to queries with low latency, as is usually the case with ad-hoc, self-service and discovery types. In this scenario, we have customers telling us answers to complex queries from less than one second to five seconds.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
For Internet of Things (IoT) data and related scenarios, Impala, together with streaming solutions such as NiFi, Kafka or Spark Streaming, and related data warehouses such as Kudu, can provide continuous pipelining with a delay time of less than ten seconds . </font><font style="vertical-align: inherit;">With built-in read / write capabilities on S3, ADLS, HDFS, Hive, HBase, and more, Impala is an excellent SQL engine to use when starting a cluster of up to 1000 nodes, and more than 100 trillion rows in tables or datasets of 50BP or more.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hive LLAP</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
‚ÄúLive Long And Process‚Äù or ‚ÄúLong Delay Analytics Processing‚Äù, also known as LLAP, is a Hive-based execution engine that supports long-running processes using the same caching and processing resources. This processing mechanism gives us a response from SQL with a very low latency, since we do not have time to start the requested resources. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/pq/uw/fc/pquwfcyzx5gn55c8d8k1otqvica.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In addition, LLAP provides and establishes control over the execution of security policies, so all LLAP work for the user is transparent, which helps Hive to compete in terms of workload performance even with the most popular and traditionally used storage media today.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Hive LLAP offers the most advanced SQL engine in the big data ecosystem. Hive LLAP was created for a huge amount of data, providing users with the wide capabilities of the Enterprise Data Warehouse (EDW), which supports the conversion of large volumes of data, the execution of long queries or heavy SQL queries with hundreds of joins. Hive supports materialized views, surrogate keys, and various restrictions similar to traditional relational database management systems, including built-in caching for querying results and querying data. Hive LLAP can reduce the burden of repeated requests by reducing response time to a fraction of a second. Hive LLAP can support federated requests for HDFS (Hadoop Distributed File System) and object stores, as well as real-time streaming,working with Kafka and Druid.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Thus, Hive LLAP is ideally suited as an Enterprise Data Warehouse (EDW) solution, in which we will be faced with a large number of long queries that require large transformations or multiple joins between tables and large datasets. </font><font style="vertical-align: inherit;">Thanks to the caching technology included in Hive LLAP, we now have customers who can join 330 billion records with 92 billion other records with or without partition key and get results in seconds.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Spark sq</font></font></h2><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Spark is a high-performance, general-purpose data processing engine that supports data processing and distribution and has a wide range of applications. </font><font style="vertical-align: inherit;">There are many Spark data libraries for data science and machine learning experts who support the higher-level programming model for quick development. </font><font style="vertical-align: inherit;">Spark SQL, MLlib, Spark Streaming and GrapX are located above Spark.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/u2/2n/gh/u22nghp80uvyrlof4kwul2pzmgu.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Spark SQL is a structured data processing module compatible with various data sources, with support for Hive, Avro, Parquet, ORC, JSON and JDBC. </font><font style="vertical-align: inherit;">Spark SQL is efficient on semi-structured datasets and integrates with Hive MetaStore and NoSQL repositories such as HBase. </font><font style="vertical-align: inherit;">Spark is often used with various software APIs in our favorite programming languages ‚Äã‚Äãsuch as Java, Python, R, and Scala.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Spark can be very useful if you need to embed SQL queries with Spark programs if it works with large amounts of data and high load. </font><font style="vertical-align: inherit;">Spark helps many of our users who work in Global 100 companies to reduce the processing of streaming data. </font><font style="vertical-align: inherit;">Combining this with MLlib, we see how many of our customers respond positively to Spark, as an excellent system capable of machine learning when working with data warehouse applications. </font><font style="vertical-align: inherit;">Thanks to its high performance, low latency and excellent integration of third-party tools, Spark SQL provides the best conditions for switching between programming and SQL.</font></font><br>
 <br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">So which SQL engine to use?</font></font></h3><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Since you can combine the same data in CDW to CDP, you can choose the right engine for each type of workload, such as data engineering, traditional EDW, ad hoc analytics, BI dashboards, Online Analytical Processing (OLAP) or Online Transaction Processing (OLTP). </font><font style="vertical-align: inherit;">The diagram below shows some principles aimed at simplifying the selection, according to which the engines and their mechanisms are well suited for each of the stated goals.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/xw/st/ff/xwstffkvxlp9ubso_swhiinvdda.png"><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Conclusion</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
If you use EDW supporting BI dashboards, Hive LLAP will give you the best results. When you need ad-hoc, self-service, and research data warehouse, turn your eyes to the benefits of Impala. If you look at Data Engineering with long-running queries and no high concurrency, Spark SQL is a great choice. If you need high concurrency support, you can take a look at Hive on Tez. Look for OLAP support with time series data, add Druid, and if you are looking for OLTP with low latency and high concurrency, then maybe you should add Phoenix.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Total - there are many SQL engines in CDW to CDP, and this is done on purpose. </font><font style="vertical-align: inherit;">Making choices before making a decision is the best way to optimize processes for high-performance applications with multi-threaded processing on massive data warehouses. </font><font style="vertical-align: inherit;">CDW in CDP provides data sharing and sharing under a single system of security, management, data tracking and metadata, which allows you to combine SQL components in optimized repositories. </font><font style="vertical-align: inherit;">Thus, this gives the user the freedom to choose the best SQL engine depending on his workloads.</font></font></div>
      
    </div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/en486124/">https://habr.com/ru/post/en486124/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../en486112/index.html">Flask web applications: how to deal with cyclic imports</a></li>
<li><a href="../en486114/index.html">Leading scientists in the field of neuroscience will gather at the annual congress of the neuronet industry union</a></li>
<li><a href="../en486116/index.html">Fermat and Miller-Rabin simplicity tests</a></li>
<li><a href="../en486120/index.html">Normalization of deviance. How bad practices are becoming the norm in our industry</a></li>
<li><a href="../en486122/index.html">Child ReactJS with 135 lines of code</a></li>
<li><a href="../en486126/index.html">Microservices with Spring Boot. Part 5. Using the Eureka name server</a></li>
<li><a href="../en486128/index.html">Quality Architect: who is it and when is it needed</a></li>
<li><a href="../en486130/index.html">GSoC 2019: Checking Bipartite Counts and Monad Transformers</a></li>
<li><a href="../en486136/index.html">Imitating network problems in Linux</a></li>
<li><a href="../en486138/index.html">‚ÄúDo you have personal data?‚Äù And if I find it? ‚Äù Webinar on the localization of personal data in Russia - February 12, 2020</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>