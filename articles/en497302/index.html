<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🌖 🔭 👋🏽 Autonomous navigation of a mobile robot 📦 👨‍💼 🛣️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="There are a huge number of ways in which a robot can receive information from the outside world in order to interact with it. Also, depending on the t...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Autonomous navigation of a mobile robot</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/497302/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">There are a huge number of ways in which a robot can receive information from the outside world in order to interact with it. </font><font style="vertical-align: inherit;">Also, depending on the tasks assigned to him, the methods of processing this information differ. </font><font style="vertical-align: inherit;">In this article I will describe the main stages of the work carried out as part of the school project, the purpose of which is to systematize information on various methods of autonomous robot navigation and apply the knowledge gained in the process of creating the robot for the “RTK Cup” competitions.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/qb/xw/dw/qbxwdwc_cwrypc3c8dthahudzmk.jpeg"><br>
<a name="habracut"></a><br>
<h2><font color="#4d7f95"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Introduction</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
At the competitions “RTK Cup” there is a block of tasks that must be completed without operator intervention. </font><font style="vertical-align: inherit;">I believe that many participants are unfairly avoiding these tasks, because the seemingly complexity of creating a robot design and writing a program hides largely simplified tasks from other competitive disciplines, combined in one training ground. </font><font style="vertical-align: inherit;">By my project I want to show possible solutions to such problems, considering as an example the following along the line. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
To achieve the project goal, the following intermediate tasks were formulated:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Analysis of the </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">rules of the</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> competition “RTK Cup”</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Analysis of existing algorithms for autonomous orientation of a mobile robot</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Software creation</font></font></li>
</ul><br>
<h2><font color="#4d7f95"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Analysis of the rules of the competition “RTK Cup”</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
At the “RTK Cup” competitions, participants are presented with a training ground on which sections of varying complexity are modeled. </font><font style="vertical-align: inherit;">The competition aims to stimulate young robotics to create devices that can work in extreme conditions, overcome obstacles, under human control, or autonomously.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/6a/sr/6w/6asr6wzvlhnqgkmz0b8dfyf2h9o.jpeg"><br>
<br>
<div class="spoiler"><b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">briefly about the elements that make up the polygon</font></font></b><div class="spoiler_text"> «»          ,    .    ,       ,     (), ,      (),   ..<br>
<br>
:<br>
<br>
<img src="https://habrastorage.org/webt/mk/ks/d3/mkksd313rprmlsilxcrq5xdytg4.png" width="300"><br>
<br>
:<br>
<br>
<img src="https://habrastorage.org/webt/rv/fp/cu/rvfpcu-6qtvdfrclsqjzlr2xok4.jpeg" width="300"><br>
<br>
  –  ,     «»  ( )  ,        . ,         ,    ,      ,         .<br>
<br>
     .    ,       ,     ,   ,     ,    ,    .<br>
</div></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Competitions are divided into two fundamentally different from each other nominations: “Seeker” and “Extreme”. This is to ensure that the competition was held between participants with a minimum difference in age and experience in developing robotic systems: Seeker for the younger level, and Extreme for participants from 14 years of age and older. In the Seeker nomination, the operator can freely move around the range and have direct eye contact with the machine, while the Extreme nomination assumes that the robot has video communication systems and / or computer vision, since the operator must navigate in the maze, relying only on the maze the camera and sensors built into the robot, while being behind a special screen.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
To qualify in competitions, the robot must either pass the task for remote control of the manipulator, or perform one of the elements of autonomy. </font><font style="vertical-align: inherit;">In the framework of the project, the task was set to fulfill autonomy tasks, as they give the most points at the lowest cost from the operator. </font><font style="vertical-align: inherit;">The elements of autonomy include:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Driving along a line with an ambient light sensor or a line of sight system</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Standalone beacon capture using distance sensor or vision systems</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Movement along a complex trajectory (for example, ascent / descent of stairs) along a line using a compass, gyroscope, accelerometer, vision system, or combined methods</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Also, points for overcoming obstacles are doubled if the robot passes them autonomously.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Within the framework of this project, the solution to the first of the tasks will be considered - movement along the line. The most common methods used when moving along the line are light sensors and a camera. The pluses of the sensors include the simplicity of creating a program - many of them are equipped with a tuning resistor, so that by setting the sensor for background lighting, it will give out 0 or 1, depending on whether it is on the line or not. For the same reason, light sensors are not demanding on the processing power of the controller used. Also, because of this, solving the problem with the help of light sensors is the least costly - the cost of the simplest sensor is 35 rubles, and for a relatively stable ride along the line, three sensors are enough (one is installed on the line, and two on the sides). However,One of the main disadvantages of such sensors is installation restrictions. Ideally, the sensor should be installed exactly in the center, at a small distance from the floor, otherwise it will give incorrect values. This is not a problem in specialized competitions where the robot must drive as fast as possible along the track, but, in the conditions of the “RTK Cup” competition, all of the above-mentioned sensor flaws can be critical - their installation primarily requires the presence of additional mechanical parts on the robot that raise and lowering the sensors, and this requires additional space on the robot, a separate engine moving the sensors, and is also a place of potential damage and increases the mass of the robot.otherwise it will give incorrect values. This is not a problem in specialized competitions where the robot must drive as fast as possible along the track, but, in the conditions of the “RTK Cup” competition, all of the above-mentioned sensor flaws can be critical - their installation primarily requires the presence of additional mechanical parts on the robot that raise and lowering the sensors, and this requires additional space on the robot, a separate engine moving the sensors, and is also a place of potential damage and increases the mass of the robot.otherwise it will give incorrect values. This is not a problem in specialized competitions where the robot must drive as fast as possible along the track, but, in the conditions of the “RTK Cup” competition, all of the above-mentioned sensor flaws can be critical - their installation primarily requires the presence of additional mechanical parts on the robot that raise and lowering the sensors, and this requires additional space on the robot, a separate engine moving the sensors, and is also a place of potential damage and increases the mass of the robot.all the above-mentioned sensor flaws can be critical - their installation primarily requires the presence of additional mechanical parts on the robot that raise and lower the sensors, and this requires additional space on the robot, a separate motor that moves the sensors, and is also a place of potential damage and increases the mass of the robot .all the above-mentioned sensor flaws can be critical - their installation primarily requires the presence of additional mechanical parts on the robot that raise and lower the sensors, and this requires additional space on the robot, a separate motor that moves the sensors, and is also a place of potential damage and increases the mass of the robot .</font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><img src="https://habrastorage.org/webt/_g/pt/zz/_gptzzip0rdk6ocgg67ttumux2k.jpeg" width="300" align="right"></a><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The camera, in turn, has the following advantages: it has a practically unlimited (in comparison with sensors) measurement radius, i.e. only one camera module is able to simultaneously see the line, both directly below the robot, and at a sufficient distance from it, which allows, for example, to evaluate its curvature and select a proportional control action. At the same time, the camera module does not interfere with the advancement of the robot in other parts of the landfill that do not require autonomy, since the camera is fixed at a distance from the floor. The main minus of the camera is that video processing requires a powerful computing complex on board the robot, and the software being developed needs more fine-tuning, because the camera receives an order of magnitude more information from the outside world than three light sensors, while the camera and computercapable of processing the information received are many times more than three sensors and “arduins”.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
For me personally the answer is obvious for me - in the nomination “extremal” the robot must have a directional camera, with which the operator will navigate. </font><font style="vertical-align: inherit;">If you use ready-made FPV solutions, then the total cost of “sensors” can be even higher, while requiring the installation of additional devices. </font><font style="vertical-align: inherit;">Moreover, a robot with raspberry pi and a camera has greater potential for the development of autonomous movement, since the camera can solve a wide range of problems and can be used not only in line movement, while not greatly complicating the design.</font></font><br>
<br>
<h2><font color="#4d7f95"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Analysis of existing computer vision algorithms</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Computer vision is the theory of creating devices that can receive images of real-world objects, process and use the data obtained to solve various kinds of applied problems without human intervention. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Computer vision systems consists of:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">one or more cameras </font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">computer complex </font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Software that provides image processing tools</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Communication channels for transmitting target and telemetry information. </font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As previously written, there are many ways to identify objects of interest to us. In the case of driving along a line, it is necessary to separate the line itself from the contrasting background (a black line on a white background or a white line on a black background for an inverse line). Algorithms using a computer vision system can be divided into several “steps” for processing the original image: </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Image acquisition</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : digital images are obtained directly from the camera, from a video stream transmitted to the device, or as separate images. Pixel values ​​usually correspond to light intensity (color or grayscale images), but can be associated with various physical measurements, such as, for example, temperature from a thermal imaging camera. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Preliminary processing</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">: Before computer vision methods can be applied to video data, pre-processing is necessary to introduce certain conditions, depending on the method used. </font><font style="vertical-align: inherit;">Examples are:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Removing noise or distortion caused by the used sensor</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Image blur used to get rid of small artifacts that occur during camera operation, decompression elements, noise, etc.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Improving the contrast so that the right information can be detected more likely</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Change exposure to crop shadows or highlights</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Scaling or cropping to better distinguish between structures in the image.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Converting an image to monochrome or changing its resolution for faster system performance</font></font></li>
</ul><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Highlighting details</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : image details of various difficulty levels are extracted from the video data. </font><font style="vertical-align: inherit;">Typical examples of such details are lines, borders, edges, individual points, areas that are characteristic for any feature. </font></font><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Detection</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : at a certain stage of the program’s work, information relevant to the program is separated from the rest of the image. </font><font style="vertical-align: inherit;">Examples are:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The selection of a certain set of points of interest in color, the number of isolated pixels that are similar in some way (curvature of the figure, color, brightness, etc.)</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Segmentation of one or more image sections that contain a characteristic object.</font></font></li>
</ul><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">High-level processing</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : at this step, the abundance of information from the image is reduced to a size that can be easily processed, for example, a set of certain pixels or the coordinates of the portion of the image in which the object of interest is supposedly located. </font><font style="vertical-align: inherit;">Examples are:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Filtering values ​​by any criterion</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Evaluation of such parameters as the physical dimensions of the object, shape, its location in the frame or relative to other characteristic objects</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Classification</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Next, it was necessary to choose the library on the basis of which the program will be created. </font><font style="vertical-align: inherit;">The key factors in my choice were:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The library’s support for the Python interface due to the relative ease in learning this language by a beginner, is simple syntax, which has a beneficial effect on the readability of the program.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Portability, i.e. </font><font style="vertical-align: inherit;">the ability to run a program using this library on raspberry pi3.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The prevalence of the library, which guarantees a well-developed community of programmers who may have already encountered problems that may arise during your work.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Among the options I examined, I highlighted the OpenCV open computer vision library, since it supports Python, has extensive online documentation. </font><font style="vertical-align: inherit;">There are many articles and instructions on the Internet that describe all the subtleties of working with this library. </font><font style="vertical-align: inherit;">There is an official forum from developers where anyone can ask a question about it. </font><font style="vertical-align: inherit;">Also, this library is implemented in C / C ++ languages, which guarantees system performance, and its structure supports various modules that can be disabled in order to increase performance.</font></font><br>
<br>
<h2><font color="#4d7f95"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Software development</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
After installing the OS and initial configuration of Raspberry pi, but before you start creating the program, you must install all the packages necessary for it. </font><font style="vertical-align: inherit;">Most of these packages, in turn, are installed using the pip package manager (in the case of Python 3, pip3)</font></font><br>
<br>
<pre><code class="bash hljs">$ sudo apt install python3-pip</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The following libraries are installed, such as:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">picamera - library for working with raspberry pi camera</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">numpy - a library for working with multidimensional data arrays, as images</font></font></li>
</ul><br>
<pre><code class="bash hljs">$ sudo pip3 install picamera<font></font>
$ sudo pip3 install numpy<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
cmake - Utility for automatically building a program from the source code </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
cmake-curses-gui - GUI package (graphical interface) for cmake</font></font><br>
<br>
<pre><code class="bash hljs">$ sudo apt-get install cmake cmake-curses-gui libgtk2.0-dev<font></font>
$ sudo apt-get install cmake cmake-curses-gui libgtk2.0-dev<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
libraries for working with different image and video formats and more</font></font><br>
<br>
<pre><code class="bash hljs">$ sudo apt-get install libavcodec-dev libavformat-dev libswscale-dev libv4l-dev libx264-dev libxvidcore-dev<font></font>
$ sudo apt-get install libjpeg-dev libpng12-dev libtiff5-dev libjasper-dev<font></font>
$ sudo apt-get install gfortran libatlas-base-dev<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
To transmit video data from the robot to the computer, GStreamer will be used - a framework designed to receive, process and transmit multimedia data:</font></font><br>
<br>
<pre><code class="bash hljs">$ sudo apt install libgstreamer1.0-0 gstreamer1.0-plugins-base gstreamer1.0-plugins-good gstreamer1.0-plugins-bad gstreamer1.0-plugins-ugly gstreamer1.0-libav gstreamer1.0-doc gstreamer1.0-tools gstreamer1.0-x gstreamer1.0-alsa gstreamer1.0-gl gstreamer1.0-gtk3 gstreamer1.0-qt5 gstreamer1.0-pulseaudio
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The next step is to install the openCV library itself from sources, configure it and build it. </font><font style="vertical-align: inherit;">To do this, an opencv working folder is created.</font></font><br>
<br>
<pre><code class="bash hljs">$ mkdir opencv<font></font>
$ <span class="hljs-built_in">cd</span> opencv
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In order to download the latest versions of the library, wget is used — a console program for downloading files from the network. </font><font style="vertical-align: inherit;">At the time of the creation of the program, the latest stable version of openCV is 4.1.0, so download and unpack the sources:</font></font><br>
<br>
<pre><code class="bash hljs">$ wget https://github.com/opencv/opencv/archive/4.1.0.zip -O opencv_source.zip<font></font>
$ unzip opencv_source.zip<font></font>
$ wget https://github.com/opencv/opencv_contrib/archive/4.1.0.zip -O opencv_contrib.zip<font></font>
$ unzip opencv_contrib.zip<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
After the unpacking process is completed, the source archives can be deleted.</font></font><br>
<br>
<pre><code class="bash hljs">$ rm opencv_source.zip<font></font>
$ rm opencv_contrib.zip<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A directory is created for assembly and configuration.</font></font><br>
<br>
<pre><code class="bash hljs">$ <span class="hljs-built_in">cd</span> /home/pi/opencv/opencv-4.1.0<font></font>
$ mkdir build<font></font>
$ <span class="hljs-built_in">cd</span> build
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Build parameters are configured using the cmake utility. </font><font style="vertical-align: inherit;">To do this, all significant parameters are passed as utility variables, along with the assigned values:</font></font><br>
<br>
<pre><code class="cmake hljs">cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local -D INSTALL_PYTHON_EXAMPLES=<span class="hljs-keyword">ON</span> -D INSTALL_C_EXAMPLES=<span class="hljs-keyword">OFF</span> -D BUILD_opencv_python2=<span class="hljs-keyword">OFF</span> -D WITH_GSTREAMER=<span class="hljs-keyword">ON</span> -D BUILD_EXAMPLES=<span class="hljs-keyword">ON</span> -DENABLE_VFPV3=<span class="hljs-keyword">ON</span> -DENABLE_NEON=<span class="hljs-keyword">ON</span> -DCPU_BASELINE=NEON ..
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
After setting up the configuration, the utility will display all the parameters. Next, you need to compile the library. To do this, use the console command make –jN, where N is the number of cores that will be involved in the compilation process. For raspberry pi 3, the number of cores is 4, but you can definitely find out this number by writing the nproc command in the console.</font></font><br>
<br>
<pre><code class="bash hljs">$ make –j4</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Due to raspberry's limited resources, compilation can take quite a while. </font><font style="vertical-align: inherit;">In some cases, raspberries can even freeze, but if you later go into the build folder and re-register make, the work will continue. </font><font style="vertical-align: inherit;">If this happens, it is worth reducing the number of cores involved, however, my compilation went without problems. </font><font style="vertical-align: inherit;">Also, at this stage, it is worth thinking about the active cooling of raspberry, because even with it the processor temperature reached about 75 degrees. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
When compilation was successful, the library needs to be installed. </font><font style="vertical-align: inherit;">This is also done using the make utility. </font><font style="vertical-align: inherit;">Then we will form all the necessary connections with the ldconfig utility:</font></font><br>
<br>
<pre><code class="bash hljs">$ sudo make install<font></font>
$ sudo ldconfig<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We verify the installation by writing the following commands in python interactive mode:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">import</span> cv2<font></font>
print(cv2.getBuildInformation())<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The following conclusion of the program will be evidence of the correct installation. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/np/vi/ng/npving2rmncveg11-8qxvhvco1q.jpeg"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
It should be noted that the above library compilation procedure must be performed both on the robot and on the PC from which it is planned to control the robot and on which broadcasts from the robot will be received. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Creating a video distribution scheme</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Before you start writing code, you need to develop a scheme according to which the algorithm will function. In the considered case of software development for a robot created for participation in the RTK Cup competitions in the Extreme nomination, the entire program will be divided into two parts: a robot and a remote control, which will be played by a computer with Linux installed. One of the most important tasks here is to create an approximate scheme of how video data will be transmitted between different parts of the algorithm. Wi-Fi will be used as a communication channel between the two devices. Data packets providing control of the robot and feedback data will be transmitted from one device to another using the UDP protocol implemented in using the socket library. Video datadue to limitations in the size of the UDP packet will be transmitted using GStreamer. For the convenience of debugging, two video streams will be implemented:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">main video stream - transfers video data directly from the robot’s camera to a computer to ensure minimal control delay.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">auxiliary video stream - transfers the video data processed by the robot, necessary for setting up and debugging a program that implements computer vision.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Two video streams will be simultaneously active on the robot, and the computer will display the desired image depending on the drive mode enabled. </font><font style="vertical-align: inherit;">The robot, in turn, depending on whether the autonomy mode is on or off, will use either control data received from a computer or generated by an image processor. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/zw/0l/uw/zw0luwrmjjesbm1ygsra6gxtkm4.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Remote control of the robot will be carried out due to the work of two parallel flows on the robot and on the computer:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The “console” in a cycle polls all available input devices and forms a control data packet consisting of the data itself and the checksum (at the time of making the final changes to the article, I refused to create checksums in order to reduce the delay, but in the source, which I laid out at the end this piece of code is left) - of a certain value calculated from a data set by the operation of some algorithm used to determine the integrity of data during transmission</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Robot - Awaits data access from the computer. </font><font style="vertical-align: inherit;">Unpacks the data, recalculates the checksum and compares it with the sent and calculated on the computer side. </font><font style="vertical-align: inherit;">If the checksums match, the data is transferred to the main program.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Before parsing the line detection algorithm, I suggest that you familiarize yourself with the design features of the robot:</font></font><br>
<br>
<div class="spoiler"><b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">about the robot</font></font></b><div class="spoiler_text">          .<br>
<br>
<img src="https://habrastorage.org/webt/vw/ao/ex/vwaoexwehb49titxcgdis_ryonc.jpeg" width="200" align="right"><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"></a> —       .      (3  )        .                 ,      .      6 ,        .           .      .          .     ,    -  .     «»   rasberry pi 3 b —      .<br>
<br>
<img src="https://habrastorage.org/webt/ho/zw/bi/hozwbiptp_fihoiqncxq2zsbpim.png" width="200" align="left"> ,       ,   ,   ,   Solidworks    petg .    ,     raspberry        .<br>
<br>
<img src="https://habrastorage.org/webt/mh/po/bd/mhpobduedmyoxzrdbhhac2ewdpq.png" width="200" align="left">          ubiquiti bullet M5 hp.     (   )      ,          .   ,   «»  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"> </a> . <br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/ti/h_/7l/tih_7l74vjx8leso89cwynfpb3o.jpeg" width="400"></div><br>
:     «»     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow">  thingiverse</a>.    ,  ,   ,      ,          .<br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/wd/df/q_/wddfq_nyi7-xcqmdiil5glkybqe.gif" width="300"></div><br>
   ,     ,   .       ,     .              ,  ,        ,     ,    .     ,       ,        ,     .<br>
<br>
<img src="https://habrastorage.org/webt/sl/ju/f9/sljuf9jwaqm2kdadelgsgubyf5o.gif" width="450"><br>
<br>
<img src="https://habrastorage.org/webt/sg/xk/_k/sgxk_kt1f0xdxkg4igwgzmbudk0.png" width="250"><br>
<br>
-     (   -  200 )    ,       ,     90       70   (     ),          ,     « ». ,            VL53L0X        ,      .<br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/jq/wh/hc/jqwhhc7et4crpin64qkaw6txgk0.png" width="250"></div><br>
 «»     ,     ,    (<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow">rds3115</a>).    — ,     ,  ,     ,     .<br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/4p/ot/ic/4poticuqt_itsiasls1of3927ma.jpeg" width="250"></div><br>
      ,      ,    ,   :<br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/qc/w5/ol/qcw5olk3klaxq75typuz41hdt18.png" width="250"></div><br>
- ,       ,          ,      .           . <img src="https://habrastorage.org/webt/he/4o/kp/he4okpaqyd5pof9x1cjwc1aalwi.jpeg" width="200" align="left">        raspberry,      ,     .       ,      .<br>
<br>
     ,   USB.            ,            ,     .<br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/rq/wv/yr/rqwvyr8kv5dtvpgz6x7rahoyfho.gif" width="200"></div><br>
<i>        </i><br>
</div></div><br>
<h3><font color="#4d7f95"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Creation of a line detection algorithm using OpenCV library methods</font></font></font></h3><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I. Receiving data</font></font></b> <br>
<br>
<img src="https://habrastorage.org/webt/ua/q7/zo/uaq7zojtflqtezqkiq2meem5mam.png" width="300" align="right"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Due to the fact that the image processor does not receive video data directly from the camera, but from the main stream, it is necessary to transfer them from the format used for translation to the format used for image processing, namely, a numpy array consisting of red values , green and blue for each of the pixels. </font><font style="vertical-align: inherit;">To do this, you need the initial data - a frame received from the raspberry pi camera module. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The easiest way to get frames from camera c for further processing is to use the picamera library. </font><font style="vertical-align: inherit;">Before you begin, you need to allow access to the camera through raspi-config -&gt; interfacing options camera -&gt; select yes.</font></font><br>
<br>
<pre><code class="bash hljs">sudo raspi-config</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
the next section of code is connected to the raspberry camera and in a cycle with a given frequency receives frames in the form of an array ready for use by the opencv library.</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> picamera.array <span class="hljs-keyword">import</span> PiRGBArray
<span class="hljs-keyword">from</span> picamera <span class="hljs-keyword">import</span> PiCamera
<span class="hljs-keyword">import</span> cv2
<span class="hljs-comment">#   </span><font></font>
camera = PiCamera()<font></font>
camera.resolution = (<span class="hljs-number">640</span>, <span class="hljs-number">480</span>) <font></font>
camera.framerate = <span class="hljs-number">30</span>
cap = PiRGBArray(camera, size=(<span class="hljs-number">640</span>, <span class="hljs-number">480</span>))<font></font>
<font></font>
<span class="hljs-keyword">for</span> frame <span class="hljs-keyword">in</span> camera.capture_continuous(cap , format=<span class="hljs-string">"bgr"</span>, use_video_port=<span class="hljs-literal">True</span>):<font></font>
	new_frame = frame.array<font></font>
	cap.truncate(<span class="hljs-number">0</span>)
	<span class="hljs-keyword">if</span> <span class="hljs-literal">False</span>: <span class="hljs-comment">#   -   </span>
		<span class="hljs-keyword">break</span>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
It is also worth noting that this method of capturing frames, although it is the simplest, but has a serious drawback: it is not very effective if you need to broadcast frames through GStreamer, as this requires several times to re-encode the video, which reduces the speed of the program. </font><font style="vertical-align: inherit;">A much faster way to obtain images will be to output frames from the video stream at the request of the image processor, however, the further stages of image processing will not depend on the method used. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
An example of an image from a robot heading camera without processing:</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/bo/yi/ez/boyiezf6vfa1nqrlcdllhwbmsgg.png" width="350"></div><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">II. Pre-processing</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
When driving on a line, it will be most simple to separate the area of ​​points that most contrast with the background color. This method is ideally suited for the RTK Cup competition, because it uses a black line on a white background (or a white line on a black background for inverse sections). In order to reduce the amount of information that needs to be processed, you can apply a binarization algorithm to it, that is, convert the image to a monochrome format, where there are only two types of pixels - dark and light. Before this, the picture should be translated into grayscale, and also blur it in order to cut off small defects and noise that inevitably appears during the operation of the camera. To blur the image, a Gaussian filter is used.</font></font><br>
<br>
<pre><code class="python hljs">gray = cv2.cvtColor(self._frame, cv2.COLOR_RGB2GRAY)<font></font>
blur = cv2.GaussianBlur(gray, (ksize, ksize), <span class="hljs-number">0</span>)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
where ksize is the size of the Gaussian core, increasing which, you can increase the degree of blur. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Example image after translation in grayscale and blur:</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/ye/5h/_d/ye5h_d7dqttbxo_af3hhkxnllce.png" width="350"></div><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">III. </font><font style="vertical-align: inherit;">Selecting details</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
After the image is translated in grayscale, it is necessary to binarize it at a given threshold. </font><font style="vertical-align: inherit;">This action allows you to further reduce the amount of data. This threshold value will be adjusted before each departure of the robot in a new place, or when the lighting conditions change. </font><font style="vertical-align: inherit;">Ideally, the task of calibration is to make sure that the outline of the line is defined on the image, but at the same time, there should not be other details on the image that are not a line:</font></font><br>
<br>
<pre><code class="python hljs">thresh = cv2.threshold(blur, self._limit, <span class="hljs-number">255</span>, cv2.THRESH_BINARY_INV)[<span class="hljs-number">1</span>]</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Here, all pixels darker than the threshold value (self._limit) are replaced by 0 (black), lighter - by 255 (white). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
After processing, the image looks as follows:</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/ne/rq/tl/nerqtlzt7p-k0q-4quw6nhsbfau.png" width="350"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As you can see, the program has identified several of the darkest parts of the image. </font><font style="vertical-align: inherit;">However, having calibrated the threshold value so as to completely “catch” the headphones, other white elements appear on the screen besides them. </font><font style="vertical-align: inherit;">Of course, you can fine-tune the threshold, and at the competitive training ground the camera will look down, not allowing unnecessary elements into the frame, but I consider it necessary for me to separate the line from everything else. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">IV.Detection</font></font></b> <br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In the binarized image, I applied a border search algorithm. </font><font style="vertical-align: inherit;">It is needed in order to determine free-standing spots and turn them into a convenient array of coordinate values ​​of points that make up the border. </font><font style="vertical-align: inherit;">In the case of opencv, as written in the documentation, the standard algorithm for finding loops uses the Suzuki85 algorithm (I could not find references to the algorithm with this name anywhere except for the opencv documentation, but I will assume that this is the </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Suzuki-Abe</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> algorithm </font><font style="vertical-align: inherit;">).</font></font><br>
<br>
<pre><code class="python hljs">contours = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)[<span class="hljs-number">0</span>]</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
And here is the frame obtained at this stage:</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/ex/3r/gx/ex3rgxc7bmefqdwhetn5wfxrtko.png" width="350"></div><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">V. High Level Processing</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Having found all the contours in the frame, the contour with the largest area is selected and taken as the contour of the line. Knowing the coordinates of all points of this contour, the coordinate of its center is found. For this, the so-called "image moments" are used. The moment is the total characteristic of the contour, calculated by summing the coordinates of all the pixels of the contour. There are several types of moments - up to the third order. For this problem, only the zero-order moment (m00) is needed - the number of all points making up the contour (the perimeter of the contour), the first-order moment (m10), which is the sum of X coordinates of all points, and m01 is the sum of Y coordinates of all points. By dividing the sum of the coordinates of the points along one of the axes by their number, the arithmetic mean is obtained — the approximate coordinate of the center of the contour. Next, the deviation of the robot from the course is calculated:the course “directly” corresponds to the coordinate of the center point along X close to the frame width divided by two. If the coordinate of the center of the line is close to the center of the frame, then the control action is minimal, and, accordingly, the robot retains its current course. If the robot deviates from one of the sides, then a control action proportional to the deviation will be introduced until it returns.</font></font><br>
<br>
<pre><code class="python hljs">mainContour = max(contours, key = cv2.contourArea)<font></font>
M = cv2.moments(mainContour)<font></font>
<span class="hljs-keyword">if</span> M[<span class="hljs-string">'m00'</span>] != <span class="hljs-number">0</span>:<span class="hljs-comment">#     (..   -  )</span>
    cx = int(M[<span class="hljs-string">'m10'</span>]/M[<span class="hljs-string">'m00'</span>])<font></font>
    cy = int(M[<span class="hljs-string">'m01'</span>]/M[<span class="hljs-string">'m00'</span>])
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Below is a schematic drawing of the position of the robot relative to the line and the frames, with the results of the program superimposed on them: the “main” contour, the lines passing through the center of the contour, as well as the point located in the center to estimate the deviation. </font><font style="vertical-align: inherit;">These elements are added using the following code:</font></font><br>
<br>
<pre><code class="python hljs">cv2.line(frame, (cx, <span class="hljs-number">0</span>), (cx, self.height), (<span class="hljs-number">255</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>), <span class="hljs-number">1</span>)    <span class="hljs-comment">#    </span>
cv2.line(frame, (<span class="hljs-number">0</span>, cy), (self.width, cy), (<span class="hljs-number">255</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>), <span class="hljs-number">1</span>)                  <font></font>
cv2.circle(frame, (self.width//<span class="hljs-number">2</span>, self.height//<span class="hljs-number">2</span>), <span class="hljs-number">3</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">255</span>), <span class="hljs-number">-1</span>) <span class="hljs-comment">#  </span>
cv2.drawContours(frame, mainContour, <span class="hljs-number">-1</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">255</span>, <span class="hljs-number">0</span>), <span class="hljs-number">2</span>, cv2.FILLED) <span class="hljs-comment">#   </span>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
For convenience of debugging, all previously described elements are added to the raw frame: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/f4/fl/os/f4flos522ouvlu-vi2b9rr_17lg.png" width="350"><br>
<br>
<img src="https://habrastorage.org/webt/uc/r1/vx/ucr1vxcecjqdv5qdswcbjsltw9w.png" width="350"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Thus, having driven the frame through the processing algorithm, we got the X and Y coordinates of the center of the object of interest to us, as well as the debug image. </font><font style="vertical-align: inherit;">Next, the position of the robot relative to the line is schematically shown, as well as the image that has passed the processing algorithm.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/4r/co/fy/4rcofyknawjnesluhuvoyp9zomu.jpeg" width="500"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The next step in the program is to convert the information obtained in the previous step into the power values ​​of two motors. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/io/s3/gn/ios3gnw-mt2xsmvh_hsfrpsdkdu.jpeg" width="250" align="right"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The easiest way to convert the difference between the shift of the center of the color spot relative to the center of the frame is the proportional regulator (There is also a relay regulator, but, due to the features of its operation, it is not very suitable for driving along the line). The principle of operation of such an algorithm is that the controller generates a control action on the object in proportion to the magnitude of the error. In addition to the proportional controller, there is also an integral one, where over time the integral component “accumulates” the error and the differential ones, the principle of which is based on the application of the regulatory influence only with a sufficient change in the controlled variable. In practice, these simplest P, I, D controllers are combined into controllers of the type PI, PD, PID.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
It is worth mentioning that on my robot I tried to “start” the PID controller, but its use did not give any serious advantages over the usual proportional controller. I admit that I could not properly adjust the regulator, but it is also possible that its advantages are not so clearly visible in the case of a heavy robot that is unable to physically develop high speeds. In the latest version of the program at the time of writing, a simple proportional regulator is used, but with a small feature that allows you to use more information from the camera: when generating the error value, not only the horizontal position of the midpoint of the spot was taken into account, but also vertically, which allowed different ways respond to line elementslocated “in the distance” and directly in front of or under the robot (the robot’s heading camera has a huge viewing angle, so turning it just 45 degrees down, you can already see a significant part of the field under the robot).</font></font><br>
<br>
<pre><code class="python hljs">error= cx / (self.width/<span class="hljs-number">2</span>) - <span class="hljs-number">1</span>  
<span class="hljs-comment">#  ( 0   )  [-1; 1]</span>
error*= cy / self.height + self.gain <span class="hljs-comment">#</span>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Most often, in the conditions of the “RTK Cup” competition, the participants use the so-called “tank circuit” - one or more engines control one side of the robot, and it works both with tracks and with wheels. Using this scheme allows you to get rid of complex transmission elements that increase the chance of breakage (differentials or cardan shafts), get the smallest possible turning radius, which gives an advantage in a confined polygon. This scheme involves the parallel control of two “sides” for movement along a complex path. To do this, the program uses two variables - the power of the right and left motor. This power depends on the base speed (BASE_SPEED), varying in the range from 0 to 100.Errors (error) - the difference between the center of the frame and the coordinate of the middle of the line and the coefficient of proportional effect (self._koof), which is calibrated by the operator. Its absolute value affects how fast the robot will try to align itself with the line. Due to the fact that on one engine the control action is subtracted from the base speed, and on the other - it is added, a turn is carried out when deviating from the course. The direction in which the reversal will be performed can be adjusted by changing the sign of the self._koof variable. Also, you may notice that as a result of the next code section, a power value may appear that is more than 100, but in my program such cases are additionally processed later.Its absolute value affects how fast the robot will try to align itself with the line. Due to the fact that on one engine the control action is subtracted from the base speed, and on the other - it is added, a turn is carried out when deviating from the course. The direction in which the reversal will be performed can be adjusted by changing the sign of the self._koof variable. Also, you may notice that as a result of the next code section, a power value may appear that is more than 100, but in my program such cases are additionally processed later.Its absolute value affects how fast the robot will try to align itself with the line. Due to the fact that on one engine the control action is subtracted from the base speed, and on the other - it is added, a turn is carried out when deviating from the course. The direction in which the reversal will be performed can be adjusted by changing the sign of the self._koof variable. Also, you may notice that as a result of the next code section, a power value may appear that is more than 100, but in my program such cases are additionally processed later.in which the reversal will be made, you can adjust by changing the sign of the self._koof variable. Also, you may notice that as a result of the next code section, a power value may appear that is more than 100, but in my program such cases are additionally processed later.in which the reversal will be made, you can adjust by changing the sign of the self._koof variable. Also, you may notice that as a result of the next code section, a power value may appear that is more than 100, but in my program such cases are additionally processed later.</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-comment">#if lineFound:</span><font></font>
leftSpeed = round(self.base_speed + error*self.koof)<font></font>
rightSpeed = round(self.base_speed - error*self.koof)<font></font>
</code></pre><br>
<h2><font color="#4d7f95"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Conclusion</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Having tested the resulting program, I can say that the main difficult moment in setting up the program is the calibration of the algorithm to the lighting features. </font><font style="vertical-align: inherit;">Since the stage of creating the article coincided with the declared self-isolation, I had to create a video with a demonstration of work in a small room. </font><font style="vertical-align: inherit;">This put the following difficulties on me:</font></font><br>
<br>
<ul>
<li> -,    ,    (   ,     ),        .        ,    ,         ,      .      ,     , ,            ,              </li>
<li> -,       —    ,   ,         </li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Despite the fact that both of these problems are absent in the conditions of real competitions, I will take measures to ensure that the work of the program minimally depends on external factors. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Also, in the future, it is planned to continue work on the implementation of algorithms using computer vision methods, creating software capable of passing through the remaining elements of autonomy described in the first part of the article (autonomous beacon capture, movement along a complex path). It is planned to expand the functionality of the robot by adding additional sensors: rangefinder, gyroscope-accelerometer, compass. Despite the fact that the publication of this article will end my work on the project as a compulsory school subject, I plan to continue to describe here the further stages of development. Therefore, I would like to receive comments about this work.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
After implementing all the steps aimed at solving the problems of the project, it is safe to say that the use of computer vision algorithms, with all its relative complexity in programming and debugging, gives the greatest gain at the stage of the competitions themselves. With the small dimensions of the camera, it has enormous potential in terms of software development, because the camera allows you to replace several "traditional" sensors at once, while receiving incredibly more information from the outside world. It was possible to realize the goal of the project - to create a program that uses computer vision to solve the problem of autonomous navigation of the robot in the conditions of the “RTK Cup” competition, as well as describe the process of creating the program and the main stages in image processing.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As I said earlier, it was not possible to recreate the complex trajectory of the house line, however, and this example shows how the algorithm fulfills turns. </font><font style="vertical-align: inherit;">The thickness of the line here corresponds to that according to the regulations, and the most curve of the turns approximately reflects the curvature of rotation by 90 degrees on the polygon:</font></font><br>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/YmHk3f-qQ5E" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
You can see the program code, as well as monitor further work on the project, on </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">my github</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> or here, if I continue.</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../en497286/index.html">Ludum Dare: checklist a week before the start</a></li>
<li><a href="../en497288/index.html">Decorative ceiling light Feron AL5000</a></li>
<li><a href="../en497290/index.html">Improving performance using uop cache on Sandy Bridge +</a></li>
<li><a href="../en497292/index.html">Technology Stack Shiro Games</a></li>
<li><a href="../en497296/index.html">Popular errors in English among IT professionals. Part 2: Pronunciation</a></li>
<li><a href="../en497304/index.html">Intercepter-NG 2.5 released for Android</a></li>
<li><a href="../en497306/index.html">DLL spoofing (DLL hijacking)</a></li>
<li><a href="../en497308/index.html">Can artificial intelligence do art?</a></li>
<li><a href="../en497310/index.html">Bipolar morphological networks: a neuron without multiplication</a></li>
<li><a href="../en497312/index.html">Question about CAN FD</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>