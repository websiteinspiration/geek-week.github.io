<!doctype html>
<html class="no-js" lang="zh-CN">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🔴 🔛 🤦🏻 您的第一个BERT：插图指南 🧑‍🤝‍🧑 🛡️ 👩🏼‍🏫</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="在过去的几年中，用于自然语言处理的机器学习取得了显着的进步。模型离开了研究实验室，并成为领先的数字产品的基础。一个很好的例证就是最近宣布BERT模型已经成为Google搜索背后的主要组成部分。 Google认为，这一步骤（即在搜索引擎中引入一种理解自然语言的高级模型）代表着“过去五年中最大的突破，也...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>您的第一个BERT：插图指南</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/498144/"><p><img src="https://habrastorage.org/webt/1t/0s/2u/1t0s2udwz_c-rqf3jvlxkigemhm.png" alt="bert-distilbert-句子分类"></p><br>
<p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">在过去的几年中，用于自然语言处理的机器学习取得了显着的进步。模型离开了研究实验室，并成为领先的数字产品的基础。一个很好的例证就是</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">最近宣布BERT模型已经成为Google搜索背后的主要组成部分</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。 Google认为，这一步骤（即在搜索引擎中引入一种理解自然语言的高级模型）代表着“过去五年中最大的突破，也是搜索引擎历史上最重要的突破之一”。</font></font></p><br>
<p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">本文是使用BERT版本之一对句子进行分类的简单指南。</font><font style="vertical-align: inherit;">我们研究的示例既简单又足以初次了解该模型，又足够先进以演示关键概念。</font></font></p><br>
<p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">除了本文之外，还准备了可以在</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">存储库中</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">查看</font><font style="vertical-align: inherit;">或在</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://colab.research.google.com/github/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Colab中</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">运行</font><font style="vertical-align: inherit;">的笔记本电脑</font><font style="vertical-align: inherit;">。</font></font></p><a name="habracut"></a><br>
<h1 id="dannye-sst2"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">数据：SST2</font></font></h1><br>
<p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">在我们的示例中，我们将使用</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SST2数据集，</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">其中包含来自电影评论的建议，每个建议都有正标签（值1）或负标签（值0）：</font></font></p><br>
<p><img src="https://habrastorage.org/webt/zn/el/tu/zneltu_kmrm0a2l1duybrs6ust8.png" alt="sst2"></p><br>
<h1 id="modeli-klassifikaciya-tonalnosti-predlozheniy"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">型号：句子分类</font></font></h1><br>
<p>  –  ,     ( ,     )    1 (      ),  0 ( ).      :</p><br>
<p><img src="https://habrastorage.org/webt/f4/mk/px/f4mkpxxli5t3izoldbz2qzfucei.png" alt="sentiment-classifier-1.png"></p><br>
<p>        :</p><br>
<ul>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow">DistilBERT</a>          . DistilBERT     BERT',         <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow">HuggingFace</a>.       ,       .</li>
<li>  –      scikit learn,     DistilBERT'        (1  0 ).</li>
</ul><br>
<p>,      ,     768.      ,     .</p><br>
<p><img src="https://habrastorage.org/webt/0v/2c/2f/0v2c2fzigdyzff3vqurbanyc2tk.png" alt="distilbert-bert-sentiment-classifier.png"></p><br>
<p> ,     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">BERT, ELMO     (  NLP   )</a>:       (     [CLS]).</p><br>
<h1 id="obuchenie-modeli"> </h1><br>
<p>  ,     ,      .   DistilBERT',         .       ,     ,     «» BERT',       .   ,     ,  BERT       ,   [CLS] . ,         ,    .   ,   , BERT           . </p><br>
<p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">transformers</a>   DistilBERT',     .</p><br>
<p><img src="https://habrastorage.org/webt/lx/du/gi/lxdugirbo9cgxehjoggycphulr8.png" alt="模型训练"></p><br>
<h1 id="obzor-rukovodstva"> </h1><br>
<p>,    .     DistilBERT'     2  .</p><br>
<p><img src="https://habrastorage.org/webt/bb/uv/_t/bbuv_tdc97jm9upvgj1gcyefemy.png" alt="bert-distilbert-教程句嵌入"></p><br>
<p>       DistilBERT'.       Scikit Learn. ,  ,         :</p><br>
<p><img src="https://habrastorage.org/webt/r_/e3/mq/r_e3mqtxvjry5xoabovb7il_iec.png" alt="伯特·迪迪伯特火车测试拆分句子嵌入"></p><br>
<p><em>        distilBERT' ( #1)   ,           ( #2).   ,    sklearn    ,    ,      75%  ,       </em></p><br>
<p>       :</p><br>
<p><img src="https://habrastorage.org/webt/wn/f6/9k/wnf69kgqr5azls-edvk6gidjoui.png" alt="伯特训练逻辑回归"></p><br>
<h1 id="kak-vychislyaetsya-predskazannoe-znachenie">   </h1><br>
<p>       ,   ,    ,      .</p><br>
<p>    «a visually stunning rumination on love».     BERT'  ,     .    ,      (   [CLS]     [SEP]   ).</p><br>
<p><img src="https://habrastorage.org/webt/rd/cu/me/rdcumeyavzwbbbleog8_tv1y0o8.png" alt="bert-distilbert-tokenization-1"></p><br>
<p>          ,       .         <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Word2vec  </a>.</p><br>
<p><img src="https://habrastorage.org/webt/-8/o5/tv/-8o5tv8c37kuyvnzvgcr9dt6vam.png" alt="bert-distilbert-tokenization-2-token-ids"></p><br>
<p>        :</p><br>
<pre><code class="python hljs">tokenizer.encode(<span class="hljs-string">"a visually stunning rumination on love"</span>, add_special_tokens=<span class="hljs-literal">True</span>)</code></pre><br>
<p>          DistilBERT'.</p><br>
<p>    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">BERT, ELMO     (  NLP   )</a>,      :</p><br>
<p><img src="https://habrastorage.org/webt/3u/i-/gl/3ui-glcku_tpcg8cjoghffb0vt0.png"></p><br>
<h1 id="prohodya-cherez-distilbert">  DistilBERT</h1><br>
<p>     DistilBERT'   ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="> BERT'</a>.        ,   768    .</p><br>
<p><img src="https://habrastorage.org/webt/oq/nf/ip/oqnfip8zglclubfblimrqvwrdz8.png" alt="伯特模型输入输出1"></p><br>
<p>      ,   ,    (  [CLS] ).           .</p><br>
<p><img src="https://habrastorage.org/webt/vr/-x/my/vr-xmyzsavuid9mrskzpysxbvhw.png" alt="伯特模型可化验输出向量cls"></p><br>
<p>        ,        ,      .         :</p><br>
<p><img src="https://habrastorage.org/webt/jm/oy/or/jmoyorskol1nxvxarefclben_ky.png" alt="bert-distilbert句子分类示例"></p><br>
<p>     ,        .</p><br>
<h1 id="kod"></h1><br>
<p>          .        <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow">Colab</a>   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow">github</a>.</p><br>
<p>    :</p><br>
<pre><code class="python hljs"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> transformers <span class="hljs-keyword">as</span> ppb <span class="hljs-comment"># pytorch transformers</span>
<span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression
<span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> cross_val_score
<span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split</code></pre><br>
<p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"></a>    github,          pandas:</p><br>
<pre><code class="python hljs">df = pd.read_csv(<span class="hljs-string">'https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv'</span>, delimiter=<span class="hljs-string">'\t'</span>, header=<span class="hljs-literal">None</span>)</code></pre><br>
<p>  df.head()  ,    5    ,     :</p><br>
<pre><code class="python hljs">df.head()</code></pre><br>
<p><img src="https://habrastorage.org/webt/t2/-_/tl/t2-_tljwx7gvvwsoqs3hlt6w7ya.png" alt="sst2-df-head"></p><br>
<h1 id="zagruzka-predobuchennoy-modeli-distilbert-i-tokenizatora">   DistilBERT  </h1><br>
<pre><code class="python hljs">model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, <span class="hljs-string">'distilbert-base-uncased'</span>)<font></font>
<font></font>
<span class="hljs-comment">##  BERT  distilBERT?   :</span>
<span class="hljs-comment">#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')</span><font></font>
<font></font>
<span class="hljs-comment">#   / </span><font></font>
tokenizer = tokenizer_class.from_pretrained(pretrained_weights)<font></font>
model = model_class.from_pretrained(pretrained_weights)</code></pre><br>
<p>     .  ,        ,     .     .             (          ,  2000).</p><br>
<h1 id="tokenizaciya"></h1><br>
<pre><code class="python hljs">tokenized = df[<span class="hljs-number">0</span>].apply((<span class="hljs-keyword">lambda</span> x: tokenizer.encode(x, add_special_tokens=<span class="hljs-literal">True</span>)))</code></pre><br>
<p>       .</p><br>
<p><img src="https://habrastorage.org/webt/le/zs/gr/lezsgrmkronylp8wh-kays3c0c8.png" alt="sst2-文本到令牌ID的例子"></p><br>
<p>      (  Series/DataFrame  pandas) .   DistilBERT    ,               0 (padding).   ,      ( ,            Python).</p><br>
<p> ,   /,    BERT':</p><br>
<p><img src="https://habrastorage.org/webt/u7/uz/ma/u7uzma5jjkbif--qi60zgb0xyy8.png" alt="伯特输入张量"></p><br>
<h1 id="obrabotka-v-distilberte">  DistilBERT'</h1><br>
<p>           DistilBERT.</p><br>
<pre><code class="python hljs">input_ids = torch.tensor(np.array(padded))<font></font>
<font></font>
<span class="hljs-keyword">with</span> torch.no_grad():<font></font>
    last_hidden_states = model(input_ids)</code></pre><br>
<p>     <strong>last_hidden_states</strong>    DistilBERT',      ( ,     ,      DistilBERT).   ,   2000 (..    2000 ), 66 (          2000 ), 278 (     DistilBERT).</p><br>
<p><img src="https://habrastorage.org/webt/yi/bf/lp/yibflpzn_dbzi72nvns_q92gaqw.png" alt="bert-distilbert-输出张量预测"></p><br>
<h1 id="raspakovka-vyhodnogo-tenzora-berta">   BERT'</h1><br>
<p>   3-d  .       :</p><br>
<p><img src="https://habrastorage.org/webt/kq/cr/k1/kqcrk198-zwu5t10mtsq4pxri3i.png" alt="伯特输出张量"></p><br>
<h1 id="puteshestvie-podhodit-k-koncu">   </h1><br>
<p>       .        :</p><br>
<p><img src="https://habrastorage.org/webt/ie/a4/ob/iea4ob8zt474ku1j4uw1uthupdq.png" alt="伯特输入到输出张量回顾"></p><br>
<h1 id="poluchenie-samoy-vazhnoy-chasti">   </h1><br>
<p>       BERT'   [CLS].          .</p><br>
<p><img src="https://habrastorage.org/webt/84/5i/pd/845ipdjygzhjk4t-k6vj4hrbmem.png" alt="伯特输出张量选择"></p><br>
<p> ,    3d   ,     2d :</p><br>
<pre><code class="python hljs"><span class="hljs-comment">#        ,      </span>
features = last_hidden_states[<span class="hljs-number">0</span>][:,<span class="hljs-number">0</span>,:].numpy()</code></pre><br>
<p>   <strong>features</strong>  2d  numpy,         .</p><br>
<p><img src="https://habrastorage.org/webt/sg/e7/_i/sge7_iy7hdtmvxyk41ajpm4xt_a.png" alt="bert输出cls感觉嵌入"></p><br>
<p><em>,      BERT'</em></p><br>
<h1 id="nabor-dannyh-dlya-logisticheskoy-regressii">    </h1><br>
<p>,       BERT',    ,        .   768 ,         .</p><br>
<p><img src="https://habrastorage.org/webt/ul/33/yn/ul33ynyfjqqgqwuk-uvwe3mty1k.png" alt="Logistic回归数据集功能标签"></p><br>
<p><em>  ,      .     BERT'   [CLS] ( #0),    (.  ).        ,    –       ,       BERT/DistilBERT</em></p><br>
<p> ,             ,             .</p><br>
<pre><code class="python hljs">labels = df[<span class="hljs-number">1</span>]<font></font>
train_features, test_features, train_labels, test_labels = train_test_split(features, labels)</code></pre><br>
<p>         :</p><br>
<p><img src="https://habrastorage.org/webt/r_/e3/mq/r_e3mqtxvjry5xoabovb7il_iec.png" alt="伯特·迪迪伯特火车测试拆分句子嵌入"></p><br>
<p>        .</p><br>
<pre><code class="python hljs">lr_clf = LogisticRegression()<font></font>
lr_clf.fit(train_features, train_labels)</code></pre><br>
<p> ,        .</p><br>
<pre><code class="python hljs">lr_clf.score(test_features, test_labels)</code></pre><br>
<p>  ,    (accuracy)  – 81%.</p><br>
<h1 id="benchmarki"></h1><br>
<p> :        – <strong>96.8</strong>. DistilBERT     ,       – ,   .    BERT     ,       (    downstream task).   DistilBERT'      <strong>90.7</strong>.   BERT'  <strong>94.9</strong>.</p><br>
<h1 id="noutbuk"></h1><br>
<p>   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"></a>     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow">Colab</a>.</p><br>
<p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">就这样！</font><font style="vertical-align: inherit;">第一次相识很好。</font><font style="vertical-align: inherit;">下一步是转到文档，然后尝试自己动手。</font><font style="vertical-align: inherit;">您还可以返回一点，从distilBERT转到BERT，看看它是如何工作的。</font></font></p><br>
<p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">感谢</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ClémentDelangue</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">，</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Victor Sanh</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">和Huggingface团队，他们对本指南的早期版本提供了反馈。</font></font></p><br>
<h1 id="avtory"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">s</font></font></h1><br>
<ul>
<li><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">原来</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">由</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">周杰伦Alammar</font></font></a></li>
<li><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">翻译</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">叶卡捷琳娜·斯米尔诺娃</font></font></a></li>
<li><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">编辑和排版</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Shkarin谢尔盖</font></font></a></li>
</ul></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN498132/index.html">PostgreSQL食谱：docker swarm中的自动故障转移和自动重新加入</a></li>
<li><a href="../zh-CN498134/index.html">如何在symfony 5 bundle中重用代码？第1部分。最小捆绑</a></li>
<li><a href="../zh-CN498136/index.html">与Polina Gurtova讨论Frontend的未来和现在。DUMP 2020组织者提出了一些重要问题。</a></li>
<li><a href="../zh-CN498138/index.html">大学学习后的生活：高等教育和工作已经存在时，我如何决定下一步的工作</a></li>
<li><a href="../zh-CN498140/index.html">复制这个。网络研讨会Apache Ignite和GridGain</a></li>
<li><a href="../zh-CN498146/index.html">需要注意的六种智能安全趋势</a></li>
<li><a href="../zh-CN498150/index.html">水管工程序员，或者一个泄漏的故事以及处理它的困难</a></li>
<li><a href="../zh-CN498154/index.html">4月20日至26日的在线免费IT活动日历</a></li>
<li><a href="../zh-CN498156/index.html">F＃，二进制图像的形态</a></li>
<li><a href="../zh-CN498158/index.html">远程马拉松周1：工作场所</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>