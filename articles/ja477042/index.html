<!doctype html>
<html class="no-js" lang="ja">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🔘 👆🏻 ⚕️ モンテカルロブラックジャック戦略の最適化 😄 🍼 👨🏽‍🚀</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="記事の翻訳は、機械学習コースの学生のために特別に準備されました。
 
 
 
 強化トレーニングには、人工知能の世界が必要でした。 AlphaGoとAlphaStarから始まって、以前は人間によって支配されていた活動の増加が、強化トレーニングに基づいてAIエージェントによって征服されるようになりまし...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>モンテカルロブラックジャック戦略の最適化</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/477042/"><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">記事の翻訳は、</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">機械学習</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">コースの学生のために特別に準備されました</font><font style="vertical-align: inherit;">。</font></font></i><br>
<hr><br>
<img src="https://habrastorage.org/webt/dj/mh/7h/djmh7hubq1mecnsb-gilezo8qyi.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
強化トレーニングには、人工知能の世界が必要でした。 AlphaGoと</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">AlphaStar</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">から始まって、</font><font style="vertical-align: inherit;">以前は人間によって支配されていた活動の増加が、強化トレーニングに基づいてAIエージェントによって征服されるようになりました。要するに、これらの成果は、最大の報酬を達成するために特定の環境でエージェントのアクションを最適化することに依存しています。</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GradientCrescentの</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">最後のいくつかの記事では</font><font style="vertical-align: inherit;">、</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">バンディット</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">システムの基本</font><font style="vertical-align: inherit;">や</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ポリシー</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ベースのアプローチ</font><font style="vertical-align: inherit;">から</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">マルコフ環境</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">での報酬ベースの動作の最適化</font><font style="vertical-align: inherit;">まで、強化学習のさまざまな基本的な側面を見てきました</font><font style="vertical-align: inherit;">。これらのすべてのアプローチには、環境に関する完全な知識が必要です。</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">たとえば、</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">動的プログラミング</font></a><font style="vertical-align: inherit;">では、考えられるすべての状態遷移の完全な確率分布が必要です。ただし、実際には、ほとんどのシステムを完全に解釈することはできず、複雑さ、固有の不確実性、または計算機能の制限により、確率分布を明示的に取得することはできません。類推として、気象学者のタスクを考えます-天気予報に関係する要素の数は非常に多く、確率を正確に計算することが不可能である場合があります。</font></font><a name="habracut"></a><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
そのような場合、モンテカルロなどのトレーニング方法が解決策です。モンテカルロという用語は、ランダムサンプリングアプローチを表すために一般的に使用されます。言い換えると、私たちは環境に関する知識を予測するのではなく、環境との相互作用の結果として得られる状態、アクション、および報酬の例示的なシーケンスを経験することにより、経験から学びます。これらの方法は、通常の操作中にモデルから返される報酬を直接観察して、その状態の平均値を判断することで機能します。興味深いことに、環境のダイナミクス（状態遷移の確率分布と見なす必要があります）の知識がなくても、報酬を最大化するための最適な動作を取得できます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
例として、12個のサイコロの出目を考えてみましょう。</font><font style="vertical-align: inherit;">これらのスローを単一の状態と見なして、これらの結果を平均化して、真の予測結果に近づけることができます。</font><font style="vertical-align: inherit;">サンプルが大きいほど、実際の期待される結果により正確に近づきます。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/xr/85/d6/xr85d6ugn6dbszaqh5b-nbq-usq.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">60ショットでの12のサイコロの平均予想額は41.57です。</font></font></i> <br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
このようなサンプルはkバンディットシステムでも実行されるため、サンプルに基づくこの種の推定は読者にとってなじみがあるように思えます。</font><font style="vertical-align: inherit;">さまざまな盗賊を比較する代わりに、モンテカルロ法を使用してマルコフ環境のさまざまなポリシーを比較し、作業が完了するまで特定のポリシーが実行されるときの状態の値を決定します。</font></font><br>
 <br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">状態値のモンテカルロ推定</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 強化学習のコンテキストでは、モンテカルロ法は、サンプル結果を平均することによってモデルの状態の有意性を評価する方法です。最終状態が必要なため、モンテカルロ法は本質的に一時的な環境に適用できます。この制限のため、モンテカルロ法は通常「自律型」と見なされ、すべての更新は最終状態に達した後に実行されます。迷路から抜け出す方法を見つけることとの簡単な類似性を与えることができます-自律的なアプローチは、迷路を通過するのにかかる時間を短縮するために得られた中間経験を使用する前に、エージェントを最後まで到達させます。一方、オンラインアプローチでは、エージェントは迷路の通過中にすでにその動作を常に変更します。おそらく、緑の廊下が行き止まりにつながることに気づき、それらを回避することにします。例えば。オンラインアプローチについては、次のいずれかの記事で説明します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
モンテカルロ法は次のように定式化できます。モンテカルロ法が</font></font><br>
<br>
<img src="https://habrastorage.org/webt/u4/kd/o_/u4kdo_uc3dj64rihmdjhnyj1r7y.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 どのように機能するかをよりよく理解するために、以下の状態遷移図を検討してください。各状態遷移の報酬は黒で表示され、0.5の割引係数が適用されます。状態の実際の値は別として、1回のスローの結果の計算に焦点を当てましょう。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/oc/he/n9/ochen91kvgbix5pgltzijpbozgg.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">状態遷移図。ステータス番号は赤で表示され、結果は黒になります。</font></font></i><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
 最終状態が0に等しい結果を返すと仮定して、最終状態（G5）から始めて、各状態の結果を計算してみましょう。割引率を0.5に設定したことに注意してください。これにより、後の州に重みが付けられます。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/uw/y6/qq/uwy6qq_amgfxpsyj2_3gpij498o.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 または、より一般的な方法：</font></font><br>
<br>
<img src="https://habrastorage.org/webt/k7/i4/ua/k7i4ua-g8vaph8w62-rfpc3r8sk.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 すべての結果をリストに保存しないようにするために、従来の勾配降下法といくつかの類似点がある方程式を使用して、モンテカルロ法で状態値を徐々に更新する手順を実行できます：</font></font><br>
<br>
<img src="https://habrastorage.org/webt/lv/tm/nk/lvtmnki_ff4csblmytygnnxjn6u.png"> <br>
 <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">増分モンテカルロ更新手順。 Sは状態、Vはその値、Gはその結果、Aはステップ値パラメーターです。</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
強化トレーニングの一環として、モンテカルロ法は、初回訪問またはすべての訪問として分類することもできます。</font><font style="vertical-align: inherit;">つまり、この2つの違いは、モンテカルロ更新の前に状態がパッセージに何度アクセスできるかです。</font><font style="vertical-align: inherit;">モンテカルロの最初の訪問方法では、作業が完了する前に各州に1回訪問した後の結果の平均値としてすべての州の値を推定します。</font><font style="vertical-align: inherit;">この記事では、比較的単純であるため、モンテカルロファーストビジットを使用します。</font></font><br>
 <br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">モンテカルロポリシー管理</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 モデルがポリシーを提供できない場合は、モンテカルロを使用して状態アクション値を評価できます。これは</font><font style="vertical-align: inherit;">、特定の状態における</font><font style="vertical-align: inherit;">各アクション</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">（q）の</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">意味の考え方</font><font style="vertical-align: inherit;">により、エージェントは不慣れな環境での観察からポリシーを自動的に作成できるため、</font><font style="vertical-align: inherit;">単に状態の意味よりも有用です</font><font style="vertical-align: inherit;">。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
より正式には、モンテカルロを使用して</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">q（s、a、pi）</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">を推定できます。これは</font><font style="vertical-align: inherit;">、状態s、アクションa、および後続のポリシー</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pi</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">から開始した場合の予想結果</font><font style="vertical-align: inherit;">です。モンテカルロ法は同じですが、特定の状態に対して実行されるアクションの次元が追加されています。状態とアクションのペア</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">（s、a）</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">状態</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">sが</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">訪問され</font><font style="vertical-align: inherit;">、アクション</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">a</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">が実行される</font><font style="vertical-align: inherit;">場合、パッセージ中に訪問</font><font style="vertical-align: inherit;">されます</font><font style="vertical-align: inherit;">。</font><font style="vertical-align: inherit;">同様に、価値行動の評価は、「最初の訪問」と「毎回の訪問」というアプローチを使用して実行できます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
動的プログラミングの場合と同様に、一般化された反復ポリシー（GPI）を使用して、ステートアクションの値を監視することからポリシーを形成できます。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/wi/pm/p5/wipmp58itbgocsyruulw0ccliay.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 政策評価と政策改善のステップを交互に行い、可能なすべての行動を確実に訪問するための調査を含めることにより、各条件に最適な政策を達成できます。</font><font style="vertical-align: inherit;">モンテカルロGPIの場合、このローテーションは通常、各パスの終了後に行われます。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/nf/b4/nw/nfb4nwijndtfex4ytwhdn2ujcyw.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">モンテカルロGPI</font></font></i><br>
 <br>
 <h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ブラックジャック戦略</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 モンテカルロ法がさまざまな状態値を評価するタスクで実際にどのように機能するかをよりよく理解するために、ブラックジャックゲームの段階的なデモを見てみましょう。</font><font style="vertical-align: inherit;">まず、ゲームのルールと条件を確認しましょう。</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">私たちはディーラーとのみ対戦し、他のプレイヤーはいないでしょう。</font><font style="vertical-align: inherit;">これにより、ディーラーの手を環境の一部と見なすことができます。</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">公称値と等しい数字のカードの値。</font><font style="vertical-align: inherit;">絵カードの値：ジャック、キング、クイーンは10です。エースの値は、プレーヤーの選択に応じて1または11になります。</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">両面とも2枚のカードを受け取ります。</font><font style="vertical-align: inherit;">プレーヤーの2枚のカードが表向きで横になり、ディーラーのカードの1枚も裏向きになります。</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ゲームの目標は、手札のカードの枚数が21以下であることです。</font><font style="vertical-align: inherit;">21より大きい値はバストです。両側の値が21の場合、ゲームは引き分けでプレイされます。</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">プレーヤーは自分のカードと最初のディーラーのカードを見た後、自分の手札のカードの合計に満足するまで、新しいカード（「まだ」）を受け取るか（「十分」）取らないかを選択できます。</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> その後、ディーラーは2枚目のカードを提示します-結果の金額が17未満の場合、彼は17ポイントに達するまでカードを取得する義務があり、その後カードを取得しません。 </font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
モンテカルロ法がこれらのルールでどのように機能するかを見てみましょう。</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ラウンド1。</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
あなたは合計19を獲得します。しかし、あなたは尻尾で運をつかまえ、チャンスを取り、3を獲得して、破産しようとします。</font><font style="vertical-align: inherit;">あなたが破産したとき、ディーラーは合計10のオープンカードを1枚しか持っていませんでした。これは次のように表すことができます。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/4o/di/sl/4odisljnoapgkqblzsixkr2a4_4.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
破産した場合、ラウンドの報酬は-1でした。</font><font style="vertical-align: inherit;">次のフォーマットを使用して、この値を最後から2番目の状態の戻り結果として設定してみましょう[エージェントの金額、ディーラーの金額、エース？]：</font></font><br>
<br>
<img src="https://habrastorage.org/webt/2q/er/g4/2qerg4mooln9blj9sfqo1ndzxbo.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
さて、私たちは運が悪くなりました。</font><font style="vertical-align: inherit;">次のラウンドに移りましょう。</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ラウンド2。</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
合計19を入力します。今回は停止することにします。</font><font style="vertical-align: inherit;">ディーラーは13をダイヤルし、カードを1つ取り、壊れました。</font><font style="vertical-align: inherit;">最後から2番目の状態は次のように説明できます。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/al/th/dv/althdvyqwkhwqd5cj2vvzcjbiam.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
このラウンドで受け取った状態と報酬について説明しましょう。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/rm/mv/n8/rmmvn88mg7za9gyrtvrvohywmwc.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
パッセージの終わりで、計算された結果を使用して、このラウンドのすべての状態の値を更新できます。</font><font style="vertical-align: inherit;">割引係数を1にすると、以前の状態遷移で行ったように、新しい報酬を手作業で配布するだけです。</font><font style="vertical-align: inherit;">状態</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">V（19、10、いいえ）は</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">以前に-1を返したため、期待される戻り値を計算し、それを状態に割り当て</font></font><br>
<br>
<img src="https://habrastorage.org/webt/u0/um/q9/u0umq9dpg6qw420ecz44cqfp9mk.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ます</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font><i><font style="vertical-align: inherit;">例として、ブラックジャックを使用したデモンストレーションのための最終状態値</font></i><font style="vertical-align: inherit;">。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">実装</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pythonを使用してゲームで考えられるすべての状態値（または手持ちのさまざまな組み合わせ）を見つけるために、最初に訪れるモンテカルロ法を使用してブラックジャックゲームを書きましょう。私たちのアプローチは</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、Sudharsan et。 al。</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。通常どおり、</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GitHubの</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">記事からすべてのコードを見つけることができます</font><font style="vertical-align: inherit;">。</font></font><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
実装を簡単にするために、OpenAIのgymを使用します。環境を、最小限のコードでブラックジャックゲームを開始するためのインターフェースと考えてください。これにより、強化学習の実装に集中できます。便利なことに、状態、アクション、および報酬について収集されたすべての情報は</font><font style="vertical-align: inherit;">、現在のゲームセッション中に蓄積</font><font style="vertical-align: inherit;">された</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">「観測」</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">変数</font><font style="vertical-align: inherit;">に</font><font style="vertical-align: inherit;">格納され</font><font style="vertical-align: inherit;">ます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
結果を取得して収集するために必要なすべてのライブラリをインポートすることから始めましょう。 </font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">import</span> gym
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">from</span> mpl_toolkits.mplot3d <span class="hljs-keyword">import</span> Axes3D
<span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> defaultdict
<span class="hljs-keyword">from</span> functools <span class="hljs-keyword">import</span> partial<font></font>
%matplotlib inline<font></font>
plt.style.use(‘ggplot’)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
次に、</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ジム</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">環境</font><font style="vertical-align: inherit;">を</font><font style="vertical-align: inherit;">初期化して</font><font style="vertical-align: inherit;">、エージェントのアクションを調整するポリシーを定義し</font><font style="vertical-align: inherit;">ましょう</font><font style="vertical-align: inherit;">。</font><font style="vertical-align: inherit;">実際には、手札が19以上になるまでカードを引き続け、その後停止します。</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-comment">#Observation here encompassess all data about state that we need, as well as reactions to it</span><font></font>
env = gym.make(‘Blackjack-v0’)<font></font>
<span class="hljs-comment">#Define a policy where we hit until we reach 19.</span>
<span class="hljs-comment"># actions here are 0-stand, 1-hit</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sample_policy</span>(<span class="hljs-params">observation</span>):</span><font></font>
  score, dealer_score, usable_ace = observation<font></font>
  <span class="hljs-keyword">return</span> <span class="hljs-number">0</span> <span class="hljs-keyword">if</span> score &gt;= <span class="hljs-number">19</span> <span class="hljs-keyword">else</span> <span class="hljs-number">1</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ポリシーを使用してパスデータを生成する方法を定義しましょう。</font><font style="vertical-align: inherit;">ステータス、実行されたアクション、アクションの報酬に関する情報を保存します。</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">generate_episode</span>(<span class="hljs-params">policy, env</span>):</span>
<span class="hljs-comment"># we initialize the list for storing states, actions, and rewards  </span><font></font>
  states, actions, rewards = [], [], []<font></font>
<span class="hljs-comment"># Initialize the gym environment  </span><font></font>
  observation = env.reset()  <font></font>
  <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:
<span class="hljs-comment"># append the states to the states list  </span><font></font>
    states.append(observation)<font></font>
<span class="hljs-comment"># now, we select an action using our sample_policy function and append the action to actions list</span><font></font>
    action = sample_policy(observation)<font></font>
    actions.append(action)<font></font>
<span class="hljs-comment"># We perform the action in the environment according to our sample_policy, move to the next state</span><font></font>
    observation, reward, done, info = env.step(action)<font></font>
    rewards.append(reward)<font></font>
<span class="hljs-comment"># Break if the state is a terminal state (i.e. done)</span>
    <span class="hljs-keyword">if</span> done:
      <span class="hljs-keyword">break</span>
  <span class="hljs-keyword">return</span> states, actions, rewards</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
最後に、最初の訪問でモンテカルロ予測関数を定義しましょう。</font><font style="vertical-align: inherit;">最初に、現在の状態値を格納するための空の辞書と、異なるパスの各状態のレコード数を格納する辞書を初期化します。</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">first_visit_mc_prediction</span>(<span class="hljs-params">policy, env, n_episodes</span>):</span>
<span class="hljs-comment"># First, we initialize the empty value table as a dictionary for storing the values of each state</span><font></font>
  value_table = defaultdict(float)<font></font>
  N = defaultdict(int)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
パスごとに、</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">generate_episode</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">メソッドを呼び出して</font><font style="vertical-align: inherit;">、状態の値と、状態の発生後に受け取った報酬に関する情報を取得します。また、変数を初期化して、増分結果を保存します。次に、通過中に訪問した各状態の報酬と現在の状態の値を取得し、変数の戻り値をステップごとの報酬の値だけ増やします。</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> range(n_episodes):
<span class="hljs-comment"># Next, we generate the epsiode and store the states and rewards</span><font></font>
  states, _, rewards = generate_episode(policy, env)<font></font>
  returns = <span class="hljs-number">0</span>
<span class="hljs-comment"># Then for each step, we store the rewards to a variable R and states to S, and we calculate</span>
  <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> range(len(states) — <span class="hljs-number">1</span>, <span class="hljs-number">-1</span>, <span class="hljs-number">-1</span>):<font></font>
    R = rewards[t]<font></font>
    S = states[t]<font></font>
    returns += R<font></font>
<span class="hljs-comment"># Now to perform first visit MC, we check if the episode is visited for the first time, if yes,</span>
<span class="hljs-comment">#This is the standard Monte Carlo Incremental equation.</span>
<span class="hljs-comment"># NewEstimate = OldEstimate+StepSize(Target-OldEstimate)</span>
    <span class="hljs-keyword">if</span> S <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> states[:t]:<font></font>
      N[S] += <span class="hljs-number">1</span><font></font>
      value_table[S] += (returns — value_table[S]) / N[S]<font></font>
<span class="hljs-keyword">return</span> value_table</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
モンテカルロの最初の訪問を実装しているので、1つのパスで1つの州を訪問します。したがって、状態ディクショナリに対して条件チェックを実行して、状態が訪問されたかどうかを確認します。この条件が満たされている場合、モンテカルロ法を使用して状態値を更新するために以前に定義された手順を使用して新しい値を計算し、この状態の観測数を1増やします。次に、最終的に結果の平均値を取得するために次のパスのプロセスを繰り返します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
私たちが持っているものを実行して結果を見てみましょう！</font></font><br>
<br>
<pre><code class="python hljs">value = first_visit_mc_prediction(sample_policy, env, n_episodes=<span class="hljs-number">500000</span>)
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">10</span>):<font></font>
  print(value.popitem())</code></pre> <br>
<img src="https://habrastorage.org/webt/xc/ow/al/xcowalpr-34tgxtf0gq8zlfllgg.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ブラックジャックの手でのさまざまな組み合わせの状態値を示すサンプルの結論。</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
5000パスのモンテカルロ観測を続け、プレーヤーとディーラーの手でのあらゆる組み合わせの値を表す状態値の分布を構築できます。</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_blackjack</span>(<span class="hljs-params">V, ax1, ax2</span>):</span>
  player_sum = np.arange(<span class="hljs-number">12</span>, <span class="hljs-number">21</span> + <span class="hljs-number">1</span>)<font></font>
  dealer_show = np.arange(<span class="hljs-number">1</span>, <span class="hljs-number">10</span> + <span class="hljs-number">1</span>)         <font></font>
  usable_ace = np.array([<span class="hljs-literal">False</span>, <span class="hljs-literal">True</span>])<font></font>
  state_values = np.zeros((len(player_sum), len(dealer_show), len(usable_ace)))<font></font>
  <span class="hljs-keyword">for</span> i, player <span class="hljs-keyword">in</span> enumerate(player_sum):
    <span class="hljs-keyword">for</span> j, dealer <span class="hljs-keyword">in</span> enumerate(dealer_show):
      <span class="hljs-keyword">for</span> k, ace <span class="hljs-keyword">in</span> enumerate(usable_ace):<font></font>
        state_values[i, j, k] = V[player, dealer, ace]<font></font>
  X, Y = np.meshgrid(player_sum, dealer_show)<font></font>
  ax1.plot_wireframe(X, Y, state_values[:, :, <span class="hljs-number">0</span>])   <font></font>
  ax2.plot_wireframe(X, Y, state_values[:, :, <span class="hljs-number">1</span>])
  <span class="hljs-keyword">for</span> ax <span class="hljs-keyword">in</span> ax1, ax2:    ax.set_zlim(<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>)<font></font>
    ax.set_ylabel(‘player sum’)<font></font>
    ax.set_xlabel(‘dealer sum’)<font></font>
    ax.set_zlabel(‘state-value’)<font></font>
fig, axes = pyplot.subplots(nrows=<span class="hljs-number">2</span>, figsize=(<span class="hljs-number">5</span>, <span class="hljs-number">8</span>),subplot_kw={<span class="hljs-string">'projection'</span>: <span class="hljs-string">'3d'</span>})<font></font>
axes[<span class="hljs-number">0</span>].set_title(<span class="hljs-string">'state-value distribution w/o usable ace'</span>)<font></font>
axes[<span class="hljs-number">1</span>].set_title(<span class="hljs-string">'state-value distribution w/ usable ace'</span>)<font></font>
plot_blackjack(value, axes[<span class="hljs-number">0</span>], axes[<span class="hljs-number">1</span>])
</code></pre><br>
<img src="https://habrastorage.org/webt/db/7t/he/db7thebzqawmtv_jemoprsaeqvg.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ブラックジャックでのさまざまな組み合わせのステータス値の視覚化。</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
それでは、学んだことを要約しましょう。</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">サンプリングベースの学習方法により、遷移のダイナミクスなしで、単にサンプリングすることで、状態値とアクション状態値を評価できます。</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">モンテカルロアプローチは、モデルのランダムサンプリングに基づいており、モデルから返される報酬を観察し、通常の操作中に情報を収集して、その状態の平均値を決定します。 </font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">モンテカルロ法を使用すると、一般化された反復ポリシーが可能です。 </font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ブラックジャックのプレーヤーとディーラーの手で可能なすべての組み合わせの価値は、最適化された戦略への道を開く複数のモンテカルロシミュレーションを使用して推定できます。</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
これで、モンテカルロ法の紹介は終わりです。</font><font style="vertical-align: inherit;">次の記事では、時間差学習という形式の指導方法に移ります。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">出典：</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
サットンら </font><font style="vertical-align: inherit;">その他、強化学習</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ホワイト他 </font><font style="vertical-align: inherit;">アル、強化学習の基礎、アルバータ大学</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
シルバ他 </font><font style="vertical-align: inherit;">al、強化学習、UCL </font></font><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">プラット他 </font><font style="vertical-align: inherit;">アル、ノースイースター大学</font></font></a> <br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
それだけです。</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">コースで</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">お会いしましょう</font><font style="vertical-align: inherit;">！</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../ja477022/index.html">お手伝いしましょうか？どのように私たちを助けることができますか？</a></li>
<li><a href="../ja477026/index.html">第7回年次JetBrainsハッカソン</a></li>
<li><a href="../ja477032/index.html">ブロックチェーンからDAGへ：仲介者を取り除く</a></li>
<li><a href="../ja477038/index.html">初心者に最適なプログラミング言語</a></li>
<li><a href="../ja477040/index.html">Gartner Chart 2019：これらすべての流行語は何ですか？</a></li>
<li><a href="../ja477044/index.html">統合情報システムのEnd-2-Endテストの自動化。パート2.技術</a></li>
<li><a href="../ja477046/index.html">Raiffeisenbank 28/11 + Broadcastでの.Meetup</a></li>
<li><a href="../ja477048/index.html">資本金550億ドルの会社が取引所を離れることを考えた理由</a></li>
<li><a href="../ja477050/index.html">ビデオ監視とクラウドのためのブラックフライデー2019。</a></li>
<li><a href="../ja477052/index.html">Reactor、WebFlux、Kotlinコルーチン、または簡単な例による非同期</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>