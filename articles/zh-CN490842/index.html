<!doctype html>
<html class="no-js" lang="zh-CN">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🧓 🥙 👩🏻 图片中的GPT-2（变形金刚语言模型的可视化） 👩🏿‍🎨 👨‍💼 👩🏾‍🚒</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="在2019年，我们见证了机器学习的卓越应用。OpenAI GPT-2模型具有出色的书写连贯和情感文本的能力，这优于我们对现代语言模型可以生成的内容的理解。GPT-2并不是特别新颖的体系结构，它使人联想到Transformer-Decoder（仅解码器的Transformer）。GPT-2之间的区别在...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>图片中的GPT-2（变形金刚语言模型的可视化）</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/490842/"><p><img src="https://habrastorage.org/webt/1k/58/ea/1k58ea5w9egy2dc5z3jtsiip3sc.png" alt="openAI-GPT-2-3"></p><br>
<p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">在2019年，我们见证了机器学习的卓越应用。</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">OpenAI GPT-2模型具有</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">出色的书写连贯和情感文本的能力，这优于我们对现代语言模型可以生成的内容的理解。</font><font style="vertical-align: inherit;">GPT-2并不是特别新颖的体系结构，它使人联想到Transformer-Decoder（仅解码器的Transformer）。</font><font style="vertical-align: inherit;">GPT-2之间的区别在于，它是基于Transformer的真正庞大的语言模型，并在令人印象深刻的数据集上进行了训练。</font><font style="vertical-align: inherit;">在本文中，我们将研究允许我们获得这种结果的模型的体系结构：我们将详细研究自我注意层以及解码Transformer用于超出语言建模的任务的用途。</font></font></p><a name="habracut"></a><br>
<p><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">内容</font></font></strong></p><br>
<ul>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">第1部分：GPT-2和语言建模</font></font></a><br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">什么是语言模型</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">语言建模变压器</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">与BERT的不同之处</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">变压器座演变</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">神经外科快速课程：深入了解GPT-2</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">深入了解</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">第一部分结束：GPT-2女士们，先生们：</font></font></li>
</ul></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">第2部分：内部注意的可视化</font></font></a><br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">内心的关注（不掩饰）</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1-创建查询，键和值向量</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2-系数计算</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3-求和</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">隐蔽内心注意的可视化</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GPT-2中的变相内心注意</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">你做到了！</font></font></li>
</ul></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">第3部分：超越语言建模</font></font></a><br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">机器翻译</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">总结</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">转移训练</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">音乐产生</font></font></li>
</ul></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">结论</font></font></a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">用料</font></font></a></li>
</ul><br>
<h1 id="chast-1-gpt-2-i-yazykovoe-modelirovaniea-namepart_1a"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">第1部分：GPT-2和语言建模</font></font></h1><br>
<p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">什么是语言建模？</font></font></p><br>
<h2 id="chto-takoe-yazykovaya-model"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">什么是语言模型</font></font></h2><br>
<p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Word2vec  </a>  ,     –  ,    ,           .     –   ,        .</p><br>
<p><img src="https://habrastorage.org/webt/98/yv/br/98yvbr3q4hpjs8hfsg1rajz3n9m.png" alt="快捷键键盘"></p><br>
<p>    ,  GPT-2        ,     ,  ,     . GPT-2        40  (WebText),  OpenAI        .      ,  ,  SwiftKey,   78 ,         GPT-2   500      ,     GPT-2 –  13   (      6,5 ).</p><br>
<p><img src="https://habrastorage.org/webt/md/qg/ve/mdqgveo0tsyxqapfzqpomuu5nzo.png" alt="gpt2尺寸"></p><br>
<p>    GPT-2   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow">AllenAI GPT-2 Explorer</a>.   GPT-2        (   ),     .</p><br>
<h2 id="transformery-dlya-yazykovogo-modelirovaniya">   </h2><br>
<p>     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">  </a>,         –     ..  .       – ,   -      .</p><br>
<p><img src="https://habrastorage.org/webt/sv/tx/hm/svtxhmcbpksa7rjn0wb0o2jj_wc.png" alt="变压器-编码器-解码器"></p><br>
<p>    ,              ,    ,   ,                (                AlphaStar).</p><br>
<p><img src="https://habrastorage.org/webt/j4/sb/4r/j4sb4rysituzcqsafgktspu0pa8.png" alt="gpt-2-transformer-xl-bert-3"></p><br>
<p>      ? ,           GPT-2  :</p><br>
<p><img src="https://habrastorage.org/webt/gg/og/nr/ggognri38aojdzkkzazvnetxfvy.png" alt="gpt2-sizes-hyperparameters-3"></p><br>
<h2 id="odno-otlichie-ot-berta">   BERT'</h2><br>
<blockquote><strong>  :</strong><br>
         ,     .</blockquote><p> GPT-2      . BERT , ,   .         .       ,  GPT-2,      ,       .   ,     GPT-2    :</p><br>
<p><img src="https://habrastorage.org/webt/so/pv/tj/sopvtjhr-crr3rgtf_zaoqomgck.gif" alt="gpt-2-output"></p><br>
<p>   :  ,     ,     .           .    «» (auto-regression)    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"> RNN   </a>. </p><br>
<p><img src="https://habrastorage.org/webt/un/7x/ir/un7xirwndekafrdeixsce0ocmmm.gif" alt="gpt-2-autoregression-2"></p><br>
<p>GPT-2      TransformerXL  XLNet    . BERT .     .  , BERT            . XLNet   ,          .</p><br>
<h2 id="evolyuciya-bloka-transformera">  </h2><br>
<p><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"> </a>     .</p><br>
<h3 id="blok-enkodera"> </h3><br>
<p> –   :</p><br>
<p><img src="https://habrastorage.org/webt/f_/ym/xb/f_ymxbcpvszucrhogiurzwqypzk.png" alt="transformer-encoder-block-2"></p><br>
<p><em>             (, 512 ).      ,        .</em></p><br>
<h3 id="blok-dekodera"> </h3><br>
<p> –   ,      .        :</p><br>
<p><img src="https://habrastorage.org/webt/gh/so/u6/ghsou6icfirnjbj1abkj7ouqohg.png" alt="transformer-decoder-block-2"></p><br>
<p>            ,          [mask] ,   BERT',             ,     ,     .</p><br>
<p>, ,      #4,   ,         :</p><br>
<p><img src="https://habrastorage.org/webt/ta/9r/td/ta9rtdiimdxkwmwpmdxy2ig-0h8.png" alt="transformer-decoder-block-self-attention-2"></p><br>
<p>     ,   BERT,     GPT-2.         .       :</p><br>
<p><img src="https://habrastorage.org/webt/43/cq/pd/43cqpdiypmyuccr1gznrhr4tioo.png" alt="self-attention-and-masked-self-attention"></p><br>
<h3 id="blok-dekodirovaniya"> </h3><br>
<p>  ,  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow">«Generating Wikipedia by Summarizing Long Sequences»</a>     ,    :    .      «-».            6   :</p><br>
<p><img src="https://habrastorage.org/webt/rd/l7/k-/rdl7k--_z3kg4ajejaycksxqpt0.png" alt="transformer-decoder-intro"></p><br>
<p><em>  .       ,     . ,       4000     –      512   .</em></p><br>
<p>       ,  ,       .       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow">«         »</a>,         /    .</p><br>
<p> GPT-2  OpenAI     .</p><br>
<h2 id="ekspress-kurs-po-neyrohirurgii-zaglyadyvaya-vnutr-gpt-2">-  :   GPT-2</h2><br>
<blockquote> ,   ,       .  ,  ,      , , . (Budgie)</blockquote><p>   GPT-2       ,   .</p><br>
<p><img src="https://habrastorage.org/webt/he/ig/hj/heighjb23joyolbax5b7__fvj6a.png" alt="gpt-2-layers-2"></p><br>
<p><em>GPT-2   1024 .           .</em></p><br>
<p>     GPT-2 –      (     )     (),        (..    ).          ,      (       &lt;|endoftext|&gt;;     &lt;|s|&gt;).</p><br>
<p><img src="https://habrastorage.org/webt/ho/yz/o0/hoyzo0yvmbi3gf-1awm__udx0fi.gif" alt="gpt2-simple-output-2"></p><br>
<p>      ,       .          ,    (score)   –  ,    (50    GPT-2).          – «the».    -  –  ,  ,         ,      ,     –       .      . GPT-2   top-k,      ,      ,      (,  ,   top-k = 1).</p><br>
<p>              :</p><br>
<p><img src="https://habrastorage.org/webt/hg/2x/ps/hg2xpsd1kuverowaqcd4z_w--ki.gif" alt="gpt-2-simple-output-3"></p><br>
<p> ,        .   GPT-2              (          ). GPT-2       .</p><br>
<h2 id="zaglyanem-poglubzhe"> </h2><br>
<h3 id="kodirovanie-vhoda"> </h3><br>
<p>   .   .     NLP-,     ,          –   ,    .</p><br>
<p><img src="https://habrastorage.org/webt/qo/ml/jl/qomljlikh-rqxyud6otmaxrtyum.png" alt="gpt2-token-embeddings-wte-2"></p><br>
<p><em>    –  ,     -   .            GPT-2.       768  /.</em></p><br>
<p>,       &lt;|s|&gt;   .        ,      – ,         .     ,        1024   . </p><br>
<p><img src="https://habrastorage.org/webt/zp/mu/cg/zpmucgftth4mabqo60oolwdohey.png" alt="gpt2-positional-encoding"></p><br>
<p>          .      ,     GPT-2.</p><br>
<p><img src="https://habrastorage.org/webt/me/r8/bx/mer8bx3ofjqcpydqnfsvo1fk4ok.png" alt="gpt2-input-embedding-positional-encoding-3"></p><br>
<p><em>                 #1.</em></p><br>
<h3 id="puteshestvie-vverh-po-steku">   </h3><br>
<p>     ,       ,      .       ,            .      ,       ,      .</p><br>
<p><img src="https://habrastorage.org/webt/ij/9e/3a/ij9e3alc1pc5ihgpd8njpbh2-vy.png" alt="gpt2-transformer-block-vectors-2"></p><br>
<h3 id="obzor-vnutrennego-vnimaniya">  </h3><br>
<p>      . ,     :</p><br>
<blockquote>    ,   <strong></strong> ,   ,  <strong> </strong>  <strong> </strong>.</blockquote><p>     ,      .         ,    .     ,    , :</p><br>
<ul>
<li><strong></strong>   ;</li>
<li><strong> </strong>      («   »);</li>
<li><strong> </strong>     .</li>
</ul><br>
<p>     :          ,      ,       (   ).      ,         ,        .</p><br>
<p> ,           «a robot»     «it». ,       ,        ,    .</p><br>
<p><img src="https://habrastorage.org/webt/yx/vw/ou/yxvwoujcadhlalw2xgq3dnoezjq.png" alt="gpt2-self-attention-example-2"></p><br>
<h3 id="algoritm-vnutrennego-vnimaniya">  </h3><br>
<p>         .     :</p><br>
<ul>
<li><strong></strong> –    ,          (  ).      ,     ;</li>
<li><strong></strong> –       .        ;</li>
<li><strong></strong> –     ;      ,   ,        .</li>
</ul><br>
<p><img src="https://habrastorage.org/webt/ds/30/gu/ds30gubjhv2r7ubicxiq-idgdlg.png" alt="self-attention-example-folders-3"></p><br>
<p>          .  –     ,   .       .   ,      –  .           ,      .</p><br>
<p>              (:      ).</p><br>
<p><img src="https://habrastorage.org/webt/br/mp/xd/brmpxdc2ngiqgrkmrgn1wb5aud4.png" alt="self-attention-example-folders-scores-3"></p><br>
<p>      ,     .</p><br>
<p><img src="https://habrastorage.org/webt/xv/7n/ez/xv7nezgfhygrknpndfsptjwucpe.png" alt="gpt2-value-vector-sum"></p><br>
<p>       ,  50%      «robot», 30%   «a»  19% –   «it».         .           .</p><br>
<h3 id="vyhod-modeli"> </h3><br>
<p>       (          ),      .</p><br>
<p><img src="https://habrastorage.org/webt/vz/tt/mf/vzttmfwinmu2z7v3m0hkxijm5o8.png" alt="gpt2-output-projection-2"></p><br>
<p>  ,           .          .</p><br>
<p><img src="https://habrastorage.org/webt/fg/ib/yq/fgibyqwocamlo1cax73ck1qxrwg.png" alt="gpt2-output-scores-2"></p><br>
<p>        (top_k = 1).    ,        .         ,      ,      (          ).   –  top_k  40:       40    .</p><br>
<p><img src="https://habrastorage.org/webt/ji/zl/mk/jizlmk0ywrvlljh6ynxq_eqacjs.png" alt="gpt2-output"></p><br>
<p> ,        .       ,       (1024 )       .</p><br>
<h2 id="konec-pervoy-chasti-gpt-2-damy-i-gospoda">  : GPT-2,   </h2><br>
<p>,    ,   GPT-2.    ,       ,       .     ,                 (  TransformerXL  XLNet).</p><br>
<p>    ,       :</p><br>
<ul>
<li>«»  «»        ;     GPT-2     (Byte Pair Encoding)     .  ,      .</li>
<li>    GPT-2    / (inference/evaluation mode).         .                .           (512),      1,     .</li>
<li>     /       .        .</li>
<li>      ,    .        <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Transformer  </a>,          .</li>
<li>          .      «zoom in», : </li>
</ul><br>
<p><img src="https://habrastorage.org/webt/q_/kq/35/q_kq35nsjwiycnwcafi8ctxl_tm.png" alt="zoom-in"></p><br>
<h1 id="chast-2-vizualizaciya-vnutrennego-vnimaniyaa-namepart_2a"> 2:   </h1><br>
<p>             ,   «it»:</p><br>
<p><img src="https://habrastorage.org/webt/ss/7h/dk/ss7hdk8hwx43lbghil2t3os95bm.png" alt="gpt2-self-attention-1-2"></p><br>
<p>       ,   .      ,      ,       .              ,      ,      .</p><br>
<h2 id="vnutrennee-vnimanie-bez-maskirovaniya">  ( )</h2><br>
<p>     ,     .    ,   4   .</p><br>
<p>       :</p><br>
<ol>
<li>  ,      ;</li>
<li>             ;</li>
<li>          .</li>
</ol><br>
<p><img src="https://habrastorage.org/webt/ab/l8/ax/abl8ax5sj4nuqdq8jihtayycpvc.png" alt="self-attention-summary"></p><br>
<h2 id="1--sozdanie-vektorov-zaprosa-klyucha-i-znacheniya">1 –   ,   </h2><br>
<p>   .          .      .              (  «» ):</p><br>
<p><img src="https://habrastorage.org/webt/qc/7c/0l/qc7c0ll_eqaebzsk5jqluip1j7k.png" alt="self-attention-1"></p><br>
<p><em>      ,            WQ, WK, WV</em></p><br>
<h2 id="2--podschet-koefficientov">2 –  </h2><br>
<p>,     ,           №2:                   .</p><br>
<p><img src="https://habrastorage.org/webt/47/tl/vm/47tlvmotgdunt0od7rzfptulwxi.png" alt="self-attention-2"></p><br>
<p><em> ( )            ,     </em></p><br>
<h2 id="3--summirovanie">3 – </h2><br>
<p>       .            ,     .</p><br>
<p><img src="https://habrastorage.org/webt/nz/aa/sl/nzaaslrrr6etmrs-qlkhlb9o6oc.png" alt="self-attention-3-2"></p><br>
<p>&nbsp;<em>         </em></p><br>
<p>  ,       –   ,            .</p><br>
<p>       ,   ,         .          (   ).</p><br>
<h2 id="vizualizaciya-maskirovannogo-vnutrennego-vnimaniya">   </h2><br>
<p>,     ,       ,      .        №2. ,           .       .        ,      :</p><br>
<p><img src="https://habrastorage.org/webt/di/h6/vx/dih6vxt0pa-5ipj_qtruxbmbcae.png" alt="masked-self-attention-2"></p><br>
<p>      ,    (attention mask). , ,     («robot must obey orders»).         4 :     ( ,    –   ). ..    ,      4    ,      (  4 )   .</p><br>
<p><img src="https://habrastorage.org/webt/ot/_d/k9/ot_dk95ismt5lcax-xeomlzxigu.png" alt="transformer-decoder-attention-mask-dataset"></p><br>
<p>     ,      .    ,   ,        ( ),    :</p><br>
<p><img src="https://habrastorage.org/webt/2_/kz/ws/2_kzws5vewj2dvdy-rtcgwjxdd8.png" alt="queries-keys-attention-mask"></p><br>
<p>   «»    .   ,    ,     – (-inf)      (, -1   GPT-2):</p><br>
<p><img src="https://habrastorage.org/webt/86/aw/oh/86awoh2nlrcsdh3_hveet0rh3vw.png" alt="transformer-attention-mask"></p><br>
<p>,      ,   ,       :</p><br>
<p><img src="https://habrastorage.org/webt/cr/-o/ku/cr-okuz6tuuuaeht--j8mjblnqo.png" alt="变压器注意屏蔽分数softmax"></p><br>
<p>    :</p><br>
<ul>
<li>        ( №1),      («robot»), 100%      .</li>
<li>        ( №2),    («robot must»),     «must» 48%     «robot»  52%    «must».</li>
<li> ..</li>
</ul><br>
<h2 id="maskirovannoe-vnutrennee-vnimanie-v-gpt-2">    GPT-2</h2><br>
<p>       GPT-2.</p><br>
<h3 id="vremya-ocenki-obrabotka-odnogo-tokena-za-raz"> :     </h3><br>
<p>   ,  GPT-2     ,    .    ,           ,          ,    .</p><br>
<p>       (  &lt;|s|&gt;).</p><br>
<p><img src="https://habrastorage.org/webt/oc/r2/ps/ocr2ps037_1vvr2uajz1idbe76g.png" alt="gpt2-self-attention-qkv-1-2"></p><br>
<p>GPT-2       «a».             :</p><br>
<p><img src="https://habrastorage.org/webt/0e/wf/vx/0ewfvx7snx6azpvwietxqfd9zzg.png" alt="gpt2-self-attention-qkv-2-2"></p><br>
<p>  ,     «robot»,      ,      «a» –    ,    :</p><br>
<p><img src="https://habrastorage.org/webt/re/qe/ff/reqeffjn9neoql-qqpiqp8hwmjq.png" alt="gpt2-self-attention-qkv-3-2"></p><br>
<h3 id="vnutrennee-vnimanie-gpt-2-1--sozdanie-vektorov-zaprosa-klyucha-i-znacheniya">  GPT-2: 1 –   ,   </h3><br>
<p>,     «it».    ,         «it» +      #9:</p><br>
<p><img src="https://habrastorage.org/webt/k1/ol/ee/k1oleegivyrvsvcx-glsjjxdt14.png" alt="gpt2-self-attention-1"></p><br>
<p>         (     ),       ,   .</p><br>
<p><img src="https://habrastorage.org/webt/ws/nc/1k/wsnc1kowztsdydbbgkevy7ycene.png" alt="gpt2-self-attention-2"></p><br>
<p><em>            (bias vector),   </em></p><br>
<p>    ,       ,      «it».</p><br>
<p><img src="https://habrastorage.org/webt/od/oc/ag/odocagmiv-m420l-3wy8sdila4w.png" alt="gpt2-self-attention-3"></p><br>
<p><em>       (     )   ,      </em></p><br>
<h3 id="vnutrennee-vnimanie-gpt-2-15--razdelenie-na-golovy-vnimaniya">  GPT-2: 1.5 –   «» </h3><br>
<p>        ,   «» .        .           (Q),  (K)   (V).   «»  –       .   GPT-2   12 «» ,      :</p><br>
<p><img src="https://habrastorage.org/webt/zt/qf/wr/ztqfwr4l3kaoq243xhlleg4ro2i.png" alt="gpt2自我注意分割注意力头1"></p><br>
<p>    ,     «» .    «» ,        (      12 «» ):</p><br>
<p><img src="https://habrastorage.org/webt/lu/8a/5c/lu8a5cqdztpnonylf6jfxpopngg.png" alt="gpt2自我注意分割注意力头2"></p><br>
<h3 id="vnutrennee-vnimanie-gpt-2-2--podschet-koefficientov">  GPT-2: 2 –  </h3><br>
<p>       (  ,      «»        ):</p><br>
<p><img src="https://habrastorage.org/webt/ab/0b/nm/ab0bnmrgkb-3knzrtitk2e64opi.png" alt="gpt2-自我注意得分"></p><br>
<p>        (    «»  #1   ):</p><br>
<p><img src="https://habrastorage.org/webt/zc/ll/nt/zcllnthmdvddhniljejpbgke8is.png" alt="gpt2-自我注意得分2"></p><br>
<h3 id="vnutrennee-vnimanie-gpt-2-3--summirovanie">  GPT-2: 3 – </h3><br>
<p>   ,         , ,      «»  #1:</p><br>
<p><img src="https://habrastorage.org/webt/dv/rf/-i/dvrf-ik3yxkidhzgihbuuqv4sww.png" alt="gpt2-self-attention-multihead-sum-1"></p><br>
<h3 id="vnutrennee-vnimanie-gpt-2-35--obedinenie-golov-vnimaniya">  GPT-2: 3.5 –  «» </h3><br>
<p>  «»  ,  ,      :</p><br>
<p><img src="https://habrastorage.org/webt/p9/jg/bg/p9jgbgr9-c8lzl-zjnqfdortvxe.png" alt="gpt2-self-attention-merge-heads-1"></p><br>
<p>        .         .</p><br>
<h3 id="vnutrennee-vnimanie-gpt-2-4--proecirovanie">  GPT-2: 4 – </h3><br>
<p>   ,         ,        .       ,    «»       :</p><br>
<p><img src="https://habrastorage.org/webt/mc/mz/qq/mcmzqqb80hq14cyooubulhh3hhu.png" alt="gpt2-self-attention-project-1"></p><br>
<p> ,   ,      :</p><br>
<p><img src="https://habrastorage.org/webt/af/tl/0o/aftl0oqxhho-m6uph1uqer2a4_c.png" alt="gpt2-self-attention-project-2"></p><br>
<h3 id="polnosvyaznaya-neyronnaya-set-gpt-2-sloy-1">   GPT-2:  #1</h3><br>
<p>   –  ,       ,         .     .    4     (  GPT-2   768,      768*4 = 3072 ).    ?      (   512   #1 – 2048). ,           ,      .</p><br>
<p><img src="https://habrastorage.org/webt/gu/4d/ci/gu4dciafj9oa7wcbnt1pwtpfcu8.gif" alt="gpt2-mlp1"></p><br>
<p><em>(   )</em></p><br>
<h3 id="polnosvyaznaya-neyronnaya-set-gpt-2-proecirovanie-na-razmernost-modeli">   GPT-2:    </h3><br>
<p>        (768   GPT-2).          .</p><br>
<p><img src="https://habrastorage.org/webt/x-/eh/kj/x-ehkj3y5dyunh4nf3v7aykhp70.gif" alt="gpt2-mlp-2"></p><br>
<p><em>(   )</em></p><br>
<h2 id="vy-sdelali-eto">  !</h2><br>
<p>     ,    - .       ,      .  , ,         :</p><br>
<p><img src="https://habrastorage.org/webt/yz/n7/20/yzn720lcfsuwwky-r3kanjys3w0.png" alt="gpt2-transformer-block-weights-2"></p><br>
<p>      .   ,            :</p><br>
<p><img src="https://habrastorage.org/webt/xl/22/mc/xl22mctq_nzefihu84kxryqj5be.png" alt="gpt2-weights-2"></p><br>
<p>       ,    :</p><br>
<p><img src="https://habrastorage.org/webt/vk/it/nh/vkitnhrnvo0djn0gn1_e0fpzhp0.png" alt="gpt2-117参数"></p><br>
<p> -    124   117.    ,  ,         (,    ).</p><br>
<h1 id="chast-3-za-predelami-yazykovogo-modelirovaniyaa-namepart_3a"> 3:    </h1><br>
<p>      ,    .      ,     .        .</p><br>
<h2 id="mashinnyy-perevod"> </h2><br>
<p>      .         :</p><br>
<p><img src="https://habrastorage.org/webt/ni/vt/bw/nivtbwcuuxla5cx5p2srhoohfgm.png" alt="解码器专用变压器翻译"></p><br>
<h2 id="summarizaciya"></h2><br>
<p>  ,        .  ,       (  ,   )   .           :</p><br>
<p><img src="https://habrastorage.org/webt/gd/3u/h5/gd3uh5vlhfzoarket8i2eeudj_4.png" alt="维基百科摘要"></p><br>
<p>         .</p><br>
<p><img src="https://habrastorage.org/webt/f2/-z/8v/f2-z8vum88usd2nyshvwwpesjq8.png" alt="仅解码器摘要"></p><br>
<h2 id="transfernoe-obuchenie"> </h2><br>
<p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow">Sample Efficient Text Summarization Using a Single Pre-Trained Transformer</a>        ,      . ,        ,     -    .</p><br>
<p>   GPT-2           .</p><br>
<h2 id="generaciya-muzyki"> </h2><br>
<p><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"> </a>          . « »      –             (,     « »).</p><br>
<p>   ,      . ,           (),    ( ).    (, ,  )       «» –  ,     .</p><br>
<p><img src="https://habrastorage.org/webt/en/x-/g2/enx-g2e1om4ctwjcuvnsspcjzcm.png" alt="音乐变压器性能编码3"></p><br>
<p> –     one-hot .   midi      .       :</p><br>
<p><img src="https://habrastorage.org/webt/q2/0s/vv/q20svvnlw_1ji4s_fjejnlnrckc.png" alt="音乐表示示例"></p><br>
<p> one-hot         :</p><br>
<p><img src="https://habrastorage.org/webt/gh/av/o5/ghavo5srziiagcrg-zqzg0vbnjg.png" alt="音乐变压器输入表示2"></p><br>
<p>         :</p><br>
<p><img src="https://habrastorage.org/webt/gg/up/ou/ggupou0idkyeu2oc-dtgvxayxsa.png" alt="音乐变形金刚自我注意2"></p><br>
<p>        , . <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"></a>.</p><br>
<h1 id="zaklyucheniea-nameconclusiona"></h1><br>
<p>     GPT-2     –  . ,              ,   ,    ,         .</p><br>
<h1 id="materialya-nameresourcesa"></h1><br>
<ul>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"> GPT-2</a>  OpenAI</li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">见</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">pytorch-transformers2</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">库</font><font style="vertical-align: inherit;">从</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">拥抱脸</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">，其中，除了GPT-2，实施BERT，变压器-XL，XLNet等先进Transformer模型。</font></font></li>
</ul><br>
<h1 id="avtory"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">s</font></font></h1><br>
<ul>
<li><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">原来</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">由</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">周杰伦Alammar</font></font></a></li>
<li><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">翻译</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">叶卡捷琳娜·斯米尔诺娃</font></font></a></li>
<li><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">编辑和排版</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Shkarin谢尔盖</font></font></a></li>
</ul></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN490830/index.html">Zimbra Collaboration Suite开源版中的密码管理</a></li>
<li><a href="../zh-CN490832/index.html">鞋标：当我们投入流通时，有200万种标码“ 1C” +“诚实标志”</a></li>
<li><a href="../zh-CN490836/index.html">如何打开评论而不被垃圾邮件淹没</a></li>
<li><a href="../zh-CN490838/index.html">WiFi 6在这里：市场提供什么以及我们为什么需要这种技术</a></li>
<li><a href="../zh-CN490840/index.html">资源规划。第4.1部分。在制定资源计划之前</a></li>
<li><a href="../zh-CN490844/index.html">书籍“ Kubernetes模式：本机云应用程序开发模式”</a></li>
<li><a href="../zh-CN490846/index.html">我们如何为卡拉什尼科夫学院开发网站并成为两项比赛的获奖者</a></li>
<li><a href="../zh-CN490850/index.html">帮助编译器帮助您</a></li>
<li><a href="../zh-CN490852/index.html">详细介绍SpinLaunch-航天工业中最热衷的秘密</a></li>
<li><a href="../zh-CN490854/index.html">使用AI对物品进行分类的仓库机器人已准备就绪</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>