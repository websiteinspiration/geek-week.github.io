<!doctype html>
<html class="no-js" lang="zh-CN">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🤴🏼 🔍 😗 使用ML-Agents的“与自己玩”方法在Unity中训练智能游戏竞争对手 ⚔️ ✌🏽 🤾🏼</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="哈Ha！
 
 正如我们的普通读者所知道的，我们已经在Unity上成功出版了很长的书。作为该主题研究的一部分，我们特别对ML-Agents Toolkit感兴趣。今天，我们引起您注意的是Unity博客文章的翻译，内容涉及如何使用“自己”方法有效地训练游戏代理商。特别是，这篇文章有助于理解为什么这种方...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>使用ML-Agents的“与自己玩”方法在Unity中训练智能游戏竞争对手</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/507212/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">哈Ha！</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
正如我们的普通读者所知道的，我们已经</font><font style="vertical-align: inherit;">在</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;">Unity</font></a><font style="vertical-align: inherit;">上</font><font style="vertical-align: inherit;">成功</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">出版了</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">很长的</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;">书</font></a><font style="vertical-align: inherit;">。作为该主题研究的一部分，我们特别对</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;">ML-Agents Toolkit</font></a><font style="vertical-align: inherit;">感兴趣</font><font style="vertical-align: inherit;">。今天，我们引起您注意的是Unity博客文章的翻译，内容涉及如何使用“自己”方法有效地训练游戏代理商。特别是，这篇文章有助于理解为什么这种方法比传统的强化学习更有效。</font><font style="vertical-align: inherit;">
享受阅读！</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"></font><br>
<br>
<img src="https://habrastorage.org/webt/vb/sz/5m/vbsz5musziyq5yighwu_iq0eggu.png"><br>
<br><font style="vertical-align: inherit;"></font><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
然后，本文概述了自我游戏技术（自己玩游戏），并演示了它如何通过</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ML-Agents Toolkit</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">帮助在足球演示环境中提供稳定有效的训练</font><font style="vertical-align: inherit;">。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
在Unity ML-Agents Toolkit的网球和足球演示环境中，特工像竞争对手一样相互竞争。在如此竞争的情况下培训代理有时是一项非常艰巨的任务。实际上，在ML-Agents工具包的早期版本中，为了使代理商能够放心地学习，需要认真研究该奖项。在</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0.14版中</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">增加了一个机会，使用户可以使用基于自我玩耍的强化学习（RL）来训练代理，该机制对于实现某些最高端的强化学习结果至关重要，例如</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">OpenAI Five</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">和</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">DeepMind的AlphaStar</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。工作中的自我玩味使代理人当前和过去的假设陷入困境。因此，我们得到了代理商的对手，代理商可以使用传统的强化学习算法来逐步改进。</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">训练有素的经纪人</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">可以成功地与高级玩家竞争。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
从人类的角度来看，自我游戏提供了一种基于与竞争相同的原理构建的学习环境。例如，一个学习打网球的人会选择以与他本人相同的水平来击打对手，因为太强或太弱的对手都不太容易掌握比赛。从发展自己的技能的角度来看，对于一个新手网球选手来说，击败相同的初学者，而不是学龄前儿童或诺瓦克·德约科维奇，具有更大的价值。第一个甚至无法击中球，第二个不会为您提供可以击败的发球。当初学者发展出足够的力量时，他可以继续前进到另一个水平，或者申请更严肃的比赛来与技能更高的对手比赛。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
在本文中，我们将考虑与游戏动态相关的一些技术细节，并考虑在虚拟环境“网球”和“足球”中工作的示例，这些示例经过重构以说明游戏本身。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">自己在游戏中玩游戏的故事</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
自己玩游戏的历史悠久，体现在开发旨在与游戏中的人竞争的人工游戏代理的实践中。</font><font style="vertical-align: inherit;">最早使用此系统的人之一是亚瑟·塞缪尔（Arthur Samuel），他在1950年代开发了国际象棋模拟器，并</font><font style="vertical-align: inherit;">于1959年</font><font style="vertical-align: inherit;">发表了</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">这项工作</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font><font style="vertical-align: inherit;">该系统成为Gerald Tesauro在TD-Gammon中进行强化学习的标志性成果的先驱；</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">总计发布</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">在1995年。 TD-加蒙（TD-Gammon）使用具有自发功能的TD（λ）时差算法来训练代理打双陆棋，以便他可以与专业人士竞争。在某些情况下，已经观察到TD-Gammon对职位的看法比世界一流的参与者更为自信。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
与RL关联的许多标志性成就反映了与自己一起玩耍的过程。重要的是要注意，与自己玩耍有助于发展</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">象棋和</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">超人能力的代理人</font><font style="vertical-align: inherit;">，</font><font style="vertical-align: inherit;">精湛的</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">DOTA 2</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">代理人</font><font style="vertical-align: inherit;">，以及</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">摔跤</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">，</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;">捉迷藏</font></a><font style="vertical-align: inherit;">等</font><font style="vertical-align: inherit;">游戏中的复杂策略和反战略。</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font><font style="vertical-align: inherit;">在通过自娱自乐获得的结果中，通常会注意到游戏代理商选择的策略会让专家们感到惊讶。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
和自己一起玩可以给代理带来一定的创造力，而独立于程序员的创造力。</font><font style="vertical-align: inherit;">代理仅接收游戏规则，然后-有关其获胜或失败的信息。</font><font style="vertical-align: inherit;">此外，根据这些基本原则，代理人必须培养胜任的行为。</font><font style="vertical-align: inherit;">根据TD-Gammon的创建者所说，这种学习方法是解放的，“ ...在某种意义上说，该程序不受人类的偏见和偏见的束缚，这可能会导致错误和不可靠。” </font><font style="vertical-align: inherit;">由于有了这种自由，代理商才能发现出色的游戏策略，这些策略完全改变了工程师对某些游戏的思考方式。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">竞技强化训练</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
在传统的强化学习任务框架内，代理人正在尝试开发一种行为方式，以使总奖励最大化。奖励信号编码代理的任务-这样的任务可以是，例如，绘制路线或收集物品。代理行为受环境限制。例如，重力，障碍以及坐席本人采取的行动的相对影响-例如，对自己的动作施加力。这些因素限制了代理人的行为，并且是他必须学会处理才能获得高额报酬的外在力量。因此，代理人要与环境动态竞争，并且必须精确地从一个州移动到另一个州，以便获得最大的回报。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/if/jf/xi/ifjfxil_iqm9pznr61qbpxujs38.png"><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">左侧显示了典型的强化训练方案：特工在环境中行动，转移到下一个状态并获得奖励。培训方案显示在右侧，代理与竞争对手竞争，从代理的角度来看，竞争对手实际上是环境的一部分。</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
在竞争性游戏的情况下，代理不仅与环境动态竞争，而且还与另一个（可能是智力）代理竞争。我们可以假定对手是内置于环境中的，他的行为直接影响代理“看到”的下一个状态以及他将获得的报酬。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/zl/kv/xo/zlkvxoa9_bt0cdsthmddwll-fvc.png"><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ML-Agents工具包中的网球示例</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
考虑一下ML-Agents Tennis的演示。蓝色球拍（左）是学习代理，紫色（右）是他的对手。要将球扔到球网上，探员必须考虑到对手飞出的球的轨迹，并考虑环境条件（重力）来调整飞球的角度和速度。但是，在与对手的比赛中，将球扔到网上仅仅是成功的一半。一个强大的对手可以用无法抗拒的打击来回应，结果，特工将失败。弱小的对手可能会将球击入网中。相等的对手可以发回发球，因此比赛将继续。在任何情况下，下一个状态和相应的奖励都取决于环境条件和对手。但是，在所有这些情况下，代理都发出相同的音高。因此，作为竞技游戏的训练，代理商推销竞争对手的行为是一个复杂的问题。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
考虑合适的对手并非易事。从上面可以清楚地看出，对手的相对实力会显着影响特定比赛的结果。如果对手太强了，那么特工可能会发现很难从头开始学习游戏。另一方面，如果对手太弱了，那么特工可以学习获胜，但是这些技能在与更强或更简单的对手的比赛中可能没有用。因此，我们需要一个与代理人实力相当的对手（坚定不移，但并非不可逾越）。此外，由于代理人的技能会随着每场比赛的完成而提高，因此我们必须在相同程度上增加对手的实力。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/fn/vk/pt/fnvkptgsipqopi2iiyflsa5ec_a.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
与自己一起玩时，过去的快照或处于当前状态的代理是构建在环境中的对手。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
这是我们自己的游戏派上用场的地方！代理人自己满足了所需对手的两个要求。他的力量绝对与他大致相当，并且他的技能会随着时间的推移而提高。在这种情况下，代理程序自己的策略已内置到环境中（见图）。那些熟悉</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">逐渐增加的复杂性教育</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">（课程学习）的人会向您展示，我们可以假定系统自然是在开发课程，随后，代理将学习与日渐强大的对手作斗争。因此，与自己一起玩可以让您利用环境本身来训练竞技游戏代理商！</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
在接下来的两个部分中，我们将考虑培训竞争性代理商的更多技术细节，尤其是有关在ML-Agents Toolkit中自行实现和使用游戏的信息。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">实际考虑</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
关于与自己一起玩的框架，出现了一些实际问题。</font><font style="vertical-align: inherit;">尤其是可以进行再训练，在这种训练中，特工仅以某种游戏方式学习获胜，以及学习过程中固有的不稳定性，这可能是由于过渡功能的不稳定（即由于不断变化的对手而引起）引起的。</font><font style="vertical-align: inherit;">之所以出现第一个问题，是因为我们希望我们的代理人具有与不同类型的对手作斗争的一般理解和能力。</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
第二个问题可以用网球环境来说明：不同的对手将以不同的速度和角度击球。从学习主体的角度来看，这意味着，随着您的学习，相同的决策将导致不同的结果，因此，主体将处于不同的后续情况。在传统的强化学习中，隐含了平稳的过渡函数。不幸的是，为解决特工准备了各种对手的选择方案以解决第一个问题，我们粗心大意会使第二个问题恶化。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
为了解决这个问题，我们将保留过去的代理政策的缓冲，从中我们将长期选择“学生”的潜在竞争对手。</font><font style="vertical-align: inherit;">从过去的政策中选择代理商，我们为他选择了各种各样的对手。</font><font style="vertical-align: inherit;">此外，允许代理与固定的对手长时间训练，我们稳定了过渡功能并创建了更一致的学习环境。</font><font style="vertical-align: inherit;">最后，可以使用超参数控制这些算法方面，这将在下一节中讨论。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">实施和使用细节</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
首先，选择与我们一起玩的超参数，我们要牢记要在对手的水平，最终政策的普遍性和培训的稳定性之间做出折衷。与一群变化缓慢或完全没有变化的对手进行比赛训练（比起与许多变化迅速的对手进行比赛训练）是一个更稳定的过程。可用的超参数使您可以控制将代理的当前策略保存为示例中的对手之一的频率，以后将保存新对手的频率，随后选择进行陪练，选择新对手的频率，保存对手的数量以及概率在这种情况下，学生将不得不与自己的异己进行比赛，而不是与池中选择的对手比赛。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
在竞技游戏中，环境颁发的“累积”奖可能不是跟踪学习进度的最有用的指标。事实是，累积奖励完全取决于对手的水平。具有一定游戏技能的坐席将分别根据熟练程度较低或技能较高的对手而获得或多或少的奖励。我们建议实施</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ELO评分系统</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">，该</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;">系统</font></a><font style="vertical-align: inherit;">可让您从零人口中计算特定人口中两名玩家的相对游戏技能。在一次训练中，该值应稳定增加。您可以使用</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">TensorBoard</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">跟踪它，以及其他学习指标，例如总体奖励</font><font style="vertical-align: inherit;">。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">在足球里玩自己</font></font></h3> <br>
<img src="https://habrastorage.org/webt/gl/m_/rh/glm_rhop1whkyve0i-wpubxfpqw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ML-Agent Toolkit的最新版本不包含用于足球学习环境的代理策略，因为其中没有内置可靠的培训过程。</font><font style="vertical-align: inherit;">但是，通过与自己一起使用游戏并进行一些重构，我们可以在非平凡的行为中训练代理。</font><font style="vertical-align: inherit;">最重大的变化是从特工的特征中删除了“游戏位置”。</font><font style="vertical-align: inherit;">在早期的足球环境中，“守门员”和“前锋”显然脱颖而出，因此整个游戏过程看起来更加合乎逻辑。</font><font style="vertical-align: inherit;">在</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">这部影片中</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">提出了一种新的环境，在该环境中，可以看到角色行为是如何自发形成的，其中一些特工开始充当攻击者，而其他人开始充当守门员。现在代理商本身正在学习扮演这些职位！将所有四个特工的奖励函数定义为：进球得分为+1.0，失球得分为-1.0，每步额外增加-0.0003的惩罚-提供该惩罚是为了刺激特工进攻。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
在这里，我们再次强调，足球学习环境中的主体本身学习协作行为，因此，没有使用与多主体行为或角色分配相关的显式算法。</font><font style="vertical-align: inherit;">此结果表明，可以使用相对简单的算法对代理进行复杂行为的训练-前提是该任务的格式合理。</font><font style="vertical-align: inherit;">为此，最重要的条件是座席可以观察其队友，即他们收到有关队友相对位置的信息。</font><font style="vertical-align: inherit;">经纪人强迫争夺球，间接告诉队友他应该防守。</font><font style="vertical-align: inherit;">相反，特工在防御中走开，挑起队友进攻。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">下一步是什么</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
如果您曾经使用过此版本中的任何新功能，请告诉我们。</font><font style="vertical-align: inherit;">我们将您的注意力吸引到</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ML-Agents GitHub问题</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">页面上</font><font style="vertical-align: inherit;">，您可以在其中谈论发现的错误，以及</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Unity ML-Agents论坛</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">页面</font><font style="vertical-align: inherit;">上的讨论一般性问题和问题。</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN507200/index.html">数据科学家欺骗您的7种方式</a></li>
<li><a href="../zh-CN507202/index.html">Avito Analytics聚会</a></li>
<li><a href="../zh-CN507204/index.html">工业设计室内厨房：从草图到盒装产品</a></li>
<li><a href="../zh-CN507206/index.html">建筑Y信使</a></li>
<li><a href="../zh-CN507210/index.html">现代Java在处理大量数据时的性能，第2部分</a></li>
<li><a href="../zh-CN507214/index.html">如何创建和修改交互式PDF表单或新技能ABBYY FineReader PDF</a></li>
<li><a href="../zh-CN507218/index.html">请读给我听，否则为什么文字没有读完</a></li>
<li><a href="../zh-CN507222/index.html">为什么每个人都应该戴口罩</a></li>
<li><a href="../zh-CN507224/index.html">如何通过视觉测试消除盲点</a></li>
<li><a href="../zh-CN507226/index.html">.NET中的OCR for PDF-如何从无法访问的PDF文档中提取文本</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>