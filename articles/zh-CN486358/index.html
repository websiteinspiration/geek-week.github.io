<!doctype html>
<html class="no-js" lang="zh-CN">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👰🏽 🙇🏽 ⚠️ 图片中的变压器 🖕🏿 🦍 💾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="在上一篇文章中，我们研究了注意力机制，这是现代深度学习模型中一种极为常见的方法，可以改善神经机器翻译应用程序的性能指标。在本文中，我们将介绍Transformer，该模型使用注意力机制来提高学习速度。此外，在许多任务上，变形金刚优于Google的神经机器翻译模型。但是，变压器的最大优点是在并行化条件...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>图片中的变压器</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/486358/"><p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">在</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">上一篇文章中，</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">我们研究了注意力机制，这是现代深度学习模型中一种极为常见的方法，可以改善神经机器翻译应用程序的性能指标。在本文中，我们将介绍Transformer，该模型使用注意力机制来提高学习速度。此外，在许多任务上，变形金刚优于Google的神经机器翻译模型。但是，变压器的最大优点是在并行化条件下具有很高的效率。甚至Google Cloud也建议在</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cloud TPU</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">上使用Transformer作为模型</font><font style="vertical-align: inherit;">。让我们尝试找出模型的组成及其执行的功能。</font></font></p><br>
<p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">“ </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">注意就是您所需要</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">的”一文中首先提出了Transformer模型</font><font style="vertical-align: inherit;">。</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tensor2Tensor</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">软件包中包含TensorFlow的实现</font><font style="vertical-align: inherit;">，此外，来自哈佛大学的一组NLP研究人员</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用PyTorch的实现</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">创建了</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;">本文</font></a><font style="vertical-align: inherit;">的</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;">指南注释</font></a><font style="vertical-align: inherit;">。</font><font style="vertical-align: inherit;">在同一指南中，我们将尝试以最简单，一致的方式概述主要思想和概念，以帮助那些对主题领域没有深入了解的人们理解该模型。</font></font></p><a name="habracut"></a><br>
<h1 id="vysokourovnevnyy-obzor"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">高层审查</font></font></h1><br>
<p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">让我们将模型视为一种黑匣子。</font><font style="vertical-align: inherit;">在机器翻译应用程序中，它接受一种语言的句子作为输入，并显示另一种语言的句子。</font></font></p><br>
<p><img src="https://habrastorage.org/webt/vo/iv/w7/voivw7zwvgnsmmjzbxkn8ypdo2s.png" alt="the_transformer_3"></p><br>
<p>     ,  ,       ,      .</p><br>
<p><img src="https://habrastorage.org/webt/mj/3a/qq/mj3aqqrifxbnb8f5gv_r5jbdf7g.png" alt="The_transformer_encoders_decoders"></p><br>
<p>  –   ;      6 ,     (  6   ,       ).   –   ,     .</p><br>
<p><img src="https://habrastorage.org/webt/-y/7z/tl/-y7ztlhyq5_817b89qhjjikwopk.png" alt="The_transformer_encoder_decoder_stack"></p><br>
<p>    ,     .      :</p><br>
<p><img src="https://habrastorage.org/webt/2_/bq/if/2_bqifdpdhluoadxjo_ncvpgdcm.png" alt="变压器编码器"></p><br>
<p> ,   ,       (self-attention),              .       .</p><br>
<p>          (feed-forward neural network).           .</p><br>
<p>     ,      ,          (   ,      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="> seq2seq</a>).</p><br>
<p><img src="https://habrastorage.org/webt/34/f7/lm/34f7lmnxvdcchib66hhxbdx8vzg.png" alt="Transformer_decoder"></p><br>
<h1 id="na-scenu-vyhodyat-tenzory">   </h1><br>
<p>,      ,    /,        ,       .</p><br>
<p>     NLP-,    ,     ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="> </a> (word embeddings).</p><br>
<p><img src="https://habrastorage.org/webt/po/lw/jl/polwjl7ap50biqcjrwegkczazy4.png" alt="嵌入"></p><br>
<p><em>      512.         .</em></p><br>
<p>      .   ,    ,  :      512 (       ,   –    ).      ,    , ,  ,        .</p><br>
<p>        ,          .</p><br>
<p><img src="https://habrastorage.org/webt/a8/o9/z-/a8o9z-cte22jxysrcoqqfmbgyqu.png" alt="encoder_with_tensors"></p><br>
<p>      :         . ,          ,        ,            .</p><br>
<p>  ,           .</p><br>
<h1 id="teper-kodiruem"> !</h1><br>
<p>   ,        ,         –     , , ,      .</p><br>
<p><img src="https://habrastorage.org/webt/s7/qm/ln/s7qmln-nbaxuocg1rswbf6fjang.png" alt="encoder_with_tensors_2"></p><br>
<p><em>         .       ,       .</em></p><br>
<h1 id="vysokourovnevyy-vzglyad-na-mehanizm-vnutrennego-vnimaniya">     </h1><br>
<p> ,   « »    -,     .            ,   «Attention is All You Need».  ,   .</p><br>
<p>   –   ,    :</p><br>
<blockquote>”The animal didn't cross the street because it was too tired”</blockquote><p>   «it»   ?   (street)    (animal)?         .</p><br>
<p>    «it»,     ,  «it»   «animal».</p><br>
<p>        (    ),             ,     .</p><br>
<p>       (RNN), ,      RNN     /,    ,    .    –  ,     ,   «»       .</p><br>
<p><img src="https://habrastorage.org/webt/pz/li/tj/pzlitjsvdvi0ehs5yuw0jmqfoje.png" alt="变压器_自我注意_可视化"></p><br>
<p><em>   «it»   #5 (   ),      «The animal»        «it».</em></p><br>
<p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">  Tensor2Tensor</a>,         ,    .</p><br>
<h1 id="mehanizm-vnutrennego-vnimaniya-v-detalyah">    </h1><br>
<p>  ,       ,     ,         .</p><br>
<p><strong> </strong>     –         (   –   ):   (Query vector),   (Key vector)    (Value vector).          ,       .</p><br>
<p>,       ,   .    64,       /     512.     ,                 (multi-head attention)  .</p><br>
<p><img src="https://habrastorage.org/webt/d1/o4/fd/d1o4fdyczdrehbvczb2m2d3whcs.png" alt="transformer_self_attention_vectors"></p><br>
<p><em> x1    WQ  q1,  «»,    .      «», «»  «»      .</em></p><br>
<p>    «», «»  «»?</p><br>
<p> ,         .  ,   ,     ,           .</p><br>
<p><strong> </strong>    –   (score). ,           – «Thinking».             .  ,               .</p><br>
<p>            .  ,          #1,      q1  k1,  —   q1  k2.</p><br>
<p><img src="https://habrastorage.org/webt/c_/wx/af/c_wxaft2ynanknfd_osg0842_us.png" alt="transformer_self_attention_score"></p><br>
<p><strong>   </strong> –     8 (    ,    – 64;          ,      ),        (softmax).     ,         1.</p><br>
<p><img src="https://habrastorage.org/webt/em/kp/4s/emkp4swx5jqhbqljq7tpm4ne-t0.png" alt="自我注意_softmax"></p><br>
<p> - (softmax score) ,            . ,        -,       ,   .</p><br>
<p><strong> </strong> –      - (  ).   :      ,    ,        (    , , 0.001).</p><br>
<p><strong> </strong> –    .             (  ).</p><br>
<p><img src="https://habrastorage.org/webt/tp/wo/s0/tpwos0dakwx0ebolkn3hzem3wsi.png" alt="自我注意输出"></p><br>
<p>     .     ,         .   , ,          . ,       ,    .</p><br>
<h1 id="matrichnye-vychisleniya-vnutrennego-vnimaniya">   </h1><br>
<p><strong> </strong> –   ,   .         X      ,    (WQ, WK, WV).</p><br>
<p><img src="https://habrastorage.org/webt/i_/ei/5g/i_ei5g0b1yt95qxkqve1jxdumrq.png" alt="自我注意矩阵计算"></p><br>
<p><em>         .         (512,  4   )   q/k/v (64,  3 ).</em></p><br>
<p><strong></strong>,      ,     2-6         .</p><br>
<p><img src="https://habrastorage.org/webt/ms/wn/cx/mswncxi-t4jjxh9mwybx9e7qmug.png" alt="自我注意矩阵计算2"></p><br>
<p><em>     .</em></p><br>
<h1 id="mnogogolovaya-gidra"> </h1><br>
<p>         ,    (multi-head attention).           :</p><br>
<ol>
<li>      . ,   , z1      ,        .       «The animal didn’t cross the street because it was too tired»,   ,     «it».</li>
<li>    « » (representation subspaces).    ,         ,     // (  8 «» ,        8    /).       .           (    /)    . </li>
</ol><br>
<p><img src="https://habrastorage.org/webt/c-/jh/8n/c-jh8nfwmfjoiciycph9wsywqf0.png" alt="transformer_attention_heads_qkv"></p><br>
<p><em>   ,    WQ/WK/WV     «»,      Q/K/V .    ,    WQ/WK/WV    Q/K/V .</em></p><br>
<p>     ,    , 8     ,    8  Z .</p><br>
<p><img src="https://habrastorage.org/webt/uc/0b/hi/uc0bhij9fzgajpjhem0sia2mna8.png" alt="transformer_attention_heads_z"></p><br>
<p>    .      ,     8  –     (   ),        Z .</p><br>
<p>  ?          WO.</p><br>
<p><img src="https://habrastorage.org/webt/s1/d7/md/s1d7md5obt_ps-nzg-adgfhr2ks.png" alt="变压器_注意_头部_重量_矩阵_o"></p><br>
<p>  ,  ,       . ,    .        ,       .</p><br>
<p><img src="https://habrastorage.org/webt/he/j3/f1/hej3f1fkxked2nrlj_xnz9k3vmy.png" alt="Translator_multi-headed_self-attention-recap"></p><br>
<p>,    «» ,      ,  ,     «»    «it»   :</p><br>
<p><img src="https://habrastorage.org/webt/t9/ax/jv/t9axjvckxd8xqxtpcygunzpf8ty.png" alt="transformer_self-attention_visualization_2"></p><br>
<p><em>      «it»,  «»      «the animal»,      —  «tired».  ,     «it»      «animal»  «tired».</em></p><br>
<p>    «»    , ,    .</p><br>
<p><img src="https://habrastorage.org/webt/w1/z1/po/w1z1pou6afrqqohpzswfk6kdnos.png" alt="transformer_self-attention_visualization_3"></p><br>
<h1 id="predstavlenie-poryadka-v-posledovatelnosti-s-pomoschyu-pozicionnogo-kodirovaniya">       </h1><br>
<p>        —       .</p><br>
<p>           .     ,                 .     ,                  Q/K/V       .</p><br>
<p><img src="https://habrastorage.org/webt/ee/bv/n7/eebvn7g_nwsete4fzv4jcuguz4c.png" alt="Translator_positional_encoding_vectors"></p><br>
<p><em> ,     ,     ,     .</em></p><br>
<p>  ,     4,       :</p><br>
<p><img src="https://habrastorage.org/webt/ts/av/pt/tsavptjlxb3tm5s-nlhmiqekspk.png" alt="transformer_positional_encoding_example"></p><br>
<p>    ?</p><br>
<p>        : ,    ,          ,   —      ..    512   -1  1.      ,     .</p><br>
<p><img src="https://habrastorage.org/webt/v9/dv/oh/v9dvohtljexbjop_vrykyyqdzbk.png" alt="transformer_positional_encoding_large_example"></p><br>
<p><em>     20  ()    512 (). ,    :      ( ),   –  ( ).          .</em></p><br>
<p>       ( 3.5).          <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">get_timing_signal_1d()</a>.        , ,        (,       ,   ,    ).</p><br>
<h1 id="ostatki-sladki"> </h1><br>
<p>    ,    ,    ,   ,    ( , )        ,       (layer-normalization step).</p><br>
<p><img src="https://habrastorage.org/webt/gp/ji/3r/gpji3ragynjcuq7y6hkhqp1ws5m.png" alt="transformer_resideual_layer_norm"></p><br>
<p>     ,    ,   :</p><br>
<p><img src="https://habrastorage.org/webt/h3/hm/94/h3hm94cg9y9acyf5odv4uxt1e5q.png" alt="transformer_resideual_layer_norm_2"></p><br>
<p>       .        ,      :</p><br>
<p><img src="https://habrastorage.org/webt/dm/wa/pi/dmwapi3jsz1arewhc4xg3_hgevo.png" alt="transformer_resideual_layer_norm_3"></p><br>
<h1 id="chto-po-chasti-dekodera">   </h1><br>
<p>,        ,    ,    .   ,       .</p><br>
<p>    .          K  V.       «-»  ,          :</p><br>
<p><img src="https://habrastorage.org/webt/l6/ry/nl/l6rynlkppwpzjaenv35ipi8vcpq.gif" alt="transformer_decoding_1"></p><br>
<p><em>      .         (   –    ).</em></p><br>
<p>      , ,       .           ,       ,    .    ,      ,        ,      .</p><br>
<p><img src="https://habrastorage.org/webt/nm/yu/oq/nmyuoqb-9wqu9bssgxkvmhv9gmc.gif" alt="transformer_decoding_2"></p><br>
<p>           .</p><br>
<p>             .          (   –inf)       .</p><br>
<p>«-»      ,  ,       ,   ,          .</p><br>
<h1 id="finishnaya-pryamaya"> </h1><br>
<p>         .       ?           .</p><br>
<p>  –     ,   ,   ,    ,    (logits vector).</p><br>
<p>    10     (« »  ),      .  ,       10 000    –       .           .</p><br>
<p>       ( ,    1).               .</p><br>
<p><img src="https://habrastorage.org/webt/hs/i1/go/hsi1go4q7rri9w6vydeuiyyqkqi.png" alt="transformer_decoder_output_softmax"></p><br>
<p><em>   ,        ,      .</em></p><br>
<h1 id="itog-obucheniya"> </h1><br>
<p>,     ,   ,      ,     .</p><br>
<p>           ,   .  ..       ,        .</p><br>
<p>  ,       6  («a», «am», «i», «thanks», «student»  «&lt;eos&gt;» (« »).</p><br>
<p><img src="https://habrastorage.org/webt/6w/qz/6w/6wqz6wpimd6gstmtqr2scouikfs.png" alt="词汇"></p><br>
<p><em>           .</em></p><br>
<p>     ,              (,   one-hot-). ,     «am»,   :</p><br>
<p><img src="https://habrastorage.org/webt/an/sf/tw/ansftwzhb-z4txzgf5zuexvyql4.png" alt="一个热门词汇示例"></p><br>
<p><em>: one-hot-   .</em></p><br>
<p>     (loss function)   – ,           ,   ,  .</p><br>
<h1 id="funkciya-poter"> </h1><br>
<p>,             .      –  «merci»  «thanks».</p><br>
<p>   ,     ,    ,    «thanks».  ..    ,      .</p><br>
<p><img src="https://habrastorage.org/webt/ev/87/xq/ev87xqtv05wyamo1jcrqlrhnxho.png" alt="Translator_logits_output_and_label"></p><br>
<p><em>   ()   ,           /.       ,    ,       ,     .</em></p><br>
<p>     ?      .    , . <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="> </a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="> -</a>.</p><br>
<p> ,     .             . ,    «je suis étudiant» –    «I am a student».  ,   ,        ,   :</p><br>
<ul>
<li>            (6    ,   –  3000  10000);</li>
<li>       ,   «i»;</li>
<li>       ,   «am»;</li>
<li> ..         ,       .</li>
</ul><br>
<p><img src="https://habrastorage.org/webt/vj/ge/o0/vjgeo0a-6nwsevrrkttu79epj3g.png" alt="output_target_probability_distributions"></p><br>
<p>           ,        :</p><br>
<p><img src="https://habrastorage.org/webt/nh/ry/zd/nhryzdfhby64om8iz-vp2l9jxvu.png" alt="output_trained_model_probability_distributions"></p><br>
<p><em>,         . ,       ,        (.: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="> </a>).  ,        ,    ,          –      ,    .</em></p><br>
<p>,   ,        ,   ,              .    ,    (greedy decoding).   – , ,  2   (  , «I»  «a»)  ,   ,   :    ,        «I»,   , ,     «a».  ,        ,    .      #2  #3  ..    « » (beam search).      (beam_size)    (..         #1  #2),  - (top_beams)    (    ).         .</p><br>
<h1 id="vyvod"></h1><br>
<p>,          .       ,   :</p><br>
<ul>
<li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Attention is All You Need</a>,    (<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Transformer: A Novel Neural Network Architecture for Language Understanding</a>)  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Tensor2Tensor announcement</a>;</li>
<li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Łukasz Kaiser’s talk</a>,      ;</li>
<li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Jupyter-   Tensor2Tensor</a>;</li>
<li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="> Tensor2Tensor</a>.</li>
</ul><br>
<p>   :</p><br>
<ul>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Depthwise Separable Convolutions for Neural Machine Translation</a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">One Model To Learn Them All</a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Discrete Autoencoders for Sequence Models</a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">通过汇总长序列来生成维基百科</font></font></a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">影像变压器</font></font></a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">变压器模型的培训技巧</font></font></a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">具有相对位置表示的自我注意</font></font></a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用离散潜在变量的序列模型中的快速解码</font></font></a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Adafactor：具有亚线性记忆成本的自适应学习率</font></font></a></li>
</ul><br>
<h1 id="avtory"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">s</font></font></h1><br>
<ul>
<li><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">原来</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">由</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">周杰伦Alammar</font></font></a></li>
<li><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">翻译</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">叶卡捷琳娜·斯米尔诺娃</font></font></a></li>
<li><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">编辑和排版</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Shkarin谢尔盖</font></font></a></li>
</ul></div>
      
    </div><p class="reference-to-source js-reference-to-source">Source: https://habr.com/ru/post/undefined/</p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN486346/index.html">关于NModbus（RTU）</a></li>
<li><a href="../zh-CN486348/index.html">在DialogFlow中使用实体进行CRUD操作（C＃）</a></li>
<li><a href="../zh-CN486350/index.html">机器人俱乐部的现金返还，或我的学生如何通过技能赚钱</a></li>
<li><a href="../zh-CN486354/index.html">乔治·波塔波夫（Georgy Potapov）：“我是专业的OpenStreetMap数据消费者”</a></li>
<li><a href="../zh-CN486356/index.html">灰烬时代-第132期</a></li>
<li><a href="../zh-CN486360/index.html">如何通过15个简单的步骤从摩卡咖啡迁移到玩笑-以及为什么</a></li>
<li><a href="../zh-CN486362/index.html">在Product Univercity Moreynis和Chernyak接受培训后，我如何没有成为产品经理</a></li>
<li><a href="../zh-CN486368/index.html">〜SMAK〜-使用MicroPython上的asyncio库的智能家居可编程控制器</a></li>
<li><a href="../zh-CN486372/index.html">捷克程序员免费编写了一个价值1600万欧元的网站吗？真相？</a></li>
<li><a href="../zh-CN486376/index.html">关卡设计师如何使用架构理论来创建游戏关卡</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>