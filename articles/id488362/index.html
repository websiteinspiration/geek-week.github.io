<!doctype html>
<html class="no-js" lang="id">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>😴 ↖️ 🛐 Klasifikasi multi-tag 👪 🤸 ♍️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Halo, habrozhiteli! Kami memutuskan untuk mengutip kutipan dari buku karya Andrei Burkov , Machine Learning Without Extra Words , yang didedikasikan u...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Klasifikasi multi-tag</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/488362/"><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><img src="https://habrastorage.org/webt/hm/vf/c_/hmvfc_yyxepplv1mj0crw1vu7pw.jpeg" align="left" alt="gambar"></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Halo, habrozhiteli! </font><font style="vertical-align: inherit;">Kami memutuskan untuk mengutip kutipan dari buku karya Andrei Burkov </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, Machine Learning Without Extra Words</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , yang didedikasikan untuk klasifikasi.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Untuk menggambarkan gambar dalam gambar, beberapa label dapat digunakan secara bersamaan: "hutan konifer", "pegunungan", "jalan". Jika jumlah nilai yang mungkin untuk label besar, tetapi semuanya memiliki sifat yang sama dengan tag, setiap sampel yang ditandai dapat dikonversi menjadi beberapa data yang ditandai, satu untuk setiap tag. Semua data baru ini akan memiliki vektor fitur yang sama dan hanya satu label. Akibatnya, tugas menjadi masalah klasifikasi multikelas. Itu dapat diselesaikan dengan menggunakan strategi “satu lawan semua”. Satu-satunya perbedaan dari masalah klasifikasi multi-kelas yang biasa adalah munculnya hyperparameter baru: ambang batas. Jika skor kesamaan untuk label di atas nilai ambang, label ini ditugaskan untuk vektor fitur input. Dalam skenario ini, beberapa label dapat ditugaskan ke satu vektor karakteristik.Nilai ambang dipilih menggunakan set kontrol.</font></font><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Untuk mengatasi masalah klasifikasi dengan banyak label, kita dapat menerapkan algoritma yang secara alami dikonversi ke multikelas (pohon keputusan, regresi logistik, jaringan saraf, dll.). Mereka mengembalikan estimasi untuk setiap kelas, sehingga kita dapat mendefinisikan ambang dan kemudian menetapkan beberapa label ke satu vektor fitur yang skor kedekatannya melebihi ambang ini. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Neural networks secara alami dapat dilatih dalam klasifikasi multi-label menggunakan binary cross-entropy sebagai fungsi biaya. Lapisan output dari jaringan saraf dalam hal ini memiliki satu simpul per label. Setiap node di lapisan output memiliki fungsi aktivasi sigmoid. Oleh karena itu, setiap label l adalah biner</font></font><img src="https://habrastorage.org/webt/2n/_n/mg/2n_nmgvuciuadryomnq3urrg1xw.jpeg" alt="gambar"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">di mana l = 1, ..., L dan i = 1, ..., N. Entropi silang biner dari probabilitas </font></font><img src="https://habrastorage.org/webt/lb/le/cf/lblecfgyuy23k8zvlasuajncsds.jpeg" alt="gambar"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">bahwa sampel xi diberi label l, yang didefinisikan sebagai </font></font><img src="https://habrastorage.org/webt/qd/1e/fx/qd1efxpmi6mmuhv_fhmsqpjgpi4.jpeg" alt="gambar"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
kriteria minimalisasi - rata-rata sederhana semua anggota lintas-entropi biner dalam semua sampel pelatihan dan semua tag mereka. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dalam kasus di mana jumlah nilai label yang mungkin kecil, Anda dapat mencoba mengubah masalah klasifikasi dengan banyak label menjadi masalah klasifikasi multi-kelas. </font><font style="vertical-align: inherit;">Bayangkan masalah berikut ini. </font><font style="vertical-align: inherit;">Anda perlu menetapkan dua jenis label untuk gambar. </font><font style="vertical-align: inherit;">Label jenis pertama dapat memiliki dua arti yang mungkin: { </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">photo, painting</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> }; </font><font style="vertical-align: inherit;">tanda tipe kedua dapat memiliki tiga arti yang mungkin: { </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">portrait, landscape, other</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">}. </font><font style="vertical-align: inherit;">Untuk setiap kombinasi dua kelas sumber, Anda bisa membuat kelas dummy baru, misalnya:</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/mz/px/zn/mzpxzn0rlrumwoql7gkk3no7ihk.jpeg" alt="gambar"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Sekarang kami memiliki data tag yang sama, tetapi kami mengganti set label benar dengan satu label dummy dengan nilai dari 1 hingga 6. Dalam praktiknya, pendekatan ini memberikan hasil yang baik ketika tidak ada terlalu banyak kemungkinan kombinasi kelas. Jika tidak, lebih banyak data pelatihan perlu digunakan untuk mengimbangi peningkatan set kelas. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Keuntungan utama dari pendekatan yang terakhir ini adalah bahwa label tetap berkorelasi, tidak seperti metode yang dijelaskan di atas, yang memprediksi masing-masing label secara independen satu sama lain. Dalam banyak tugas, korelasi antara label dapat menjadi faktor yang signifikan. Misalnya, bayangkan Anda ingin mengklasifikasikan email sebagai </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">spam</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> dan </font><i><font style="vertical-align: inherit;">non- </font></i><i><font style="vertical-align: inherit;">spam</font></i></font><i><font style="vertical-align: inherit;"></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, dan pada saat yang sama seperti biasa dan penting. </font><font style="vertical-align: inherit;">Anda mungkin ingin mengecualikan perkiraan seperti [ </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">spam, penting</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ].</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">7.5. </font><font style="vertical-align: inherit;">Pelatihan ansambel</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Algoritma fundamental yang kita bahas pada Bab 3 memiliki keterbatasan. Karena kesederhanaannya, terkadang mereka tidak dapat membuat model yang cukup efektif untuk tugas Anda. Dalam kasus seperti itu, Anda dapat mencoba menggunakan jaringan saraf yang dalam. Namun, dalam praktiknya, jaringan syaraf dalam membutuhkan sejumlah besar data berlabel, yang mungkin tidak Anda miliki. Cara lain untuk meningkatkan efektivitas algoritma pembelajaran sederhana adalah dengan menggunakan </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">pelatihan ensemble</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pelatihan Ensemble adalah paradigma pelatihan yang didasarkan pada pelatihan tidak hanya satu model super-benar, tetapi sejumlah besar model dengan akurasi rendah dan menggabungkan perkiraan yang diberikan oleh </font><font style="vertical-align: inherit;">model </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">lemah</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ini </font><font style="vertical-align: inherit;">untuk mendapatkan </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">metamodel yang</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> lebih benar </font><font style="vertical-align: inherit;">.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Model dengan akurasi rendah biasanya dilatih oleh </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">algoritma pembelajaran yang lemah</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> yang tidak mampu melatih model yang kompleks dan karenanya menunjukkan kecepatan tinggi pada tahap pelatihan dan perkiraan. Paling sering, algoritma pembelajaran pohon keputusan digunakan sebagai algoritma yang lemah, yang biasanya berhenti memecah set pelatihan setelah beberapa iterasi. Hasilnya adalah pohon kecil dan tidak terlalu teratur, tetapi, seperti ide pelatihan ansambel mengatakan, jika pohon tidak identik dan setiap pohon setidaknya sedikit lebih baik daripada tebakan acak, kita bisa mendapatkan akurasi tinggi dengan menggabungkan sejumlah besar pohon tersebut. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Untuk mendapatkan perkiraan akhir untuk entri </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, perkiraan semua model yang lemah dikombinasikan menggunakan beberapa metode pemungutan suara tertimbang. </font><font style="vertical-align: inherit;">Bentuk spesifik pembobotan suara tergantung pada algoritme, tetapi esensinya tidak bergantung padanya: jika, secara kolektif, model yang lemah memperkirakan bahwa email tersebut adalah spam, kami memberikan </font><font style="vertical-align: inherit;">label </font><i><font style="vertical-align: inherit;">spam </font></i></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> pada sampel </font><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">
Dua metode utama untuk ansambel pelatihan adalah </font><b><font style="vertical-align: inherit;">meningkatkan</font></b><font style="vertical-align: inherit;"> dan </font><b><font style="vertical-align: inherit;">mengantongi</font></b><font style="vertical-align: inherit;"> (agregasi). </font><font style="vertical-align: inherit;">Terjemahan dari istilah meningkatkan dan mengantongi tidak akurat dan tidak biasa.</font></font><i><font style="vertical-align: inherit;"></font></i><font style="vertical-align: inherit;"></font><br>
<br><font style="vertical-align: inherit;"></font><b><font style="vertical-align: inherit;"></font></b><font style="vertical-align: inherit;"></font><b><font style="vertical-align: inherit;"></font></b><font style="vertical-align: inherit;"></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">7.5.1. </font><font style="vertical-align: inherit;">Meningkatkan dan mengantongi</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Metode meningkatkan adalah dengan menggunakan data pelatihan awal dan berulang kali membuat beberapa model menggunakan algoritma yang lemah. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Setiap model baru berbeda dari yang sebelumnya dalam hal itu, membangunnya, algoritma yang lemah mencoba untuk "memperbaiki" kesalahan yang dibuat oleh model sebelumnya. </font><font style="vertical-align: inherit;">Model ansambel akhir adalah kombinasi dari banyak model iteratif yang lemah ini. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Inti dari mengantongi adalah membuat banyak "salinan" data pelatihan (setiap salinan sedikit berbeda dari yang lain) dan kemudian menerapkan algoritma yang lemah untuk setiap salinan untuk mendapatkan beberapa model yang lemah, dan kemudian menggabungkannya. </font><font style="vertical-align: inherit;">Algoritma pembelajaran mesin yang digunakan secara luas dan efisien berdasarkan ide mengantongi adalah </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">hutan acak</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">7.5.2. </font><font style="vertical-align: inherit;">Hutan acak</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Algoritma bagging "klasik" berfungsi sebagai berikut. </font><font style="vertical-align: inherit;">Sampel acak B dibuat dari set pelatihan yang ada </font></font><img src="https://habrastorage.org/webt/-d/6r/gn/-d6rgnsn5wk-yzl3dbk0i7x2ole.jpeg" alt="gambar"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(untuk setiap b = 1, ..., B) dan </font></font><img src="https://habrastorage.org/webt/-d/6r/gn/-d6rgnsn5wk-yzl3dbk0i7x2ole.jpeg" alt="gambar"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">model </font></font><img src="https://habrastorage.org/webt/nd/ew/4z/ndew4zvx7r0jpeilfknakojmrbs.jpeg" alt="gambar"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">pohon keputusan </font><font style="vertical-align: inherit;">dibangun </font><font style="vertical-align: inherit;">berdasarkan masing-masing sampel </font><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Untuk mendapatkan sampel </font></font><img src="https://habrastorage.org/webt/-d/6r/gn/-d6rgnsn5wk-yzl3dbk0i7x2ole.jpeg" alt="gambar"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">untuk beberapa b, </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">sampel dibuat dengan penggantian</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Artinya, pertama sampel kosong dibuat, dan kemudian sampel acak dipilih dari set pelatihan, dan salinannya ditempatkan </font></font><img src="https://habrastorage.org/webt/-d/6r/gn/-d6rgnsn5wk-yzl3dbk0i7x2ole.jpeg" alt="gambar"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, sementara sampel itu sendiri tetap dalam set pelatihan asli. </font><font style="vertical-align: inherit;">Pemilihan data berlanjut sampai kondisi terpenuhi. </font></font><img src="https://habrastorage.org/webt/se/au/-5/seau-5gwous1c1cshmrx8rwubig.jpeg" alt="gambar"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Sebagai hasil pelatihan, </font><font style="vertical-align: inherit;">pohon keputusan </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">B</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> diperoleh </font><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Perkiraan untuk sampel </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x baru</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , dalam kasus regresi, ditentukan sebagai rata-rata </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">B</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> perkiraan</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/re/zp/mj/rezpmjqa9lo7w4dxvqd6njwwqcm.jpeg" alt="gambar"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
atau dengan suara terbanyak dalam hal klasifikasi.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Hutan acak hanya memiliki satu perbedaan dari pengemasan klasik. Ini menggunakan algoritma pembelajaran pohon yang dimodifikasi yang, dengan setiap pemisahan dalam proses pembelajaran, memeriksa subset fitur secara acak. Hal ini dilakukan untuk menghilangkan korelasi antar pohon: jika satu atau lebih fitur memiliki kemampuan prediksi yang besar, banyak pohon akan memilihnya untuk memisahkan data. Ini akan menyebabkan munculnya di "hutan" sejumlah besar pohon berkorelasi. Tanda korelasi dengan kemampuan prediksi tinggi mencegah akurasi prediksi meningkat. Efisiensi tinggi dari ansambel model dijelaskan oleh fakta bahwa model yang baik kemungkinan besar setuju dengan perkiraan yang sama, dan model yang buruk tidak mungkin setuju dan akan memberikan perkiraan yang berbeda. Korelasi akan membuat model yang buruk lebih mungkin untuk setuju,yang akan mengubah pola pemungutan suara atau mempengaruhi rata-rata.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Hyperparameter yang paling penting untuk penyetelan adalah jumlah pohon B dan ukuran subset acak fitur yang harus dipertimbangkan untuk setiap pemisahan. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Hutan acak adalah salah satu algoritma pembelajaran ensemble yang paling banyak digunakan. Apa yang menentukan efektivitasnya? Alasannya adalah bahwa dengan menggunakan beberapa sampel dari dataset asli, kami mengurangi </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">varians dari</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> model akhir. Ingat bahwa varians rendah berarti kecenderungan lemah untuk </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">berlatih kembali</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Pelatihan ulang terjadi ketika model mencoba menjelaskan variasi kecil dalam kumpulan data karena kumpulan data hanyalah contoh kecil dari semua contoh yang mungkin dari fenomena yang kami coba simulasikan. </font><font style="vertical-align: inherit;">Dalam kasus pendekatan yang gagal untuk pembentukan set pelatihan, beberapa artefak yang tidak diinginkan (tetapi tidak terhindarkan) dapat jatuh ke dalamnya: kebisingan, data yang abnormal dan berlebihan atau tidak cukup representatif. </font><font style="vertical-align: inherit;">Dengan membuat beberapa sampel acak dengan penggantian set pelatihan, kami mengurangi pengaruh artefak ini.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">7.5.3. </font><font style="vertical-align: inherit;">Peningkatan Gradien</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Algoritma pelatihan ensemble lain yang efektif berdasarkan ide boosting adalah gradient boosting. </font><font style="vertical-align: inherit;">Pertama, pertimbangkan penggunaan peningkatan gradien dalam regresi. </font><font style="vertical-align: inherit;">Kami akan mulai membangun model regresi yang efektif dengan model konstan </font></font><img src="https://habrastorage.org/webt/7u/7x/jd/7u7xjdufljpsjwwu45j9_gkc3r0.jpeg" alt="gambar"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(seperti yang kami lakukan di ID3):</font></font><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/em/vj/bv/emvjbvtmptxl_d3bihzev4wbh7o.jpeg" alt="gambar"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Kemudian ganti label di semua sampel i = 1, ..., N di set pelatihan:</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/va/wc/ah/vawcahk0zsdpgnumh_bfjuozw_e.jpeg" alt="gambar"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
di mana </font></font><img src="https://habrastorage.org/webt/rf/1x/pk/rf1xpkgfxmcivy-1tqpwv2vgroi.jpeg" alt="gambar"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">disebut </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">sisa</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> dan merupakan label baru dari sampel </font></font><img src="https://habrastorage.org/webt/dk/ey/2r/dkey2rj3yf-zkei2029wfa2ujso.jpeg" alt="gambar"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Sekarang kita menggunakan set pelatihan yang dimodifikasi dengan sisa-sisa bukannya label asli untuk membangun model baru pohon keputusan. </font></font><img src="https://habrastorage.org/webt/mx/fw/um/mxfwumzpc5tq1wjpdate2rxra48.jpeg" alt="gambar"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Model peningkatan sekarang didefinisikan sebagai di </font></font><img src="https://habrastorage.org/webt/cy/c2/vz/cyc2vz0tmrrihcm6kta_7rnwmui.jpeg" alt="gambar"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">mana α adalah kecepatan belajar (hyperparameter). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Kemudian, kami menghitung ulang residu menggunakan Persamaan 7.2, mengganti label dalam data pelatihan lagi, mengajarkan model baru pohon keputusan, </font></font><img src="https://habrastorage.org/webt/p4/wk/nl/p4wknlhlvwiqtr7zz7lx_y5oq3s.jpeg" alt="gambar"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">mendefinisikan kembali model dorongan saat </font></font><img src="https://habrastorage.org/webt/n-/mn/ht/n-mnhtck0rar7pz4anzlbdc-bmo.jpeg" alt="gambar"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">kami mengulangi proses, sampai kami menggabungkan jumlah maksimum </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">M</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> pohon yang </font><font style="vertical-align: inherit;">telah ditentukan </font><font style="vertical-align: inherit;">.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Mari secara intuitif memahami apa yang terjadi di sini. Dengan menghitung residu, kami menentukan seberapa baik (atau buruk) tujuan dari setiap sampel pelatihan diprediksi oleh model saat ini f. Kemudian kami melatih pohon lain untuk memperbaiki kesalahan model saat ini (itulah sebabnya kami menggunakan sisa daripada label yang sebenarnya) dan menambahkan pohon baru ke model yang ada dengan beberapa bobot α. Akibatnya, setiap pohon baru ditambahkan ke model sebagian memperbaiki kesalahan yang dibuat oleh pohon sebelumnya. Proses berlanjut sampai jumlah maksimum M (hyperparameter lain) dari pohon-pohon tersebut digabungkan.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Sekarang mari kita coba menjawab pertanyaan mengapa algoritma ini disebut gradient boosting. Dalam meningkatkan gradien, kami tidak menghitung gradien, tidak seperti apa yang kami lakukan di bab 4, memecahkan masalah regresi linier. Untuk melihat kesamaan antara peningkatan gradien dan penurunan gradien, ingat mengapa kami menghitung gradien dalam regresi linier: untuk mengetahui arah nilai parameter untuk meminimalkan fungsi biaya MSE. Gradien menunjukkan arah, tetapi tidak menunjukkan seberapa jauh untuk pergi ke arah ini, jadi di setiap iterasi kami mengambil langkah kecil, dan kemudian menentukan arah. Hal yang sama terjadi pada peningkatan gradien, tetapi alih-alih secara langsung menghitung gradien, kami menggunakan estimasi dalam bentuk residual: mereka menunjukkan bagaimana model harus disesuaikan untuk mengurangi kesalahan (residual).</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dalam meningkatkan gradien, tiga hiperparameter utama tersedia untuk penyetelan: jumlah pohon, kecepatan belajar, dan kedalaman pohon. Ketiganya mempengaruhi keakuratan model. Kedalaman pohon juga mempengaruhi kecepatan belajar dan perkiraan: semakin kecil kedalamannya, semakin cepat. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dapat ditunjukkan bahwa pembelajaran dengan residual mengoptimalkan keseluruhan model f untuk standar kesalahan standar. Di sini Anda dapat melihat perbedaan dari mengantongi: meningkatkan mengurangi bias (atau kurangnya pendidikan) alih-alih varian. Akibatnya, peningkatan dapat dilakukan pelatihan ulang. Namun, dengan menyesuaikan kedalaman dan jumlah pohon, pelatihan ulang sebagian besar dapat dihindari.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Gradient boost sama untuk tugas grading, tetapi langkah-langkahnya sedikit berbeda. Pertimbangkan kasus klasifikasi biner. Misalkan ada pohon keputusan regresi M. Dengan analogi dengan regresi logistik, perkiraan ansambel pohon keputusan dimodelkan menggunakan fungsi sigmoid:</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/wy/pd/gw/wypdgwjjgpzuojrelehnadtdggc.jpeg" alt="gambar"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
di mana </font></font><img src="https://habrastorage.org/webt/w6/3d/kb/w63dkbj3f9j-ik9hrhqbexyxtem.jpeg" alt="gambar"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">pohon regresi. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dan lagi, seperti dalam regresi logistik, ketika mencoba menemukan model untuk memaksimalkan </font></font><img src="https://habrastorage.org/webt/c4/lv/tt/c4lvttexfzr8disbsv1ph_drzka.jpeg" alt="gambar"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, prinsip kemungkinan maksimum diterapkan. Demikian pula, untuk menghindari limpahan numerik, kami memaksimalkan jumlah dari logaritma kemungkinan, bukan produk dari kemungkinan. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Algoritma dimulai dengan model konstanta awal di </font></font><img src="https://habrastorage.org/webt/hn/7j/ep/hn7jepdxhudnxvjjgbulvfwtpyw.jpeg" alt="gambar"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">mana </font></font><img src="https://habrastorage.org/webt/gj/6s/f6/gj6sf6i874m3gq3_fbbxbkrudae.jpeg" alt="gambar"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(Dapat ditunjukkan bahwa inisialisasi tersebut optimal untuk fungsi sigmoid.) Kemudian, pada setiap iterasi m, pohon fm baru ditambahkan ke model. Untuk menemukan pohon terbaik </font></font><img src="https://habrastorage.org/webt/b0/5w/rv/b05wrvcuk5hzvnrmkpebdnk_kxu.jpeg" alt="gambar"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Untuk menemukan pohon terbaik </font></font><img src="https://habrastorage.org/webt/xm/u4/dp/xmu4dpm1hi3-podtuiayxggydti.jpeg" alt="gambar"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, turunan parsial dari </font></font><img src="https://habrastorage.org/webt/ep/hx/gu/ephxgusen4ou8pc8atm4ksqathq.jpeg" alt="gambar"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">model saat ini </font><font style="vertical-align: inherit;">pertama </font><font style="vertical-align: inherit;">- </font><font style="vertical-align: inherit;">tama dihitung </font><font style="vertical-align: inherit;">untuk setiap i = 1, ..., N:</font></font><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/wg/v_/oo/wgv_oogvmupu4q5j6g3rq7dphvk.jpeg" alt="gambar"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
di mana f adalah model dari ensemble classifier yang dibangun di atas iterasi sebelumnya m - 1. Untuk menghitung </font></font><img src="https://habrastorage.org/webt/ep/hx/gu/ephxgusen4ou8pc8atm4ksqathq.jpeg" alt="gambar"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, kita perlu menemukan turunan dari </font></font><img src="https://habrastorage.org/webt/pv/pe/lb/pvpelb2jplsmq_jmkbdfyl-pzom.jpeg" alt="gambar"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">sehubungan dengan f untuk semua i. </font><font style="vertical-align: inherit;">Perhatikan bahwa </font></font><img src="https://habrastorage.org/webt/nk/zn/0l/nkzn0lh0l0brnzl_vh0xxlmttrq.jpeg" alt="gambar"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">turunan sehubungan dengan f dari istilah yang tepat dalam persamaan sebelumnya adalah</font></font><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/xx/en/iu/xxeniu2qr35ln17asbfk2fqb3sc.jpeg" alt="gambar"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Kemudian, set pelatihan diubah dengan mengganti label asli dari </font></font><img src="https://habrastorage.org/webt/a1/fv/cu/a1fvcukqqvsu5wpv3x8i1zj5smc.jpeg" alt="gambar"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">turunan parsial yang sesuai </font></font><img src="https://habrastorage.org/webt/ep/hx/gu/ephxgusen4ou8pc8atm4ksqathq.jpeg" alt="gambar"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, dan pohon baru dibangun berdasarkan set pelatihan yang dikonversi. </font></font><img src="https://habrastorage.org/webt/w1/8f/41/w18f41gto37doyvyakgs4np_fyy.jpeg" alt="gambar"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Selanjutnya, langkah pembaruan optimal ditentukan </font></font><img src="https://habrastorage.org/webt/1n/cf/sj/1ncfsjxfe-tao3ep_csuw-s-arw.jpeg" alt="gambar"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">sebagai:</font></font><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/co/qc/hw/coqchwpctxgy2ukbdwaybkouxx0.jpeg" alt="gambar"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pada akhir iterasi m, kami memperbarui model ensemble dengan </font></font><img src="https://habrastorage.org/webt/r7/ox/vy/r7oxvyc5mesbifpwfjtkcrtdr2q.jpeg" alt="gambar"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">menambahkan pohon baru</font></font><img src="https://habrastorage.org/webt/s2/vp/u0/s2vpu0-7pmzuzv0n55f1z1fgktu.jpeg" alt="gambar"><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/vg/ar/qn/vgarqnddik0vjhxxfsesr1t5qs8.jpeg" alt="gambar"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Iterasi berlanjut sampai kondisi m = M terpenuhi, setelah itu pelatihan berhenti dan model ensemble f diperoleh. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Meningkatkan gradien adalah salah satu algoritma pembelajaran mesin yang paling kuat. </font><font style="vertical-align: inherit;">Bukan hanya karena ia menciptakan model yang sangat akurat, tetapi juga karena ia mampu memproses kumpulan data besar dengan jutaan data dan fitur. </font><font style="vertical-align: inherit;">Sebagai aturan, ini lebih unggul dalam akurasi dibandingkan dengan hutan acak, tetapi karena sifatnya yang konsisten, ia dapat belajar lebih lambat.</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id488346/index.html">Menginstal atau-alat dengan SCIP dan GLPK di lingkungan virtual Python 3.7 di Linux</a></li>
<li><a href="../id488348/index.html">Webinar "Sepuluh Tantangan Agile Top dan Cara Mengatasi Mereka dalam Satu Jam" 17 Februari pukul 20:00 waktu Moskow</a></li>
<li><a href="../id488352/index.html">Perbandingan Biaya VDI: On-premise versus Cloud Publik</a></li>
<li><a href="../id488356/index.html">Pelatihan di Universitas Teknik Kelautan State Petersburg untuk produk Dassault Systèmes</a></li>
<li><a href="../id488360/index.html">Mitos Data Besar dan Budaya Digital</a></li>
<li><a href="../id488366/index.html">Dan lagi tentang "Informasi zona waktu salah untuk zona waktu Rusia" [. Net bug, ID: 693286]</a></li>
<li><a href="../id488368/index.html">Apa yang saya pelajari saat mengerjakan proyek skala besar pertama saya</a></li>
<li><a href="../id488370/index.html">TDD untuk mikrokontroler. Bagian 2: Bagaimana mata-mata menyingkirkan kecanduan</a></li>
<li><a href="../id488374/index.html">Telegram + 1C + Webhooks + Apache + Sertifikat yang ditandatangani sendiri</a></li>
<li><a href="../id488376/index.html">Ketika prinsip "ke neraka dengan segalanya, ambil dan lakukan itu!" tidak bekerja: catatan penunda</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>