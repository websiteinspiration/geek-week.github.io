<!doctype html>
<html class="no-js" lang="fr">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🌙 🤕 👴🏽 Apprentissage automatique Unity: apprendre aux agents MO à sauter par-dessus les murs 🙍🏾 🎯 🚶🏿</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Il y a eu des percées majeures dans l'apprentissage par renforcement (RL) au cours des dernières années: de la première utilisation réussie de celui-c...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Apprentissage automatique Unity: apprendre aux agents MO à sauter par-dessus les murs</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/pixonic/blog/492548/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Il y a eu des percées majeures dans l'apprentissage par renforcement (RL) au cours des dernières années: de la première utilisation réussie de celui-ci dans la formation de pixels bruts à la formation de robots AI ouverts, et des environnements de plus en plus sophistiqués sont nécessaires pour de nouveaux progrès, auxquels L'unité vient. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
L'outil Unity ML-Agents est un nouveau plugin dans le moteur de jeu Unity, vous permettant d'utiliser Unity comme constructeur d'environnement pour former des agents MO. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
De jouer au football à marcher, sauter des murs et apprendre à maîtriser l'IA avec un bâton, Unity ML-Agents Toolkit fournit un large éventail de conditions d'entraînement pour les agents. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dans cet article, nous verrons comment fonctionnent les agents Unity MO, puis nous apprendrons à l'un de ces agents à sauter par-dessus les murs.</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/9c6/5ad/bdb/9c65adbdbb38b939262c10e5ac728dde.gif" alt="image"><br>
<br>
<a name="habracut"></a><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Qu'est-ce que les agents Unity ML?</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Unity ML-Agents est un nouveau plugin pour le moteur de jeu Unity, qui vous permet de créer ou d'utiliser des environnements prêts à l'emploi pour former nos agents. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Le plugin se compose de trois composants: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/zo/mx/m_/zomxm_3g9svsej0iarjpw1vbnta.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Le premier - un </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">environnement d'apprentissage</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ( </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l'environnement d'apprentissage</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ), contenant des scènes d'Unity et des éléments environnementaux. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La seconde est l' </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">API Python</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , dans laquelle se trouvent les algorithmes RL (tels que PPO - Proximal Policy Optimization et SAC - Soft Actor-Critic). </font><font style="vertical-align: inherit;">Nous utilisons cette API pour lancer la formation, les tests, etc. Elle est connectée à l'environnement d'apprentissage via le troisième composant - </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">un communicateur externe</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En quoi consiste l'environnement d'apprentissage</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Le volet formation comprend différents éléments: </font></font><br>
 <br>
<img src="https://habrastorage.org/webt/hy/1u/pt/hy1upt9en2esnt-amk-iipklmn4.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
le premier agent est l'acteur de la scène. </font><font style="vertical-align: inherit;">C'est lui que nous formerons en optimisant un composant appelé «cerveau», dans lequel il est enregistré quelles actions doivent être effectuées dans chacun des états possibles. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Le troisième élément, l'Académie, gère les agents et leurs processus décisionnels et traite les demandes de l'API Python. </font><font style="vertical-align: inherit;">Pour mieux comprendre son rôle, rappelons le processus RL. </font><font style="vertical-align: inherit;">Il peut être représenté comme un cycle qui fonctionne comme suit: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/5y/ge/ef/5ygeefzasafou6bkwggdnaq7ows.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Supposons qu'un agent ait besoin d'apprendre à jouer à un jeu de plateforme. </font><font style="vertical-align: inherit;">Le processus RL dans ce cas ressemblera à ceci:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">L'agent reçoit l'état </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">S </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0</font></font></sub></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> de l'environnement - ce sera la première image de notre jeu.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sur la base de l'état </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">S </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0, l'</font></font></sub></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> agent exécute l'action </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0</font></font></sub></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> et se décale vers la droite.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">L'environnement passe dans un nouvel état </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">S </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">L'agent reçoit </font><i><sub><font style="vertical-align: inherit;">une</font></sub></i><font style="vertical-align: inherit;"> récompense </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">R </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> pour ne pas être mort ( </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">récompense positive</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> +1).</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ce cycle RL forme une séquence d'état, d'action et de récompense. </font><font style="vertical-align: inherit;">L'objectif de l'agent est de maximiser la récompense totale attendue. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/qm/mr/ml/qmmrmlkz7urj8cywqpsg1ejajve.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ainsi, Academy envoie des instructions aux agents et assure la synchronisation de leur exécution, à savoir:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Collection d'observations;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Le choix de l'action conformément aux instructions fixées;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Exécution de l'action;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Réinitialisez si le nombre d'étapes a été atteint ou si l'objectif a été atteint.</font></font></li>
</ul><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nous apprenons à l'agent à sauter à travers les murs</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Maintenant que nous savons comment fonctionnent les agents Unity, nous allons en former un à sauter à travers les murs. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Les modèles déjà formés peuvent également être téléchargés sur </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GitHub</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Environnement d'apprentissage du saut de mur</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Le but de cet environnement est d'apprendre à l'agent à se rendre à la tuile verte. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Prenons trois cas: </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
1. Il n'y a pas de murs, et notre agent n'a qu'à se rendre sur le carreau. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/17a/8fd/da8/17a8fdda8bbe767f64a34d8d05fd9be9.png" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
2. L'agent doit apprendre à sauter pour atteindre la tuile verte. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/d2c/149/0d0/d2c1490d0b90c4fdefb8eb4922f446b2.png" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
3. Le cas le plus difficile: le mur est trop haut pour que l'agent puisse sauter dessus, il doit donc d'abord sauter sur le bloc blanc. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/458/175/9b6/4581759b653a175597054fcf60a0050e.png" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nous apprendrons à l'agent deux scénarios de comportement en fonction de la hauteur du mur:</font></font><br>
<br>
<ul>
<li><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SmallWallJump</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> dans les cas sans murs ou à faible hauteur de mur;</font></font></li>
<li><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">BigWallJump</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> dans le cas de hauts murs.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Voici à quoi ressemblera le système de récompense: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/rt/lo/hm/rtlohmnturcucyikrnhfsadztgi.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dans nos observations, nous n'utilisons pas une trame régulière, mais 14 reykast, chacun pouvant détecter 4 objets possibles. </font><font style="vertical-align: inherit;">Dans ce cas, le reykast peut être perçu comme des faisceaux laser qui peuvent déterminer s'ils traversent un objet. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nous utiliserons également la position d'agent mondial dans notre programme. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/a06/0fc/fa2/a060fcfa26e80154ef988d76dc9b8f05.png" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Quatre options sont possibles dans notre espace: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ni/g6/zs/nig6zskfzterisy_hskg3k-rsnk.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
L'objectif est de réaliser une tuile verte </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">avec une récompense moyenne de 0,8</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Alors, commençons!</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Tout d'abord, ouvrez le projet </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">UnitySDK</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Parmi les exemples, vous devez trouver et ouvrir la scène </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">WallJump</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Comme vous pouvez le voir, il y a de nombreux agents sur scène, chacun provenant du même préfabriqué, et ils ont tous le même «cerveau». </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/bad/195/820/bad195820da3e5b2c74309e9f44e9935.png" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Comme dans le cas du Deep Reinforcement Learning classique, après avoir lancé plusieurs instances du jeu (par exemple, 128 environnements parallèles), il ne nous reste plus qu'à copier et coller les agents pour avoir des états plus différents. Et puisque nous voulons former notre agent à partir de zéro, nous devons d'abord retirer le «cerveau» de l'agent. Pour ce faire, accédez au dossier prefabs et ouvrez Prefab.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ensuite, dans la hiérarchie Prefab, vous devez sélectionner l'agent et accéder aux paramètres. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dans les paramètres de comportement, vous devez supprimer le modèle. Si nous avons plusieurs GPU à notre disposition, vous pouvez utiliser le périphérique d'inférence du CPU comme GPU. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/d0d/bdb/6ec/d0dbdb6ec05655b6fecdf675d629d126.png" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dans le composant Wall Jump Agent, vous devez supprimer les cerveaux pour un boîtier sans murs, ainsi que pour les murs bas et hauts. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/618/e81/c8c/618e81c8c3c0cb15ae5fbca791c33c8c.png" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Après cela, vous pouvez commencer à former votre agent à partir de zéro. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pour notre première formation, nous modifions simplement le nombre total d'étapes de formation pour deux scénarios de comportement: SmallWallJump et BigWallJump. Ainsi, nous pouvons atteindre l'objectif en seulement 300 000 étapes. Pour ce faire, dans </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">config / trainer config.yaml,</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> remplacez max_steps par 3e5 pour les cas SmallWallJump et BigWallJump.</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/b80/bbb/1b2/b80bbb1b21f0f9e445fa8d2583c39d52.png" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pour former notre agent, nous utiliserons </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PPO</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (Proximal Policy Optimization). L'algorithme comprend l'accumulation d'expérience dans l'interaction avec l'environnement et son utilisation pour mettre à jour les politiques décisionnelles. Après sa mise à jour, les événements précédents sont supprimés et la collecte de données suivante est déjà effectuée selon les termes de la politique mise à jour. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Donc, tout d'abord, en utilisant l'API Python, nous devons appeler un communicateur externe afin qu'il ordonne à l'Académie de lancer des agents. Pour ce faire, ouvrez le terminal où se trouve ml-agents-master et saisissez-le: </font></font><br>
<br>
<code>mlagents-learn config/trainer_config.yaml — run-id=”WallJump_FirstTrain” — train</code><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Cette commande vous demandera de démarrer la scène Unity. Pour ce faire, appuyez sur ► en haut de l'éditeur. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/212/874/4fb/2128744fb143fec6308d95938e0383f7.png" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Vous pouvez regarder la formation de vos agents dans Tensorboard avec la commande suivante:</font></font><br>
<br>
<code>tensorboard — logdir=summaries</code><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Une fois la formation terminée, vous devez déplacer les fichiers de modèle enregistrés contenus dans </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ml-agents-master / models vers UnitySDK / Assets / ML-Agents / examples / WallJump / TFModels</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Ensuite, ouvrez à nouveau l'éditeur Unity et sélectionnez la scène </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">WallJump</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , où nous ouvrons l'objet </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">WallJumpArea</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> terminé </font><font style="vertical-align: inherit;">. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Après cela, sélectionnez l'agent et dans ses paramètres de comportement, faites glisser le fichier </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SmallWallJump.nn</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> dans l' </font><i><font style="vertical-align: inherit;">espace réservé</font></i><font style="vertical-align: inherit;"> au modèle. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/d0d/bdb/6ec/d0dbdb6ec05655b6fecdf675d629d126.png" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Déplacer également:</font></font><br>
<br>
<ol>
<li><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SmallWallJump.nn</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> à No Wall Brain Placeholder.</font></font></li>
<li><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SmallWallJump.nn</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> dans l' </font><i><font style="vertical-align: inherit;">espace réservé</font></i><font style="vertical-align: inherit;"> au petit mur de cerveau.</font></font></li>
<li><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">BigWallJump.nn</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> à No Wall Brain Placeholder.</font></font></li>
</ol><br>
<img src="https://habrastorage.org/getpro/habr/post_images/618/e81/c8c/618e81c8c3c0cb15ae5fbca791c33c8c.png" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Après cela, appuyez sur le bouton ► en haut de l'éditeur et vous avez terminé! </font><font style="vertical-align: inherit;">L'algorithme de configuration de la formation des agents est maintenant terminé.</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/9c6/5ad/bdb/9c65adbdbb38b939262c10e5ac728dde.gif" alt="image"><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Temps d'expérimentation</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La meilleure façon d'apprendre est d'essayer constamment d'apporter quelque chose de nouveau. </font><font style="vertical-align: inherit;">Maintenant que nous avons déjà obtenu de bons résultats, nous allons essayer de poser quelques hypothèses et de les tester.</font></font><br>
<br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Réduire le coefficient d'actualisation à 0,95</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nous savons donc que:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Plus le gamma est grand, plus la remise est faible. </font><font style="vertical-align: inherit;">Autrement dit, l'agent est plus préoccupé par les récompenses à long terme.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En revanche, plus le gamma est petit, plus la remise est importante. </font><font style="vertical-align: inherit;">Dans ce cas, la priorité de l'agent est la rémunération à court terme.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
L'idée de cette expérience est que si nous augmentons la remise en diminuant la gamme de 0,99 à 0,95, la récompense à court terme sera une priorité pour l'agent - ce qui peut l'aider à aborder rapidement la politique de comportement optimale. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/u_/cm/hv/u_cmhvpc0dk-npolzz49loxofiq.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Fait intéressant, dans le cas d'un saut à travers un muret, l'agent s'efforcera d'obtenir le même résultat. </font><font style="vertical-align: inherit;">Cela peut s'expliquer par le fait que ce cas est assez simple: l'agent n'a qu'à se déplacer vers la tuile verte et, si nécessaire, à sauter s'il y a un mur devant. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/yc/ic/tt/ycicttb0mkwxmwn5c1ghgm2iiiu.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En revanche, dans le cas de Big Wall Jump, cela fonctionne moins bien, car notre agent se soucie plus de la récompense à court terme et ne comprend donc pas qu'il doit grimper sur le bloc blanc pour sauter par-dessus le mur.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Augmentation de la complexité du réseau neuronal</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Enfin, nous émettons l'hypothèse que notre agent deviendra plus intelligent si nous augmentons la complexité du réseau neuronal. </font><font style="vertical-align: inherit;">Pour ce faire, augmentez la taille du niveau caché de 256 à 512. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Et nous constatons que dans ce cas le nouvel agent fonctionne moins bien que notre premier agent. </font><font style="vertical-align: inherit;">Cela signifie que cela n'a aucun sens pour nous d'augmenter la complexité de notre réseau, car sinon le temps d'apprentissage augmentera également. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/zo/g7/qm/zog7qm1uemu7l7taqc5uuwlqauw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nous avons donc formé l'agent à sauter par-dessus les murs, et c'est tout pour aujourd'hui. </font><font style="vertical-align: inherit;">Rappelons que pour comparer les résultats, des modèles formés peuvent être téléchargés </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ici</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/af6/f00/ce3/af6f00ce3784c60469522296276d2651.gif" alt="image"></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr492530/index.html">Combiner les MVVM basés dans les applications UIKit et SwiftUI pour les développeurs UIKit</a></li>
<li><a href="../fr492534/index.html">Existe-t-il ou non de vrais ouragans à Moscou? Nous analysons le cas du 13 mars 2020 en pleine poursuite</a></li>
<li><a href="../fr492538/index.html">Wrike TechClub: Infrastructure de livraison - processus et outils (DevOps + QAA). Articles en anglais</a></li>
<li><a href="../fr492540/index.html">Le jeu "Attendez un instant!" sur arduino</a></li>
<li><a href="../fr492546/index.html">Vérification de la vulnérabilité de tout site utilisant Nikto</a></li>
<li><a href="../fr492552/index.html">Comment vivre et travailler en quarantaine à Barcelone</a></li>
<li><a href="../fr492558/index.html">Bonjour, c'est COVID19: le coronavirus vit-il à la surface d'un smartphone?</a></li>
<li><a href="../fr492560/index.html">Table de hachage simple pour GPU</a></li>
<li><a href="../fr492562/index.html">Trois webinaires Apache Ignite utiles dans votre programme de quarantaine</a></li>
<li><a href="../fr492566/index.html">Analyse de la combinaison d'un algorithme de recherche de clics gourmand avec énumération partielle des sommets des graphes</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>