<!doctype html>
<html class="no-js" lang="ja">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👩🏻‍🎨 🦁 ↙️ 強化学習または進化戦略？- どちらも 🤸🏻 ↘️ 🐈</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="こんにちは、ハブル！
 
 コードや明確な学術的焦点なしに、2年前にテキストの翻訳をここに投稿することはめったにありませんが、今日は例外とします。記事のタイトルで提起されたジレンマが多くの読者の関心事であり、この記事が既に論争されている進化的戦略に関する基本的な研究を読んだか、今読んだことを願ってい...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>強化学習または進化戦略？- どちらも</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/456160/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">こんにちは、ハブル！</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
コードや明確な学術的焦点なしに、2年前にテキストの翻訳をここに投稿することはめったにありませんが、今日は例外とします。</font><font style="vertical-align: inherit;">記事のタイトルで提起されたジレンマが多くの読者の関心事であり、この記事が既に論争されている進化的戦略に関する基本的な研究を読んだか、今読んだことを願っています。</font><font style="vertical-align: inherit;">猫へようこそ！</font></font><br>
<br>
<img src="https://habrastorage.org/webt/-n/u-/i6/-nu-i6enynr12ma1d7utan_7ml8.jpeg"><br>
<a name="habracut"></a> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
2017年3月、OpenAIは「</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">強化学習のスケーラブルな代替手段としての進化戦略</font></a><font style="vertical-align: inherit;">」という記事を公開することで、ディープラーニングコミュニティで大騒ぎしました。</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">…」この研究では、強化（RL）を使用したトレーニングでは光が収束しなかったという事実を支持して印象的な結果が説明されました。複雑なニューラルネットワークをトレーニングするときは、他の方法を試すことをお勧めします。次に、強化学習の重要性と、問題を解決するための学習における「強制」テクノロジーのステータスにどれだけの価値があるかについての議論が生じました。ここで、これら2つのテクノロジーを競合するものと見なすべきではないという事実についてお話ししたいと思います。逆に、それらは最終的にお互いを補完します。実際、</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">一般的なAI</font></a><font style="vertical-align: inherit;">を作成するために何が必要かについて少し考えれば</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">そして、その存在を通じて学習、判断、計画を立てることができるようなシステムでは、ほぼ確実に、これまたはその組み合わせたソリューションが必要になるという結論に達します。</font><font style="vertical-align: inherit;">ちなみに、進化の過程で哺乳類や他の高等動物の複雑な知能が与えられ、複合的な決定に至ったのは自然なのです。</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">進化戦略</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
OpenAI記事の主な論文は、従来の逆伝播と組み合わせて強化学習を使用する代わりに、いわゆる「進化的戦略」（ES）を使用して複雑な問題を解決するためにニューラルネットワークを正常にトレーニングしたというものでした。このようなESアプローチは、ネットワークスケールでの重み値の分布を維持することにあり、多くのエージェントが並行して動作し、この分布から選択されたパラメーターを使用します。各エージェントは独自の環境で動作し、一定数のエピソードまたはエピソードのステージが完了すると、アルゴリズムは、フィットネススコアとして表される合計報酬を返します。この値を指定すると、パラメーターの分布をより成功したエージェントにシフトし、成功しなかったエージェントを奪うことができます。何百人ものエージェントが関与するこのような操作を何百回も繰り返すと、重みの分布を空間に移動して、エージェントがタスクを解決するための品質ポリシーを策定できるようになります。実際、記事で提示された結果は印象的です。1,000のエージェントを並行して実行すると、2本の脚の擬人化動作を30分未満で調査できることが示されています（最も高度なRLメソッドでも1時間以上必要です）。より詳細なレビューについては、優れた1000のエージェントを並行して実行すると、2本の足の擬人化動作を30分未満で調査できます（最も高度なRLメソッドでも1時間以上必要です）。詳細なレビューについては、私は優れたを読むことをお勧めします1000のエージェントを並行して実行すると、2本の脚の擬人化動作を30分未満で調査できます（最も高度なRLメソッドでも1時間以上必要です）。詳細なレビューについては、私は優れたを読むことをお勧めします</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">実験の著者からの</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">投稿</font></a><font style="vertical-align: inherit;">、および</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">科学記事</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">自体</font><font style="vertical-align: inherit;">。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/0j/lp/ms/0jlpmsa3-jz8ono405nv8c79ve8.gif"><br>
 <br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">OpenAIのESメソッドを使用して研究された擬人化直立姿勢のさまざまな学習戦略。</font></font></i><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ブラックボックス</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
この方法の大きな利点は、並列化が簡単なことです。 A3CなどのRLメソッドでは、ワークフローとパラメーターサーバー間の情報交換が必要ですが、ESで必要なのは、パラメーターの分布に関する妥当性の見積もりと一般化された情報だけです。このような単純さのおかげで、このメソッドはスケーラビリティにおいて最新のRLメソッドをバイパスします。ただし、これらすべてが無駄ではありません。ブラックボックスの原則に基づいてネットワークを最適化する必要があります。この場合、「ブラックボックス」とは、トレーニング中にネットワークの内部構造が完全に無視され、全体の結果（エピソードに対する報酬）のみが使用されることを意味し、特定のネットワークの重みが将来の世代に継承されるかどうかに依存します。状況で環境から顕著なフィードバックが得られない場合、および従来のRL関連の多くのタスクを解決する場合、報酬フローは非常に希薄化します。問題は「部分的にブラックボックス」から「完全にブラックボックス」に変わります。この場合、生産性を大幅に向上させることができるので、当然、そのような妥協は正当化されます。 「まだ絶望的にうるさいのであれば、誰が勾配を必要としますか？」 -これは一般的な意見です。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ただし、フィードバックがより活発な状況では、ESの問題がうまくいかなくなり始めています。</font><font style="vertical-align: inherit;">OpenAIチームは、ESを使用して単純分類ネットワークMNISTがどのようにトレーニングされたかを説明し、今回はトレーニングが1000倍遅くなりました。</font><font style="vertical-align: inherit;">実際、画像の分類における勾配信号は、ネットワークをより適切に分類する方法について非常に有益です。</font><font style="vertical-align: inherit;">したがって、この問題はRLテクニックとはあまり関係がありませんが、ノイズの多い勾配を生成する環境では、報酬はまばらです。</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">自然が見つけたソリューション</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
自然の例から学び、AIを開発する方法を考えてみると、AIは</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">問題指向のアプローチ</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">として表現できる場合があります</font><font style="vertical-align: inherit;">。結局のところ、自然はコンピュータ科学者が持っていないような制限内で動作します。特定の問題を解決するための純粋に理論的なアプローチは、経験的な選択肢よりも効果的な解決策を提供できるという意見があります。それにもかかわらず、特定の制限（地球）の条件下で動作する動的システムが、柔軟で複雑な動作が可能なエージェント（動物、特に哺乳動物）を形成した方法を確認することをお勧めします。これらの制限の一部は、データサイエンスのシミュレーションの世界では適用できませんが、他の制限は非常に優れています。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
哺乳類の知的行動を調べたところ、密接に関連する2つのプロセスの複雑な相互作用の結果として形成されていることがわかりました。</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">他者の経験</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">からの</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">学習と私たち自身の経験からの学習</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。最初は自然選択による進化としばしば識別されますが、ここでは、エピジェネティクス、微生物叢、および互いに遺伝的に関連しない生物間の経験の交換を保証する他のメカニズムを考慮に入れるために、より広い用語を使用します。 2番目のプロセスである直接学習は、動物が生涯を通じて同化するすべての情報であり、この情報は、この動物と外界との相互作用に直接関連しています。このカテゴリには、オブジェクトの認識を学習することから、教育プロセスに固有のコミュニケーションを習得することまで、すべてが含まれます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
大まかに言えば、自然界で発生するこれら2つのプロセスは、ニューラルネットワークを最適化するための2つのオプションと比較できます。身体に関する情報を更新するために勾配情報が使用される進化的戦略は、他の誰かの経験から学ぶことに近づきます。同様に、1つまたは別の経験を積むことでエージェントの行動に1つまたは別の変化が生じる勾配法は、経験から学ぶことに相当します。知的行動の多様性、またはこれら2つのアプローチのそれぞれが動物で発達させる能力について考える場合、そのような比較はより顕著になります。どちらの場合も、「進化的方法」は、特定のフィットネス（生き続けるために十分）の発達を可能にする反応行動の研究に貢献します。捕われの身から歩いたり脱出したりすることを学ぶことは、多くの場合、遺伝レベルで多くの動物に「固定されている」より「本能的な」行動に相当します。さらに、この例は、進化的方法がシグナル報酬が非常にまれな場合（たとえば、子牛の飼育の成功など）に適用できることを確認しています。そのような場合、報酬をこの事実が始まる前に何年も前に犯されたかもしれない特定の一連の行動と関連付けることは不可能です。一方、ESが失敗した場合、つまり画像の分類を考えると、100年以上に及ぶ数えきれないほどの行動心理実験の結果として得られた動物訓練の結果と非常によく似ています。この例は、シグナル報酬が非常にまれな場合（たとえば、子の子育てが成功した場合など）にも、進化的手法が適用できることを示しています。そのような場合、報酬をこの事実が始まる前に何年も前に犯されたかもしれない特定の一連の行動と関連付けることは不可能です。一方、ESが失敗した場合、つまり画像の分類を考えると、100年以上に及ぶ無数の行動心理実験の動物訓練の結果と非常によく似た結果になります。この例は、シグナル報酬が非常にまれな場合（たとえば、子の子育てが成功した場合など）にも、進化的手法が適用できることを示しています。そのような場合、報酬をこの事実が始まる前に何年も前に犯されたかもしれない特定の一連の行動と関連付けることは不可能です。一方、ESが失敗した場合、つまり画像の分類を考えると、100年以上に及ぶ無数の行動心理実験の動物訓練の結果と非常によく似た結果になります。そのような場合、報酬をこの事実が始まる前に何年も前に犯されたかもしれない特定の一連の行動と関連付けることは不可能です。一方、ESが失敗した場合、つまり画像の分類を考えると、100年以上に及ぶ数えきれないほどの行動心理実験の結果として得られた動物訓練の結果と非常によく似ています。そのような場合、報酬をこの事実が始まる前に何年も前に犯されたかもしれない特定の一連の行動と関連付けることは不可能です。一方、ESが失敗した場合、つまり画像の分類を考えると、100年以上に及ぶ無数の行動心理実験の動物訓練の結果と非常によく似た結果になります。100年以上で過ごしました。100年以上で過ごしました。</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">動物の訓練</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
強化学習で使用される方法は、多くの場合、</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">オペラント条件付け</font></a><font style="vertical-align: inherit;">に関する心理学的文献から直接取得され</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ます</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">そして、オペラント条件付けは動物心理学に基づいて調査されました。ちなみに、強化訓練の創設者2人の1人であるリチャードサットンは、心理学の学士号を取得しています。オペラント条件付けのコンテキストでは、動物は報酬または罰を特定の行動パターンと関連付けることを学びます。トレーナーと研究者は、何らかの形で報酬との関連を操作して、動物に工夫や特定の行動を促すことができます。ただし、動物の研究で使用されるオペラント条件付けは、動物が一生を通じて訓練されることに基づいて、その条件付けのより顕著な形態に過ぎません。私たちは常に環境からポジティブな強化信号を受け取り、それに応じて行動を調整します。実際、多くの神経生理学者や認知科学者は、実際に人や他の動物はさらに1レベル高いレベルで行動し、将来の状況での行動の結果を予測することを常に学習し、潜在的な報酬を期待しています。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
自己学習における予測の中心的な役割は、上記のダイナミクスを最も重要な方法で変更することです。以前は非常に希薄化された（一時的な報酬）と見なされていた信号は非常に密集しています。理論的には、状況はおおよそ次のとおりです。動物は単にこのストリームに浸されているだけで、哺乳動物の脳はいつでも、感覚刺激とアクションの複雑なストリームに基づいて結果を計算します。この場合、動物の最終的な行動は密な信号を与え、予測の修正と行動の発達によって導かれなければなりません。脳はこれらのすべての信号を使用して、将来の予測（したがって、実行されるアクションの品質）を最適化します。このアプローチの概要は、優れた書籍「</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">Surfing Uncertainty</font></a><font style="vertical-align: inherit;">」に記載されています。</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">「認知と哲学者のアンディクラーク。</font><font style="vertical-align: inherit;">そのような推論が人工エージェントのトレーニングに外挿される場合、強化トレーニングは根本的な欠陥を明らかにします。このパラダイムで使用される信号は、あり得る（またはそうあるべきである）と比較して絶望的に弱いです。</font><font style="vertical-align: inherit;">信号の飽和度を上げることが不可能な場合（定義上、弱いか、低レベルの反応性に関連している可能性があるため）、ESなどの並列化されたトレーニング方法を選択することをお勧めします。</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ニューラルネットワークのより良い学習</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
絶えず予測に従事している哺乳類の脳に固有のより高い神経活動の原則に基づいて、最近、そのような予測の重要性を考慮した強化訓練で特定の成功を収めることが可能になりました。</font><font style="vertical-align: inherit;">私はあなたに2つの類似した作品をお勧めします：</font></font><br>
<br>
<ul>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">未来を予測して行動することを学ぶ</font></font></a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">教師なし補助タスクを使用した強化学習</font></font></a></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
これらの記事の両方で、著者は典型的なデフォルトのニューラルネットワークポリシーを、将来の環境条件に関する予測結果で補足しています。最初の記事では、予測はさまざまな測定変数に適用され、2番目の記事では、環境の変化とそのようなエージェントの動作などが適用されます。どちらの場合も、ポジティブ補強に関連するスパース信号ははるかに飽和して情報が多くなり、加速された学習とより複雑な行動モデルの同化の両方が提供されます。このような改善は、勾配信号を使用するメソッドを使用する場合にのみ利用できます。ESなどの「ブラックボックス」の原理で動作するメソッドでは利用できません。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
さらに、直接学習と勾配法の方がはるかに効果的です。 ES法を使用して強化トレーニングを使用するよりも早く特定の問題を研究することができた場合でも、RLを使用した場合よりも何倍も多くのデータがES戦略に含まれるため、利益が得られました。この場合、動物での学習の原則について考えると、外国の例での訓練の結果は何世代にもわたって現れますが、動物が経験した単一のイベントで動物が永久にレッスンを学ぶのに十分な場合もあります。このような</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">例の</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ない</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">トレーニング</font></a><font style="vertical-align: inherit;">は、従来の勾配法にはまだ完全には適合していませんが、ESよりもはるかにわかりやすくなっています。たとえば、</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">神経の一時的な制御</font></a><font style="vertical-align: inherit;">などのアプローチがあります</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、ここでQ値は学習プロセスに格納されます。その後、アクションを実行する前にプログラムがそれらと一緒にチェックされます。</font><font style="vertical-align: inherit;">これにより、以前よりもはるかに速く問題を解決する方法を学ぶことができる勾配法がわかります。</font><font style="vertical-align: inherit;">エピソード神経制御に関する記事で、著者は人間の海馬に言及しています。これは、一度経験した経験でもイベントに関する情報を保存できるため、</font><font style="vertical-align: inherit;">リコールプロセスで</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">重要な役割を果たし</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ます。</font><font style="vertical-align: inherit;">このようなメカニズムでは、エージェントの内部組織へのアクセスが必要です。これは、ESのパラダイムでは定義上不可能です。</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">では、なぜそれらを組み合わせないのですか？</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
おそらく、この記事のほとんどは、私がRLメソッドを主張した印象を残していた可能性があります。ただし、実際には、長期的には、両方の方法を組み合わせることが最善の解決策であり、それぞれが最も適した状況で使用されると思います。多くのリアクティブポリシーの場合、またはポジティブな強化の信号が非常にまばらな状況では、特に大量並列トレーニングを実行できる計算能力がある場合、ESが勝ることは明らかです。一方、強化学習または教師トレーニングを使用した勾配法は、広範囲にわたるフィードバックが利用可能であり、問​​題の解決策を迅速かつ少ないデータで学習する必要がある場合に役立ちます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
自然に目を向けると、最初の方法が本質的に2番目の方法の基礎を築いていることがわかります。</font><font style="vertical-align: inherit;">そのため、進化の過程で、哺乳類は環境からの複雑な信号の素材から非常に効率的な学習を可能にする脳を発達させました。</font><font style="vertical-align: inherit;">したがって、問題は未解決のままです。</font><font style="vertical-align: inherit;">おそらく、進化的戦略は、勾配学習法に役立つ効果的な学習アーキテクチャを発明するのに役立ちます。</font><font style="vertical-align: inherit;">結局のところ、自然界で発見された解決策は確かに非常に成功しています。</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../ja456150/index.html">ささいな喜び＃4：ラドン-数で測定されたコード品質</a></li>
<li><a href="../ja456152/index.html">2Dゲームのレベルデザインパターン</a></li>
<li><a href="../ja456154/index.html">製品を作成する際のコアUX機能とMVP</a></li>
<li><a href="../ja456156/index.html">だから学校代数が必要です。</a></li>
<li><a href="../ja456158/index.html">核燃料源について少し</a></li>
<li><a href="../ja456162/index.html">Google、Tesla、Uberからの移民によって設立された会社であるAuroraは、自動車関連会社と協力し始めました</a></li>
<li><a href="../ja456164/index.html">ルーン風船は、マグニチュード8.0の地震の後、ペルーのネットワークとインターネットへの緊急接続を提供します</a></li>
<li><a href="../ja456168/index.html">何百万年も前にあなたの家はどこにありましたか？</a></li>
<li><a href="../ja456170/index.html">金融アプリケーションの作成方法：開発者を支援する5つのAPI</a></li>
<li><a href="../ja456172/index.html">パート2：RocketChip：RAMの接続</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>