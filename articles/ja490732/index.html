<!doctype html>
<html class="no-js" lang="ja">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>♿️ ⚡️ 💥 音声認識：非常に短い入門コース 🙅 🍮 🤱🏿</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="コンピュータの音声認識とそれをテキストに変換する作業について、素人にできるだけ簡単に伝えることはほとんど不可能です。複雑な式や数学的な用語がなければ、これに関する単一の話は完全ではありません。車が人間の声を認識することを学んだとき、スマートフォンが音声をどのように理解するか、そしてこの技術が予期しな...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>音声認識：非常に短い入門コース</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/toshibarus/blog/490732/"><img src="https://habrastorage.org/webt/tz/sh/ll/tzshllxzf2iddwai7sredy3edie.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
コンピュータの音声認識とそれをテキストに変換する作業について、素人にできるだけ簡単に伝えることはほとんど不可能です。</font><font style="vertical-align: inherit;">複雑な式や数学的な用語がなければ、これに関する単一の話は完全ではありません。</font><font style="vertical-align: inherit;">車が人間の声を認識することを学んだとき、スマートフォンが音声をどのように理解するか、そしてこの技術が予期しない領域でどのように使用されているかを、できるだけ明確かつ少し単純に説明します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
必要な警告：あなたが開発者、または特に数学者であれば、投稿から何か新しいことを学んだり、資料の不十分な科学的性質について不平を言ったりすることはほとんどありません。</font><font style="vertical-align: inherit;">私たちの目標は、初心者の読者に音声技術を最も簡単な方法で紹介し、東芝が音声AIの作成をどのように、そしてなぜ始めたのかを説明することです。</font></font><br>
<a name="habracut"></a><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">音声認識の歴史における重要なマイルストーン</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
電子機械による人間の音声認識の歴史は、通常の考えよりも少し早く始まりました。ほとんどの場合、1952年からカウントダウンするのが慣例ですが、実際、音声コマンドに応答した最初のデバイスの1つは、</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">すでに説明し</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">たTelevoxロボットでした</font><font style="vertical-align: inherit;">。 1927年に米国で作成されたHerbert Televoxロボットは、さまざまなリレーがさまざまな周波数の音に反応するシンプルなデバイスでした。ロボットには3つの音叉があり、それぞれが音色の原因でした。機能した音叉に応じて、1つまたは別のリレーが作動しました。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/90/pq/4i/90pq4ixpys3c8uevjp-ovvydfny.jpeg" alt="画像"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">実際、コマンド認識システムを含むTelevoxの「充填物」全体は、「ロボット」の本体の領域にあるラックに配置されていました。蓋を閉めることは不可能でした。さもなければ、音叉は音を正しく「聞く」ことができませんでした。出典：</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Acme Telepictures / Wikimedia。</font></font></a></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
笛を使って個別の信号として、短い口頭でTelevoxと通信することができました</font><i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">。</font></a></i><font style="vertical-align: inherit;">それらの音叉も一連の音でレイアウトされていました。ロボットの作成者であるロイウェンズリーは、当時の素晴らしいデモを披露し、「セサミ、開く」というコマンドを発し、テレボックスがドアを開くためのリレーをオンにしました。デジタルテクノロジー、ニューラルネットワーク、AI、機械学習はありません。アナログテクノロジーだけです。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
人間の発話の真の認識への道を開いた次の主要な発明は、1952年にベル研究所イノベーションフォージで開発されたオードリーマシンでした。巨大なオードリーは多くの電力を消費し、優れたキャビネットの大きさでしたが、そのすべての機能は、0から9までの話し言葉を認識することに帰着しました。はい、10語だけですが、オードリーがアナログマシンであることを忘れないでください。</font></font><br>
<img src="https://habrastorage.org/webt/vd/1q/eb/vd1qebrer6czotgwty3tdyfp15i.png" alt="画像"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">残念ながら、このストーリーはオードリーの公開写真を保存しておらず、コンセプトのみが存在します。紙の上では単純で、翻訳が難しい-同時代の回顧録によると、オードリーのコンポーネントはキャビネット全体を占めていました。出典：Bell Labs</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
それは次のように機能しました：アナウンサーがマイクに数字をスポークし、単語間に少なくとも350 msの間隔を空けて、オードリーが聞いた音を電気信号に変換し、それらをアナログメモリに記録されたサンプルと比較しました。比較の結果によると、車はダッシュボードで番号を強調表示しました。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
それは画期的でしたが、オードリーからの実質的な利益はありませんでした-マシンは97％の精度で作成者の声を認識し、他の特別に訓練されたスピーカーは70-80％の精度を受け取りました。オードリーに最初に接触した見知らぬ人は、どんなに頑張っても、スコアボードで自分の番号を目にしたのはわずか50％でした。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
その時代の革命的な結果にもかかわらず、オードリーは見つけられず、実用的なアプリケーションを見つけることができませんでした。</font><font style="vertical-align: inherit;">電話オペレーターの代わりにシステムを適応させることができると想定されていましたが、それでも、オードリーよりもヒューマンサービスの方が便利で、高速で、はるかに信頼性がありました。</font></font><br>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/rQco1sa9AwU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Audreyに似たプレゼンテーションですが、はるかに小さいマシンです-IBM Shoebox。</font><font style="vertical-align: inherit;">靴箱の速度ははっきりと見えます。</font><font style="vertical-align: inherit;">マシンは、加算と減算の単純な数学演算も実行できます</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
1960年代初頭、音声認識用のマシンを作成する作業が日本、イギリス、アメリカ、さらにはUSSRでも行われ、タイムラインの動的変換（DTW）のための非常に重要なアルゴリズムを発明しました。その助けを借りて、約200語を認識するシステムを構築することができました。しかし、すべての開発は互いに類似しており、認識の原則は共通の欠点になりました。単語は統合された音の指紋として認識され、サンプルのベースで検証されました（辞書）。単語の発音の速度、音質、明瞭さの変化は、認識の品質に大きな影響を与えました。科学者には新しい課題があります。機械に個々の音、音素、音節を聞いてから、それらから言葉を作るように教えることです。このようなアプローチは、話者に応じて認識レベルが急激に変化する場合に、話者を変更する効果を平準化することを可能にする。</font></font><br>
<br>
<i> —     ,           . ,   « »  «»       «».   «»   « »  « »      «»,    —  «».  ,  ,   . </i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
1971年、国防総省高等研究計画局（DARPA）は、予算が1,500万ドルの5年間のプログラムを立ち上げました。このプログラムは、少なくとも1000語を認識する認識システムの作成を任されていました。 1976年までに、カーネギーメロン大学は1011語の辞書を操作できるハーピーを導入しました。ハーピーは完全に聞こえた単語をサンプルと比較しませんでしたが、それらをアロフォン（周囲の文字に応じた音素の音のサンプル）に分けました。これはもう1つの成功であり、将来は単語全体ではなく個々の音素を認識することにあることが確認されました。ただし、Harpyの欠点の中には、異音（音素の発音）の正確な認識レベルが約47％という非常に低いレベルがあります。このような高いエラーでは、辞書のボリュームが原因でエラーの割合が増加しました。</font></font><br>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/32KKg3aP3Vw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Harpyの動作の説明。プログラムのビデオは生き残りませんでした。</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
ハーピーの経験は、全体的な音の指紋の辞書を構築することは役に立たないことを示しています-それは認識時間を増加させ、正確さを大幅に低下させるだけなので、世界中の研究者が別の道をたどりました-音素の認識。 1980年代の半ば、IBMタンゴラマシンはアクセント、方言、発音を使って任意の話者のスピーチを理解できるようになり、20分のトレーニングだけで、その間に音素と異音サンプルのデータベースが蓄積されました。</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">隠れマルコフモデルを</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用すると、</font><font style="vertical-align: inherit;">IBMタンゴラの語彙も20,000ワードという印象的なものになりました。これは、ハーピーが持っていた語彙の20倍で、すでにティーンエイジャーの語彙に匹敵します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
1950年代から1990年代半ばまでのすべての音声認識システムは、人の自然な話し言葉の読み方を知りませんでした-単語を別々に発音し、それらの間で一時停止する必要がありました。真に革命的な出来事は、1980年代に開発された隠れマルコフモデルの導入でした。これは、既知の要素に基づいて未知の要素に関する正確な仮定を構築した統計モデルです。簡単に言えば、認識された音素が1つの単語に数個あるだけで、隠れマルコフモデルは欠落している音素を非常に正確に選択するため、音声認識の精度が大幅に向上します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
1996年、最初の商用プログラムが登場しました。これは、個々の単語ではなく、自然な発話の継続的な流れを識別することができるものです-IBM MedSpeak / Radiology。 IBMは、研究中に医師によって提供されたX線の結果を簡単に説明するために医学で使用された特殊な製品でした。ここで、コンピュータの能力がようやく「オンザフライ」で個々の単語を認識するのに十分になりました。さらに、アルゴリズムがより完全になり、話し言葉間のマイクロポーズの正しい認識が現れました。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
自然な音声を認識するための最初のユニバーサルエンジンは、1997年にDragon NaturallySpeakingプログラムでした。彼女と一緒に作業する場合、アナウンサー（つまりユーザー）は、MedSpeakの場合のようにトレーニングを受けたり、特定の語彙で操作したりする必要はありませんでした。子供であっても、誰でもNaturallySpeakingを使用でき、プログラムは発音規則を設定していません。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/xj/m6/w-/xjm6w-kpgryltox7wquuvpl8db4.png" alt="画像"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dragon NaturallySpeakingの独自性にもかかわらず、ITブラウザーは自然な発話を認識することにそれほど熱意を示しませんでした。欠点の中には、プログラム自体に宛てられたコマンドの認識エラーと不正な処理が指摘されていました。出典：</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">itWeek</font></font></a></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
認識エンジンは1980年代に準備ができていたことは注目に値しますが、不十分なコンピューター能力のために、Dragon Systems開発（現在はNuance Communicationsが所有）は、自然な音声を認識するために必要なオンザフライの単語間のスペースを決定する時間がありませんでした。</font><font style="vertical-align: inherit;">これがないと、たとえば、「治療中」という言葉は、コンピュータによって「不自由」と聞こえる可能性があります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
音声認識システム、ニューラルネットワークの人気の高まり、モバイルデバイスでのGoogle音声検索の登場、そして最後に、Siri音声アシスタントは、音声をテキストに変換するだけでなく、自然な方法で作成されたクエリに適切に応答しました。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">言われたことを聞く方法と聞こえなかったものを考える方法？</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
今日、音声認識エンジンを作成するための最良のツールは、リカレントニューラルネットワーク（RNN）です。RNNでは、音声、音楽、画像、顔、オブジェクト、テキストを認識するための最新のサービスがすべて構築されています。 RNNを使用すると、単語を非常に正確に理解できるだけでなく、認識されなかった場合にコンテキストのコンテキストで最も可能性の高い単語を予測できます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ニューラルネットワークのモデルの時間的分類（CTC）は、記録されたオーディオストリーム（単語、フレーズ）内の個々の音素を選択し、発音された順に並べます。繰り返し分析した後、CTCは特定の音素を非常に明確に識別し、それらのテキスト記録はニューラルネットワーク内の単語のデータベースと比較されて、認識された単語に変わります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
彼らの仕事の原理は人間の脳の仕事に似ているので、ニューラルネットワークはそう呼ばれています。ニューラルネットワークのトレーニングは、人間のトレーニングとよく似ています。たとえば、非常に小さな子供が車を認識してオートバイと区別することを学ぶためには、少なくとも数回彼の注意をさまざまな車に引き付け、毎回対応する単語を発音する必要があります。これは大きく、赤は車で、この低い黒は車ですが、これらはオートバイです。ある時点で、子供はさまざまな車のパターンと一般的な兆候を発見し、通りの広告ポスターでそれらを見ても、車の場所、ジープ、オートバイ、ATVの場所を正しく認識することを学びます。同様に、ニューラルネットワークは、例のベースでトレーニングする必要があります。つまり、各単語、文字、音素の数百、数千の発音のバリエーションを「学習」する必要があります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
音声認識用のリカレントニューラルネットワークは、さまざまな発音の基礎を長時間トレーニングした後、発音の質や性質に関係なく、音素を単語から区別し、それらから単語を作成することを学ぶため、優れています。また、背景のノイズやあいまいな発音のために、単語のコンテキスト内で明確に認識できなかった単語も、正確に「考え」ます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
しかし、RNNの予測にはニュアンスがあります。リカレントニューラルネットワークは、約5語の最も近いコンテキストに依存することによってのみ、欠落している語を「考える」ことができます。このスペースの外では、分析は行われません。そして時には彼はとても必要です！たとえば、認識のために、「偉大なロシアの詩人アレクサンドルセルゲイエビッチ</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">プーシキン</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">」、「プーシキン」（特にイタリック体）という言葉が聞き取れないように言われたため、AIはそれを正確に認識できませんでした。しかし、トレーニング中に得られた経験に基づくリカレントニューラルネットワークでは、「プーシキン」という単語が「ロシア語」、「詩人」、「アレクサンダー」、「セルゲイエヴィッチ」という単語の隣に最も頻繁に見られることが示唆されます。非常に具体的なコンテキストにより、最高の精度で仮定を行うことができるため、これはロシア語のテキストでトレーニングされたRNNのかなり単純なタスクです。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
そして、コンテキストが曖昧な場合はどうなりますか？ 1つの単語を認識できない別のテキストを考えてみましょう。「私たちのすべて、アレクサンダーセルゲイビッチプーシキンは、ダンテスとの決闘の後、彼の人生の最盛期に悲劇的に亡くなりました。プーシキン劇場祭は詩人にちなんで名付けられました。 「プーシキンスキー」という単語を削除すると、RNNは、提案のコンテキストに基づいて、それを単に推測することができなくなります。これは、演劇祭と、未知の詩人の名前への参照のみが言及されているためです。選択肢はたくさんあります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
これは、1997年に作成されたリカレントニューラルネットワーク用のLSTM（Long Short-term Memory）アーキテクチャ（LSTM </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">に関する詳細な記事</font></a><font style="vertical-align: inherit;">）の出番</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">です。</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">）これは、処理中のイベントから離れたコンテキストを考慮に入れるRNNの機能を追加するために特別に開発されました。以前の問題（つまり、単語認識）を解決した結果は、モノローグの長さに関係なく、認識プロセス全体を通過し、疑わしいケースごとに考慮されます。さらに、除去距離はアーキテクチャの効率にほとんど影響しません。 LSTMの助けを借りて、必要に応じて、単語ネットワークはタスクのフレームワーク内で利用可能なすべてのエクスペリエンスを考慮に入れます。この例では、RNNは前の文を調べ、プーシキンとダンテスが前述のように言及されていることを見つけます。ダンテスシアターフェスティバルの存在を示す証拠はないので、私たちはプーシキンスキーについて話しています（認識されない単語の音の痕跡は非常に似ているのでなおさらです）-そのようなフェスティバルはニューラルネットワークのトレーニングのベースにありました。</font></font><br>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/P325_hrGsDI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">「音声アシスタントの告白」</font><font style="vertical-align: inherit;">よく訓練されたニューラルネットワークが機能するようになると、音声アシスタントは「グリーンスリッパ」で何をする必要があるかを正確に把握できます</font></font></i><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">音声認識はどのようにして世界をより良い場所にしていますか？</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
いずれの場合も、アプリケーションは異なります-それは誰かがガジェットと通信するのに役立ち、</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PricewaterhouseCooper</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">のスマートフォンユーザーの半数以上がデバイスに音声コマンドを提供します-大人（25-49歳）の間で、常に音声インターフェイスを使用しているユーザーの割合です。若者よりも高い（18-25）-59％に対して65％。</font><font style="vertical-align: inherit;">そして、ロシアでは少なくとも1回、人口の少なくとも71％がSiri、Google Assitant、またはAliceと連絡を取っていました。</font><font style="vertical-align: inherit;">4500万人のロシア人が常にアリスのYandexと通信しており、Yandex.Maps / Yandex.Navigatorはリクエストの30％しか占めていません。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
音声認識は職場で誰かを本当に助けます-例えば、前述のように、医師にとって：1996年（IBM MedSpeakが出たとき）の医学では、認識は既往歴を記録するために使用され、画像を検査するとき-医師はコンピュータまたは紙のカード。ちなみに、医学のディクテーションに関する作業は西洋だけでなく、ロシアにも「Center for Speech Technologies」のVoice2Medプログラムがあります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
私たちのものを含む他の例があります。東芝の事業を組織するには、完全に包括する必要があります。つまり、聴覚障害のある従業員を含む、さまざまな健康状態の人々に対する平等な権利と機会です。ユニバーサルデザインアドバイザーシステムと呼ばれる企業向けプログラムがあります。このシステムでは、さまざまな種類の障害を持つ人々が東芝製品の開発に参加し、障害を持つ人々の利便性を向上させるための提案を行っています。と従業員のレビュー。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
数年前、日本の東芝本社では、新しい音声認識システムの開発を必要とする非常に興味深い課題に直面しました。ユニバーサルデザインアドバイザーシステムの運用中に、私たちは重要な洞察を得ました。聴覚障害のある従業員は、会議や講義での議論にリアルタイムで参加したいと考えています。処理された筆記録を数時間または数日後に読むことに限定されません。そのような場合、スマートフォンで音声認識を開始すると、結果が非​​常に弱いため、東芝の専門家が専用の認識システムを開発する必要がありました。そしてもちろん、すぐに問題が発生しました。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
会話は書き言葉とは大きく異なります。私たちは手紙の書き方を話しません。テキストに翻訳された実際の会話は非常にずさんで読みづらいものに見えます。つまり、朝の計画での会話を高精度のテキストに変換したとしても、言葉の寄生虫、間投詞、思慮深い「aaa」、「uh」、「mm​​m」で溢れる一貫性のないハッシュが得られます。テキスト内の不要な音、単語、感情の表現の転写を取り除くために、一部の単語の感情的な色付け（たとえば、「はい、まあ」は懐疑論のように聞こえるかもしれません）心からの驚き、そしてこれらは文字通り反対の意味です）。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/6z/vv/od/6zvvodwnihcvdqdqfv4uuprbtb4.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">これは、東芝AIを使用した音声認識用の周辺機器セットを備えたラップトップ（左）と、エンドデバイス用の結果を備えたアプリケーション（右）のように見えます。出典：</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
ここでは</font><i><font style="vertical-align: inherit;">東芝</font></i><font style="vertical-align: inherit;"> LSTMが役に立ちました。これがないと、受信したテキストを努力なしに読んで理解するには認識精度が不十分でした。さらに、LSTMは、コンテキスト内の単語をより正確に予測するためだけでなく、文の真ん中のポーズと間投詞-寄生虫の正しい処理にも役立ちました。このため、口語的発話に自然なこれらの寄生虫とポーズをニューラルネットワークに教えました。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
これは、ニューラルネットワークが筆記録から間投詞を削除できることを意味しますか？</font><font style="vertical-align: inherit;">はい、できますが、これは必須ではありません。</font><font style="vertical-align: inherit;">事実は、（別の洞察を受け取った）聴覚障害を持つ人々が、話者の唇の動きを含めて案内されることです。</font><font style="vertical-align: inherit;">唇が動くが、その動きに対応するテキストが画面に表示されない場合、認識システムが会話の一部を逃したような感覚があります。</font><font style="vertical-align: inherit;">つまり、聞こえない人にとっては、不運なポーズや失神など、会話について可能な限り多くの情報を取得することが重要です。</font><font style="vertical-align: inherit;">したがって、東芝エンジンはこれらの要素をトランスクリプトに残しますが、リアルタイムで文字の明るさを暗くし、これらはテキストを理解するためのオプションの詳細であることを明確にします。</font></font><br>
<br>
<div class="oembed"><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">https://www.toshiba-clip.com/en/detail/7655</font></font></a></div><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">これは、オンザフライでの認識結果がクライアントデバイスでどのように見えるかです。</font><font style="vertical-align: inherit;">モノローグの意味のない部分は灰色に塗られて</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
おり、東芝AIは英語、日本語、中国語の音声に対応しており、その場で言語間の翻訳も可能です。</font><font style="vertical-align: inherit;">オンザフライでその場で使用する必要はありません。AIは、音声アシスタントと連携するように調整できます。音声アシスタントは、人がコマンドを発したときに、間投詞、一時停止、吃音を適切に認識することを学びます。</font><font style="vertical-align: inherit;">2019年3月、このシステムは、日本のIPSJ全国大会放送に字幕を追加するために使用されました。</font><font style="vertical-align: inherit;">近い将来-東芝AIの公共サービスへの変換と、本番環境での音声認識の実装の経験。</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../ja490720/index.html">モーター！またはゲームの物理学とは</a></li>
<li><a href="../ja490722/index.html">ITにおける性別休暇。注意する方法</a></li>
<li><a href="../ja490726/index.html">公開鍵を使用したSSHによるネットワーク機器での認証</a></li>
<li><a href="../ja490728/index.html">Security Week 10：RSA Conference and Cyber​​security Awareness</a></li>
<li><a href="../ja490730/index.html">Intel x86 Root of Trust：信頼の喪失</a></li>
<li><a href="../ja490734/index.html">スイスの地形図上のイースターエッグ</a></li>
<li><a href="../ja490736/index.html">英語のボキャブラリーを学び、刺激するための9つの明確なツール</a></li>
<li><a href="../ja490738/index.html">Liskの置換原理</a></li>
<li><a href="../ja490740/index.html">*** sの欠陥は単なるランダム化ではありません</a></li>
<li><a href="../ja490742/index.html">ロボット工学の新時代が始まりました</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>