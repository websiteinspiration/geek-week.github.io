<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üéôÔ∏è ü§≥üèΩ üßëüèΩ‚Äçü§ù‚Äçüßëüèº Neural networks and Process Mining: trying to make friends üîÆ üö£üèª ü§õüèº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Process Mining is a data analysis field that allows you to analyze processes based on the logs of information systems. Since there are very few public...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Neural networks and Process Mining: trying to make friends</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/vtb/blog/493152/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Process Mining is a data analysis field that allows you to analyze processes based on the logs of information systems. Since there are very few publications on the topic of the use of machine learning in this field on Habr√©, we decided to share our experience in developing predictive models for solving process-oriented problems. As part of the VTB program, an IT junior for beginning IT professionals, interns from the Process mining team tested the machine learning methods in the context of the tasks of researching bank processes. Under the cut, we will talk about when and how we had the idea of ‚Äã‚Äãsolving such problems, what we did and what results we got.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/6n/hi/lg/6nhilgitc0hyxrpmzuypu6w0irg.jpeg"><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
IT Junior Program is an annual internship program for beginner IT professionals at VTB Bank, which first started in September 2019. </font><font style="vertical-align: inherit;">The internship lasts six months. </font><font style="vertical-align: inherit;">According to the results of the 2019 program, more than half of the interns joined the staff and became employees of the company. </font><font style="vertical-align: inherit;">More information about the program, the start of selection and requirements for candidates can be found </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">here</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">This is how the trainees of this program approached the bank‚Äôs tasks. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In the classical coordinate system, in order to understand and formalize the process, it is necessary:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">conduct an interview with employees;</font></font><br>
</li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">analyze available reports and documentation.</font></font><br>
</li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In the Process Mining approach, a digital process model is formed on the basis of not only the expert opinion of the process participants, but also relevant data from information systems. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As a result, we get an objective digital model of the process, which is a reflection of the movement of real data in the IT systems of the process. The resulting model works in real time and allows you to display the current state of the process with the necessary degree of detail. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In our previous </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">article</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We talked about our Process Mining platform and the real tasks of the Bank, which are solved with its help. </font><font style="vertical-align: inherit;">The implemented solution allowed us to significantly reduce the time required to prepare mandatory reports to government agencies, and also helped to identify and optimize process imperfections, to establish a daily presentation of the current status of purchases in work. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Subsequently, our customers had a need not only to qualitatively determine the current state of the process, but also to predict its future conditions.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Next, we will describe step by step how we solved the problem of predicting the duration of the procurement process (using the BPI Challenge 2019 dataset as an example) using a set of well-known events using the high-performance DGX Station, kindly provided to us by NVIDIA for research.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Machine Learning Application for Process Mining</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
To solve the problem, we built a baseline using CatBoostRegressor, and then developed a solution with a neural network and embedding categorical variables. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Due to the presence of categorical and material features in the source data, it was decided to use boosting, which could process categorical features without coding, and also solve the problem on a discrete and material input. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The nets were used to build completely material attributes and solve the problem on the whole material input, and then compare these two approaches and decide whether to bother with the nets.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Data Description</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
It was decided to use external data that would suit us in the business area and possess a similar set of characteristics. </font><font style="vertical-align: inherit;">The used BPI Challenge 2019 dataset includes 250 thousand cases - this is 1.5 million events. </font><font style="vertical-align: inherit;">The initial data are described by a set of 21 signs: 18 categorical (there are index signs), two Boolean and one real. </font><font style="vertical-align: inherit;">The execution time of the procurement process was selected as the target variable, which corresponded to the real needs of the business. </font><font style="vertical-align: inherit;">For a detailed description of the characteristics, you can refer to the </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">description of the dataset</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Baseline</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Prior to model training, the data were divided into training (train) and test (test) samples in the ratio of 0.8 / 0.2. Moreover, the division did not occur according to events, but according to cases. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
To determine how appropriate it is to use a complex native solution in the form of a neural network, baseline was built using CatBoost, an advanced library of gradient boosting on decision trees. To build a baseline, minimal data preprocessing was carried out (coding of categorical features with the corresponding frequency in the data), a target variable (case duration) and a number of new features were developed (in addition to those already in the original dataset):</font></font><br>
<br>
<ul>
<li>   .  ,        :      ,         ,     ,          ,   ,      .<br>
</li>
<li>Exponential Moving Average     . EMA     ,         .<br>
</li>
<li>     (, ,  ).<br>
</li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
After training the CatBoostRegressor in the training set, we got the following result: MAE (Mean Absolute Error) = 17.5 days (that is, the value of the predicted target variable is on average 17.5 days different from the true value). This result was used to test the effectiveness of a neural network.&nbsp; </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
One of the important details here is the development of the target variable for baseline.&nbsp;</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Let us have a case. We denote it by c_i from the set C (the set of all cases in our dataset). Each case is an ordered sequence of events, i.e. c_i = (e_0, ‚Äã‚Äã..., e_ni), where ni is the length of the ith case. For each event, we have a time stamp - the exact time it started. Using these temporary stamps, you can calculate the duration of the case without the last event. However, assigning such a target to each event, that is, making a correspondence ek ‚àà ‚Äã‚Äãci, ek ‚Üí ti (ti is the duration of the ith case), is not very good. Firstly, similar events (typical) can occur in cases of different durations. Secondly, we want to predict the duration of the case from a certain subsequence (ordered in time) of events (this is motivated by the fact that we don‚Äôt know the whole sequence of events, that is, we don‚Äôt know the case beforehow it happened, but we want to make an assessment of the duration of the whole case according to some known (occurred) events from this case).</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Therefore, we need to break each case into subsequences of length from one to the length of the case of time-ordered events and assign a target variable equal to the duration of the case from which these subsequences are obtained, that is, the correspondences ci ‚àà C, ci ‚Üí {sub_cj} ni (ni as before, the length of the ith case), j = 1 and len (sub_cj) = j. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Thus, we break each case into subsequences and assign the duration of the entire case to each such subsequence. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">More about subsequences</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
As mentioned earlier, we break the case into subsequences and assign the duration of the case to each of them. We are going to use boosting which is exacting to the size of the input data. So now we have X = {{sub_c </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">k</font></font></sup><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">i</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> } </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ni </font></font></sup> <sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">k = 1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> } </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">t = 1 N</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , sub_c </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ik</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> is the k-th subsequence of the i-th case, t </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">i</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> is the length of the i-th case, N is the number of cases. That is, the dimension [‚àë </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">N </font></font></sup> <sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">t = 1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> n </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">i</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , sc, 17], </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">sc</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> is a variable equal to the length of the subsequence of the corresponding case.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
After coding categorical variables by their frequency, we have real and Boolean variables, as well as coded categorical ones (index variables will not be used in the learning process). We can also average values ‚Äã‚Äãover a subsequence, while in categorical features we get the average frequency of occurring categorical values, which can also be considered as a characteristic describing the aggregation of a subset of events in a case, that is, as a characteristic describing a subsequence. Leave it and see what happens. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
After averaging sc over the dimension, we obtain the following dimension: [‚àë </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">N </font></font></sup> <sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">t = 1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> n </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">i</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , 17]. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Model building</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Based on the cases, we divide train into another train and a validation sample, take a CatBoostRegressor with default parameters, pass it a training sample, validate on a validation sample, take the best iteration, use MAE as a validation metric. We get the following (in the figure below) on the test (we‚Äôll separately prepare the test for the same pipeline that the train was built on. All the signs are based on the data that is in the test, that is, we don‚Äôt have any signs that are focused on the target variable. The only caveat: if the categorical features in the test do not meet the value that we saw in train, then we consider the frequency of this value on the test and update the dictionary for encoding). </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Baseline results</font></font></b><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/90b/f18/386/90bf183863f8ad2cdb50098c58d06a37.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
‚Ä¢ Iterations: 500. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
‚Ä¢ Learning rate: 0.1. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Training Parameters:</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
‚Ä¢ Training time: less than 2 minutes. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
‚Ä¢ Iron: Tesla k80 (from colab). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Results: </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
‚Ä¢ Test MAE: 17.5 days. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
‚Ä¢ The average duration of the case in the test: 66.3 days.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Neural network</font></font></h2><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Setup</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
To train the neural network, the data were improved: embeddings for categorical variables were built, and the distribution of the target variable was adjusted. </font><font style="vertical-align: inherit;">Next, the neural network was trained on the NVIDIA Tesla K80 (Google Colab) and on the NVIDIA DGX Station. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The following results were obtained:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Training time on NVIDIA K80 (Google Colab): 20 minutes.</font></font><br>
</li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Training time at NVIDIA DGX Station: 8 minutes.</font></font><br>
</li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The training time of the neural network is due to the difference in the technical characteristics of the GPUs used:</font></font><br>
<div class="scrollable-table"><table>
<tbody><tr>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">NVIDIA Tesla K80 (Google Colab)</font></font><br>
</td>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">NVIDIA DGX Station</font></font><br>
</td>
</tr>
<tr>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1X NVIDIA Tesla K80 12GB</font></font><br>
</td>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">4X NVIDIA Tesla V100 32GB</font></font><br>
</td>
</tr>
</tbody></table></div><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Preprocessing</font></font></h3><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">New signs</font></font></b><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">EMA on the value of the event: we want to catch the trend in the cost of activities for each case.</font></font><br>
</li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Type of flaw: in the dataset description you can find information about four types of some descriptive statistics of the purchase (event) - these types are divided into the values ‚Äã‚Äãof two variables in the original dataset. </font><font style="vertical-align: inherit;">We simply aggregate it back (if you look at the description of the dataset, it will be clear what we are talking about).</font></font><br>
</li>
</ul><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Categorical signs We</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
simply encode the unique values ‚Äã‚Äãof categorical signs with natural numbers in order, so that later we can teach embeddings. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Embeddings for categorical variables</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
We determine the dimension of embeddings for each categorical variable:</font></font><br>
<br>
<ul>
<li>,   ÃÜ  ÃÜ ÃÜ.    ÃÜ ,         ÃÜ  ÃÜ ,    ÃÜ,  .  : MUi&nbsp; = min(CAT_EMBEDDING_DIM; (len(uniquei) + 1) // 2), CAT_EMBEDDING_DIM ‚Äî , uniquei ‚Äî   i-  .<br>
</li>
<li> ,      3,       i-ÃÜ ÃÜ   max(3;MUi)+1,  1,        ,     train,      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=">unk</a>-.<br>
</li>
</ul><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We adjust the distribution of the target on the train sample. The</font></font></b>&nbsp;<br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
initial distribution turned out to be very much shifted to the left due to outliers (cases that lasted 250 thousand days) and a large number of short cases, so we count the 0.05 and 0.95 percentiles and leave the data from train with the target between these rapids.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
After that, we still have cases that last about one and about 100 days, that is, the target variable goes through several orders of magnitude. Therefore, the assumption that the variance is constant in the distribution of the target variable around the decision algorithm is hardly fulfilled, that is, the distribution of the target variable is close to normal, but the variance is not constant due to the fact that the target variable can be either less than 1 or more than 100. Therefore, to at least somehow level this effect, we normalize the data. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The result is shown in the graph below (black line is the normal distribution).</font></font><br>
<br>
<img src="https://habrastorage.org/webt/k4/qs/vz/k4qsvz1egedfnm9q4y6nljidoam.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Then we divide by case our data into train and validation. </font><font style="vertical-align: inherit;">There is also an obvious nuance here: we normalize the target with the average and deviation, calculated according to all the data, and then divide by train and validation, that is, it turns out like a face in train, but since we solve an auxiliary problem here, this face does not seem critical.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Building signs</font></font></h3><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Idea</font></font></b><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We take only categorical signs from our train, encoded by natural numbers.</font></font><br>
</li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We take not substrings from cases, but simply events, that is, one line in our data for embeddings - this is one event characterized by coded categorical features.</font></font><br>
</li>
<li> :     ,   ÃÜ,      ,   ,        ,            ÃÜ  ÃÜ    ÃÜ. - ,         ÃÜ,  ,  ,     ÃÜ (    ),      (     ,  - ÃÜ  ).<br>
</li>
<li>       ÃÜ        .<br>
</li>
<li>     ,     8-ÃÜÃÜ   elu   ÃÜ,   (  ,   ,     ,  L2-)     .<br>
</li>
<li>,           , ‚Äî    ,      ,    .<br>
</li>
<li>Summary:    ‚Äî ÃÜ  ÃÜ ÃÜ ÃÜ        ‚Äî  ÃÜ.<br>
</li>
</ul><br>
<h3>   </h3><br>
<ul>
<li>Batch size = 1000</li>
<li>Learning rate = 3e-04.</li>
<li>  = 15.</li>
<li>: Tesla k80 (colab) + Nvidia DGX Station .</li>
<li> (colab) ‚Äì 50 .</li>
<li> (Nvidia DGX Station) ‚Äî 18 .</li>
</ul><br>
<img src="https://habrastorage.org/webt/53/4y/gh/534yghkinehatjl8cm1xwg5j0oi.png"><br>
<br>
<h3> </h3><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Data preparation</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Now we have embeddings for categorical variables (there is a nuance here: we honestly took unique values ‚Äã‚Äãof categorical variables on our train (not on the one we allocated for training embeddings, but on the one we allocated at the very beginning for training), therefore there is a chance that on the test data there will be a value of categorical variables that we did not see on the train, that is, we do not have trained embedding for this value.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
For such values, a separate line is created in the embedding matrices, but in our case the problem is that during the training it is not involved, and therefore does not study. Based on this, if we meet a value of a categorical variable that was not seen before, then we take this vector, but in fact it is simply taken from the initializing distribution. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In how to train this vector, there is a direction for improving the model. The idea is that very rare values ‚Äã‚Äãin train can be encoded with this vector, because if we see a new value only in the test, which conditionally makes up 20% of the entire initial sample, then this value is rare and probably behaves same as rare values ‚Äã‚Äãin train.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In each event, we replace the categorical variables with the corresponding embedding, connect with the real and Boolean attributes, we obtain a matrix of size [N, F], where F is the sum of the dimensions of the embeds for categorical variables, the number of real and Boolean attributes. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We carry out a grouping of events in a subsequence (as previously done). </font><font style="vertical-align: inherit;">The target variable for the subsequence is the duration of the case from which the subsequence was obtained. </font><font style="vertical-align: inherit;">Add the number of events and the sum of the costs of events in this subsequence to the vector of the subsequence. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Now we have a matrix of a fixed size - you can feed into the model (before that we normalize the matrix). </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Parallelization method</font></font></b><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We make a tower for each gpu.</font></font><br>
</li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">At each step, we divide the parameters between the towers.</font></font><br>
</li>
<li>       .<br>
</li>
<li> ÃÜ   ,   .<br>
</li>
<li>       (      ,       ÃÜ ÃÜ         ).<br>
</li>
<li>        .<br>
</li>
<li>     ,   -  (   ,  word2vec-style,     ).<br>
</li>
</ul><br>
<b></b><br>
<br>
<ul>
<li>          ()   ()  ().<br>
</li>
<li>ÃÜ :   ‚Äî   ,     gpu     ,    ,   gpu     .<br>
</li>
</ul><br>
<b></b><br>
<br>
<ul>
<li>: 7-ÃÜÃÜ    elu.<br>
</li>
<li>   ÃÜ ,    .<br>
</li>
<li>Batch size = 1000.<br>
</li>
<li>Learning rate = 3e-04.<br>
</li>
<li>  = 15.<br>
</li>
<li>: Tesla k80 (colab) + Nvidia DGX Station.<br>
</li>
<li> (colab) = 20 .<br>
</li>
<li> (Nvidia DGX Station) = 8 .<br>
</li>
</ul><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A piece of the model graph. </font></font></b><br>
<br>
<img src="https://habrastorage.org/webt/u5/4e/3e/u54e3eoth5n5kmcdryndbpqgcua.jpeg"><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Consumption of resources and parallelization.</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Neural network training on a CPU takes about four times as much time as on an NVIDIA DGX Station. In this case, the difference seems insignificant - eight minutes on the NVIDIA DGX Station and 32 minutes on the CPU. However, this is a small model with a small amount of data. When implementing real projects, where there will be several times more cases and events, training on the CPU will take at least a week. In this case, the use of NVIDIA DGX Station will reduce the training time to two days, which will greatly increase work efficiency.&nbsp; </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
It was also revealed that the speed of the learning process greatly depends on the number of GPUs used, which shows the advantage of NVIDIA DGX Station.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
This is confirmed by previous experiments on the NVIDIA DGX Station CPU and GPU using the original data set without any preliminary processing:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Learning time on the CPU: 6 minutes 18 seconds.</font></font><br>
</li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GPU training time: 34 seconds.</font></font><br>
</li>
</ul><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GPU </font></font></b><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/6f1/015/d82/6f1015d82101dda1e0af40e599ee96a2.png"><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">load visualization CPU load visualization</font></font></b><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/efe/3ae/f3b/efe3aef3b801dcbd14655b3851ae9ad3.png"><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Neural network results</font></font></h3><br>
<img src="https://habrastorage.org/webt/-g/2g/p1/-g2gp1o1sdktbwtspcurjfdiwqu.png"><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Test MAE = 10 days.</font></font><br>
</li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The average duration of the case on the test = 67 days.</font></font><br>
</li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Inference time = 20 seconds.</font></font><br>
</li>
</ul><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">findings</font></font></h2><br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We implemented a pilot to evaluate machine learning methods in the context of Process Mining tasks.</font></font><br>
</li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We tested and expanded the list of our tools with which we will solve problems that are important for business.</font></font><br>
</li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">One of the interesting results was the writing of our own implementation of parallel computing on 4 Tesla v100 cards with which the DGX station is equipped: the use of several gpu speeds up learning almost in line from the number of gpu (the code is parallelized).</font></font><br>
</li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The transition to a fully continuous entry and the use of a neural network made it possible to take a week off baseline.</font></font><br>
</li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Time increases from a few minutes to an hour and a half (training in final architecture and embeddings, but embeddings can be used pre-trained, so the time is reduced to 20 minutes).</font></font><br>
</li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The described experiments show that in the field of Process mining, machine and deep learning algorithms can be successfully applied. </font><font style="vertical-align: inherit;">In addition, it was revealed that the speed of the learning process greatly depends on the number of GPUs used, which shows the advantage of NVIDIA DGX Station.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">What and how can be improved</font></font></h2><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Word2vec-style embeddings for events</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
When we made our model, including embeddings for categorical variables, we did not take into account the sequence of events relative to each other, that is, the peculiar semantics of events inside cases. </font><font style="vertical-align: inherit;">To learn something useful from the order of events inside the cases, you need to train embeddings for these events. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Idea</font></font></b><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We take one categorical feature and one real, divide the real by bucket, then each transaction will be characterized by the value of the categorical variable and the bucket in which the value of the real variable falls. </font><font style="vertical-align: inherit;">Combine these two values, we get, as it were, an analogue of the word for the event.</font></font><br>
</li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We consider the case as a sentence (the set of words in the sentence corresponds to the set of events in the case).</font></font><br>
</li>
<li> ÃÜ      ,          ÃÜ   ÃÜ,    ,             .<br>
</li>
<li>   ÃÜ,     Skipgram  CBOW   .<br>
</li>
<li>  ,      ÃÜ,            .<br>
</li>
</ul><br>
<b> </b><br>
<br>
<ul>
<li>  Skipgram.<br>
</li>
<li>  ‚Äî 5.<br>
</li>
</ul><br>
<ul>
<li>Batch size = 1000.<br>
</li>
<li>Learning rate = 3e-04.<br>
</li>
<li>  = 10.<br>
</li>
<li>: Tesla k80 (colab) + Nvidia DGX Station.<br>
</li>
<li> (colab) ‚Äî 20 .<br>
</li>
<li> (Nvidia DGX Station) ‚Äî 8 .<br>
</li>
<li>Test MAE    : 10 ÃÜ.&nbsp;<br>
</li>
</ul><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Count</font></font></b><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/438/137/cb8/438137cb87ac22637e44a01da4a56dfc.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Using features from embeddings gives an increase of a couple of tenths of a day. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Eventually</font></font></b><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Embeddings turned out, of course, uneducated, because they trained a little.</font></font><br>
</li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">There are about 290 features from categorical embeddings, and 20 features from semantic embeddings (it makes no more sense to do because the size of the dictionary is small), so the influence of these semantic features can be leveled due to an imbalance in the proportion of features.</font></font><br>
</li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The semantics between events need to be somehow added to the training set, because due to the fact that the sequences of events (cases) are ordered, the order matters, and information can be extracted from this.</font></font><br>
</li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">You can use more sophisticated architectures for embeddings.</font></font><br>
</li>
<li>      ,       ,    ‚Äî     .<br>
</li>
</ul></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../en493138/index.html">Talk to me: what voice bots can do today</a></li>
<li><a href="../en493140/index.html">The Adventures of Cipollino: Quarantine IT Quest from Flant</a></li>
<li><a href="../en493142/index.html">‚ÄúLet's use Kubernetes!‚Äù You now have 8 problems</a></li>
<li><a href="../en493146/index.html">Exciting side projects that you can do today</a></li>
<li><a href="../en493150/index.html">"Remote" with a Cisco router</a></li>
<li><a href="../en493158/index.html">Loghouse 0.3 - the long-awaited update of our logging system in Kubernetes</a></li>
<li><a href="../en493160/index.html">How we improved Smart Search on hh.ru in 2019: infographics</a></li>
<li><a href="../en493162/index.html">An open list of online meetings and conferences</a></li>
<li><a href="../en493164/index.html">It turns out that the online business is surviving in the current environment. Why? DNA deleted</a></li>
<li><a href="../en493166/index.html">Ontol: a selection of articles on "burnout" [100+]</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>