<!doctype html>
<html class="no-js" lang="pt">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ‘ƒğŸ½ ğŸ’ªğŸ¿ ğŸ‘¨ğŸ¿â€ğŸš€ Aprendizagem reforÃ§ada atravÃ©s de redes neurais competitivas ğŸ“ ğŸ’Œ âœğŸ½</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="No jogo clÃ¡ssico "jogo da velha", hÃ¡ a oportunidade de apresentar todos os movimentos provÃ¡veis â€‹â€‹- e nunca perder. Usei essa oportunidade como uma mÃ©...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Aprendizagem reforÃ§ada atravÃ©s de redes neurais competitivas</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/505574/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">No jogo clÃ¡ssico "jogo da velha", hÃ¡ a oportunidade de apresentar todos os movimentos provÃ¡veis â€‹â€‹- e nunca perder. Usei essa oportunidade como uma mÃ©trica do meu treinamento na rede neural do jogo. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O treinamento reforÃ§ado serÃ¡ Ãºtil para tarefas com uma decisÃ£o ambÃ­gua, complicada pelas muitas opÃ§Ãµes para escolher uma aÃ§Ã£o com resultados diferentes para cada uma. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Obviamente, tic-tac-toe nÃ£o parece um jogo difÃ­cil para treinÃ¡-los com reforÃ§os. No entanto, Ã© adequado para dominar a metodologia de treinamento por meio de redes competitivas, o que melhorarÃ¡ a qualidade e reduzirÃ¡ o tempo gasto no treinamento da rede. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A seguir, descreverei o algoritmo geral de aprendizado com reforÃ§o por meio de redes competitivas no contexto de um jogo da velha com uma demonstraÃ§Ã£o da rede treinada para fazer movimentos "significativos", ou seja, para jogar.</font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Gravando um jogo de uma rede treinada </font></font></a><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Treine a rede do zero </font></font></a><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fontes</font></font></a><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
VocÃª tambÃ©m pode inserir um modelo prÃ©- </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow"><font style="vertical-align: inherit;">treinado</font></a><font style="vertical-align: inherit;"> no GitHub clicando no botÃ£o correspondente para comeÃ§ar imediatamente a testar uma rede neural.</font></font><br>
<a name="habracut"></a><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Os primeiros passos no treinamento de redes neurais</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
AlÃ©m do fato de um neurÃ´nio ter uma funÃ§Ã£o de ativaÃ§Ã£o, que altera a soluÃ§Ã£o resultante de uma rede neural, tambÃ©m podemos dizer que os neurÃ´nios sÃ£o memÃ³ria de rede. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Cada camada aumenta o tempo de treinamento devido Ã  propagaÃ§Ã£o do erro inverso atravÃ©s das camadas "para cima", e o sinal desaparece gradualmente antes de atingir as camadas "superiores", que iniciam o caminho da tomada de decisÃ£o. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ApÃ³s vÃ¡rias opÃ§Ãµes de configuraÃ§Ãµes de rede, cheguei Ã  conclusÃ£o de que, para um jogo simples com um campo 3x3, seria suficiente usar uma rede de camada Ãºnica com 128 neurÃ´nios. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A rede nÃ£o deve ter muita memÃ³ria, isso pode levar Ã  reciclagem - a memorizaÃ§Ã£o completa de todas as opÃ§Ãµes para o resultado do jogo. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A forÃ§a das redes neurais na expressividade da aproximaÃ§Ã£o de uma soluÃ§Ã£o baseada em dados de entrada em condiÃ§Ãµes de memÃ³ria limitada.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Regras gerais para agentes promotores</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para previsÃ£o relativa por uma rede neural, cada cÃ©lula possui uma recompensa dinÃ¢mica, dependendo de sua significÃ¢ncia para o agente no momento. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Na saÃ­da, a rede neural prediz o Ã­ndice da cÃ©lula para onde o agente irÃ¡. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Recompensas de cÃ©lulas tÃªm a seguinte distribuiÃ§Ã£o: </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As menos movimentos feitos, quanto maior a recompensa de 0,1 a 1,0 Uma </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ocupada cÃ©lula tem uma recompensa de -1,0 Uma </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
cÃ©lula perder irÃ¡ receber uma recompensa de -0,4 Uma </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
cÃ©lula livre tem uma recompensa de 0,1 A </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
mudanÃ§a para uma cÃ©lula livre aumenta a sua recompensa para 0,2 A </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
cÃ©lula de vencimento de um adversÃ¡rio tem uma recompensa de 0,5 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A vaga vencedora trarÃ¡ uma recompensa 1.0</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Treinamento competitivo em redes neurais</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Na competiÃ§Ã£o, os agentes serÃ£o treinados em um ambiente competitivo, o que levarÃ¡ a novos resultados do jogo e melhorarÃ¡ a qualidade do treinamento para novas situaÃ§Ãµes. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Cada agente tem seu prÃ³prio campo para treinar para ir a uma cÃ©lula livre e criar combinaÃ§Ãµes vencedoras de jogadas. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Os agentes jogam 9 jogos em casa, depois passam para o campo competitivo por 1 jogo, onde o jogo Ã© disputado atÃ© o vencedor com um limite de 9 jogadas, e tudo Ã© repetido novamente. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
No final de cada jogo, as duas redes sÃ£o treinadas em uma nova experiÃªncia de rivalidade em um campo de jogo comum.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PrevenÃ§Ã£o de Oponentes</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A rede precisa ser treinada para competir pela vitÃ³ria em campo, ou seja, </font><font style="vertical-align: inherit;">recompensa pela prevenÃ§Ã£o bem-sucedida de ganhar um oponente aumentando a recompensa celular. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Outra mÃ©trica para o treinamento de redes neurais sÃ£o os indicadores de vitÃ³ria nas competiÃ§Ãµes. </font><font style="vertical-align: inherit;">Se a margem de vitÃ³ria de um jogador for muito grande, Ã© provÃ¡vel que a rede esteja aprendendo incorretamente, e a razÃ£o para isso sÃ£o recompensas incorretas pelas aÃ§Ãµes dos agentes ou algumas outras aÃ§Ãµes e suas recompensas nÃ£o foram levadas em consideraÃ§Ã£o. </font><font style="vertical-align: inherit;">O melhor resultado do treinamento pode ser considerado uma situaÃ§Ã£o em que as redes vÃ£o quase iguais, ganhando e perdendo quase o mesmo nÃºmero de vezes.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aprendizagem competitiva com um homem</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A implementaÃ§Ã£o de treinar uma rede neural para brincar com humanos nÃ£o Ã© muito diferente da competiÃ§Ã£o entre agentes. </font><font style="vertical-align: inherit;">A Ãºnica diferenÃ§a sÃ©ria Ã© que a pessoa inicialmente joga razoavelmente. </font><font style="vertical-align: inherit;">Uma parte com esse oponente cria situaÃ§Ãµes adicionais para o agente, o que afetarÃ¡ favoravelmente sua experiÃªncia de jogo e, consequentemente, seu treinamento.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ConclusÃ£o</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A rede neural aprendeu a jogar tic-tac-toe somente apÃ³s a introduÃ§Ã£o de um algoritmo competitivo, o que lhe permitiu aprender a fazer movimentos em resposta aos movimentos do oponente, embora nÃ£o perfeitamente, como planejado originalmente. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Em geral, acho que o projeto foi concluÃ­do com sucesso - o objetivo foi alcanÃ§ado.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Obrigado pela atenÃ§Ã£o!</font></font></h2><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ps Treine redes competitivas, isso permite que vocÃª veja jogos simples de um Ã¢ngulo diferente.</font></font></i></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt505554/index.html">Na competiÃ§Ã£o â€œJovens TÃ©cnicos e Inventoresâ€, nÃ£o sÃ£o necessÃ¡rios verdadeiros jovens inventores</a></li>
<li><a href="../pt505556/index.html">Apple rastreia iPhones saqueados e dÃ¡ saqueadores da polÃ­cia</a></li>
<li><a href="../pt505558/index.html">CÃ¢mera de aprendizado profundo do Amazon DeepLens. Descompactando, Conectando e Implementando um Projeto</a></li>
<li><a href="../pt505560/index.html">O segundo conjunto para um programa de gerenciamento de produtos no centro de CS: o que os alunos dizem</a></li>
<li><a href="../pt505568/index.html">Transferindo arquivos usando pipes e outras pequenas coisas no Delphi</a></li>
<li><a href="../zh-CN486176/index.html">ä¼ä¸šç”µå­é‚®ä»¶é€šè®¯å¤‡å¿˜å½•</a></li>
<li><a href="../zh-CN486178/index.html">FOSSæ–°é—»1-2020å¹´1æœˆ27æ—¥è‡³2æœˆ2æ—¥å…è´¹å’Œå¼€æºæ–°é—»çš„å›é¡¾</a></li>
<li><a href="../zh-CN486180/index.html">åˆ›å»ºæ— æœåŠ¡å™¨åº”ç”¨ç¨‹åºçš„æç¤ºå’Œèµ„æº</a></li>
<li><a href="../zh-CN486184/index.html">å¦‚ä½•æœ‰æ•ˆä½¿ç”¨æœç´¢</a></li>
<li><a href="../zh-CN486186/index.html">ç¾éš¾æ€§äº‘ï¼šå¦‚ä½•å·¥ä½œ</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>