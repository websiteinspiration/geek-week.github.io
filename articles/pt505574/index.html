<!doctype html>
<html class="no-js" lang="pt">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👃🏽 💪🏿 👨🏿‍🚀 Aprendizagem reforçada através de redes neurais competitivas 📍 💌 ✍🏽</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="No jogo clássico "jogo da velha", há a oportunidade de apresentar todos os movimentos prováveis ​​- e nunca perder. Usei essa oportunidade como uma mé...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Aprendizagem reforçada através de redes neurais competitivas</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/505574/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">No jogo clássico "jogo da velha", há a oportunidade de apresentar todos os movimentos prováveis ​​- e nunca perder. Usei essa oportunidade como uma métrica do meu treinamento na rede neural do jogo. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O treinamento reforçado será útil para tarefas com uma decisão ambígua, complicada pelas muitas opções para escolher uma ação com resultados diferentes para cada uma. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Obviamente, tic-tac-toe não parece um jogo difícil para treiná-los com reforços. No entanto, é adequado para dominar a metodologia de treinamento por meio de redes competitivas, o que melhorará a qualidade e reduzirá o tempo gasto no treinamento da rede. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A seguir, descreverei o algoritmo geral de aprendizado com reforço por meio de redes competitivas no contexto de um jogo da velha com uma demonstração da rede treinada para fazer movimentos "significativos", ou seja, para jogar.</font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Gravando um jogo de uma rede treinada </font></font></a><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Treine a rede do zero </font></font></a><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fontes</font></font></a><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Você também pode inserir um modelo pré- </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow"><font style="vertical-align: inherit;">treinado</font></a><font style="vertical-align: inherit;"> no GitHub clicando no botão correspondente para começar imediatamente a testar uma rede neural.</font></font><br>
<a name="habracut"></a><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Os primeiros passos no treinamento de redes neurais</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Além do fato de um neurônio ter uma função de ativação, que altera a solução resultante de uma rede neural, também podemos dizer que os neurônios são memória de rede. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Cada camada aumenta o tempo de treinamento devido à propagação do erro inverso através das camadas "para cima", e o sinal desaparece gradualmente antes de atingir as camadas "superiores", que iniciam o caminho da tomada de decisão. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Após várias opções de configurações de rede, cheguei à conclusão de que, para um jogo simples com um campo 3x3, seria suficiente usar uma rede de camada única com 128 neurônios. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A rede não deve ter muita memória, isso pode levar à reciclagem - a memorização completa de todas as opções para o resultado do jogo. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A força das redes neurais na expressividade da aproximação de uma solução baseada em dados de entrada em condições de memória limitada.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Regras gerais para agentes promotores</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para previsão relativa por uma rede neural, cada célula possui uma recompensa dinâmica, dependendo de sua significância para o agente no momento. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Na saída, a rede neural prediz o índice da célula para onde o agente irá. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Recompensas de células têm a seguinte distribuição: </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As menos movimentos feitos, quanto maior a recompensa de 0,1 a 1,0 Uma </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ocupada célula tem uma recompensa de -1,0 Uma </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
célula perder irá receber uma recompensa de -0,4 Uma </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
célula livre tem uma recompensa de 0,1 A </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
mudança para uma célula livre aumenta a sua recompensa para 0,2 A </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
célula de vencimento de um adversário tem uma recompensa de 0,5 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A vaga vencedora trará uma recompensa 1.0</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Treinamento competitivo em redes neurais</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Na competição, os agentes serão treinados em um ambiente competitivo, o que levará a novos resultados do jogo e melhorará a qualidade do treinamento para novas situações. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Cada agente tem seu próprio campo para treinar para ir a uma célula livre e criar combinações vencedoras de jogadas. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Os agentes jogam 9 jogos em casa, depois passam para o campo competitivo por 1 jogo, onde o jogo é disputado até o vencedor com um limite de 9 jogadas, e tudo é repetido novamente. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
No final de cada jogo, as duas redes são treinadas em uma nova experiência de rivalidade em um campo de jogo comum.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Prevenção de Oponentes</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A rede precisa ser treinada para competir pela vitória em campo, ou seja, </font><font style="vertical-align: inherit;">recompensa pela prevenção bem-sucedida de ganhar um oponente aumentando a recompensa celular. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Outra métrica para o treinamento de redes neurais são os indicadores de vitória nas competições. </font><font style="vertical-align: inherit;">Se a margem de vitória de um jogador for muito grande, é provável que a rede esteja aprendendo incorretamente, e a razão para isso são recompensas incorretas pelas ações dos agentes ou algumas outras ações e suas recompensas não foram levadas em consideração. </font><font style="vertical-align: inherit;">O melhor resultado do treinamento pode ser considerado uma situação em que as redes vão quase iguais, ganhando e perdendo quase o mesmo número de vezes.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aprendizagem competitiva com um homem</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A implementação de treinar uma rede neural para brincar com humanos não é muito diferente da competição entre agentes. </font><font style="vertical-align: inherit;">A única diferença séria é que a pessoa inicialmente joga razoavelmente. </font><font style="vertical-align: inherit;">Uma parte com esse oponente cria situações adicionais para o agente, o que afetará favoravelmente sua experiência de jogo e, consequentemente, seu treinamento.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Conclusão</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A rede neural aprendeu a jogar tic-tac-toe somente após a introdução de um algoritmo competitivo, o que lhe permitiu aprender a fazer movimentos em resposta aos movimentos do oponente, embora não perfeitamente, como planejado originalmente. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Em geral, acho que o projeto foi concluído com sucesso - o objetivo foi alcançado.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Obrigado pela atenção!</font></font></h2><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ps Treine redes competitivas, isso permite que você veja jogos simples de um ângulo diferente.</font></font></i></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt505554/index.html">Na competição “Jovens Técnicos e Inventores”, não são necessários verdadeiros jovens inventores</a></li>
<li><a href="../pt505556/index.html">Apple rastreia iPhones saqueados e dá saqueadores da polícia</a></li>
<li><a href="../pt505558/index.html">Câmera de aprendizado profundo do Amazon DeepLens. Descompactando, Conectando e Implementando um Projeto</a></li>
<li><a href="../pt505560/index.html">O segundo conjunto para um programa de gerenciamento de produtos no centro de CS: o que os alunos dizem</a></li>
<li><a href="../pt505568/index.html">Transferindo arquivos usando pipes e outras pequenas coisas no Delphi</a></li>
<li><a href="../zh-CN486176/index.html">企业电子邮件通讯备忘录</a></li>
<li><a href="../zh-CN486178/index.html">FOSS新闻1-2020年1月27日至2月2日免费和开源新闻的回顾</a></li>
<li><a href="../zh-CN486180/index.html">创建无服务器应用程序的提示和资源</a></li>
<li><a href="../zh-CN486184/index.html">如何有效使用搜索</a></li>
<li><a href="../zh-CN486186/index.html">灾难性云：如何工作</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>