<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üïã üóìÔ∏è üë©üèª‚Äçüîß Training smart gaming rivals in Unity using the "play with yourself" method using ML-Agents üö® üë©‚Äç‚úàÔ∏è üí≤</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello, Habr! 
 
 As our regular readers know, we have long and successfully published books on Unity . As part of the study of the topic, we were inte...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Training smart gaming rivals in Unity using the "play with yourself" method using ML-Agents</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/507212/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hello, Habr! </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As our regular readers know, we have long and successfully </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">published books</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> on </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Unity</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . As part of the study of the topic, we were interested in, in particular, the </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ML-Agents Toolkit</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Today we bring to your attention a translation of an article from the Unity blog about how to effectively train game agents using the ‚Äúwith oneself‚Äù method; in particular, the article helps to understand why this method is more effective than traditional reinforced learning. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/vb/sz/5m/vbsz5musziyq5yighwu_iq0eggu.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Enjoy reading!</font></font><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
This article then gives an overview of the self-play technology (playing with oneself) and demonstrates how it helps to provide stable and effective training in the Soccer demo environment from the </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ML-Agents Toolkit</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In the Tennis and Soccer demo environments from the Unity ML-Agents Toolkit, agents pit each other like rivals. Training agents in such a competitive scenario is sometimes a very non-trivial task. In fact, in previous releases of the ML-Agents Toolkit, in order for the agent to confidently learn, a serious study of the award was required. In </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">version 0.14</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">a feature was added that allows the user to train agents using reinforcement learning (RL) based on self-play, a mechanism critical to achieving one of the most high-end reinforcement learning outcomes, such as </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">OpenAI Five</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> and </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">DeepMind's AlphaStar</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Self-play at work pits with each other the current and past hypostases of the agent. Thus, we get an adversary for our agent, who can gradually improve using traditional reinforcement learning algorithms. </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A fully trained agent</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> can successfully compete with advanced human players.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Self-play provides a learning environment that is built on the same principles as competition from a human perspective. For example, a person who learns to play tennis will choose to spar opponents at about the same level as himself, since an opponent too strong or too weak is not so convenient for mastering the game. From the point of view of developing their own skills, it can be much more valuable for a novice tennis player to beat the same beginners, rather than, say, a preschool child or Novak Djokovic. The first one will not even be able to hit the ball, and the second will not give you such a serve that you can beat. When a beginner develops sufficient strength, he can move on to the next level or apply for a more serious tournament to play against more skilled opponents.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In this article, we will consider some technical subtleties associated with the dynamics of the game with ourselves, and also consider examples of working in virtual environments Tennis and Soccer, refactored in such a way as to illustrate the game with itself.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The story of a game with yourself in games</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The phenomenon of playing with oneself has a long history, reflected in the practice of developing artificial game agents designed to compete with people in games. </font><font style="vertical-align: inherit;">One of the first to use this system was Arthur Samuel, who developed a chess simulator in the 1950s and published </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">this work</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> in 1959. </font><font style="vertical-align: inherit;">This system became the forerunner of a landmark result in reinforcement learning achieved by Gerald Tesauro in TD-Gammon; </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">totals published</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">in 1995. TD-Gammon used the TD (Œª) time-difference algorithm with the function of playing with itself to train the agent to play backgammon so that he could compete with a professional person. In some cases, it has been observed that TD-Gammon has a more confident vision of positions than world-class players. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Playing with yourself is reflected in many of the iconic achievements associated with RL. It is important to note that playing with yourself helped the development of agents for playing </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">chess and go</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> with superhuman abilities, elite </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">DOTA 2</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> agents </font><font style="vertical-align: inherit;">, as well as complex strategies and counter-strategies in games such as </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">wrestling</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> and </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;">hide</font></a><font style="vertical-align: inherit;"> and </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">seek</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">In the results achieved by playing with oneself, it is often noted that game agents choose strategies that surprise expert people. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Playing with yourself gives agents a certain creativity that is independent of the creativity of programmers. </font><font style="vertical-align: inherit;">The agent receives only the rules of the game, and then - information about whether he won or lost. </font><font style="vertical-align: inherit;">Further, based on these basic principles, the agent must develop competent behavior. </font><font style="vertical-align: inherit;">According to the creator of TD-Gammon, such an approach to learning liberates, "... in the sense that the program is not constrained by human inclinations and prejudices, which may turn out to be erroneous and unreliable." </font><font style="vertical-align: inherit;">Thanks to this freedom, agents discover brilliant game strategies that completely change the way engineers think about some games.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Competitive reinforcement training</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Within the framework of the traditional task of reinforced learning, the agent is trying to develop a line of behavior that maximizes the total reward. The rewarding signal encodes the agent's task - such a task may be, for example, plotting a course or collecting items. Agent behavior is subject to environmental restrictions. Such, for example, gravity, obstacles, as well as the relative influence of actions taken by the agent himself - for example, the application of force for one‚Äôs own movement. These factors limit the agent‚Äôs behavior and are external forces that he must learn to handle in order to receive a high reward. Thus, the agent competes with the dynamics of the environment, and must move from state to state precisely so that the maximum reward is achieved.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/if/jf/xi/ifjfxil_iqm9pznr61qbpxujs38.png"><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The typical reinforcement training scenario is shown on the left: the agent acts in the environment, transfers to the next state and receives a reward. The training scenario is shown on the right, where the agent competes with a rival, which, from the point of view of the agent, is actually an element of the environment.</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
In the case of competitive games, the agent competes not only with the dynamics of the environment, but also with another (possibly intellectual) agent. We can assume that the opponent is built into the environment, and his actions directly affect the next state that the agent ‚Äúsees‚Äù, as well as the reward that he will receive. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/zl/kv/xo/zlkvxoa9_bt0cdsthmddwll-fvc.png"><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Example Tennis from ML-Agents Toolkit</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Consider the demo of ML-Agents Tennis. The blue racket (left) is the learning agent, and the purple (right) is his opponent. To throw the ball over the net, the agent must take into account the trajectory of the ball flying from the opponent, and make an adjustment to the angle and speed of the flying ball, taking into account environmental conditions (gravity). However, in a competition with an opponent, throwing the ball over the net is only half the battle. A strong opponent can respond with an irresistible blow, and as a result, the agent will lose. A weak opponent may hit the ball into the net. An equal opponent can return the serve, and therefore the game will continue. In any case, both the next state and the corresponding reward depend on both environmental conditions and the opponent. However, in all these situations, the agent makes the same pitch. Therefore, as training in competitive games,and pumping rival behaviors by an agent is a complex problem.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Considerations for a suitable opponent are not trivial. As is clear from the above, the relative strength of the opponent significantly affects the outcome of a particular game. If the opponent is too strong, then the agent may find it difficult to learn how to play from scratch. On the other hand, if the opponent is too weak, then the agent can learn to win, but these skills may be useless in competition with a stronger or simply different opponent. Therefore, we need an opponent who will be approximately equal in strength to the agent (unyielding, but not insurmountable). In addition, since the skills of our agent improve with each game completed, we must increase the strength of his opponent to the same extent. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/fn/vk/pt/fnvkptgsipqopi2iiyflsa5ec_a.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
When playing with yourself, a snapshot from the past or an agent in its current state is the opponent built into the environment.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
This is where the game with ourselves comes in handy! The agent himself satisfies both requirements for the desired opponent. He is definitely roughly equal in strength to himself, and his skills improve over time. In this case, the agent‚Äôs own policy is built into the environment (see the figure). Those who are familiar with </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">gradually increasing complexity education</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (curriculum learning), show you that we can assume that the system is naturally developing the curriculum, following which, the agent learns to fight against increasingly powerful opponents. Accordingly, playing with yourself allows you to use the environment itself to train competitive agents for competitive games!</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In the next two sections, we will consider more technical details of training competitive agents, in particular, regarding the implementation and use of the game with oneself in the ML-Agents Toolkit.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Practical considerations</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Some practical problems arise regarding the framework for playing with yourself. </font><font style="vertical-align: inherit;">In particular, retraining is possible, in which the agent learns to win only with a certain style of play, as well as the instability inherent in the learning process, which can arise due to the unsteadiness of the transition function (that is, due to constantly changing opponents). </font><font style="vertical-align: inherit;">The first problem arises because we want our agents to have a general understanding and ability to fight opponents of different types.</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The second problem can be illustrated in the Tennis environment: different opponents will hit the ball at different speeds and at different angles. From the point of view of the learning agent, this means that, as you learn, the same decisions will lead to different outcomes and, accordingly, the agent will be in different subsequent situations. In traditional reinforcement learning, stationary transition functions are implied. Unfortunately, having prepared a selection of various opponents for the agent in order to solve the first problem, we, being careless, can aggravate the second.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
To cope with this, we will maintain a buffer with past agent policies, from which we will choose potential rivals for our "student" for the long term. </font><font style="vertical-align: inherit;">Choosing an agent from past policies, we get for him a selection of diverse opponents. </font><font style="vertical-align: inherit;">Moreover, allowing the agent to train with a fixed opponent for a long time, we stabilize the transition function and create a more consistent learning environment. </font><font style="vertical-align: inherit;">Finally, these algorithmic aspects can be controlled using hyperparameters, which are discussed in the next section.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Implementation and Usage Details</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Choosing hyperparameters for playing with ourselves, we, first of all, keep in mind a compromise between the level of the opponent, the universality of the final policy and the stability of training. Training in rivalry with a group of opponents that change slowly or do not change at all, and, therefore, give a smaller scatter of results, is a more stable process than training in rivalry with many diverse rivals that change quickly. Available hyperparameters allow you to control how often the current policy of the agent will be saved for later use as one of the opponents in the sample, how often the new opponent will be saved, subsequently chosen for sparring, how often the new opponent will be selected, the number of opponents saved, as well as the probabilitythat in this case, the student will have to play against his own alter ego, and not against an opponent selected from the pool.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In competitive games, the ‚Äúcumulative‚Äù award issued by the environment is perhaps not the most informative metric for tracking learning progress. The fact is that the accumulative award entirely depends on the level of the opponent. An agent with a certain game skill will receive a greater or lesser reward, depending on a less skilled or more skillful opponent, respectively. We propose the implementation </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">of the ELO rating system</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , which allows you to calculate the relative game skill of two players from a certain population when playing with a zero amount. During a single training run, this value should steadily increase. You can track it, along with other learning metrics, for example, the overall award, using </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">TensorBoard</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Playing with yourself in Soccer</font></font></h3> <br>
<img src="https://habrastorage.org/webt/gl/m_/rh/glm_rhop1whkyve0i-wpubxfpqw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The latest releases of the ML-Agent Toolkit do not include agent policies for the Soccer learning environment, since the reliable training process was not built in it. </font><font style="vertical-align: inherit;">However, using the game with ourselves and some refactoring, we can train the agent in non-trivial behaviors. </font><font style="vertical-align: inherit;">The most significant change is the removal of ‚Äúgaming positions‚Äù from the agent‚Äôs characteristics. </font><font style="vertical-align: inherit;">Earlier in the Soccer environment, the ‚Äúgoalkeeper‚Äù and ‚Äústriker‚Äù clearly stood out, so the whole gameplay looked more logical. </font><font style="vertical-align: inherit;">In </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">this video</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">a new environment is presented in which one can see how role behavior is spontaneously formed, in which some agents begin to act as attackers and others as goalkeepers. Now the agents themselves are learning to play these positions! The reward function for all four agents is defined as +1.0 for a goal scored and -1.0 for a goal conceded, with an additional penalty of -0.0003 per step - this penalty is provided to encourage agents to attack.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Here we emphasize once again that the agents in the Soccer learning environment themselves learn cooperative behavior, and for this, no explicit algorithm is used related to multi-agent behavior or role assignment. </font><font style="vertical-align: inherit;">This result demonstrates that an agent can be trained in complex behaviors using relatively simple algorithms - provided that the task is well-formulated. </font><font style="vertical-align: inherit;">The most important condition for this is that agents can observe their teammates, that is, they receive information about the relative position of the teammate. </font><font style="vertical-align: inherit;">Forcing an aggressive fight for the ball, the agent indirectly tells the teammate that he should move in defense. </font><font style="vertical-align: inherit;">On the contrary, moving away in defense, the agent provokes a teammate to attack.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">What's next</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
If you have ever used any of the new features from this release - tell us about them. </font><font style="vertical-align: inherit;">We draw your attention to the </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ML-Agents GitHub issues</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> page </font><font style="vertical-align: inherit;">, where you can talk about bugs found, as well as to the </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Unity ML-Agents forums</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> page </font><font style="vertical-align: inherit;">, where general questions and problems are discussed.</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../en507200/index.html">7 Ways Data Scientists Fool You</a></li>
<li><a href="../en507202/index.html">Avito Analytics meetup</a></li>
<li><a href="../en507204/index.html">Industrial design interior kitchen: from sketch to product in box</a></li>
<li><a href="../en507206/index.html">Architecture Y messenger</a></li>
<li><a href="../en507210/index.html">The performance of modern Java when working with large amounts of data, part 2</a></li>
<li><a href="../en507214/index.html">How to create and modify interactive PDF forms, or the new skill ABBYY FineReader PDF</a></li>
<li><a href="../en507218/index.html">Read me, or why the text is not read to the end</a></li>
<li><a href="../en507222/index.html">Why everyone should wear masks</a></li>
<li><a href="../en507224/index.html">How to eliminate blind spots with visual testing</a></li>
<li><a href="../en507226/index.html">OCR for PDF in .NET - How to extract text from inaccessible PDF documents</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>