<!doctype html>
<html class="no-js" lang="ja">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👩🏾‍✈️ 🛌🏼 🧑🏿‍🤝‍🧑🏼 「申し訳ありませんが、...を認識しました」またはTensorflow Object Detection APIを使用してラズベリーとコントローラを認識しました ⛄️ ⛵️ 🗻</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="昨年末に、ニューラルネットワークを使用して画像内のオブジェクトを認識する機能に興味をそそられた記事を書きました。その記事では、PyTorchを使用して、ラズベリーまたはアルドゥイーノのようなコントローラーをビデオで分類しました。そして、PyTorchが好きだったにもかかわらず、すぐにTensorFl...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>「申し訳ありませんが、...を認識しました」またはTensorflow Object Detection APIを使用してラズベリーとコントローラを認識しました</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/494804/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">昨年末に、</font><font style="vertical-align: inherit;">ニューラルネットワークを使用して画像内のオブジェクトを認識する機能に興味をそそられ</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">た記事</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">を書きまし</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">た</font></a><font style="vertical-align: inherit;">。その記事では、PyTorchを使用して、ラズベリーまたはアルドゥイーノのようなコントローラーをビデオで分類しました。そして、PyTorchが好きだったにもかかわらず、すぐにTensorFlowに対応できなかったので、彼に頼りました。しかし、私はビデオ内のオブジェクトの認識の問題に戻ることを約束しました。約束を守る時がきたようです。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
この記事では、ローカルマシンを試して、Tensorflow 1.13の完成モデルと独自の画像セットのオブジェクト検出APIを再トレーニングし、OpenCVを使用してWebカメラのビデオストリーム内のベリーとコントローラーを認識します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
夏までにベリー認識スキルを向上させたいですか？</font><font style="vertical-align: inherit;">その後、猫の下で大歓迎です。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/fu/do/rd/fudordve5xz-8gwdnbvlnkkjusm.png"><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
内容：</font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">パートI：</font></font></a><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">はじめに</font></font></a><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">パートII：</font></font></a><font style="vertical-align: inherit;"><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">TenosrFlowでのモデルのトレーニング</font></a><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">パートIII：OpenCVでのモデルの適用</font></a></font><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">パートIV：結論</font></font></a><br>
<a name="I"></a><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">パートI：はじめに</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
PyTorchに関する以前の記事を読んだことのある人は、私がニューラルネットワークの問題で素人であることをすでに知っています。したがって、この記事を究極の真実と見なさないでください。しかし、どういうわけか私は誰かがTensorflow Object Detection APIを使用してビデオ認識の基本に対処するのを助けることができることを願っています。</font></font><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">今回はチュートリアルを作らなかったので、記事は通常より短くなります。</font></font></i><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
まず</font><font style="vertical-align: inherit;">、ローカルマシンでObject Detection APIを使用することに関する</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">公式のチュートリアル</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">は、控えめに言っても、完全ではありません。初心者として、私は完全に不十分であり、ブログ記事に集中する必要がありました。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
正直なところ、TensorFlow 2.0を試してみたいと思いますが、ほとんどの出版物では、この記事の執筆時点では、移行の問題は完全には解決されていません。</font><font style="vertical-align: inherit;">したがって、結局、私はTF 1.13.2に落ち着きました。</font></font><br>
<a name="II"></a><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">パートII：TensorFlowでモデルを教える </font></font></h2><br><font style="vertical-align: inherit;"></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">この記事</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、またはその前半から、JavaScriptが適用されるまで</font><font style="vertical-align: inherit;">
、モデルを教えるための手順を描きました</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">（英語を話せない場合は</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、Habréで</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">同じトピックに関する記事を見ることができます</font><font style="vertical-align: inherit;">）</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
確かに、私の場合、いくつかの違いがあります：</font></font><br>
<br>
<ol>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Linuxを使用したのは、Anaconda for Linuxがprotobufとpycocoapiをすでにビルドしていて、自分でビルドする必要がなかったからです。</font></font></li>
<li>   TensorFlow 1.13.2,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow">Object Detection API 1.13</a> ,       TensorFlow 1.13.2.   master        TF 1.15,         1.13.</li>
<li>      numpy — 1.17.5,  1.18    .</li>
<li>  faster_rcnn_inception_v2_coco    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow">ssd_mobilenet_v2_coco</a>,    ,     .</li>
</ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 念のため、グラフィックアクセラレータは使用しなかったと言います。トレーニングは、プロセッサの容量についてのみ実行されました。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
画像のセット、構成ファイル、保存されたグラフ、およびOpenCVを使用して画像を認識するためのスクリプトは、いつものように</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GitHub</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">からダウンロードでき</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"><font style="vertical-align: inherit;">ます</font></a><font style="vertical-align: inherit;">。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
23時間のモデルトレーニングが終了し、家の中のお茶はすべて飲まれました。どこ？いつ？"検査し、今私の忍耐力はようやく終わりました。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
トレーニングを停止してモデルを保存します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
次のコマンドを使用して、「Anaconda」と同じ環境にOpenCVをインストールします。</font></font><br>
<br>
<pre><code class="plaintext hljs">conda install -c conda-forge opencv</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
最終的にバージョン4.2をインストールしました。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
さらに、</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">この記事</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">の説明</font><font style="vertical-align: inherit;">は不要になります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
モデルを保存した後、私には明らかではない1つの間違いを犯しました。つまり、関数のトレーニング/フォルダーで先ほど使用したgraph.pbtxtファイルをすぐに置き換えようとしました。</font></font><br>
<br>
<pre><code class="python hljs">cv2.dnn.readNetFromTensorflow()</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
残念ながら、これはこの方法では機能せず、OpenCVのgraph.pbtxtを取得するには、もう1つ操作が必要になります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
おそらく、私が今アドバイスしているという事実はあまり良い方法ではありませんが、私にとってはうまくいきます。</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"><font style="vertical-align: inherit;">tf_text_graph_ssd.pyを</font></a></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
ダウンロードし</font><font style="vertical-align: inherit;">、さらに</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"><font style="vertical-align: inherit;">tf_text_graph_common.py</font></a><font style="vertical-align: inherit;">を保存したグラフが置かれているフォルダー（このinference_graphフォルダーがあります）に置きます。</font><font style="vertical-align: inherit;">
次に、このフォルダーのコンソールに移動し、そこからおよそ次の内容のコマンドを実行します。</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"></font><br><font style="vertical-align: inherit;"></font><br>
<br>
<pre><code class="plaintext hljs">python tf_text_graph_ssd.py --input frozen_inference_graph.pb --config pipeline.config --output graph.pbtxt</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
そして、モデルをOpenCVにアップロードするのはこれで終わりです。</font></font><br>
<br>
<a name="III"></a><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">パートIII：OpenCVでモデルを適用する </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
OpenCVでの作業に関するPyTorchに関する記事のように、私は</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">この出版物の</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">プログラムコードを基礎として使用しました</font><font style="vertical-align: inherit;">。</font></font><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
もう少し簡単にするために小さな変更を加えましたが、コードを完全に理解していないので、コメントしません。</font><font style="vertical-align: inherit;">うまくいきます。</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">コードの方が優れていた可能性があることは明らかですが、OpenCVチュートリアルに参加する時間はまだありません</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font></font><br>
<br>
<div class="spoiler" role="button" tabindex="0">
                        <b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">OpenCVコード</font></font></b>
                        <div class="spoiler_text"><pre><code class="python hljs">
<span class="hljs-comment"># USAGE</span>
<span class="hljs-comment"># based on this code https://proglib.io/p/real-time-object-detection/</span>
<span class="hljs-comment"># import the necessary packages</span>
<span class="hljs-keyword">from</span> imutils.video <span class="hljs-keyword">import</span> VideoStream
<span class="hljs-keyword">from</span> imutils.video <span class="hljs-keyword">import</span> FPS
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> imutils
<span class="hljs-keyword">import</span> time
<span class="hljs-keyword">import</span> cv2<font></font>
<font></font>
prototxt=<span class="hljs-string">"graph.pbtxt"</span>
model=<span class="hljs-string">"frozen_inference_graph.pb"</span>
min_confidence = <span class="hljs-number">0.5</span><font></font>
<font></font>
<span class="hljs-comment"># initialize the list of class labels MobileNet SSD was trained to</span>
<span class="hljs-comment"># detect, then generate a set of bounding box colors for each class</span>
CLASSES = [<span class="hljs-string">"background"</span>, <span class="hljs-string">"duino"</span>,<span class="hljs-string">"raspb"</span>]<font></font>
COLORS = [(<span class="hljs-number">40</span>,<span class="hljs-number">50</span>,<span class="hljs-number">60</span>),((<span class="hljs-number">140</span>,<span class="hljs-number">55</span>,<span class="hljs-number">130</span>)),(<span class="hljs-number">240</span>,<span class="hljs-number">150</span>,<span class="hljs-number">25</span>)]<font></font>
<font></font>
<span class="hljs-comment"># load our serialized model from disk</span>
print(<span class="hljs-string">"[INFO] loading model..."</span>)<font></font>
<font></font>
net =cv2.dnn.readNetFromTensorflow(model,prototxt)<font></font>
<font></font>
<span class="hljs-comment"># initialize the video stream, allow the cammera sensor to warmup,</span>
<span class="hljs-comment"># and initialize the FPS counter</span>
print(<span class="hljs-string">"[INFO] starting video stream..."</span>)<font></font>
vs = VideoStream(src=<span class="hljs-number">0</span>).start()<font></font>
time.sleep(<span class="hljs-number">0.5</span>)<font></font>
fps = FPS().start()<font></font>
<font></font>
<span class="hljs-comment"># loop over the frames from the video stream</span>
<span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:
	<span class="hljs-comment"># grab the frame from the threaded video stream and resize it</span>
	<span class="hljs-comment"># to have a maximum width of 400 pixels</span><font></font>
	frame = vs.read()<font></font>
	frame = imutils.resize(frame, width=<span class="hljs-number">300</span>)<font></font>
<font></font>
	<span class="hljs-comment"># grab the frame dimensions and convert it to a blob</span>
	(h, w) = frame.shape[:<span class="hljs-number">2</span>]<font></font>
	blob = cv2.dnn.blobFromImage(frame, size=(<span class="hljs-number">300</span>, <span class="hljs-number">300</span>), swapRB=<span class="hljs-literal">True</span>)<font></font>
<font></font>
	<span class="hljs-comment"># pass the blob through the network and obtain the detections and</span>
	<span class="hljs-comment"># predictions</span><font></font>
	net.setInput(blob)<font></font>
	detections = net.forward()<font></font>
<font></font>
	<span class="hljs-comment"># loop over the detections</span>
	<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> np.arange(<span class="hljs-number">0</span>, detections.shape[<span class="hljs-number">2</span>]):
		<span class="hljs-comment"># extract the confidence (i.e., probability) associated with</span>
		<span class="hljs-comment"># the prediction</span>
		<span class="hljs-keyword">print</span> (detections)<font></font>
		confidence = detections[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, i, <span class="hljs-number">2</span>]<font></font>
<font></font>
		<span class="hljs-keyword">if</span> confidence &gt; min_confidence:
			<span class="hljs-comment"># extract the index of the class label from the</span>
			<span class="hljs-comment"># `detections`, then compute the (x, y)-coordinates of</span>
			<span class="hljs-comment"># the bounding box for the object</span>
			idx = int(detections[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, i, <span class="hljs-number">1</span>])<font></font>
			box = detections[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, i, <span class="hljs-number">3</span>:<span class="hljs-number">7</span>] * np.array([w, h, w, h])<font></font>
			(startX, startY, endX, endY) = box.astype(<span class="hljs-string">"int"</span>)<font></font>
<font></font>
			<span class="hljs-comment"># draw the prediction on the frame</span>
			label = <span class="hljs-string">"{}: {:.2f}%"</span>.format(CLASSES[idx],<font></font>
				confidence * <span class="hljs-number">100</span>)<font></font>
			cv2.rectangle(frame, (startX, startY), (endX, endY),<font></font>
				COLORS[idx], <span class="hljs-number">2</span>)<font></font>
			y = startY - <span class="hljs-number">15</span> <span class="hljs-keyword">if</span> startY - <span class="hljs-number">15</span> &gt; <span class="hljs-number">15</span> <span class="hljs-keyword">else</span> startY + <span class="hljs-number">15</span>
			cv2.putText(frame, label, (startX, y+<span class="hljs-number">3</span>),<font></font>
				cv2.FONT_HERSHEY_SIMPLEX, <span class="hljs-number">0.5</span>, COLORS[idx], <span class="hljs-number">1</span>)<font></font>
<font></font>
	<span class="hljs-comment"># show the output frame</span>
	cv2.imshow(<span class="hljs-string">"Frame"</span>, frame)<font></font>
	key = cv2.waitKey(<span class="hljs-number">1</span>) &amp; <span class="hljs-number">0xFF</span><font></font>
<font></font>
	<span class="hljs-comment"># if the `q` key was pressed, break from the loop</span>
	<span class="hljs-keyword">if</span> key == ord(<span class="hljs-string">"q"</span>):
		<span class="hljs-keyword">break</span><font></font>
<font></font>
	<span class="hljs-comment"># update the FPS counter</span><font></font>
	fps.update()<font></font>
<font></font>
<span class="hljs-comment"># stop the timer and display FPS information</span><font></font>
fps.stop()<font></font>
print(<span class="hljs-string">"[INFO] elapsed time: {:.2f}"</span>.format(fps.elapsed()))<font></font>
print(<span class="hljs-string">"[INFO] approx. FPS: {:.2f}"</span>.format(fps.fps()))<font></font>
<font></font>
<span class="hljs-comment"># do a bit of cleanup</span><font></font>
cv2.destroyAllWindows()<font></font>
vs.stop()<font></font>
</code></pre><br>
</div>
                    </div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
これで準備はすべて整いました。モデルを起動し、レンズを古いCraftDuinoに向けて、その結果を楽しんでいます。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/bw/hj/yd/bwhjyd9pddoeop9yaz7fxbqozzo.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
一見したところ、まったく悪くはありませんが、一見しただけです。</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
23時間後にはモデルが再トレーニングされたように見えるため、オブジェクトの定義時に重大なエラーが発生します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ここに視覚的なデモンストレーションがあり</font></font><br>
<br>
<img src="https://habrastorage.org/webt/1w/3y/gf/1w3ygfo-ufytpuyct1kaarpsgls.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ます</font><font style="vertical-align: inherit;">：</font><font style="vertical-align: inherit;">ご覧のとおり、ナイフだけでなく黒い背景でも、このモデルはそれをArduinoのようなコントローラーとして定義します。おそらくこれは、トレーニングデータにArduinoとその類似体の暗い画像があり、モデルが23時間でぶつかったためです。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
その結果、コンピューターをさらに8時間ロードし、新しいモデルをトレーニングする必要がありました。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
物事は彼女とはるかに良いです。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
以下はCraftDuinoの例です。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/_c/8m/62/_c8m62y2q6as-l8sun5ah5ivppk.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
生きているラズベリーは手元にありません。</font><font style="vertical-align: inherit;">写真を印刷しなければなりませんでした。</font><font style="vertical-align: inherit;">電話やモニターの画面からも認識できますが、紙からの方が便利でした。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/63/_k/ou/63_koujmchte7jor0ulqzxcvgcs.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
モデルがArduino nanoをどのように認識するかを確認してみましょう。</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" class="user_link"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Drzugrik</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">私にとっては、センサー付きのメガデバイスにはんだ付けしました。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/ub/33/61/ub3361ozwkiwvl2sosx6yldvsou.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ご覧のとおり、認識は非常に良好ですが、角度が非常に悪く、暖かい照明下では、ラズベリーのような断片を認識できます。しかし実際には、エラーのあるフレームはレンズで捉えるのが困難でした。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
次に、彼女が訓練されていないオブジェクトをどのように分類するかを確認しましょう。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
繰り返しになりますが、ナイフと黒い背景の例：</font></font><br>
<br>
<img src="https://habrastorage.org/webt/io/ja/6a/ioja6aexferclondu4228nsr06y.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
今回はすべて正常に動作します。</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">前回の記事で説明</font></a></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
したCanny 3の小さなコントローラーを認識するモデルを提供します</font><font style="vertical-align: inherit;">。</font><font style="vertical-align: inherit;">
私たちのモデルはラズベリーとarduinoのようなコントローラー以外は何も知らないため、モデルはCannyコントローラーを非常にうまく認識したと言えます。</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"></font><br>
<br>
<img src="https://habrastorage.org/webt/xp/ay/14/xpay14o7clhp1y1twu4vyltiay4.png"><br>
<br><font style="vertical-align: inherit;"></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
確かに、Arduino nanoの場合と同様に、角度と照明に大きく依存します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
白熱灯の暖かい光と角度の不成功により、コントローラーは認識されないだけでなく、ラズベリーとして定義される場合もあります。</font><font style="vertical-align: inherit;">確かに、過去の場合と同様に、これらの角度はまだレンズを捉えようとする必要がありました。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/01/ut/h_/01uth_-raiwnzasg7ypn-aoxezs.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
さて、最後のケースは、PyTorch </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">での画像</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">の</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">分類</font></a><font style="vertical-align: inherit;">についての記事に対する一種のお粗末なものです</font><font style="vertical-align: inherit;">。</font><font style="vertical-align: inherit;">前回と同様に、Raspberry Pi 2シングルボードコンピューターとそのロゴは1つのフレームで互換性があります。</font><font style="vertical-align: inherit;">分類の問題を解決し、画像に最も可能性の高いオブジェクトを1つ選択した前回の記事とは異なり、この場合、ロゴとラズベリー自体の両方が認識されます。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/vx/fv/us/vxfvusfgitn6vk1pe6o4rvoen9i.png"><br>
<br>
<a name="IV"></a><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> パートIV：結論 </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
結論として、Tensorflow Object Detection APIを使用したこの小さな例の経験が浅いにもかかわらず、休日と月曜日の両方にかかったと後悔していません。それを使用する方法の少なくとも少しの理解がめちゃくちゃ好奇心をそそられるとき。学習プロセスでは、モデルを生きているモデルと見なし始め、その成功と失敗を追跡します。</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ですから、ある日このことに慣れていない人には、自分の何かを試してみることをお勧めします。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
さらに、その過程で上昇しているため、実際のウェブカメラを購入する必要すらありません。実際のところ、記事の作成中にウェブカメラを壊すことができ（フォーカスメカニズムを壊し）、すでにすべてを放棄しなければならないと考えていました。しかし、Droidcamの助けを借りて、ウェブカメラの代わりにスマートフォンを使用できることがわかりました（広告には数えません）。さらに、撮影の品質は壊れたカメラの品質よりもはるかに優れていることがわかり、これは画像内のオブジェクトの認識の品質に大きく影響しました。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ちなみに、アナコンダは通常の</font><b><font style="vertical-align: inherit;">pycocotools</font></b><font style="vertical-align: inherit;">ビルドを持っているので</font></font><b><font style="vertical-align: inherit;"></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">私はLinuxのみを見つけ、オペレーティングシステムを切り替えるのが面倒なので、この記事全体をオープンソースソフトウェアのみを使用して作成しました。</font><font style="vertical-align: inherit;">WordとPhotoshopの両方に類似しており、プリンターのドライバーさえありました。</font><font style="vertical-align: inherit;">私の人生で初めてこれが起こりました。</font><font style="vertical-align: inherit;">Linux OSの最新バージョンとアプリケーションプログラムは、Microsoft OSを25年以上使用している人にとっても非常に便利であることがわかりました。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
PS誰かが</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Tensorflowバージョン2以降の</font><font style="vertical-align: inherit;">オブジェクト検出APIを適切に実行する方法を知っている場合</font><font style="vertical-align: inherit;">は、PMまたはコメントで購読を解除してください。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
良い一日と健康を！</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../ja494788/index.html">メールの仕組み</a></li>
<li><a href="../ja494792/index.html">Yandex.Routing：物流に突入し、未来を変えることにした方法</a></li>
<li><a href="../ja494794/index.html">私たちは長い腕を持っています</a></li>
<li><a href="../ja494798/index.html">Snowden：パンデミックは終了しますが、人口の監視は残ります</a></li>
<li><a href="../ja494800/index.html">中国のUSB IRトランシーバープロトコルのリバースエンジニアリング</a></li>
<li><a href="../ja494806/index.html">サイバーは2019年をトレンド2020としてターゲット-ハッカーは焦点を変えました</a></li>
<li><a href="../ja494808/index.html">製品アナリスト：何をするか、どれだけ稼ぐか、ビジネスがもたらすメリット</a></li>
<li><a href="../ja494810/index.html">3Dの概要：Three.jsの基本</a></li>
<li><a href="../ja494814/index.html">Slurmは役に立ちますか？</a></li>
<li><a href="../ja494818/index.html">取引所で作業する取引端末を選択する方法</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>