<!doctype html>
<html class="no-js" lang="de">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🎣 👩🏾‍🏫 🥘 Verstärktes Lernen durch wettbewerbsfähige neuronale Netze 🎱 🈷️ 📀</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Im klassischen Spiel "Tic-Tac-Toe" besteht die Möglichkeit, alle wahrscheinlichen Züge zu präsentieren - und nie zu verlieren. Ich nutzte diese Gelege...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Verstärktes Lernen durch wettbewerbsfähige neuronale Netze</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/505574/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Im klassischen Spiel "Tic-Tac-Toe" besteht die Möglichkeit, alle wahrscheinlichen Züge zu präsentieren - und nie zu verlieren. Ich nutzte diese Gelegenheit als Maß für mein Training im neuronalen Netzwerk des Spiels. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die Verstärkung des Trainings ist nützlich für Aufgaben, bei denen eine mehrdeutige Entscheidung getroffen wird, was aufgrund der vielen Optionen für die Auswahl einer Aktion mit jeweils unterschiedlichen Ergebnissen kompliziert ist. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Natürlich sieht Tic-Tac-Toe nicht nach einem schwierigen Spiel aus, um sie mit Verstärkungen zu trainieren. Es ist jedoch gut geeignet, um die Trainingsmethodik durch wettbewerbsfähige Netzwerke zu beherrschen, wodurch die Qualität verbessert und der Zeitaufwand für das Training des Netzwerks verringert wird. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Als nächstes beschreibe ich den allgemeinen Lernalgorithmus mit Verstärkung durch wettbewerbsfähige Netzwerke im Kontext eines Tic-Tac-Toe-Spiels mit einer Demonstration eines trainierten Netzwerks, um „sinnvolle“ Bewegungen auszuführen, dh zu spielen.</font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aufzeichnen eines Spiels eines trainierten Netzwerks. </font></font></a><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Trainieren Sie das Netzwerk von Grund auf. </font></font></a><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Quellen.</font></font></a><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Sie können auch ein vorab </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;">trainiertes</font></a><font style="vertical-align: inherit;"> Modell von GitHub aus eingeben, indem Sie auf die entsprechende Schaltfläche klicken, um sofort mit dem Testen eines neuronalen Netzwerks zu beginnen.</font></font><br>
<a name="habracut"></a><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die ersten Schritte beim Training neuronaler Netze</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Zusätzlich zu der Tatsache, dass ein Neuron eine Aktivierungsfunktion hat, die die resultierende Lösung eines neuronalen Netzwerks ändert, können wir auch sagen, dass Neuronen Netzwerkspeicher sind. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Jede Schicht erhöht die Trainingszeit aufgrund der Ausbreitung des Umkehrfehlers durch die Schichten "nach oben", und das Signal verblasst allmählich, bevor es die "oberen" Schichten erreicht, die den Weg der Entscheidungsfindung beginnen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nach mehreren Optionen für Netzwerkeinstellungen kam ich zu dem Schluss, dass es für ein einfaches Spiel mit einem 3x3-Feld ausreichen würde, ein einschichtiges Netzwerk mit 128 Neuronen zu verwenden. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das Netzwerk sollte nicht zu viel Speicher haben, dies kann zu einer Umschulung führen - dem vollständigen Speichern aller Optionen für das Ergebnis des Spiels. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die Stärke neuronaler Netze in der Ausdruckskraft der Annäherung einer Lösung basierend auf Eingabedaten unter Bedingungen mit begrenztem Speicher.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Allgemeine Regeln für die Förderung von Agenten</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Für die relative Vorhersage durch ein neuronales Netzwerk hat jede Zelle eine dynamische Belohnung, die von ihrer Bedeutung für den Agenten im Moment abhängt. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Am Ausgang sagt das neuronale Netzwerk den Index der Zelle voraus, in die der Agent gehen wird. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Belohnungen von Zellen haben die folgende Verteilung: </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Je weniger Züge gemacht werden, desto höher ist die Belohnung von 0,1 auf 1,0. Eine </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
besetzte Zelle hat eine Belohnung von -1,0. Eine </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
verlorene Zelle erhält eine Belohnung von -0,4. Eine </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
freie Zelle hat eine Belohnung. 0,1 Ein </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Zug in eine freie Zelle erhöht ihre Belohnung auf 0,2. Eine </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
gewinnende Zelle eines Gegners hat eine Belohnung von 0,5 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Der Gewinn eines Slots bringt eine Belohnung von 1,0</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wettbewerbsfähiges neuronales Netzwerktraining</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Während des Wettbewerbs werden die Agenten in einem Wettbewerbsumfeld geschult, was zu neuen Ergebnissen des Spiels führt und die Qualität des Trainings für neue Situationen verbessert. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Jeder Agent hat sein eigenes Feld, um zu trainieren, in eine freie Zelle zu gehen und gewinnbringende Kombinationen von Zügen zu erstellen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Agenten spielen 9 Spiele zu Hause und wechseln dann für 1 Spiel in das Wettbewerbsfeld, wo das Spiel bis zum Sieger mit einem Limit von 9 Zügen gespielt wird. Dann wird alles erneut wiederholt. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Am Ende jedes Spiels werden beide Netzwerke auf eine neue Erfahrung der Rivalität auf einem gemeinsamen Spielfeld trainiert.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Gegnerprävention</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das Netzwerk muss trainiert werden, um auf dem Feld um den Sieg zu kämpfen, d. H. </font><font style="vertical-align: inherit;">Belohnung für die erfolgreiche Verhinderung des Gewinnens eines Gegners durch Erhöhung der Zellbelohnung. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Eine weitere Metrik für das Training neuronaler Netze sind die Indikatoren für den Sieg bei Wettbewerben. </font><font style="vertical-align: inherit;">Wenn die Gewinnspanne für einen Spieler zu groß ist, lernt das Netzwerk wahrscheinlich falsch, und der Grund dafür sind falsche Belohnungen für die Aktionen von Agenten, oder andere Aktionen und deren Belohnungen wurden nicht berücksichtigt. </font><font style="vertical-align: inherit;">Das beste Ergebnis des Trainings kann als eine Situation angesehen werden, in der die Netzwerke fast gleich sind und ungefähr gleich oft gewinnen und verlieren.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wettbewerbsfähiges Lernen mit einem Mann</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die Implementierung des Trainings eines neuronalen Netzwerks zum Spielen mit Menschen unterscheidet sich nicht wesentlich von der Konkurrenz zwischen Agenten. </font><font style="vertical-align: inherit;">Der einzige gravierende Unterschied ist, dass die Person zunächst vernünftig spielt. </font><font style="vertical-align: inherit;">Eine Partei mit einem solchen Gegner schafft zusätzliche Situationen für den Agenten, die sich günstig auf seine Spielerfahrung und dementsprechend auf sein Training auswirken.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fertigstellung</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das neuronale Netzwerk lernte das Tic-Tac-Toe erst nach Einführung des Wettbewerbsalgorithmus, wodurch es lernte, wie man als Reaktion auf die Bewegungen des Gegners Bewegungen ausführt, wenn auch nicht perfekt, wie ursprünglich geplant. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Generell denke ich, dass das Projekt erfolgreich abgeschlossen wurde - das Ziel wurde erreicht.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Vielen Dank für Ihre Aufmerksamkeit!</font></font></h2><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ps Trainiere wettbewerbsfähige Netzwerke, damit du einfache Spiele aus einem anderen Blickwinkel betrachten kannst.</font></font></i></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de505554/index.html">Beim Wettbewerb „Junge Techniker und Erfinder“ werden echte junge Erfinder nicht benötigt</a></li>
<li><a href="../de505556/index.html">Apple verfolgt geplünderte iPhones und gibt Plünderer der Polizei</a></li>
<li><a href="../de505558/index.html">Amazon DeepLens Deep Learning Kamera. Ein Projekt entpacken, verbinden und bereitstellen</a></li>
<li><a href="../de505560/index.html">Der zweite Satz für ein Produktmanagementprogramm im CS-Center: Was die Schüler sagen</a></li>
<li><a href="../de505568/index.html">Übertragen von Dateien mit Pipes und anderen kleinen Dingen auf Delphi</a></li>
<li><a href="../en486014/index.html">SLAC Tour: US Department of Energy National Accelerator Laboratory at Stanford</a></li>
<li><a href="../en486018/index.html">Session Survey Results</a></li>
<li><a href="../en486024/index.html">Really simple graphics in R for science and journalism</a></li>
<li><a href="../en486028/index.html">Neural networks in the production of dentures</a></li>
<li><a href="../en486030/index.html">TL-SREET 55 5K LED Street Light Review</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>