<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üîÄ ü§∏ üñãÔ∏è Configuring the loss function for a neural network based on seismic data üë®üèΩ üë©üèø‚Äçü§ù‚Äçüë®üèª üßîüèø</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In a previous article, we described an experiment to determine the minimum amount of manually labeled sections for training a neural network using sei...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Configuring the loss function for a neural network based on seismic data</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ods/blog/488852/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In a </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">previous article,</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> we described an experiment to determine the minimum amount of manually labeled sections for training a neural network using seismic data. </font><font style="vertical-align: inherit;">Today we continue this topic by choosing the most appropriate loss function. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Two basic classes of functions are considered - Binary cross entropy and Intersection over Union - in 6 variants with selection of parameters, as well as combinations of functions of different classes. </font><font style="vertical-align: inherit;">Additionally, the regularization of the loss function is considered. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Spoiler: managed to significantly improve the quality of the network forecast.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/nq/rn/qt/nqrnqtl1xaabhj0vtklokcrbtpc.jpeg"><br>
<a name="habracut"></a><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Business Research Goals</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We will not repeat the description of the specifics of the seismic survey, the data obtained and the tasks of their interpretation. All this is described in </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">our previous article</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The idea of ‚Äã‚Äãthis study was prompted by the results of the </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">competition for the search for salt deposits on 2D slices</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . According </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">to the participants of the competition</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , in solving this problem, a whole zoo of various loss functions was used, moreover, with different successes. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Therefore, we asked ourselves: is it really possible for such problems on such data to select the loss function can give a significant gain in quality? Or is this characteristic only for the conditions of the competition, when there is a struggle for the fourth or fifth decimal place for the metrics predefined by the organizers?</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Typically, in tasks solved with the help of neural networks, the tuning of the learning process is based mainly on the experience of the researcher and some heuristics. For example, for the problems of image segmentation, loss functions are most often used, based on assessing the coincidence of the shapes of recognized zones, the so-called Intersection over Union. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Intuitively, based on an understanding of behavior and research results, these types of functions will give better results than those that are not sharpened for images, such as cross-entropy ones. Nevertheless, experiments in search of the best option for this type of task as a whole and each task individually continue.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The seismic data prepared for interpretation have a number of features that can have a significant impact on the behavior of the loss function. </font><font style="vertical-align: inherit;">For example, the horizons separating the geological layers are smooth, more sharply changing only in the places of faults. </font><font style="vertical-align: inherit;">In addition, the distinguished zones have a sufficiently large area relative to the image, i.e. </font><font style="vertical-align: inherit;">small spots on interpretation results are most often considered a recognition error. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As part of this experiment, we tried to find answers to the following local questions:</font></font><br>
<br>
<ol>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Is the loss function of the Intersection over Union class really the best result for the problem considered below? </font><font style="vertical-align: inherit;">It seems the answer is obvious, but which one? </font><font style="vertical-align: inherit;">And how much is the best from a business point of view?</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Is it possible to improve the results by combining functions of different classes? </font><font style="vertical-align: inherit;">For example, Intersection over Union and cross-entropy with different weights.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Is it possible to improve the results by adding to the loss function various additions designed specifically for seismic data?</font></font></li>
</ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
And to a more global question: </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Is it worth bothering with the selection of the loss function for the tasks of interpreting seismic data, or is the gain in quality not comparable with the time loss for conducting such studies? </font><font style="vertical-align: inherit;">Maybe it‚Äôs worth intuitively choosing any function and spending energy on the selection of more significant training parameters?</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">General description of the experiment and the data used</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
For the experiment, we took the same task of isolating geological layers on 2D slices of a seismic cube (see Figure 1). </font></font><br>
<br>
<img src="https://habrastorage.org/webt/w6/j3/gn/w6j3gnflms5vqyxc3mbjdnzsaam.png"> <br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Figure 1. Example of a 2D slice (left) and the result of the marking of the corresponding geological layers (right) ( </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">source</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> )</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
And the same set of completely labeled data from the Dutch sector of the North Sea. Source seismic data are available on the </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Open Seismic Repository: Project Netherlands Offshore F3 Block</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> website </font><font style="vertical-align: inherit;">. A brief description can be found in </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Silva et al. "Netherlands Dataset: A New Public Dataset for Machine Learning in Seismic Interpretation</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;">"</font></a></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Since in our case we are talking about 2D slices, we did not use the original 3D cube, but the already made ‚Äúslicing‚Äù, available here:</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Netherlands F3 Interpretation Dataset</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
During the experiment, we solved the following problems:</font></font><br>
<br>
<ol>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We looked at the source data and selected the slices, which are closest in quality to manual marking (similar to the previous experiment).</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We recorded the architecture of the neural network, the methodology and parameters of training, and the principle of selecting slices for training and validation (similar to the previous experiment).</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We chose the studied loss functions.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We selected the best parameters for the parameterized loss functions.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We trained neural networks with different functions on the same data volume and chose the best function.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We trained neural networks with different combinations of the selected function with functions of another class on the same amount of data.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We trained neural networks with regularization of the selected function on the same amount of data.</font></font></li>
</ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
For comparison, we used the results of a previous experiment in which the loss function was chosen exclusively intuitively and was a combination of functions of different classes with coefficients also chosen ‚Äúby eye‚Äù. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The results of this experiment in the form of estimated metrics and predicted by the networks of slice masks are presented below.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Task 1. Data selection</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As initial data, we used ready-made inlines and crosslines of a seismic cube from the Dutch sector of the North Sea. </font><font style="vertical-align: inherit;">As in the previous experiment, simulating the work of the interpreter, for training the network, we chose only clean masks, having looked at all the slices. </font><font style="vertical-align: inherit;">As a result, 700 crosslines and 400 inlines from ~ 1600 source images were selected.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Task 2. Fixing the parameters of the experiment</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
This and the following sections are of interest, first of all, for specialists in Data Science, therefore, appropriate terminology will be used. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
For training, we chose 5% of the total number of slices, moreover, inlines and crosslines in equal shares, i.e. 40 + 40. Slices were selected evenly throughout the cube. For validation, 1 slice was used between adjacent images of the training sample. Thus, the validation sample consisted of 39 inlines and 39 crosslines. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
321 inline and 621 crossline fell into the delayed sample, on which the results were compared. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Similar to the previous experiment, image preprocessing was not performed, and the same UNet architecture with the same training parameters was used.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The target slice masks were represented as binary cubes of dimension HxWx10, where the last dimension corresponds to the number of classes, and each value of the cube is 0 or 1, depending on whether this pixel in the image belongs to the class of the corresponding layer or not. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Each network forecast was a similar cube, each value of which relates to the probability that a given image pixel belongs to the class of the corresponding layer. In most cases, this value was converted into the probability itself by using a sigmoid. However, this should not be done for all loss functions, therefore activation was not used for the last layer of the network. Instead, the corresponding conversions were performed in the functions themselves.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
To reduce the influence of the randomness of the choice of initial weights on the results, the network was trained for 1 era with binary cross-entropy as a function of losses. </font><font style="vertical-align: inherit;">All other training started with these weights received.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Task 3. The choice of loss functions</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
For the experiment, 2 base classes of functions were selected in 6 variants: </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Binary cross entropy</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> :</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">binary cross entropy;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">weighted binary cross entropy;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">balanced binary cross entropy.</font></font></li>
</ul><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Intersection over Union</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> :</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Jaccard loss;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tversky loss;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Lov√°sz loss.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A brief description of the listed functions with code for Keras is given in the </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">article</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Here we present the most important with links (where possible) to a detailed description of each function. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
For our experiment, the consistency of the function used during training is important with the metric by which we evaluate the result of the network forecast on the delayed sample. </font><font style="vertical-align: inherit;">Therefore, we used our code implemented on TensorFlow and Numpy, written directly using the formulas below. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The following notation is used in the formulas:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">pt - for the binary target mask (Ground Truth);</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">pp - for network prediction mask.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
For all functions, unless otherwise specified, it is assumed that the network prediction mask contains probabilities for each pixel in the image, i.e. </font><font style="vertical-align: inherit;">values ‚Äã‚Äãin the interval (0, 1).</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Binary cross entropy</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Description: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br>
<img src="https://habrastorage.org/webt/fc/-1/ep/fc-1epf3rdg8gfdq0ycdtywgxee.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
This function seeks to bring the network forecast distribution closer to the target, penalizing not only erroneous predictions, but also uncertain ones.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Weighted binary cross entropy</font></font></h3><br>
<img src="https://habrastorage.org/webt/gm/ud/n3/gmudn3reniryckosw6v_myxzyh0.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
This function coincides with binary cross-entropy with a beta value of 1. It is recommended for strong class imbalances. </font><font style="vertical-align: inherit;">For beta&gt; 1, the number of false negative forecasts (False Negative) decreases and the completeness (Recall) increases, for beta &lt;1 the number of false positive forecasts (False Positive) decreases and the accuracy increases (Precision).</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Balanced binary cross entropy</font></font></h3><br>
<img src="https://habrastorage.org/webt/pd/pn/sd/pdpnsd3ah6jwiofykm1ftkggfec.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
This function is similar to weighted cross-entropy, but it corrects the contribution of not only single, but also zero values ‚Äã‚Äãof the target mask. </font><font style="vertical-align: inherit;">Coincides (up to a constant) with binary cross-entropy at a value of coefficient beta = 0.5.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Jaccard loss </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The Jacquard coefficient (aka Intersection over Union, IoU) determines the measure of the ‚Äúsimilarity‚Äù of the two areas. </font><font style="vertical-align: inherit;">The Dice index does the same thing: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ej/ak/to/ejakto7j-ghcfffxulq6o6i7kz0.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
It makes no sense to consider both of these functions. </font><font style="vertical-align: inherit;">We chose Jacquard. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
For the case when both areas are specified using binary masks, the above formula can be easily rewritten in terms of the values ‚Äã‚Äãof the masks: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/n-/6w/zh/n-6wzhefudinbxqnol6vaibdbaw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
For non-binary forecasts, optimization of the Jacquard coefficient is a non-trivial task. </font><font style="vertical-align: inherit;">We will use the same formula for probabilities in the forecast mask as a certain imitation of the initial coefficient and, accordingly, the following loss function:</font></font><br>
<br>
<img src="https://habrastorage.org/webt/eq/yi/8f/eqyi8fefrizsnxxu909mlx9wmgm.png"><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tversky loss</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Description: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">https://arxiv.org/pdf/1706.05721.pdf</font></font></a><br>
<br>
<img src="https://habrastorage.org/webt/hr/3k/cc/hr3kccuxmb3qfogjzaotqbz1ewa.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
This function is a parameterized version of the optimization of the Jacquard coefficient that coincides with it at alpha = beta = 1 and with the Dice index at alpha = beta = 0.5. </font><font style="vertical-align: inherit;">For other non-zero and non-coincident values, we can shift the emphasis towards accuracy or completeness in the same way as in the functions of weighted and balanced cross-entropy. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The emphasis shift problem can be rewritten using a single coefficient lying in the interval (0, 1). </font><font style="vertical-align: inherit;">The resulting loss function will look like this:</font></font><br>
<br>
<img src="https://habrastorage.org/webt/fd/re/og/fdreog2gzjmosed7ggczhjwj9oo.png"><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Lov√°sz loss</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
It is difficult to give a formula for this function, since it is an option for optimizing the Jacquard coefficient by an algorithm based on sorted errors. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
You can see the description of the function </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">here</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , one of the code options is </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">here</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Important explanation! </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
To simplify the comparison of values ‚Äã‚Äãand graphs hereinafter, under the term "Jacquard coefficient" we will further understand the unit minus the coefficient itself. </font><font style="vertical-align: inherit;">Jaccard loss is one way to optimize this ratio, along with Tversky loss and Lov√°sz loss.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Task 4. Choosing the best parameters for parameterized loss functions</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
To select the best loss function on the same data set, an evaluation criterion is needed. In his quality, we chose the average / median number of connected components on the resulting masks. In addition, we used the Jacquard coefficient for predictive masks converted to single-layer argmax and again divided into binarized layers. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The number of connected components (i.e., solid spots of the same color) on each forecast obtained is an indirect criterion for assessing the volume of its subsequent refinement by the interpreter. If this value is 10, then the layers are selected correctly and we are talking about a maximum of minor horizon correction. If there are not many more, then you only need to "clean" small areas of the image. If there are substantially more of them, then everything is bad and may even need a complete re-layout.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The Jacquard coefficient, in turn, characterizes the coincidence of image zones assigned to one class and their boundaries.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Weighted binary cross entropy</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
According to the experimental results was selected parameter beta = 2: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/iu/uo/hs/iuuohsoebw2mam6axgianwbi_se.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Figure 2. Comparison of the quality of the network prediction and core loss function chosen criteria </font></font></i><br>
<br>
<img src="https://habrastorage.org/webt/dm/-c/3t/dm-c3teowwucpbfd2cckbspwbi0.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Figure 3. Statistics for the number of connected components on the part of the values of the parameter beta</font></font><br>
</i><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Balanced binary cross entropy</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
According to the results of the experiments, the value of the parameter beta = 0.7 was chosen: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ae/cl/p8/aeclp8omibpajaurlwmrg9sttai.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Figure 4. Comparison of the quality of the network forecast by the main loss function and the selected criteria </font></font></i><br>
<br>
<img src="https://habrastorage.org/webt/lj/r0/fm/ljr0fmrrxbyarastm-wudodn0w0.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Figure 5. Statistics for the number of connected components</font></font><br>
</i><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tversky loss</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
According to the results of the experiments, the value of the parameter beta = 0.7 was chosen: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/3a/rk/vt/3arkvtetkqy6i9k3ih8zyo8is78.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Figure 6. Comparison of the quality of the network forecast by the main loss function and the selected criteria </font></font><br>
</i><br>
<img src="https://habrastorage.org/webt/ct/qx/bh/ctqxbh-6f4hy2ek2k-phurk4zmg.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Figure 7. Comparison of the quality of the network forecast by the main loss function and the selected criteria</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Two conclusions can be drawn from the above figures. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
First, the selected criteria correlate fairly well with each other, i.e. </font><font style="vertical-align: inherit;">the Jacquard coefficient is consistent with an estimate of the volume of necessary refinement. </font><font style="vertical-align: inherit;">Secondly, the behavior of the cross-entropy loss functions is quite logically different from the behavior of the criteria, i.e. </font><font style="vertical-align: inherit;">using training only this category of functions without additional evaluation of the results is still not worth it.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Task 5. Choosing the best loss function.</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Now compare the results that showed the selected 6 loss functions on the same data set. </font><font style="vertical-align: inherit;">For completeness, we added the predictions of the network obtained in the previous experiment. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/wc/xr/my/wcxrmy--zexfjmduml37g4smpvc.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Figure 8. Comparison projections networks trained with different loss functions for the selected criteria</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Table 1. The mean values of criteria </font></font><br>
<br>
<img src="https://habrastorage.org/webt/kx/89/87/kx8987wn5narwnkfza5k2nemhbm.png"><br>
<br>
<img src="https://habrastorage.org/webt/6s/f4/_k/6sf4_kelwapket3g27sjs2eukyw.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Figure 9. Comparison networks projections on the number of predictions of the indicated number of connected components</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
from the presented diagrams and tables, the following conclusions regarding the use of "solo" loss functions:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In our case, the "Jacquard" functions of the Intersection over Union class really show better values ‚Äã‚Äãthan cross-entropy ones. </font><font style="vertical-align: inherit;">Moreover, significantly better.</font></font></li>
<li>              Lovazh loss.</li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Let us visually compare the forecasts for the slices with one of the best and one of the worst Lovazh loss values ‚Äã‚Äãand the number of connected components. The target mask is displayed in the upper right corner, the forecast obtained in the previous experiment in the lower right: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/xp/xi/ra/xpxirarzwu1-6gfacc2c1cjh5yy.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Figure 10. Network forecasts for one of the best slices </font></font><br>
 </i><br>
<img src="https://habrastorage.org/webt/xq/13/dr/xq13drinuno-gjldxvuknxurm-m.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Figure 11. Network forecasts for one of the worst slices</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
It can be seen that all networks work equally well on easily recognizable slices. But even on a poorly recognizable slice where all networks are wrong, the forecast for Lovazh loss is visually better than the forecasts of other networks. Although it is one of the worst losses for this function. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
So, at this step, we have decided on a clear leader - Lovazh loss, the results of which can be described as follows:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">about 60% of forecasts are close to ideal, i.e. </font><font style="vertical-align: inherit;">require no more than adjustments to individual sections of the horizons;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">approximately 30% of forecasts contain no more than 2 extra spots, i.e. </font><font style="vertical-align: inherit;">require minor improvements;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">approximately 1% of forecasts contain from 10 to 25 extra spots, i.e. </font><font style="vertical-align: inherit;">requires substantial improvement.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
At this step, only replacing the loss function, we achieved a significant improvement in the results compared to the previous experiment. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Can it still be improved by a combination of different functions? </font><font style="vertical-align: inherit;">Check it out.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Task 6. Choosing the best combination of loss function</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The combination of loss functions of various nature is used quite often. However, finding the best combination is not easy. A good example is the result of a previous experiment, which turned out to be even worse than the "solo" function. The purpose of such combinations is to improve the result by optimizing according to different principles. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Let's try to sort through different options of the function selected in the previous step with others, but not with all in a row. We confine ourselves to combinations of functions of different types, in this case, with cross-entropy ones. It makes no sense to consider combinations of functions of the same type.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Total, we checked 3 pairs with 9 possible coefficients each (from 0.1 \ 0.9 to 0.9 \ 0.1). In the figures below, the X axis shows the coefficient before Lovazh loss. The coefficient before the second function is equal to one minus the coefficient before the first. The left value is only a cross-entropy function, the right value is only Lovazh loss. </font></font><br>
 <br>
<img src="https://habrastorage.org/webt/gq/rq/-d/gqrq-d_mwnnwg5dgvyzysvlxzqw.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Figure 12. Evaluation of the forecast results of networks trained on BCE + Lovazh </font></font><br>
 </i><br>
<img src="https://habrastorage.org/webt/1p/as/9p/1pas9pcxuskc8hnyj2igbzjibkc.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Figure 13. Evaluation of the forecast </font></font></i><br>
 <br>
<img src="https://habrastorage.org/webt/dy/22/8z/dy228z6wlagamlmywmivsvbvzmg.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">results of networks trained on BCE + Lovazh</font></font><br>
</i><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
It can be seen that the selected ‚Äúsolo‚Äù function was not improved by adding cross-entropy. Reducing some values ‚Äã‚Äãof the Jacquard coefficient by 1-2 thousandths may be important in a competitive environment, but does not compensate for a business deterioration in the criterion for the number of connected components. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
To verify the typical behavior of a combination of functions of different types, we conducted a similar series of training for Jaccard loss. For only one pair, it was possible to slightly improve the values ‚Äã‚Äãof both criteria simultaneously: </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
0.8 * JaccardLoss + 0.2 * BBCE </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Average of connected components: 11.5695 -&gt; 11.2895 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Average of Jaccard: 0.0307 -&gt; 0.0283 </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
But even these values ‚Äã‚Äãare worse than the ‚Äúsolo‚Äù Lovazh loss.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Thus, it makes sense to investigate combinations of functions of different nature on our data only in competition conditions or in the presence of free time and resources. </font><font style="vertical-align: inherit;">To achieve a significant increase in quality is unlikely to succeed.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Task 7. Regularization of the best loss function.</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
At this step, we tried to improve the previously selected loss function with an addition designed specifically for seismic data. This is the regularization described in the article: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Neural-networks for geophysicists and their application to seismic data interpretation</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;">"</font></a></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
The article mentions that standard regularizations like weights decay on seismic data do not work well. Instead, an approach based on the norm of the gradient matrix is ‚Äã‚Äãproposed, which is aimed at smoothing the boundaries of classes. The approach is logical if we recall that the boundaries of the geological layers should be smooth.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
However, when using such regularization, one should expect some deterioration in the results by the Jacquard criterion, since smoothed class boundaries will less likely coincide with possible abrupt transitions obtained with manual markup. But we have one more criterion for verification - by the number of connected components. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We trained 13 networks with the regularization described in the article and the coefficient in front of it, taking values ‚Äã‚Äãfrom 0.1 to 0.0001. The figures below show some of the ratings for both criteria. </font></font><br>
 <br>
<img src="https://habrastorage.org/webt/hz/ud/no/hzudnopqafa6t6chsygeikxsvwy.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Figure 15. Comparison of the quality of the network forecast by the selected criteria </font></font><br>
 </i><br>
<img src="https://habrastorage.org/webt/rq/v7/2h/rqv72hpfiaplc9m3_nfurrxk3xq.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Figure 16. Statistics for the number of connected components in terms of the coefficient values ‚Äã‚Äãbefore regularization</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
It is seen that regularization with a coefficient of 0.025 significantly reduced the statistics for the criterion for the number of connected components. However, the Jacquard criterion in this case expectedly increased to 0.0357. However, this is a slight increase compared to a reduction in manual refinement. </font></font><br>
 <br>
<img src="https://habrastorage.org/webt/oa/-1/ow/oa-1owbvsfhyuzxqsugvjmlk1ro.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Figure 17. Comparison of network forecasts by the number of predictions with the specified number of connected components.</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Finally, we compare the class boundaries on the target and predicted masks for the previously selected worst cut. </font></font><br>
 <br>
<img src="https://habrastorage.org/webt/gh/sp/1o/ghsp1oiatktuohroonfpakr3teq.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Figure 18. The network forecast for one of the worst slices. </font></font></i><br>
 <br>
<img src="https://habrastorage.org/webt/ge/0b/zg/ge0bzgqe3gpsuktjwgmxannuxca.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Figure 19. Overlaying part of the horizon of the target mask and forecast</font></font><br>
</i><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As can be seen from the figures, the forecast mask, of course, is mistaken in some places, but at the same time it smooths the oscillations of the target horizons, i.e. </font><font style="vertical-align: inherit;">corrects minor errors in the initial markup. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Summary characteristics of the selected loss function with regularization:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">about 87% of forecasts are close to ideal, i.e. </font><font style="vertical-align: inherit;">require no more than adjustments to individual sections of the horizons;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">approximately 10% of forecasts contain 1 extra spot, i.e. </font><font style="vertical-align: inherit;">require minor improvements;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">about 3% of forecasts contain from 2 to 5 extra spots, i.e. </font><font style="vertical-align: inherit;">require a little more substantial refinement.</font></font></li>
</ul><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">findings</font></font></h2><br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Only by adjusting one learning parameter - the loss function - we were able to significantly improve the quality of the network forecast and reduce the amount of necessary refinement by about three times. </font></font></li>
<li>             Intersection over Union (     Lovazh loss)      .            -,       .</li>
<li>    -,      .         , ..     .</li>
</ul><br>
<h2>  :</h2><br>
<ol>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=">Reinaldo Mozart Silva, Lais Baroni, Rodrigo S. Ferreira, Daniel Civitarese, Daniela Szwarcman, Emilio Vital Brazil. Netherlands Dataset: A New Public Dataset for Machine Learning in Seismic Interpretation</a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=">Lars Nieradzik. Losses for Image Segmentation</a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=">Daniel Godoy. Understanding binary cross-entropy / log loss: a visual explanation</a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=">Seyed Sadegh Mohseni Salehi, Deniz Erdogmus, and Ali Gholipour. Tversky loss function for image segmentation using 3D fully convolutional deep networks</a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=">Maxim Berman, Amal Rannen Triki, Matthew B. Blaschko. The Lovasz-Softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks</a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=">Bas Peters, Eldad Haber, and Justin Granek. Neural-networks for geophysicists and their application to seismic data interpretation</a></li>
</ol></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../en488842/index.html">Catch an electron: observing a process that takes a quintillionth of a second</a></li>
<li><a href="../en488844/index.html">Borders for border guards: US court has established rules for checking devices - discuss the situation</a></li>
<li><a href="../en488846/index.html">Postgresso 19</a></li>
<li><a href="../en488848/index.html">CTO all startup</a></li>
<li><a href="../en488850/index.html">Using RabbitMQ with MonsterMQ Part 1</a></li>
<li><a href="../en488854/index.html">10. Fortinet Getting Started v6.0. Escort</a></li>
<li><a href="../en488856/index.html">Well CRM and CRM. Everything is easier than you think.</a></li>
<li><a href="../en488860/index.html">Import substitution and shipbuilding</a></li>
<li><a href="../en488866/index.html">Validating Images with Gitlab CI / CD</a></li>
<li><a href="../en488868/index.html">Announcement of Mobius 2020 Piter: what excites mobile developers?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>