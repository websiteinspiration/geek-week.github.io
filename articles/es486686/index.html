<!doctype html>
<html class="no-js" lang="es">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèæ‚ÄçüöÄ üé£ ‚ôüÔ∏è Acerca de implementar una biblioteca de aprendizaje profundo en Python üõ∂ üë©üèø‚Äçüè´ üë®üèø‚Äçüî¨</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Las tecnolog√≠as de aprendizaje profundo han recorrido un largo camino en un corto per√≠odo de tiempo, desde redes neuronales simples hasta arquitectura...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Acerca de implementar una biblioteca de aprendizaje profundo en Python</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ruvds/blog/486686/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Las tecnolog√≠as de aprendizaje profundo han recorrido un largo camino en un corto per√≠odo de tiempo, desde redes neuronales simples hasta arquitecturas bastante complejas. Para apoyar la r√°pida difusi√≥n de estas tecnolog√≠as, se han desarrollado varias bibliotecas y plataformas de aprendizaje profundo. Uno de los objetivos principales de tales bibliotecas es proporcionar a los desarrolladores interfaces simples para crear y entrenar modelos de redes neuronales. Dichas bibliotecas permiten a sus usuarios prestar m√°s atenci√≥n a las tareas que se resuelven y no a las sutilezas de la implementaci√≥n del modelo. Para hacer esto, es posible que deba ocultar la implementaci√≥n de mecanismos b√°sicos detr√°s de varios niveles de abstracci√≥n. Y esto, a su vez, complica la comprensi√≥n de los principios b√°sicos en los que se basan las bibliotecas de aprendizaje profundo.</font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><img src="https://habrastorage.org/webt/bp/yi/sl/bpyislfb1o7e-qh7exvklh1oxuw.jpeg"></a><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El art√≠culo, cuya traducci√≥n estamos publicando, tiene como objetivo analizar las caracter√≠sticas del dispositivo de bloques de construcci√≥n de bajo nivel de bibliotecas de aprendizaje profundo. </font><font style="vertical-align: inherit;">Primero, hablamos brevemente sobre la esencia del aprendizaje profundo. </font><font style="vertical-align: inherit;">Esto nos permitir√° comprender los requisitos funcionales para el software respectivo. </font><font style="vertical-align: inherit;">Luego buscamos desarrollar una biblioteca de aprendizaje profundo simple pero funcional en Python usando NumPy. </font><font style="vertical-align: inherit;">Esta biblioteca es capaz de proporcionar capacitaci√≥n integral para modelos simples de redes neuronales. </font><font style="vertical-align: inherit;">En el camino, hablaremos sobre los diversos componentes de los marcos de aprendizaje profundo. </font><font style="vertical-align: inherit;">La biblioteca que consideraremos es bastante peque√±a, menos de 100 l√≠neas de c√≥digo. </font><font style="vertical-align: inherit;">Y esto significa que ser√° bastante simple resolverlo. </font><font style="vertical-align: inherit;">El c√≥digo completo del proyecto, que trataremos, se puede encontrar </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">aqu√≠</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<a name="habracut"></a><br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Informaci√≥n general</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Por lo general, las bibliotecas de aprendizaje profundo (como TensorFlow y PyTorch) constan de los componentes que se muestran en la siguiente figura.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c49/b9f/396/c49b9f39652c0260a9e30ee4e5dea146.jpg"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Componentes del marco de aprendizaje profundo</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Analicemos estos componentes.</font></font><br>
<br>
<h3><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ñç Operadores</font></font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Los conceptos de "operador" y "capa" (capa) generalmente se usan indistintamente. </font><font style="vertical-align: inherit;">Estos son los componentes b√°sicos de cualquier red neuronal. </font><font style="vertical-align: inherit;">Los operadores son funciones vectoriales que transforman datos. </font><font style="vertical-align: inherit;">Entre los operadores de uso frecuente, se pueden distinguir como las capas de activaci√≥n lineal y de convoluci√≥n, capas de submuestreo (agrupaci√≥n), semi-lineal (ReLU) y sigmoide (sigmoide).</font></font><br>
<br>
<h3><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ñçOptimizadores (optimizadores)</font></font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Los optimizadores son la base de las bibliotecas de aprendizaje profundo. </font><font style="vertical-align: inherit;">Describen m√©todos para ajustar los par√°metros del modelo utilizando ciertos criterios y teniendo en cuenta el objetivo de la optimizaci√≥n. </font><font style="vertical-align: inherit;">Entre los optimizadores conocidos, se pueden observar SGD, RMSProp y Adam.</font></font><br>
<br>
<h3><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ñç Funciones de p√©rdida</font></font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Las funciones de p√©rdida son expresiones matem√°ticas anal√≠ticas y diferenciables que se utilizan como sustituto del objetivo de optimizaci√≥n al resolver un problema. </font><font style="vertical-align: inherit;">Por ejemplo, la funci√≥n de entrop√≠a cruzada y la funci√≥n lineal por partes se usan generalmente en problemas de clasificaci√≥n.</font></font><br>
<br>
<h3><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ñç Inicializadores</font></font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Los inicializadores proporcionan valores iniciales para los par√°metros del modelo. </font><font style="vertical-align: inherit;">Son estos valores los que tienen los par√°metros al comienzo del entrenamiento. </font><font style="vertical-align: inherit;">Los inicializadores desempe√±an un papel importante en el entrenamiento de las redes neuronales, ya que los par√°metros iniciales fallidos pueden significar que la red aprender√° lentamente o que no podr√° aprender en absoluto. </font><font style="vertical-align: inherit;">Hay muchas formas de inicializar los pesos de una red neuronal. </font><font style="vertical-align: inherit;">Por ejemplo, puede asignarles peque√±os valores aleatorios de la distribuci√≥n normal. </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aqu√≠ hay una</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> p√°gina donde puede aprender sobre los diferentes tipos de inicializadores.</font></font><br>
<br>
<h3><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ñç Regularizadores</font></font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Los regularizadores son herramientas que evitan el reentrenamiento de la red y ayudan a la red a generalizarse. Puede lidiar con el reentrenamiento de la red de manera expl√≠cita o impl√≠cita. Los m√©todos expl√≠citos implican limitaciones estructurales en los pesos. Por ejemplo, minimizando su Norma L1 y la Norma L2, lo que, en consecuencia, hace que los valores de peso est√©n mejor dispersos y distribuidos de manera m√°s uniforme. Los m√©todos impl√≠citos est√°n representados por operadores especializados que realizan la transformaci√≥n de representaciones intermedias. Esto se hace a trav√©s de la normalizaci√≥n expl√≠cita, por ejemplo, usando la t√©cnica de normalizaci√≥n de paquetes (BatchNorm), o cambiando la conectividad de red usando los algoritmos DropOut y DropConnect.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Los componentes anteriores generalmente pertenecen a la parte de la interfaz de la biblioteca. </font><font style="vertical-align: inherit;">Aqu√≠, por "parte de la interfaz" me refiero a las entidades con las que el usuario puede interactuar. </font><font style="vertical-align: inherit;">Le proporcionan herramientas convenientes para dise√±ar eficientemente una arquitectura de red neuronal. </font><font style="vertical-align: inherit;">Si hablamos de los mecanismos internos de las bibliotecas, pueden proporcionar soporte para el c√°lculo autom√°tico de gradientes de la funci√≥n de p√©rdida, teniendo en cuenta varios par√°metros del modelo. </font><font style="vertical-align: inherit;">Esta t√©cnica se llama com√∫nmente diferenciaci√≥n autom√°tica (AD).</font></font><br>
<br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Diferenciaci√≥n autom√°tica</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Cada biblioteca de aprendizaje profundo proporciona al usuario algunas capacidades de diferenciaci√≥n autom√°tica. Esto le da la oportunidad de centrarse en la descripci√≥n de la estructura del modelo (gr√°fico de c√°lculos) y transferir la tarea de calcular los gradientes al m√≥dulo AD. Tomemos un ejemplo que nos permitir√° saber c√≥mo funciona todo. Supongamos que queremos calcular las derivadas parciales de la siguiente funci√≥n con respecto a sus variables de entrada X‚ÇÅ y X‚ÇÇ: </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Y = sin (x‚ÇÅ) + X‚ÇÅ * X‚ÇÇ </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La siguiente figura, que tom√© prestada </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">de aqu√≠</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , muestra la gr√°fica de c√°lculos y el c√°lculo de derivadas usando una regla de cadena.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/605/635/189/605635189c56a2f87927ec1a0c5b6318.png"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Gr√°fico computacional y c√°lculo de derivados por una regla de cadena</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Lo que ves aqu√≠ es algo as√≠ como un "modo inverso" de diferenciaci√≥n autom√°tica. </font><font style="vertical-align: inherit;">El conocido algoritmo de propagaci√≥n de error de retorno es un caso especial del algoritmo anterior para el caso en que la funci√≥n ubicada en la parte superior es una funci√≥n de p√©rdida. </font><font style="vertical-align: inherit;">AD explota el hecho de que cualquier funci√≥n compleja consiste en operaciones aritm√©ticas elementales y funciones elementales. </font><font style="vertical-align: inherit;">Como resultado, los derivados se pueden calcular aplicando una regla de cadena a estas operaciones.</font></font><br>
<br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Implementaci√≥n</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En la secci√≥n anterior, examinamos los componentes necesarios para crear una biblioteca de aprendizaje profundo dise√±ada para crear y capacitar de extremo a extremo las redes neuronales. Para no complicar el ejemplo, imito el patr√≥n de dise√±o de la biblioteca </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Caffe</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> aqu√≠ </font><font style="vertical-align: inherit;">. Aqu√≠ declaramos dos clases abstractas - </font></font><code>Function</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">y </font></font><code>Optimizer</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Adem√°s, hay una clase </font></font><code>Tensor</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, que es una estructura simple que contiene dos matrices NumPy multidimensionales. Uno de ellos est√° dise√±ado para almacenar valores de par√°metros, el otro, para almacenar sus gradientes. Todos los par√°metros en diferentes capas (operadores) ser√°n de tipo </font></font><code>Tensor</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Antes de continuar, eche un vistazo al esquema general de la biblioteca.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d05/98c/068/d0598c068139ecda1f2aacbd9ea5f068.jpg"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Diagrama UML de la biblioteca</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Al momento de escribir este material, esta biblioteca contiene una implementaci√≥n de la capa lineal, la funci√≥n de activaci√≥n ReLU, la capa SoftMaxLoss y el optimizador SGD. Como resultado, resulta que la biblioteca se puede usar para entrenar modelos de clasificaci√≥n que consisten en capas completamente conectadas y que usan una funci√≥n de activaci√≥n no lineal. Ahora veamos algunos detalles sobre las clases abstractas que tenemos.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Una clase abstracta</font></font><code>Function</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">proporciona una interfaz para operadores. Aqu√≠ est√° su c√≥digo:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword">class</span>&nbsp; <span class="hljs-title">Function</span>(<span class="hljs-params">object</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self</span>):</span>&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">raise</span> NotImplementedError<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backward</span>(<span class="hljs-params">self</span>):</span>&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">raise</span> NotImplementedError<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getParams</span>(<span class="hljs-params">self</span>):</span>&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span> []</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Todos los operadores se implementan a trav√©s de la herencia de una clase abstracta </font></font><code>Function</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Cada operador debe proporcionar una implementaci√≥n de los m√©todos </font></font><code>forward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">y </font></font><code>backward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Los operadores pueden contener una implementaci√≥n de un m√©todo opcional </font></font><code>getParams()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">que devuelve sus par√°metros (si los hay). El m√©todo </font></font><code>forward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">recibe datos de entrada y devuelve el resultado de su transformaci√≥n por parte del operador. Adem√°s, resuelve los problemas internos necesarios para calcular gradientes. El m√©todo </font></font><code>backward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">acepta las derivadas parciales de la funci√≥n de p√©rdida con respecto a las salidas del operador e implementa el c√°lculo de las derivadas parciales de la funci√≥n de p√©rdida con respecto a los datos de entrada del operador y los par√°metros (si los hay). Tenga en cuenta que el m√©todo</font></font><code>backward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, en esencia, proporciona a nuestra biblioteca la capacidad de realizar diferenciaciones autom√°ticas. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para tratar todo esto con un ejemplo espec√≠fico, echemos un vistazo a la implementaci√≥n de la funci√≥n </font></font><code>Linear</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Linear</span>(<span class="hljs-params">Function</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,in_nodes,out_nodes</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.weights = Tensor((in_nodes,out_nodes))<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.bias&nbsp; &nbsp; = Tensor((<span class="hljs-number">1</span>,out_nodes))<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.type = <span class="hljs-string">'linear'</span><font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self,x</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;output = np.dot(x,self.weights.data)+self.bias.data<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.input = x&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span> output<font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backward</span>(<span class="hljs-params">self,d_y</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.weights.grad += np.dot(self.input.T,d_y)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.bias.grad&nbsp; &nbsp; += np.sum(d_y,axis=<span class="hljs-number">0</span>,keepdims=<span class="hljs-literal">True</span>)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;grad_input &nbsp; &nbsp; &nbsp; &nbsp; = np.dot(d_y,self.weights.data.T)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span> grad_input<font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getParams</span>(<span class="hljs-params">self</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span> [self.weights,self.bias]</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El m√©todo </font></font><code>forward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">implementa la transformaci√≥n de la vista </font></font><code>Y = X*W+b</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">y devuelve el resultado. Adem√°s, guarda el valor de entrada </font></font><code>X</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, ya que es necesario para calcular la derivada parcial de </font></font><code>dY</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">la funci√≥n de p√©rdida con respecto al valor de salida </font></font><code>Y</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">en el m√©todo </font></font><code>backward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. El m√©todo </font></font><code>backward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">recibe las derivadas parciales, calculadas con respecto al valor de entrada </font></font><code>X</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">y los par√°metros </font></font><code>W</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">y </font></font><code>b</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Adem√°s, devuelve las derivadas parciales calculadas con respecto al valor de entrada </font></font><code>X</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, que se transferir√° a la capa anterior. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Una clase abstracta </font></font><code>Optimizer</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">proporciona una interfaz para optimizadores:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Optimizer</span>(<span class="hljs-params">object</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,parameters</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.parameters = parameters<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">step</span>(<span class="hljs-params">self</span>):</span>&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">raise</span> NotImplementedError<font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">zeroGrad</span>(<span class="hljs-params">self</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> self.parameters:<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;p.grad = <span class="hljs-number">0.</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Todos los optimizadores se implementan heredando de la clase base </font></font><code>Optimizer</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Una clase que describe una optimizaci√≥n particular debe proporcionar una implementaci√≥n del m√©todo </font></font><code>step()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Este m√©todo actualiza los par√°metros del modelo utilizando sus derivadas parciales calculadas en relaci√≥n con el valor optimizado de la funci√≥n de p√©rdida. </font><font style="vertical-align: inherit;">Se proporciona un enlace a varios par√°metros del modelo en la funci√≥n </font></font><code>__init__()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Tenga en cuenta que la funcionalidad universal para restablecer los valores de gradiente se implementa en la propia clase base. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ahora, para comprender mejor todo esto, considere un ejemplo espec√≠fico: la implementaci√≥n del algoritmo de descenso de gradiente estoc√°stico (SGD) con soporte para ajustar el impulso y reducir los pesos:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">SGD</span>(<span class="hljs-params">Optimizer</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,parameters,lr=<span class="hljs-number">.001</span>,weight_decay=<span class="hljs-number">0.0</span>,momentum = <span class="hljs-number">.9</span></span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;super().__init__(parameters)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.lr &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = lr<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.weight_decay = weight_decay<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.momentum &nbsp; &nbsp; = momentum<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.velocity &nbsp; &nbsp; = []<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> parameters:<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.velocity.append(np.zeros_like(p.grad))<font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">step</span>(<span class="hljs-params">self</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> p,v <span class="hljs-keyword">in</span> zip(self.parameters,self.velocity):<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v = self.momentum*v+p.grad+self.weight_decay*p.data<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;p.data=p.data-self.lr*v</code></pre><br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La soluci√≥n al problema real.</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ahora tenemos todo lo necesario para entrenar el modelo de red neuronal (profunda) utilizando nuestra biblioteca. </font><font style="vertical-align: inherit;">Para esto necesitamos las siguientes entidades:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Modelo: gr√°fico de c√°lculo.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Datos y valor objetivo: datos para la formaci√≥n de redes.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Funci√≥n de p√©rdida: sustituto del objetivo de optimizaci√≥n.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Optimizer: un mecanismo para actualizar los par√°metros del modelo.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El siguiente pseudoc√≥digo describe un ciclo de prueba t√≠pico:</font></font><br>
<br>
<pre><code class="python hljs">model <span class="hljs-comment"># </span>
data,target <span class="hljs-comment"># </span>
loss_fn <span class="hljs-comment"># </span>
optim <span class="hljs-comment">#,         </span>
Repeat:<span class="hljs-comment">#   ,    ,     </span>
&nbsp;&nbsp;&nbsp;optim.zeroGrad() <span class="hljs-comment">#    </span>
&nbsp;&nbsp;&nbsp;output = model.forward(data) <span class="hljs-comment">#   </span>
&nbsp;&nbsp;&nbsp;loss &nbsp; = loss_fn(output,target) <span class="hljs-comment"># </span>
&nbsp;&nbsp;&nbsp;grad &nbsp; = loss.backward() <span class="hljs-comment">#      </span>
&nbsp;&nbsp;&nbsp;model.backward(grad) <span class="hljs-comment">#    </span>
&nbsp;&nbsp;&nbsp;optim.step() <span class="hljs-comment">#  </span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Aunque esto no es necesario en la biblioteca de aprendizaje profundo, puede ser √∫til incluir la funcionalidad anterior en una clase separada. </font><font style="vertical-align: inherit;">Esto nos permitir√° no repetir los mismos pasos al aprender nuevos modelos (esta idea corresponde a la filosof√≠a de abstracciones de alto nivel de marcos como </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Keras</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ). </font><font style="vertical-align: inherit;">Para lograr esto, declare una clase </font></font><code>Model</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Model</span>():</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.computation_graph = []<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.parameters&nbsp; &nbsp; &nbsp; &nbsp; = []<font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">add</span>(<span class="hljs-params">self,layer</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.computation_graph.append(layer)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.parameters+=layer.getParams()<font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__innitializeNetwork</span>(<span class="hljs-params">self</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> self.computation_graph:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">if</span> f.type==<span class="hljs-string">'linear'</span>:<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;weights,bias = f.getParams()<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;weights.data = <span class="hljs-number">.01</span>*np.random.randn(weights.data.shape[<span class="hljs-number">0</span>],weights.data.shape[<span class="hljs-number">1</span>])<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bias.data&nbsp; &nbsp; = <span class="hljs-number">0.</span><font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fit</span>(<span class="hljs-params">self,data,target,batch_size,num_epochs,optimizer,loss_fn</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loss_history = []<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.__innitializeNetwork()<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;data_gen = DataGenerator(data,target,batch_size)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;itr = <span class="hljs-number">0</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(num_epochs):
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> X,Y <span class="hljs-keyword">in</span> data_gen:<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;optimizer.zeroGrad()<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> self.computation_graph: X=f.forward(X)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loss = loss_fn.forward(X,Y)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;grad = loss_fn.backward()<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> self.computation_graph[::<span class="hljs-number">-1</span>]: grad = f.backward(grad)&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loss_history+=[loss]<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(<span class="hljs-string">"Loss at epoch = {} and iteration = {}: {}"</span>.format(epoch,itr,loss_history[<span class="hljs-number">-1</span>]))<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;itr+=<span class="hljs-number">1</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;optimizer.step()<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span> loss_history<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span>(<span class="hljs-params">self,data</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;X = data<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> self.computation_graph: X = f.forward(X)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span> X</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Esta clase incluye la siguiente funcionalidad:</font></font><br>
<br>
<ul>
<li>  :  <code>add()</code>   ,    .        <code>computation_graph</code>.</li>
<li> : ,   ,       ,    .</li>
<li> :    <code>fit()</code>       .       ,    .</li>
<li>  :  <code>predict()</code>   ,       ,   .</li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Como esta clase no es el componente b√°sico de los sistemas de aprendizaje profundo, la implement√© en un m√≥dulo separado </font></font><code>utilities.py</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Tenga en cuenta que el m√©todo </font></font><code>fit()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">utiliza una clase </font></font><code>DataGenerator</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">cuya implementaci√≥n est√° en el mismo m√≥dulo. </font><font style="vertical-align: inherit;">Esta clase es solo un contenedor para los datos de entrenamiento y genera mini paquetes para cada iteraci√≥n de entrenamiento.</font></font><br>
<br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Entrenamiento modelo</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ahora considere el √∫ltimo fragmento de c√≥digo en el que se entrena el modelo de red neuronal utilizando la biblioteca descrita anteriormente. </font><font style="vertical-align: inherit;">Voy a entrenar una red multicapa en datos dispuestos en espiral. </font><font style="vertical-align: inherit;">Me impuls√≥ </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">esta</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> publicaci√≥n. </font><font style="vertical-align: inherit;">El c√≥digo para generar estos datos y para visualizarlos se puede encontrar en el archivo </font></font><code>utilities.py</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/895/eb0/856/895eb085662368f9c1d171153a301fb0.png"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Datos con tres clases dispuestas en espiral&nbsp;</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
La figura anterior muestra la visualizaci√≥n de los datos en los que entrenaremos el modelo. </font><font style="vertical-align: inherit;">Estos datos son no linealmente separables. </font><font style="vertical-align: inherit;">Podemos esperar que una red con una capa oculta pueda encontrar correctamente l√≠mites de decisi√≥n no lineales. </font><font style="vertical-align: inherit;">Si re√∫ne todo lo que mencionamos, obtendr√° el siguiente fragmento de c√≥digo que le permite entrenar el modelo:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">import</span> dl_numpy <span class="hljs-keyword">as</span> DL
<span class="hljs-keyword">import</span> utilities<font></font>
<font></font>
batch_size&nbsp; &nbsp; &nbsp; &nbsp; = <span class="hljs-number">20</span>
num_epochs&nbsp; &nbsp; &nbsp; &nbsp; = <span class="hljs-number">200</span>
samples_per_class = <span class="hljs-number">100</span>
num_classes &nbsp; &nbsp; &nbsp; = <span class="hljs-number">3</span>
hidden_units&nbsp; &nbsp; &nbsp; = <span class="hljs-number">100</span><font></font>
data,target &nbsp; &nbsp; &nbsp; = utilities.genSpiralData(samples_per_class,num_classes)<font></font>
model &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = utilities.Model()<font></font>
model.add(DL.Linear(<span class="hljs-number">2</span>,hidden_units))<font></font>
model.add(DL.ReLU())<font></font>
model.add(DL.Linear(hidden_units,num_classes))<font></font>
optim &nbsp; = DL.SGD(model.parameters,lr=<span class="hljs-number">1.0</span>,weight_decay=<span class="hljs-number">0.001</span>,momentum=<span class="hljs-number">.9</span>)<font></font>
loss_fn = DL.SoftmaxWithLoss()<font></font>
model.fit(data,target,batch_size,num_epochs,optim,loss_fn)<font></font>
predicted_labels = np.argmax(model.predict(data),axis=<span class="hljs-number">1</span>)<font></font>
accuracy &nbsp; &nbsp; &nbsp; &nbsp; = np.sum(predicted_labels==target)/len(target)<font></font>
print(<span class="hljs-string">"Model Accuracy = {}"</span>.format(accuracy))<font></font>
utilities.plot2DDataWithDecisionBoundary(data,target,model)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La siguiente imagen muestra los mismos datos y los l√≠mites decisivos del modelo entrenado.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/502/b0b/84f/502b0b84fee2c960c918494e8d63e33a.png"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Datos y l√≠mites de decisi√≥n del modelo entrenado</font></font></font></i><br>
 <br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Resumen</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dada la creciente complejidad de los modelos de aprendizaje profundo, existe una tendencia a aumentar las capacidades de las bibliotecas respectivas y a aumentar la cantidad de c√≥digo necesario para implementar estas capacidades. Pero la funcionalidad m√°s b√°sica de tales bibliotecas todav√≠a se puede implementar en una forma relativamente compacta. Aunque la biblioteca que creamos se puede utilizar para la capacitaci√≥n integral de redes simples, todav√≠a es, en muchos sentidos, limitada. Estamos hablando de limitaciones en el campo de las capacidades que permiten que los marcos de aprendizaje profundo se utilicen en √°reas como la visi√≥n artificial, el reconocimiento de voz y texto. Esto, por supuesto, las posibilidades de tales marcos no son limitadas. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Creo que todos pueden bifurcar el </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">proyecto</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, cuyo c√≥digo examinamos aqu√≠ y, como ejercicio, introducimos en √©l lo que les gustar√≠a ver en √©l. </font><font style="vertical-align: inherit;">Aqu√≠ hay algunos mecanismos que puede intentar implementar usted mismo:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Operadores: convoluci√≥n, submuestreo.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Optimizadores: Adam, RMSProp.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Reguladores: BatchNorm, DropOut.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Espero que este material le haya permitido al menos ver de reojo lo que est√° sucediendo en las entra√±as de las bibliotecas para el aprendizaje profundo. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">¬°Queridos lectores! </font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">¬øQu√© bibliotecas de aprendizaje profundo utilizas?</font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><img src="https://habrastorage.org/files/1ba/550/d25/1ba550d25e8846ce8805de564da6aa63.png"></a></div>
      
    </div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es486686/">https://habr.com/ru/post/es486686/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es486674/index.html">El resumen de materiales frescos del mundo del front-end para la √∫ltima semana No. 400 (27 de enero - 2 de febrero de 2020)</a></li>
<li><a href="../es486676/index.html">Inevitabilidad de la penetraci√≥n de FPGA en centros de datos</a></li>
<li><a href="../es486678/index.html">Cuarzo en ASP.NET Core</a></li>
<li><a href="../es486680/index.html">ML, VR y Robots (y un poco de nube)</a></li>
<li><a href="../es486682/index.html">Docker Compose: Simplifique usando Makefile</a></li>
<li><a href="../es486688/index.html">Node.js, Tor, Titiritero y Cheerio: raspado web an√≥nimo</a></li>
<li><a href="../es486690/index.html">5 consejos para escribir funciones de flecha de calidad</a></li>
<li><a href="../es486692/index.html">Funciones de la consola Chrome que quiz√°s nunca hayas usado</a></li>
<li><a href="../es486694/index.html">Noticias del mundo de OpenStreetMap No. 496 (14/01/2020/20/01/2020)</a></li>
<li><a href="../es486702/index.html">Eventos digitales en Mosc√∫ del 3 al 9 de febrero.</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>