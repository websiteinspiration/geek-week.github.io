<!doctype html>
<html class="no-js" lang="pt">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👩🏾‍🤝‍👨🏻 👨🏼‍⚕️ 👩🏾‍🤝‍👨🏿 Reconhecimento de Fala: Um Curso Introdutório Muito Curto ✋🏿 🤱🏽 👩🏻‍⚕️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="É quase impossível dizer ao leigo o mais simples possível sobre o trabalho de reconhecimento de fala por computador e convertê-lo em texto. Nem uma ún...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Reconhecimento de Fala: Um Curso Introdutório Muito Curto</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/toshibarus/blog/490732/"><img src="https://habrastorage.org/webt/tz/sh/ll/tzshllxzf2iddwai7sredy3edie.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
É quase impossível dizer ao leigo o mais simples possível sobre o trabalho de reconhecimento de fala por computador e convertê-lo em texto. </font><font style="vertical-align: inherit;">Nem uma única história é completa sem fórmulas complexas e termos matemáticos. </font><font style="vertical-align: inherit;">Vamos tentar explicar da maneira mais clara e simplista possível o modo como o smartphone entende a fala, quando os carros aprenderem a reconhecer a voz humana e em que áreas inesperadas essa tecnologia é usada. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Aviso necessário: se você é um desenvolvedor ou, especialmente, um matemático, é improvável que você aprenda algo novo no post e até se queixe da natureza científica insuficiente do material. </font><font style="vertical-align: inherit;">Nosso objetivo é apresentar aos leitores não iniciados as tecnologias da fala da maneira mais simples e contar como e por que a Toshiba adotou a criação da sua IA de voz.</font></font><br>
<a name="habracut"></a><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Marcos importantes na história do reconhecimento de fala</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A história do reconhecimento da fala humana por máquinas eletrônicas começou um pouco mais cedo do que se costuma pensar: na maioria dos casos, é uma contagem regressiva desde 1952, mas, na verdade, um dos primeiros dispositivos que responderam aos comandos de voz foi o robô Televox, sobre o </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">qual já escrevemos</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Criado em 1927 nos EUA, o robô Herbert Televox era um dispositivo simples no qual vários relés reagiam a sons de diferentes frequências. O robô tinha três diapasões, cada um dos quais era responsável por seu tom. Dependendo de qual diapasão funcionou, um ou outro relé foi ativado.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/90/pq/4i/90pq4ixpys3c8uevjp-ovvydfny.jpeg" alt="imagem"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">De fato, todo o "preenchimento" da Televox, incluindo o sistema de reconhecimento de comando, estava localizado em um rack na área do corpo do "robô". Era impossível fechar a tampa; caso contrário, os diapasões não poderiam "ouvir" corretamente os sons. Fonte: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Acme Telepictures / Wikimedia</font></font></a></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Era possível se comunicar com a Televox como sinais separados com um apito e em breves sinais verbais - seus diapasões também eram dispostos em uma sequência de sons. O criador do robô, Roy Wensley, até fez uma demonstração fantástica para aqueles tempos, dizendo o comando “Gergelim, aberto”, através do qual a Televox ligou o relé responsável por abrir a porta. Sem tecnologia digital, redes neurais, IA e aprendizado de máquina - apenas tecnologia analógica!</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A próxima invenção chave que abriu o caminho para o verdadeiro reconhecimento da fala humana foi a máquina Audrey, desenvolvida em 1952 no Bell Labs Innovation Forge. O enorme Audrey consumia muita eletricidade e era do tamanho de um bom gabinete, mas toda a sua funcionalidade se resumia ao reconhecimento de números falados de zero a nove. Apenas dez palavras, sim, mas não vamos esquecer que Audrey era uma máquina analógica. </font></font><br>
<img src="https://habrastorage.org/webt/vd/1q/eb/vd1qebrer6czotgwty3tdyfp15i.png" alt="imagem"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Infelizmente, a história não preservou fotografias públicas de Audrey, existe apenas um conceito. Simples no papel, difícil de traduzir - de acordo com as memórias dos contemporâneos, os componentes de Audrey ocupavam um gabinete inteiro. Fonte: Bell Labs</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Funcionou assim: o locutor falou números no microfone, fazendo intervalos de pelo menos 350 ms entre as palavras, Audrey converteu os sons que ouvia em sinais elétricos e os comparou com amostras gravadas na memória analógica. De acordo com os resultados da comparação, o carro destacou o número no painel. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Foi um avanço, mas não houve benefício real de Audrey - a máquina reconheceu a voz de seu criador com uma precisão de 97%, enquanto outros falantes especialmente treinados receberam uma precisão de 70-80%. Estranhos que entraram em contato com Audrey, por mais que tentassem, viram seu número no placar em apenas 50% dos casos.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Apesar dos resultados revolucionários da época, Audrey não encontrou e não encontrou aplicação prática. </font><font style="vertical-align: inherit;">Supunha-se que o sistema pudesse ser adaptado em vez das operadoras de telefonia, mas, no entanto, os serviços humanos eram mais convenientes, mais rápidos e muito mais confiáveis ​​do que Audrey.</font></font><br>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/rQco1sa9AwU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Apresentação semelhante à Audrey, apenas máquinas muito menores - IBM Shoebox. </font><font style="vertical-align: inherit;">A velocidade da caixa de sapatos é claramente visível. </font><font style="vertical-align: inherit;">A máquina também pode executar operações matemáticas simples de adição e subtração</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
No início dos anos 60, o trabalho de criação de máquinas para reconhecimento de fala foi realizado no Japão, Reino Unido, EUA e até na URSS, onde eles inventaram um algoritmo muito importante para a transformação dinâmica da linha do tempo (DTW), com a ajuda da qual foi possível construir um sistema com cerca de 200 palavras. Mas todos os desenvolvimentos foram semelhantes entre si, e o princípio do reconhecimento tornou-se uma desvantagem comum: as palavras foram percebidas como impressões digitais sonoras integrais e, em seguida, foram verificadas na base de amostras (dicionário). Quaisquer mudanças na velocidade, timbre e clareza da pronúncia das palavras afetaram significativamente a qualidade do reconhecimento. Os cientistas têm uma nova tarefa: ensinar a máquina a ouvir sons individuais, fonemas ou sílabas e depois criar palavras a partir deles. Essa abordagem tornaria possível nivelar o efeito da alteração do alto-falante quando, dependendo do alto-falante, o nível de reconhecimento variasse acentuadamente.</font></font><br>
<br>
<i> —     ,           . ,   « »  «»       «».   «»   « »  « »      «»,    —  «».  ,  ,   . </i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Em 1971, a Agência de Projetos de Pesquisa Avançada do Departamento de Defesa (DARPA) lançou um programa de cinco anos com um orçamento de US $ 15 milhões, encarregado de criar um sistema de reconhecimento que tivesse pelo menos mil palavras. Em 1976, a Universidade Carnegie Mellon introduziu o Harpy, capaz de operar um dicionário de 1011 palavras. Harpy não comparou as palavras completamente ouvidas com as amostras, mas as dividiu em alofones (uma amostra do som de um fonema, dependendo das letras ao seu redor). Esse foi outro sucesso, confirmando que o futuro está no reconhecimento de fonemas individuais, e não em palavras inteiras. No entanto, entre as desvantagens da Harpy, havia um nível extremamente baixo de reconhecimento correto de alofones (pronúncia de fonemas) - cerca de 47%. Com um erro tão alto, a parcela de erros aumentou após o volume do dicionário.</font></font><br>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/32KKg3aP3Vw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Descrição de como a Harpy funciona. O vídeo do programa não sobreviveu.</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
A experiência de Harpy mostrou que a criação de dicionários de impressões digitais holísticas do som é inútil - apenas aumenta o tempo de reconhecimento e reduz drasticamente a precisão, de modo que pesquisadores de todo o mundo adotaram um caminho diferente - o reconhecimento de fonemas. Em meados da década de 1980, a máquina IBM Tangora aprendeu a entender a fala de qualquer falante com sotaque, dialeto e pronúncia, exigindo apenas um treinamento de 20 minutos, durante o qual um banco de dados de amostras de fonemas e alofones foi acumulado. O uso do </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">modelo oculto de Markov</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> também aumentou o vocabulário do IBM Tangora para impressionantes 20.000 palavras - 20 vezes mais do que Harpy, e já é comparável ao vocabulário do adolescente.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Todos os sistemas de reconhecimento de fala da década de 1950 a meados da década de 90 não sabiam ler a linguagem natural falada de uma pessoa - eles tinham que pronunciar as palavras separadamente, fazendo uma pausa entre elas. Um evento verdadeiramente revolucionário foi a introdução do modelo oculto de Markov, desenvolvido na década de 1980 - um modelo estatístico que construiu suposições precisas sobre elementos desconhecidos com base nos conhecidos. Simplificando, com apenas alguns fonemas reconhecidos em uma palavra, o modelo oculto de Markov seleciona com muita precisão os fonemas ausentes, aumentando significativamente a precisão do reconhecimento de fala.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Em 1996, o primeiro programa comercial apareceu, capaz de distinguir não palavras individuais, mas um fluxo contínuo de fala natural - IBM MedSpeak / Radiology. A IBM era um produto especializado usado na medicina para descrever em poucas palavras os resultados de um raio-x emitido por um médico durante o estudo. Aqui, o poder dos computadores finalmente se tornou suficiente para reconhecer palavras individuais "on the fly". Além disso, os algoritmos se tornaram mais perfeitos, o reconhecimento correto de micro-pausas entre as palavras faladas apareceu.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O primeiro mecanismo universal para o reconhecimento da fala natural foi o programa Dragon NaturallySpeaking, em 1997. Ao trabalhar com ela, o locutor (ou seja, o usuário) não precisou receber treinamento ou operar com um vocabulário específico, como no caso do MedSpeak, qualquer pessoa, mesmo uma criança, poderia trabalhar com o NaturallySpeaking, o programa não estabeleceu nenhuma regra de pronúncia. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/xj/m6/w-/xjm6w-kpgryltox7wquuvpl8db4.png" alt="imagem"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Apesar da singularidade do Dragon NaturallySpeaking, os navegadores de TI não demonstraram muito entusiasmo por reconhecer a fala natural. Entre as deficiências, foram observados erros de reconhecimento e processamento incorreto de comandos endereçados ao próprio programa. Fonte: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">itWeek</font></font></a></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Vale ressaltar que o mecanismo de reconhecimento estava pronto na década de 1980, mas devido à falta de energia do computador, o desenvolvimento da Dragon Systems (agora de propriedade da Nuance Communications) não teve tempo para determinar os espaços entre as palavras em tempo real, o que é necessário para o reconhecimento da fala natural. </font><font style="vertical-align: inherit;">Sem isso, as palavras "enquanto estavam sendo tratadas", por exemplo, podiam ser ouvidas pelo computador como "aleijadas". </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
À frente estava a crescente popularidade de sistemas de reconhecimento de voz, redes neurais, o surgimento da pesquisa por voz do Google em dispositivos móveis e, finalmente, o assistente de voz Siri, não apenas convertendo a fala em texto, mas também respondendo adequadamente às consultas construídas de maneira natural.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Como ouvir o que foi dito e pensar no que era inaudível?</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Atualmente, a melhor ferramenta para criar um mecanismo de reconhecimento de fala é a rede neural recorrente (RNN), na qual são construídos todos os serviços modernos para reconhecimento de voz, música, imagens, rostos, objetos e texto. A RNN permite que você entenda as palavras com extrema precisão, além de prever a palavra mais provável no contexto do contexto, se não for reconhecida. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A classificação temporal do modelo de rede neural (CTC) seleciona fonemas individuais no fluxo de áudio gravado (palavra, frase) e os organiza na ordem em que foram pronunciados. Após análise repetida, o CTC identifica claramente certos fonemas, e sua gravação de texto é comparada com o banco de dados de palavras na rede neural e depois se transforma em uma palavra reconhecida.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As redes neurais são assim chamadas porque o princípio de seu trabalho é semelhante ao trabalho do cérebro humano. O treinamento em redes neurais é muito semelhante ao treinamento humano. Por exemplo, para que uma criança muito pequena aprenda a reconhecer carros e distingui-los das motocicletas, é necessário chamar várias vezes sua atenção para vários carros e pronunciar a palavra correspondente: este é grande e vermelho - o carro e esse preto baixo - o carro, mas isso e estas são motocicletas. Em algum momento, a criança descobrirá padrões e sinais comuns para carros diferentes e aprenderá a reconhecer corretamente onde está o carro, onde o jipe, onde a motocicleta e o ATV, mesmo que de passagem os veja em um cartaz publicitário na rua. Da mesma forma, a rede neural precisa ser treinada com base em exemplos - para fazer centenas e milhares de variantes de pronúncia de cada palavra, letra, fonema "aprender".</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Uma rede neural recorrente para reconhecimento de fala é boa porque, após um longo treinamento na base de várias pronúncias, aprende com alta precisão a distinguir fonemas e formar palavras a partir deles, independentemente da qualidade e natureza da pronúncia. E até “pense” com alta precisão, dentro do contexto da palavra, palavras que não puderam ser reconhecidas sem ambiguidade devido a ruídos de fundo ou pronúncia imprecisa. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Mas há uma nuance nas previsões da RNN - uma rede neural recorrente pode "pensar" em uma palavra que falta apenas confiando no contexto mais próximo de cerca de cinco palavras. Fora deste espaço, a análise não será realizada. E às vezes ele é tão necessário! Por exemplo, para reconhecimento, pronunciamos a frase “O grande poeta russo Alexander Sergeyevich </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pushkin</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">”, Em que a palavra“ Pushkin ”(especialmente em itálico) foi dita tão inaudível que a IA não a reconheceu com precisão. Mas uma rede neural recorrente, baseada na experiência adquirida durante o treinamento, pode sugerir que a palavra "Pushkin" seja encontrada com mais frequência ao lado das palavras "russo", "poeta", "Alexander" e "Sergeyevich". Essa é uma tarefa bastante simples para uma RNN treinada em textos em russo, porque um contexto muito específico nos permite fazer suposições com a maior precisão.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
E se o contexto é vago? Tomemos outro texto em que uma palavra não pode ser reconhecida: “Nosso tudo, Alexander Sergeyevich Pushkin, morreu tragicamente no auge de sua vida após um duelo com Dantes. O Festival de Teatro Pushkin recebeu o nome do poeta. ” Se você remover a palavra "Pushkinsky", a RNN simplesmente não consegue adivinhar, com base no contexto da proposta, porque apenas menciona um festival de teatro e uma referência ao nome de um poeta desconhecido - existem inúmeras opções possíveis! </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
É aqui que entra em cena a arquitetura de memória de longo prazo (LSTM) para redes neurais recorrentes, criada em 1997 (um </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;">artigo detalhado sobre LSTM</font></a><font style="vertical-align: inherit;"> ) </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">) Foi desenvolvido especialmente para adicionar a capacidade da RNN de levar em consideração o contexto remoto do evento que está sendo processado - os resultados da solução de problemas anteriores (ou seja, reconhecimento de palavras) passam por todo o processo de reconhecimento, independentemente do tempo do monólogo, e são levados em consideração em cada caso de dúvida. Além disso, a distância de remoção quase não afeta a eficiência da arquitetura. Com a ajuda do LSTM, se necessário, uma rede de palavras levará em conta toda a experiência disponível no âmbito da tarefa: em nosso exemplo, a RNN analisará a frase anterior, descobrirá que Pushkin e Dantes foram mencionados anteriormente, portanto, “Pelo nome do poeta” provavelmente aponta para um deles. Como não há evidências da existência do Festival de Teatro de Dantes,estamos falando de Pushkinsky (tanto mais que a impressão sonora de uma palavra não reconhecida é muito semelhante) - esse festival estava na base do treinamento da rede neural.</font></font><br>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/P325_hrGsDI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Confissão de um assistente de voz." </font><font style="vertical-align: inherit;">Quando uma rede neural bem treinada entra em ação, um assistente de voz pode descobrir exatamente o que precisa ser feito com "chinelos verdes"</font></font></i><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Como o reconhecimento de fala faz do mundo um lugar melhor?</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Em cada caso, o aplicativo é diferente - ajuda alguém a se comunicar com os gadgets e, de </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">acordo com</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> mais da metade dos usuários de smartphones da </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;">PricewaterhouseCooper,</font></a><font style="vertical-align: inherit;"> dá comandos de voz aos dispositivos - entre adultos (25 a 49 anos), a porcentagem daqueles que usam constantemente interfaces de voz, mesmo superior ao dos jovens (18-25) - 65% contra 59%. </font><font style="vertical-align: inherit;">E na Rússia, pelo menos uma vez, pelo menos 71% da população se comunicou com Siri, Google Assitant ou Alice. </font><font style="vertical-align: inherit;">45 milhões de russos se comunicam constantemente com o Yandex de Alice e o Yandex.Maps / Yandex.Navigator representam apenas 30% dos pedidos.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O reconhecimento de fala realmente ajuda alguém no trabalho - por exemplo, como dissemos acima, para médicos: na medicina desde 1996 (quando o IBM MedSpeak foi lançado), o reconhecimento é usado para registrar anamnese e estudar imagens - um médico pode continuar trabalhando sem se distrair com gravações em computador ou cartão de papel. A propósito, o trabalho sobre ditado em medicina é realizado não apenas no Ocidente - na Rússia, há um programa Voice2Med do "Center for Speech Technologies".</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Existem outros exemplos, incluindo o nosso. A organização de um negócio da Toshiba envolve a inclusão total, isto é, igualdade de direitos e oportunidades para pessoas com várias condições de saúde, inclusive para funcionários com deficiência auditiva. Temos um programa corporativo chamado Universal Design Advisor System, no qual pessoas com vários tipos de deficiências participam do desenvolvimento dos produtos Toshiba, fazendo sugestões para melhorar sua conveniência para pessoas com deficiências - ou seja, não assumimos como podemos melhorar, mas operamos com experiência real e avaliações de funcionários.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Alguns anos atrás, na sede da Toshiba no Japão, enfrentamos uma tarefa muito interessante, exigindo o desenvolvimento de um novo sistema de reconhecimento de fala. Durante a operação do Universal Design Advisor System, recebemos um insight importante: os funcionários com deficiência auditiva desejam participar de discussões em reuniões e palestras em tempo real e não se limitam a ler a transcrição processada horas ou dias depois. Iniciar o reconhecimento de voz por meio de um smartphone nesses casos fornece um resultado muito fraco, então os especialistas da Toshiba tiveram que começar a desenvolver um sistema de reconhecimento especializado. E, é claro, imediatamente tivemos problemas.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A conversa difere enormemente da fala escrita - não falamos da maneira como escrevemos cartas, e uma conversa real traduzida em texto parece muito desleixada e até ilegível. Ou seja, mesmo se convertermos conversas no plano da manhã em texto com alta precisão, obteremos um hash incoerente repleto de parasitas verbais, interjeições e "aaa", "uh" e "mmm". Para se livrar da transcrição de sons, palavras e expressões de emoções desnecessárias no texto, decidimos desenvolver uma IA capaz de reconhecer com precisão máxima nem sempre os elementos necessários do discurso coloquial, incluindo a coloração emocional de algumas palavras (por exemplo, "sim, bem" pode parecer ceticismo ou como surpresa sincera, e estes são significados literalmente opostos).</font></font><br>
<br>
<img src="https://habrastorage.org/webt/6z/vv/od/6zvvodwnihcvdqdqfv4uuprbtb4.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Parece um laptop com um conjunto de periféricos para reconhecimento de voz usando o Toshiba AI (à esquerda) e um aplicativo com os resultados para dispositivos finais (à direita). Fonte: Toshiba</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
LSTM foi útil aqui, sem o qual a precisão do reconhecimento era insuficiente para que o texto recebido fosse lido e entendido sem esforço. Além disso, o LSTM foi útil não apenas para uma previsão mais precisa das palavras no contexto, mas também para o processamento correto de pausas no meio de frases e interjeições-parasitas - por isso ensinamos à rede neural esses parasitas e pausas que são naturais para a fala coloquial.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Isso significa que agora a rede neural pode remover interjeições de transcrições? </font><font style="vertical-align: inherit;">Sim, pode, mas isso não é necessário. </font><font style="vertical-align: inherit;">O fato é que (outra percepção recebida) as pessoas com deficiência auditiva são guiadas, inclusive pelos movimentos dos lábios do falante. </font><font style="vertical-align: inherit;">Se os lábios se moverem, mas o texto correspondente a esses movimentos não aparecer na tela, há uma sensação de que o sistema de reconhecimento perdeu parte da conversa. </font><font style="vertical-align: inherit;">Ou seja, para alguém que não pode ouvir, é importante obter o máximo de informações possível sobre a conversa, incluindo pausas infelizes e mejometia. </font><font style="vertical-align: inherit;">Portanto, o mecanismo da Toshiba deixa esses elementos na transcrição, mas em tempo real diminui o brilho das letras, deixando claro que esses são detalhes opcionais para a compreensão do texto.</font></font><br>
<br>
<div class="oembed"><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">https://www.toshiba-clip.com/en/detail/7655</font></font></a></div><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">É assim que o resultado do reconhecimento em tempo real aparece no dispositivo cliente. </font><font style="vertical-align: inherit;">As partes do monólogo que não são significativas são pintadas de cinza.Agora, o</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Toshiba AI trabalha com fala em inglês, japonês e chinês, e até mesmo a tradução entre idiomas em tempo real é possível. </font><font style="vertical-align: inherit;">Não é necessário usá-lo para abreviar rapidamente - a IA pode ser adaptada para trabalhar com assistentes de voz, que finalmente aprendem a perceber adequadamente interjeições, pausas e gaguejos quando uma pessoa pronuncia um comando. </font><font style="vertical-align: inherit;">Em março de 2019, o sistema foi usado com sucesso para adicionar legendas à transmissão da Convenção Nacional da IPSJ no Japão. </font><font style="vertical-align: inherit;">Em um futuro próximo - a transformação da Toshiba AI em um serviço público e experiências com a implementação do reconhecimento de voz na produção.</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt490720/index.html">Motor! ou O que é física de jogo</a></li>
<li><a href="../pt490722/index.html">Férias de gênero em TI. Como observar</a></li>
<li><a href="../pt490726/index.html">Autenticação em equipamentos de rede por SSH usando chaves públicas</a></li>
<li><a href="../pt490728/index.html">Semana 10 de segurança: Conferência RSA e conscientização sobre segurança cibernética</a></li>
<li><a href="../pt490730/index.html">Raiz de confiança Intel x86: perda de confiança</a></li>
<li><a href="../pt490734/index.html">Ovos de Páscoa nos mapas topográficos da Suíça</a></li>
<li><a href="../pt490736/index.html">9 ferramentas claras para aprender e aprimorar o vocabulário em inglês</a></li>
<li><a href="../pt490738/index.html">Princípio de substituição de Lisk</a></li>
<li><a href="../pt490740/index.html">Deserte *** *** não é apenas randomização</a></li>
<li><a href="../pt490742/index.html">Uma nova era na robótica começou</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>