<!doctype html>
<html class="no-js" lang="ja">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🧑🏾‍🤝‍🧑🏼 🍲 🤛🏽 Ceph-「膝の上」から「本番」まで 👩🏽‍🤝‍👨🏾 👨🏿‍🤝‍👨🏽 🧛🏾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="CEPHの選択。パート1
 

5つのラック、10の光スイッチ、構成済みのBGP、すべての色とサイズの数十のSSDとSASディスクの束があり、proxmoxと独自のS3ストレージにすべての静的を入れたいという要望がありました。これがすべて仮想化に必要だったわけではありませんが、オープンソースの使用を...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Ceph-「膝の上」から「本番」まで</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/456446/"><h1 id="vybor-ceph-chast-1"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">CEPHの選択。</font><font style="vertical-align: inherit;">パート1</font></font></h1><br>
<p><em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">5つのラック、10の光スイッチ、構成済みのBGP、すべての色とサイズの数十のSSDとSASディスクの束があり、proxmoxと独自のS3ストレージにすべての静的を入れたいという要望がありました。これがすべて仮想化に必要だったわけではありませんが、オープンソースの使用を開始したら、趣味で最後まで進んでください。気になったのはBGPだけだった。世界には、内部BGPルーティングほど無力で無責任で不道徳な人はいません。そして、すぐにそれに突入することを知っていました。</font></font></em></p><br>
<p><img src="https://habrastorage.org/getpro/habr/post_images/09e/a36/178/09ea3617814a9598a6aa9784abc14a76.jpg"></p><br>
<p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">タスクは当たり前でした-CEPHがありましたが、うまく機能しませんでした。</font><font style="vertical-align: inherit;">「よく」する必要がありました。</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
私が入手したクラスターは、異種混合で、ホイップアップされており、実際には調整されていません。</font><font style="vertical-align: inherit;">異なるノードの2つのグループで構成され、1つの共通グリッドがクラスターとパブリックネットワークの両方の役割を果たす。</font><font style="vertical-align: inherit;">ノードには4種類のディスクが詰め込まれました。2種類のSSDが2つの個別の配置ルールに組み立てられ、2種類の異なるサイズのHDDが3つ目のグループに組み立てられました。</font><font style="vertical-align: inherit;">さまざまなサイズの問題は、さまざまなOSDの重みによって解決されました。</font></font></p><br>
<p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">-設定自体は二つの部分に分かれていた</font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">オペレーティングシステムをチューニング</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">し、</font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">CEPH自体チューニング</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">とその設定を。</font></font></p><a name="habracut"></a><br>
<h2 id="prokachka-os"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">レベリングOS</font></font></h2><br>
<h3 id="network"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">通信網</font></font></h3><br>
<p> latency    ,    .   — ,        ,          .      CRUSH map        ,    .</p><br>
<p>       ,       . </p><br>
<p>     .    :</p><br>
<p> :</p><br>
<div class="spoiler"><b class="spoiler_title">ethtool -l ens1f1</b><div class="spoiler_text"><pre><code class="plaintext hljs">root@ceph01:~# ethtool -l ens1f1<font></font>
Channel parameters for ens1f1:<font></font>
Pre-set maximums:<font></font>
RX:     0<font></font>
TX:     0<font></font>
Other:      1<font></font>
Combined:   63<font></font>
Current hardware settings:<font></font>
RX:     0<font></font>
TX:     0<font></font>
Other:      1<font></font>
Combined:   1<font></font>
root@ceph01:~# ethtool -g ens1f1<font></font>
Ring parameters for ens1f1:<font></font>
Pre-set maximums:<font></font>
RX:     4096<font></font>
RX Mini:    0<font></font>
RX Jumbo:   0<font></font>
TX:     4096<font></font>
Current hardware settings:<font></font>
RX:     256<font></font>
RX Mini:    0<font></font>
RX Jumbo:   0<font></font>
TX:     256<font></font>
root@ceph01:~# ethtool -l ens1f1<font></font>
Channel parameters for ens1f1:<font></font>
Pre-set maximums:<font></font>
RX:     0<font></font>
TX:     0<font></font>
Other:      1<font></font>
Combined:   63<font></font>
Current hardware settings:<font></font>
RX:     0<font></font>
TX:     0<font></font>
Other:      1<font></font>
Combined:   1</code></pre></div></div><br>
<p>,  current    maximums. :</p><br>
<pre><code class="plaintext hljs">root@ceph01:~#ethtool -G ens1f0 rx 4096<font></font>
root@ceph01:~#ethtool -G ens1f0 tx 4096<font></font>
root@ceph01:~#ethtool -L ens1f0 combined 63</code></pre><br>
<p>  </p><br>
<p><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=">https://blog.packagecloud.io/eng/2017/02/06/monitoring-tuning-linux-networking-stack-sending-data/</a></p><br>
<p>    <strong>txqueuelen</strong>  1000  10 000</p><br>
<pre><code class="plaintext hljs">root@ceph01:~#ip link set ens1f0  txqueuelen 10000</code></pre><br>
<p>     ceph</p><br>
<p><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=">https://ceph.com/geen-categorie/ceph-loves-jumbo-frames/</a></p><br>
<p> <strong>MTU</strong>  9000.</p><br>
<pre><code class="plaintext hljs">root@ceph01:~#ip link set dev ens1f0  mtu 9000</code></pre><br>
<p>  /etc/network/interfaces,      </p><br>
<div class="spoiler"><b class="spoiler_title">cat /etc/network/interfaces</b><div class="spoiler_text"><pre><code class="plaintext hljs">root@ceph01:~# cat /etc/network/interfaces<font></font>
auto lo<font></font>
iface lo inet loopback<font></font>
<font></font>
auto ens1f0<font></font>
iface ens1f0 inet manual<font></font>
post-up /sbin/ethtool -G ens1f0 rx 4096<font></font>
post-up /sbin/ethtool -G ens1f0 tx 4096<font></font>
post-up /sbin/ethtool -L ens1f0 combined 63<font></font>
post-up /sbin/ip link set ens1f0  txqueuelen 10000<font></font>
mtu 9000<font></font>
<font></font>
auto ens1f1<font></font>
iface ens1f1 inet manual<font></font>
post-up /sbin/ethtool -G ens1f1 rx 4096<font></font>
post-up /sbin/ethtool -G ens1f1 tx 4096<font></font>
post-up /sbin/ethtool -L ens1f1 combined 63<font></font>
post-up /sbin/ip link set ens1f1  txqueuelen 10000<font></font>
mtu 9000</code></pre></div></div><br>
<p> ,    ,      4.15. ,    128G RAM,      <strong>sysctl</strong></p><br>
<div class="spoiler"><b class="spoiler_title">cat /etc/sysctl.d/50-ceph.conf</b><div class="spoiler_text"><pre><code class="plaintext hljs">net.core.rmem_max = 56623104  <font></font>
#         54M<font></font>
<font></font>
net.core.wmem_max = 56623104<font></font>
#        54M<font></font>
<font></font>
net.core.rmem_default = 56623104<font></font>
#        . 54M<font></font>
<font></font>
net.core.wmem_default = 56623104<font></font>
#         54M  <font></font>
#   <font></font>
<font></font>
net.ipv4.tcp_rmem = 4096 87380 56623104<font></font>
# (,  , )    tcp_rmem<font></font>
#  3  ,      TCP.<font></font>
# :   TCP       <font></font>
#   .     <font></font>
#       (moderate memory pressure).<font></font>
#       8  (8192).<font></font>
#  :  ,    <font></font>
#   TCP  .    <font></font>
#  /proc/sys/net/core/rmem_default,   .<font></font>
#       ( ) <font></font>
#  87830 .     65535  <font></font>
#     tcp_adv_win_scale  tcp_app_win = 0, <font></font>
#  ,       tcp_app_win.<font></font>
# :   ,    <font></font>
#     TCP.     , <font></font>
#    /proc/sys/net/core/rmem_max.  «»<font></font>
#     SO_RCVBUF     .<font></font>
<font></font>
net.ipv4.tcp_wmem = 4096 65536 56623104<font></font>
net.core.somaxconn = 5000    <font></font>
#    ,  .<font></font>
<font></font>
net.ipv4.tcp_timestamps=1<font></font>
#     (timestamps),    RFC 1323.<font></font>
<font></font>
net.ipv4.tcp_sack=1<font></font>
#     TCP<font></font>
<font></font>
net.core.netdev_max_backlog=5000 ( 1000)<font></font>
#       ,  <font></font>
#    ,     .<font></font>
<font></font>
net.ipv4.tcp_max_tw_buckets=262144<font></font>
#   ,    TIME-WAIT .<font></font>
#     – «»    <font></font>
#    .<font></font>
<font></font>
net.ipv4.tcp_tw_reuse=1<font></font>
#   TIME-WAIT   ,<font></font>
#     .<font></font>
<font></font>
net.core.optmem_max=4194304<font></font>
#   - ALLOCATABLE<font></font>
#    (4096 )<font></font>
<font></font>
net.ipv4.tcp_low_latency=1<font></font>
#  TCP/IP     <font></font>
#     .<font></font>
<font></font>
net.ipv4.tcp_adv_win_scale=1<font></font>
#          ,<font></font>
#    TCP-    .<font></font>
#   tcp_adv_win_scale ,    <font></font>
#   :<font></font>
# Bytes- bytes\2  -tcp_adv_win_scale<font></font>
#  bytes –     .   tcp_adv_win_scale<font></font>
# ,       :<font></font>
# Bytes- bytes\2  tcp_adv_win_scale<font></font>
#    .  - – 2, <font></font>
# ..     ¼  ,  <font></font>
# tcp_rmem.<font></font>
<font></font>
net.ipv4.tcp_slow_start_after_idle=0<font></font>
#    ,     <font></font>
# ,       .<font></font>
#   SSR  ,    <font></font>
#  .<font></font>
<font></font>
net.ipv4.tcp_no_metrics_save=1<font></font>
#    TCP      .<font></font>
<font></font>
net.ipv4.tcp_syncookies=0<font></font>
#   syncookie<font></font>
<font></font>
net.ipv4.tcp_ecn=0<font></font>
#Explicit Congestion Notification (   )  <font></font>
# TCP-.      «» <font></font>
#       .    <font></font>
# -       <font></font>
#    .<font></font>
<font></font>
net.ipv4.conf.all.send_redirects=0<font></font>
#   ICMP Redirect …  .   <font></font>
#   ,        .<font></font>
#    .<font></font>
<font></font>
net.ipv4.ip_forward=0<font></font>
#  .   ,     ,<font></font>
#    .<font></font>
<font></font>
net.ipv4.icmp_echo_ignore_broadcasts=1<font></font>
#   ICMP ECHO ,   <font></font>
<font></font>
net.ipv4.tcp_fin_timeout=10<font></font>
#      FIN-WAIT-2  <font></font>
#   .  60<font></font>
<font></font>
net.core.netdev_budget=600 # ( 300)<font></font>
#        ,<font></font>
#          <font></font>
#  .    NIC ,    .<font></font>
# ,     SoftIRQs<font></font>
# ( )  CPU.    netdev_budget. <font></font>
#    300.    SoftIRQ <font></font>
# 300   NIC     CPU<font></font>
<font></font>
net.ipv4.tcp_fastopen=3<font></font>
# TFO TCP Fast Open<font></font>
#        TFO,     <font></font>
#    TCP .     , <font></font>
#  )</code></pre></div></div><br>
<p><strong>luster network</strong>     10Gbps      .         <strong>mellanox</strong> 10/25 Gbps,     10Gbps .     OSPF,    lacp -       16 Gbps,     ospf        .      ROCE   ,   .     :</p><br>
<ol>
<li>       BGP,     — <em>(         <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=">frr=6.0-1</a> )</em>  . </li>
<li>         —   4 .              BGP,  —             OSPF</li>
</ol><br>
<p>   OSPF:   —      fault tolerance.<br>
         — 10.10.10.0/24  10.10.20.0/24 </p><br>
<pre><code class="plaintext hljs">1: ens1f0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9000 qdisc mq state UP group default qlen 1000<font></font>
    inet 10.10.10.2/24 brd 10.10.10.255 scope global ens1f0<font></font>
<font></font>
2: ens1f1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9000 qdisc mq state UP group default qlen 1000<font></font>
    inet 10.10.20.2/24 brd 10.10.20.255 scope global ens1f1</code></pre><br>
<p>     . </p><br>
<h3 id="disk">DISK</h3><br>
<p>     .  SSD    <strong>noop</strong>,  HDD — <strong>deadline</strong>.   —  NOOP    "   —   ",      "FIFO (First In, First Out)".        . DEADLINE    ,             .       —         — OSD daemon.<br>
(    -     :<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=">http://www.admin-magazine.com/HPC/Articles/Linux-I-O-Schedulers</a></p><br>
<p>   : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=">https://www.opennet.ru/base/sys/linux_shedulers.txt.html</a>)</p><br>
<p>      -  nr_request</p><br>
<blockquote><em>nr_requests<br>
The value of nr_requests determines the amount of I/O requests that get buffered before the I/O scheduler sends / receives data to the block device, if you are using a RAID card / Block Device that can handle a larger queue than what the I/O scheduler is set to, raising the value of nr_requests may help to improve throughout and reduce server load when large amounts of I/O occur on the server. If you are using Deadline or CFQ as the scheduler, it is suggested that you should set the nr_request value to 2 times the value of queue depth.</em></blockquote><p>!    CEPH  ,      </p><br>
<p><img src="https://habrastorage.org/getpro/habr/post_images/b7e/fd3/bd0/b7efd3bd03fedc88307e200905c8c6a9.gif"></p><br>
<div class="spoiler"><b class="spoiler_title">WBThrottle / nr_requests</b><div class="spoiler_text"><blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">WBThrottleおよび/またはnr_requests</font></font><br>
       / ;              .          ,               Linux.     OSD       SSD    .            /         ,             .     ,        /               / .</blockquote><p>,           Ceph        .          /               / ,      Ceph.       ,       ,          .</p><br>
<p>     Ceph          (writeback)   WBThrottle.        /   ,                       .  ,  ,              ,         .                  .    :          ,            .            ,       .</p><br>
<p>      ,          / ,   WBThrottle,             .  ,                    .<br>
  ,    Ceph             .               Ceph,            / .   :</p><br>
<pre><code class="plaintext hljs">echo 8 &gt; /sys/block/sda/queue/nr_requests</code></pre><br>
<p><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=">http://onreader.mdl.ru/MasteringCeph/content/Ch09.html#030202</a></p></div></div><br>
<h3 id="common">COMMON</h3><br>
<p>    ,   <del>    </del>      </p><br>
<div class="spoiler"><b class="spoiler_title">cat /etc/sysctl.d/60-ceph2.conf</b><div class="spoiler_text"><pre><code class="plaintext hljs"> kernel.pid_max = 4194303<font></font>
#     25,      <font></font>
kernel.threads-max=2097152<font></font>
# , , .<font></font>
vm.max_map_count=524288<font></font>
#      . <font></font>
#        <font></font>
#        <font></font>
# malloc,    mmap, mprotect  madvise,    <font></font>
#  .<font></font>
fs.aio-max-nr=50000000<font></font>
#   input-output<font></font>
#  Linux     - (AIO),<font></font>
#       -<font></font>
# ,    -  . <font></font>
#     , <font></font>
#      -.<font></font>
#  aio-max-nr     <font></font>
#  .<font></font>
vm.min_free_kbytes=1048576<font></font>
#       .<font></font>
#  1Gb,       , <font></font>
#    OOM Killer   OSD.    <font></font>
#    ,     <font></font>
vm.swappiness=10<font></font>
#       10% .<font></font>
#   128G ,  10%  12 .     .<font></font>
#    60%   ,   ,<font></font>
#      <font></font>
vm.vfs_cache_pressure=1000<font></font>
#    100.    <font></font>
#     .<font></font>
vm.zone_reclaim_mode=0<font></font>
#         <font></font>
#  ,     . <font></font>
#     ,     .<font></font>
#      <font></font>
# ,    , zone_reclaim_mode<font></font>
#  ,   , <font></font>
# ,   ,   .<font></font>
vm.dirty_ratio=20<font></font>
#   ,     "" <font></font>
#    : <font></font>
#   128  .<font></font>
#   20  SSD,     CEPH  <font></font>
#     3G .<font></font>
#   40  HDD,      1G<font></font>
# 20%  128  25.6 . ,     ,<font></font>
#    2.4G .        <font></font>
#    -    DevOps   .<font></font>
vm.dirty_background_ratio=3<font></font>
#   ,    dirty pages  ,<font></font>
#    pdflush/flush/kdmflush    <font></font>
fs.file-max=524288<font></font>
#      ,,   ,    . </code></pre></div></div><br>
<h2 id="pogruzhenie-v--ceph">  CEPH</h2><br>
<p>,      :</p><br>
<div class="spoiler"><b class="spoiler_title">cat /etc/ceph/ceph.conf</b><div class="spoiler_text"><pre><code class="plaintext hljs">osd:<font></font>
    journal_aio: true               #  ,  <font></font>
    journal_block_align: true       #  i/o<font></font>
    journal_dio: true               #  <font></font>
    journal_max_write_bytes: 1073714824 #    <font></font>
                                        #     <font></font>
    journal_max_write_entries: 10000    #     <font></font>
    journal_queue_max_bytes: 10485760000 <font></font>
    journal_queue_max_ops: 50000<font></font>
    rocksdb_separate_wal_dir: true      #    wal                                                                            <font></font>
                                        #                                                                                                                                                                                           <font></font>
                                        # NVMe<font></font>
    bluestore_block_db_create: true     #      <font></font>
    bluestore_block_db_size: '5368709120 #5G'<font></font>
    bluestore_block_wal_create: true<font></font>
    bluestore_block_wal_size: '1073741824   #1G' <font></font>
    bluestore_cache_size_hdd: '3221225472   # 3G' <font></font>
                                            #     <font></font>
                                            #    <font></font>
    bluestore_cache_size_ssd: '9663676416   # 9G' <font></font>
<font></font>
    keyring: /var/lib/ceph/osd/ceph-$id/keyring<font></font>
    osd_client_message_size_cap: '1073741824 #1G'<font></font>
    osd_disk_thread_ioprio_class: idle<font></font>
    osd_disk_thread_ioprio_priority: 7<font></font>
    osd_disk_threads: 2 #       <font></font>
    osd_failsafe_full_ratio: 0.95<font></font>
    osd_heartbeat_grace: 5<font></font>
    osd_heartbeat_interval: 3<font></font>
    osd_map_dedup: true<font></font>
    osd_max_backfills: 2 #       .<font></font>
    osd_max_write_size: 256<font></font>
    osd_mon_heartbeat_interval: 5<font></font>
    osd_op_threads: 16<font></font>
    osd_op_num_threads_per_shard: 1<font></font>
    osd_op_num_threads_per_shard_hdd: 2<font></font>
    osd_op_num_threads_per_shard_ssd: 2<font></font>
    osd_pool_default_min_size: 1     #  .   <font></font>
    osd_pool_default_size: 2         #  ,                                                                                                                                                         <font></font>
                                     #     <font></font>
                                     #  <font></font>
    osd_recovery_delay_start: 10.000000<font></font>
    osd_recovery_max_active: 2<font></font>
    osd_recovery_max_chunk: 1048576<font></font>
    osd_recovery_max_single_start: 3<font></font>
    osd_recovery_op_priority: 1<font></font>
    osd_recovery_priority: 1            #      <font></font>
    osd_recovery_sleep: 2<font></font>
    osd_scrub_chunk_max: 4</code></pre></div></div><br>
<p> ,    QA   12.2.12,    ceph 12.2.2,   <strong>osd_recovery_threads.</strong>          12.2.12.        12.2.2  12.2.12,    rolling update.</p><br>
<h3 id="testovyy-klaster"> </h3><br>
<p>,      -     ,              . ,         (<strong>1393</strong>     <strong>1436</strong>   ),     (  ,     )</p><br>
<p>,      —   <strong>ceph-deploy,</strong>    (  )     .     ,        ,     <strong>1.5.39</strong></p><br>
<p>  ceph-disk     deprecated  -, ,  ceph-volume —    OSD   ,     . </p><br>
<p>   —     SSD ,     OSD, ,   ,    SAS.           .</p><br>
<p>    </p><br>
<div class="spoiler"><b class="spoiler_title">cat /etc/ceph/ceph.conf</b><div class="spoiler_text"><pre><code class="plaintext hljs">root@ceph01-qa:~# cat /etc/ceph/ceph.conf #    <font></font>
[client]<font></font>
rbd_cache = true<font></font>
rbd_cache_max_dirty = 50331648<font></font>
rbd_cache_max_dirty_age = 2<font></font>
rbd_cache_size = 67108864<font></font>
rbd_cache_target_dirty = 33554432<font></font>
rbd_cache_writethrough_until_flush = true<font></font>
rbd_concurrent_management_ops = 10<font></font>
rbd_default_format = 2<font></font>
[global]<font></font>
auth_client_required = cephx<font></font>
auth_cluster_required = cephx<font></font>
auth_service_required = cephx<font></font>
cluster network = 10.10.10.0/24<font></font>
debug_asok = 0/0<font></font>
debug_auth = 0/0<font></font>
debug_buffer = 0/0<font></font>
debug_client = 0/0<font></font>
debug_context = 0/0<font></font>
debug_crush = 0/0<font></font>
debug_filer = 0/0<font></font>
debug_filestore = 0/0<font></font>
debug_finisher = 0/0<font></font>
debug_heartbeatmap = 0/0<font></font>
debug_journal = 0/0<font></font>
debug_journaler = 0/0<font></font>
debug_lockdep = 0/0<font></font>
debug_mon = 0/0<font></font>
debug_monc = 0/0<font></font>
debug_ms = 0/0<font></font>
debug_objclass = 0/0<font></font>
debug_objectcatcher = 0/0<font></font>
debug_objecter = 0/0<font></font>
debug_optracker = 0/0<font></font>
debug_osd = 0/0<font></font>
debug_paxos = 0/0<font></font>
debug_perfcounter = 0/0<font></font>
debug_rados = 0/0<font></font>
debug_rbd = 0/0<font></font>
debug_rgw = 0/0<font></font>
debug_throttle = 0/0<font></font>
debug_timer = 0/0<font></font>
debug_tp = 0/0<font></font>
fsid = d0000000d-4000-4b00-b00b-0123qwe123qwf9<font></font>
mon_host = ceph01-q, ceph02-q, ceph03-q<font></font>
mon_initial_members = ceph01-q, ceph02-q, ceph03-q<font></font>
public network = 8.8.8.8/28 #  ,  ))<font></font>
rgw_dns_name = s3-qa.mycompany.ru #    <font></font>
rgw_host = s3-qa.mycompany.ru #   <font></font>
[mon]<font></font>
mon allow pool delete = true<font></font>
mon_max_pg_per_osd = 300 #    <font></font>
                          #    <font></font>
                     #  , ,    ,<font></font>
                     #     OSD.     PG<font></font>
                        #     -   <font></font>
mon_osd_backfillfull_ratio = 0.9<font></font>
mon_osd_down_out_interval = 5<font></font>
mon_osd_full_ratio = 0.95 #   SSD    <font></font>
                          #   -     <font></font>
                          #   5%   (   1.2Tb)<font></font>
                          #    ,    <font></font>
                          # bluestore_block_db_size     <font></font>
                          #  <font></font>
mon_osd_nearfull_ratio = 0.9<font></font>
mon_pg_warn_max_per_osd = 520<font></font>
[osd]<font></font>
bluestore_block_db_create = true<font></font>
bluestore_block_db_size = 5368709120 #5G<font></font>
bluestore_block_wal_create = true<font></font>
bluestore_block_wal_size = 1073741824 #1G<font></font>
bluestore_cache_size_hdd = 3221225472 # 3G<font></font>
bluestore_cache_size_ssd = 9663676416 # 9G<font></font>
journal_aio = true<font></font>
journal_block_align = true<font></font>
journal_dio = true<font></font>
journal_max_write_bytes = 1073714824<font></font>
journal_max_write_entries = 10000<font></font>
journal_queue_max_bytes = 10485760000<font></font>
journal_queue_max_ops = 50000<font></font>
keyring = /var/lib/ceph/osd/ceph-$id/keyring<font></font>
osd_client_message_size_cap = 1073741824 #1G<font></font>
osd_disk_thread_ioprio_class = idle<font></font>
osd_disk_thread_ioprio_priority = 7<font></font>
osd_disk_threads = 2<font></font>
osd_failsafe_full_ratio = 0.95<font></font>
osd_heartbeat_grace = 5<font></font>
osd_heartbeat_interval = 3<font></font>
osd_map_dedup = true<font></font>
osd_max_backfills = 4<font></font>
osd_max_write_size = 256<font></font>
osd_mon_heartbeat_interval = 5<font></font>
osd_op_num_threads_per_shard = 1<font></font>
osd_op_num_threads_per_shard_hdd = 2<font></font>
osd_op_num_threads_per_shard_ssd = 2<font></font>
osd_op_threads = 16<font></font>
osd_pool_default_min_size = 1<font></font>
osd_pool_default_size = 2<font></font>
osd_recovery_delay_start = 10.0<font></font>
osd_recovery_max_active = 1<font></font>
osd_recovery_max_chunk = 1048576<font></font>
osd_recovery_max_single_start = 3<font></font>
osd_recovery_op_priority = 1<font></font>
osd_recovery_priority = 1<font></font>
osd_recovery_sleep = 2<font></font>
osd_scrub_chunk_max = 4<font></font>
osd_scrub_chunk_min = 2<font></font>
osd_scrub_sleep = 0.1<font></font>
rocksdb_separate_wal_dir = true</code></pre></div></div><br>
<pre><code class="plaintext hljs">#  <font></font>
root@ceph01-qa:~#ceph-deploy mon create ceph01-q<font></font>
#       <font></font>
root@ceph01-qa:~#ceph-deploy gatherkeys ceph01-q<font></font>
#   .       - ,       <font></font>
# mon_initial_members = ceph01-q, ceph02-q, ceph03-q<font></font>
#        <font></font>
root@ceph01-qa:~#ceph-deploy mon create-initial<font></font>
#       <font></font>
root@ceph01-qa:~#cat ceph.bootstrap-osd.keyring &gt; /var/lib/ceph/bootstrap-osd/ceph.keyring <font></font>
root@ceph01-qa:~#cat ceph.bootstrap-mgr.keyring &gt; /var/lib/ceph/bootstrap-mgr/ceph.keyring <font></font>
root@ceph01-qa:~#cat ceph.bootstrap-rgw.keyring &gt; /var/lib/ceph/bootstrap-rgw/ceph.keyring<font></font>
#     <font></font>
root@ceph01-qa:~#ceph-deploy admin ceph01-q<font></font>
#  ,  <font></font>
root@ceph01-qa:~#ceph-deploy mgr create ceph01-q</code></pre><br>
<p>,        ceph-deploy    12.2.12 —      OSD  db    -</p><br>
<pre><code class="plaintext hljs">root@ceph01-qa:~#ceph-volume lvm create --bluestore --data /dev/sde --block.db /dev/md0<font></font>
blkid could not detect a PARTUUID for device: /dev/md1</code></pre><br>
<p>, blkid   PARTUUID,    :</p><br>
<pre><code class="plaintext hljs">root@ceph01-qa:~#parted /dev/md0 mklabel GPT <font></font>
#   , <font></font>
#  GPT    <font></font>
#        = bluestore_block_db_size: '5368709120 #5G'<font></font>
#    20  OSD,    <font></font>
#   <font></font>
root@ceph01-qa:~#for i in {1..20}; do echo -e "n\n\n\n+5G\nw" | fdisk /dev/md0; done</code></pre><br>
<p>  ,     OSD     (, ,    )</p><br>
<p>  OSD  bluestore     WAL,    db</p><br>
<pre><code class="plaintext hljs">root@ceph01-qa:~#ceph-volume lvm create --bluestore --data /dev/sde --block.db /dev/md0<font></font>
 stderr: 2019-04-12 10:39:27.211242 7eff461b6e00 -1 bluestore(/var/lib/ceph/osd/ceph-0/) _read_fsid unparsable uuid<font></font>
 stderr: 2019-04-12 10:39:27.213185 7eff461b6e00 -1 bdev(0x55824c273680 /var/lib/ceph/osd/ceph-0//block.wal) open open got: (22) Invalid argument<font></font>
 stderr: 2019-04-12 10:39:27.213201 7eff461b6e00 -1 bluestore(/var/lib/ceph/osd/ceph-0/) _open_db add block device(/var/lib/ceph/osd/ceph-0//block.wal) returned: (22) Invalid argument<font></font>
 stderr: 2019-04-12 10:39:27.999039 7eff461b6e00 -1 bluestore(/var/lib/ceph/osd/ceph-0/) mkfs failed, (22) Invalid argument<font></font>
 stderr: 2019-04-12 10:39:27.999057 7eff461b6e00 -1 OSD::mkfs: ObjectStore::mkfs failed with error (22) Invalid argument<font></font>
 stderr: 2019-04-12 10:39:27.999141 7eff461b6e00 -1  ** ERROR: error creating empty object store in /var/lib/ceph/osd/ceph-0/: (22) Invalid argumen</code></pre><br>
<p>    -  (   ,  )      WAL      OSD —     (    WAL,  , ,   ).</p><br>
<p>,         WAL  NVMe,     .</p><br>
<pre><code class="plaintext hljs">root@ceph01-qa:~#ceph-volume lvm create --bluestore --data /dev/sdf --block.wal  /dev/md0p2 --block.db /dev/md1p2</code></pre><br>
<p> ,   OSD.      ,        —    SSD  ,     SAS.</p><br>
<p>      20 ,     ,  — .<br>
, ,   :</p><br>
<div class="spoiler"><b class="spoiler_title">ceph osd tree</b><div class="spoiler_text"><p>root@eph01-q:~# ceph osd tree<br>
ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF<br>
-1 14.54799 root default<br>
-3 9.09200 host ceph01-q<br>
0 ssd 1.00000 osd.0 up 1.00000 1.00000<br>
1 ssd 1.00000 osd.1 up 1.00000 1.00000<br>
2 ssd 1.00000 osd.2 up 1.00000 1.00000<br>
3 ssd 1.00000 osd.3 up 1.00000 1.00000<br>
4 hdd 1.00000 osd.4 up 1.00000 1.00000<br>
5 hdd 0.27299 osd.5 up 1.00000 1.00000<br>
6 hdd 0.27299 osd.6 up 1.00000 1.00000<br>
7 hdd 0.27299 osd.7 up 1.00000 1.00000<br>
8 hdd 0.27299 osd.8 up 1.00000 1.00000<br>
9 hdd 0.27299 osd.9 up 1.00000 1.00000<br>
10 hdd 0.27299 osd.10 up 1.00000 1.00000<br>
11 hdd 0.27299 osd.11 up 1.00000 1.00000<br>
12 hdd 0.27299 osd.12 up 1.00000 1.00000<br>
13 hdd 0.27299 osd.13 up 1.00000 1.00000<br>
14 hdd 0.27299 osd.14 up 1.00000 1.00000<br>
15 hdd 0.27299 osd.15 up 1.00000 1.00000<br>
16 hdd 0.27299 osd.16 up 1.00000 1.00000<br>
17 hdd 0.27299 osd.17 up 1.00000 1.00000<br>
18 hdd 0.27299 osd.18 up 1.00000 1.00000<br>
19 hdd 0.27299 osd.19 up 1.00000 1.00000<br>
-5 5.45599 host ceph02-q<br>
20 ssd 0.27299 osd.20 up 1.00000 1.00000<br>
21 ssd 0.27299 osd.21 up 1.00000 1.00000<br>
22 ssd 0.27299 osd.22 up 1.00000 1.00000<br>
23 ssd 0.27299 osd.23 up 1.00000 1.00000<br>
24 hdd 0.27299 osd.24 up 1.00000 1.00000<br>
25 hdd 0.27299 osd.25 up 1.00000 1.00000<br>
26 hdd 0.27299 osd.26 up 1.00000 1.00000<br>
27 hdd 0.27299 osd.27 up 1.00000 1.00000<br>
28 hdd 0.27299 osd.28 up 1.00000 1.00000<br>
29 hdd 0.27299 osd.29 up 1.00000 1.00000<br>
30 hdd 0.27299 osd.30 up 1.00000 1.00000<br>
31 hdd 0.27299 osd.31 up 1.00000 1.00000<br>
32 hdd 0.27299 osd.32 up 1.00000 1.00000<br>
33 hdd 0.27299 osd.33 up 1.00000 1.00000<br>
34 hdd 0.27299 osd.34 up 1.00000 1.00000<br>
35 hdd 0.27299 osd.35 up 1.00000 1.00000<br>
36 hdd 0.27299 osd.36 up 1.00000 1.00000<br>
37 hdd 0.27299 osd.37 up 1.00000 1.00000<br>
38 hdd 0.27299 osd.38 up 1.00000 1.00000<br>
39 hdd 0.27299 osd.39 up 1.00000 1.00000<br>
-7 6.08690 host ceph03-q<br>
40 ssd 0.27299 osd.40 up 1.00000 1.00000<br>
41 ssd 0.27299 osd.41 up 1.00000 1.00000<br>
42 ssd 0.27299 osd.42 up 1.00000 1.00000<br>
43 ssd 0.27299 osd.43 up 1.00000 1.00000<br>
44 hdd 0.27299 osd.44 up 1.00000 1.00000<br>
45 hdd 0.27299 osd.45 up 1.00000 1.00000<br>
46 hdd 0.27299 osd.46 up 1.00000 1.00000<br>
47 hdd 0.27299 osd.47 up 1.00000 1.00000<br>
48 hdd 0.27299 osd.48 up 1.00000 1.00000<br>
49 hdd 0.27299 osd.49 up 1.00000 1.00000<br>
50 hdd 0.27299 osd.50 up 1.00000 1.00000<br>
51 hdd 0.27299 osd.51 up 1.00000 1.00000<br>
52 hdd 0.27299 osd.52 up 1.00000 1.00000<br>
53 hdd 0.27299 osd.53 up 1.00000 1.00000<br>
54 hdd 0.27299 osd.54 up 1.00000 1.00000<br>
55 hdd 0.27299 osd.55 up 1.00000 1.00000<br>
56 hdd 0.27299 osd.56 up 1.00000 1.00000<br>
57 hdd 0.27299 osd.57 up 1.00000 1.00000<br>
58 hdd 0.27299 osd.58 up 1.00000 1.00000<br>
59 hdd 0.89999 osd.59 up 1.00000 1.00000</p></div></div><br>
<p>         :</p><br>
<pre><code class="plaintext hljs">root@ceph01-q:~#ceph osd crush add-bucket rack01 root #  root<font></font>
root@ceph01-q:~#ceph osd crush add-bucket ceph01-q host #  <font></font>
root@ceph01-q:~#ceph osd crush move ceph01-q root=rack01 #    <font></font>
root@ceph01-q:~#osd crush add 28 1.0 host=ceph02-q #    <font></font>
<font></font>
#      <font></font>
root@ceph01-q:~# ceph osd crush remove osd.4<font></font>
root@ceph01-q:~# ceph osd crush remove rack01</code></pre><br>
<p>,      <strong></strong> ,            —  <strong>ceph osd crush move ceph01-host root=rack01</strong> ,      .    CTRL+C     . </p><br>
<p>   : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=">https://tracker.ceph.com/issues/23386</a></p><br>
<p>   crushmap     <strong>rule replicated_ruleset</strong> </p><br>
<pre><code class="plaintext hljs">root@ceph01-prod:~#ceph osd getcrushmap -o crushmap.row #    <font></font>
root@ceph01-prod:~#crushtool -d crushmap.row -o crushmap.txt #  <font></font>
root@ceph01-prod:~#vim  crushmap.txt #,  rule replicated_ruleset<font></font>
root@ceph01-prod:~#crushtool -c crushmap.txt  -o new_crushmap.row # <font></font>
root@ceph01-prod:~#ceph osd setcrushmap -i  new_crushmap.row #  </code></pre><br>
<p><strong>:</strong>      placement group  OSD.    ,   .</p><br>
<p> ,        —  ,     OSD ,        ,    root default.<br>
 ,   ,      root  ssd     ,          default root.   OSD     .<br>
<em>     ,     .     </em></p><br>
<h3 id="kak-my-delali-razlichnye-gruppy-po-tipam-diskov">       .</h3><br>
<p>    root- —  ssd   hdd</p><br>
<pre><code class="plaintext hljs">root@ceph01-q:~#ceph osd crush add-bucket ssd-root root<font></font>
root@ceph01-q:~#ceph osd crush add-bucket hdd-root root</code></pre><br>
<p>       —         </p><br>
<pre><code class="plaintext hljs"># :<font></font>
root@ceph01-q:~#ceph osd crush add-bucket ssd-rack01 rack<font></font>
root@ceph01-q:~#ceph osd crush add-bucket ssd-rack02 rack<font></font>
root@ceph01-q:~#ceph osd crush add-bucket ssd-rack03 rack<font></font>
<font></font>
root@ceph01-q:~#ceph osd crush add-bucket hdd-rack01 rack<font></font>
root@ceph01-q:~#ceph osd crush add-bucket hdd-rack01 rack<font></font>
root@ceph01-q:~#ceph osd crush add-bucket hdd-rack01 rack<font></font>
<font></font>
# <font></font>
root@ceph01-q:~#ceph osd crush add-bucket ssd-ceph01-q host<font></font>
root@ceph01-q:~#ceph osd crush add-bucket ssd-ceph02-q host<font></font>
root@ceph01-q:~#ceph osd crush add-bucket ssd-ceph03-q host<font></font>
<font></font>
root@ceph01-q:~#ceph osd crush add-bucket hdd-ceph01-q host<font></font>
root@ceph01-q:~#ceph osd crush add-bucket hdd-ceph02-q host<font></font>
root@ceph01-q:~#ceph osd crush add-bucket hdd-ceph02-q host</code></pre><br>
<p>        </p><br>
<pre><code class="plaintext hljs">root@ceph01-q:~#   0  3  SSD,   ceph01-q,     <font></font>
root@ceph01-q:~#  ssd-ceph01-q<font></font>
root@ceph01-q:~#ceph osd crush add 0 1 host=ssd-ceph01-q<font></font>
root@ceph01-q:~#ceph osd crush add 1 1 host=ssd-ceph01-q<font></font>
root@ceph01-q:~#ceph osd crush add 2 1 host=ssd-ceph01-q<font></font>
root@ceph01-q:~#ceph osd crush add 3 1 host=ssd-ceph01-q<font></font>
root-ceph01-q:~#    </code></pre><br>
<p>    ssd-root  hdd-root   root-default ,    </p><br>
<pre><code class="plaintext hljs">root-ceph01-q:~#ceph osd crush remove default</code></pre><br>
<p>    ,        —      root          —        ,     (    root,    )</p><br>
<p>       :<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=">http://docs.ceph.com/docs/jewel/rados/operations/crush-map/#crushmaprules</a></p><br>
<pre><code class="plaintext hljs">root-ceph01-q:~#ceph osd crush rule create-simple rule-ssd ssd-root host firstn<font></font>
root-ceph01-q:~#ceph osd crush rule create-simple rule-hdd hdd-root host firstn<font></font>
root-ceph01-q:~#    ,     <font></font>
root-ceph01-q:~#   -        ,<font></font>
root-ceph01-q:~#      <font></font>
root-ceph01-q:~#  ,   ,   <font></font>
root-ceph01-q:~#        :<font></font>
root-ceph01-q:~# ##ceph osd crush rule create-simple rule-ssd ssd-root rack firstn</code></pre><br>
<p>   ,            — PROXMOX:</p><br>
<pre><code class="plaintext hljs">    root-ceph01-q:~# #ceph osd pool create {NAME} {pg_num}  {pgp_num}<font></font>
    root-ceph01-q:~# ceph osd pool create ssd_pool 1024 1024 <font></font>
    root-ceph01-q:~# ceph osd pool create hdd_pool 1024 1024</code></pre><br>
<p>       </p><br>
<pre><code class="plaintext hljs"> root-ceph01-q:~#ceph osd crush rule ls #   <font></font>
    root-ceph01-q:~#ceph osd crush rule dump rule-ssd | grep rule_id # ID <font></font>
    root-ceph01-q:~#ceph osd pool set ssd_pool crush_rule 2<font></font>
</code></pre><br>
<p>              —     ,    (    )   ,    .</p><br>
<p>     300    ,        —        10 Tb    10 PG —      (pg)   —           ).</p><br>
<p>       PG —         —     .</p><br>
<p>   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="> </a>,    CEPH.</p><br>
<p> :</p><br>
<p><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">https://blog.packagecloud.io/eng/2017/02/06/monitoring-tuning-linux-networking-stack-sending-data </font></font></a><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">http://www.admin-magazine.com/HPC/Articles/Linux-IO -スケジューラ</font></font></a><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">http://onreader.mdl.ru/MasteringCeph/content/Ch09.html#030202 </font></font></a><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">https://tracker.ceph.com/issues/23386 </font></font></a><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">https://ceph.com/pgcalc/</font></font></a></p></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../ja456432/index.html">セキュリティウィーク25：Evernoteの脆弱性と何百ものハッキングされたオンラインストア</a></li>
<li><a href="../ja456434/index.html">未来の職業：「あなたは誰が火星で働きますか？」</a></li>
<li><a href="../ja456436/index.html">月曜日の短いJSタスク</a></li>
<li><a href="../ja456440/index.html">とらえどころのないマルヴァリの冒険、パートI</a></li>
<li><a href="../ja456442/index.html">YandexとJetBrainsの支援を得て、サンクトペテルブルク州立大学の学部プログラムへの入学</a></li>
<li><a href="../ja456448/index.html">JSフレームワークを選択するためのルール</a></li>
<li><a href="../ja456450/index.html">DO-RA.Aviaは、航空宇宙線の監視に使用されます</a></li>
<li><a href="../ja456452/index.html">範囲の前後のC ++コード例</a></li>
<li><a href="../ja456462/index.html">Webコンポーネント角度コンポーネントライブラリを構築する</a></li>
<li><a href="../ja456466/index.html">今日のPHPジェネリック（まあ、ほぼ）</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>