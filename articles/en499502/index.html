<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®‚Äçüë¶ üì≤ üëåüèæ Why OceanStor Dorado V6 is the fastest and most reliable storage ‚úåüèº üå•Ô∏è ü§∂üèæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Please do not rush to conclusions because of the header! We have weighty arguments in support of it, and we packed them as compactly as possible. We b...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Why OceanStor Dorado V6 is the fastest and most reliable storage</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/huawei/blog/499502/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Please do not rush to conclusions because of the header! </font><font style="vertical-align: inherit;">We have weighty arguments in support of it, and we packed them as compactly as possible. </font><font style="vertical-align: inherit;">We bring to your attention a post on the concept and principles of operation of our new data storage system, which was released in January 2020.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/gm/jm/gy/gmjmgygu6blyuruew5mcs3aqbjy.jpeg"><br>
<a name="habracut"></a><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In our opinion, the main competitive advantage of the Dorado V6 storage family is provided by the performance and reliability mentioned in the header. </font><font style="vertical-align: inherit;">Yes, it‚Äôs so simple, but due to some tricky and not very tricky decisions we managed to achieve this ‚Äújust‚Äù, we‚Äôll talk today. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In order to better reveal the potential of the new generation systems, we will talk about senior representatives of the model range (models 8000, 18000). </font><font style="vertical-align: inherit;">Unless otherwise indicated, they are implied.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/fw/oc/tl/fwoctl2wdueuf_ua9f0qk0sn_c0.jpeg"><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A few words about the market</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In order to better understand the place of Huawei solutions on the market, we turn to the proven measure - the ‚Äú </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">magic quadrants</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ‚Äù of Gartner. Two years ago, in the general-purpose disk array sector, our company confidently entered the group of leaders, second only to NetApp and Hewlett Packard Enterprise. In 2018, Huawei‚Äôs position in the solid-state storage market was characterized by the ‚Äúapplicant‚Äù status, but something was missing to achieve leadership positions. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In 2019, Gartner, in his research, combined both of the above sectors into one - ‚ÄúMain storage‚Äù. As a result, Huawei again found itself in the quadrant of leaders, next to suppliers such as IBM, Hitachi Vantara and Infinidat.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
To complete the picture, we note that Gartner collects 80% of the data for analysis in the US market, and this leads to a noticeable bias in favor of those companies that are well represented in the USA. </font><font style="vertical-align: inherit;">In the meantime, suppliers targeting European and Asian markets find themselves in a notably less advantageous position. </font><font style="vertical-align: inherit;">And despite this, last year, Huawei products took their rightful place in the upper right quadrant and, according to the Gartner verdict, ‚Äúcan be recommended for use.‚Äù</font></font><br>
<br>
<img src="https://habrastorage.org/webt/wo/jy/aq/wojyaqxgxloltozqspxrezxl02i.jpeg"><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">What's New in Dorado V6</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The Dorado V6 product line, in particular, is represented by the entry-level systems of the 3000 series. Originally equipped with two controllers, they can be horizontally expanded to 16 controllers, 1200 disks and 192 GB of cache. Also, the system will be equipped with external Fiber Channel (8/16/32 Gb / s) and Ethernet (1/10/25/40/100 Gb / s) ports.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Note that the use of protocols that have no commercial success is being phased out, so at the start we decided to abandon the support of Fiber Channel over Ethernet (FCoE) and Infiniband (IB). They will be added in later firmware versions. Support for NVMe over Fabric (NVMe-oF) is available out of the box on top of Fiber Channel. The next firmware, which is scheduled for release in June, is scheduled to support NVMe over Ethernet mode. In our opinion, the above set will more than cover the needs of most Huawei customers. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
There is no file access in the current firmware version and will appear in one of the following updates closer to the end of the year. Implementation is expected at the native level, by the controllers themselves with Ethernet ports, without the use of additional equipment.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The main difference between the Dorado V6 3000 series and older models is that on the backend it supports one protocol - SAS 3.0. Accordingly, the drives there can only be used with the named interface. From our point of view, the performance provided is quite enough for a device of this type. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dorado V6 5000 and 6000 Series systems are mid-range solutions. They are also made in the 2U form factor and are equipped with two controllers. They differ from each other in performance, number of processors, maximum number of disks and cache size. However, in the architectural and engineering terms, Dorado V6 5000 and 6000 are identical and look the same.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The hi-end class includes Dorado V6 systems of the 8000 and 18000 series. Designed in frame sizes 4U, by default they have a separate architecture in which the controllers and drives are separated. In the minimum configuration, they can also be equipped with only two controllers, although customers, as a rule, are asked to install four or more. </font></font><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dorado V6 8000 scales horizontally to 16 controllers, and Dorado V6 18000 - to 32. These systems have different processors with different number of cores and cache size. For all that, the identity of engineering solutions is preserved, as in mid-end models.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
2U shelves with drives are connected via RDMA with a bandwidth of 100 Gb / s. </font><font style="vertical-align: inherit;">The older Dorado V6 backend also supports SAS 3.0, but rather in case SSDs with such an interface drop in price. </font><font style="vertical-align: inherit;">Then there will be economic feasibility of their use, even taking into account lower productivity. </font><font style="vertical-align: inherit;">At the moment, the difference in cost between SSDs with SAS and NVMe interfaces is so small that we are not ready to recommend such a solution.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/to/yi/gr/toyigrvocm6fdvmapyswlqk_k5i.jpeg"><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Inside the controller</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dorado V6 controllers are made on our own element base. </font><font style="vertical-align: inherit;">No Intel processors, no Broadcom ASICs. </font><font style="vertical-align: inherit;">Thus, every single component of the motherboard, as well as itself, is completely removed from the influence of risks associated with sanctions pressure from American companies. </font><font style="vertical-align: inherit;">Those who saw with our own eyes any of our equipment must have noticed shields with a red stripe under the logo. </font><font style="vertical-align: inherit;">It means that the product lacks American components. </font><font style="vertical-align: inherit;">This is the official course of Huawei - the transition to components of its own production or, in any case, manufactured in countries that do not follow the US policy. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Here's what you can see on the controller board itself.</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Universal network interface (Hisilicon 1822 chip), responsible for connecting to Fiber Channel or Ethernet.</font></font></li>
<li>    BMC-,   Hisilicon 1710,       .          .</li>
<li> ,        ARM  Kunpeng 920  Huawei.      ,               ,     . .           . ,    Dorado V6     .</li>
<li> SSD ( Hisilicon 1812e),     SAS-,   NVMe-. , Huawei   SSD,      NAND,             . ,      Huawei  ,        .</li>
<li>   ‚Äî Ascend 310.           ,     ,    .       ,       .          ,        .</li>
</ul><br>
<img src="https://habrastorage.org/webt/ef/qf/qc/efqfqcjue5oaufiyfsw-4leydtw.jpeg"><br>
<br>
<h2>   Kunpeng</h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The Kunpeng processor is a system on a chip (SoC), where in addition to the computing unit there are hardware modules that accelerate various processes, such as calculating checksums or executing ‚Äúerasure coding‚Äù. </font><font style="vertical-align: inherit;">It also implements hardware support for SAS, Ethernet, DDR4 (from six to eight channels), etc. All this allows Huawei to create storage controllers that are not inferior in performance to classical Intel solutions. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In addition, its own solutions based on the ARM architecture give Huawei the opportunity to create full-fledged server solutions and offer them to their customers as an alternative to x86.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/m9/sw/2t/m9sw2trly1xkjyvuefek2ueacbo.jpeg"><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The new architecture of Dorado V6 ...</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The internal architecture of the older Dorado V6 storage system is represented by four main subdomains (factories). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The first factory is a common frontend (network interfaces responsible for communicating with a SAN factory or hosts). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The second is a set of controllers, each of which can reach the frontend network card as well as the neighboring ‚Äúengine‚Äù, which is a box with four controllers, as well as power and cooling units common to them, using the RDMA protocol. Now Dorado V6 hi-end models can be equipped with two such "engines" (respectively, eight controllers). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The third factory is responsible for the backend and consists of RDMA 100G network cards.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Finally, the fourth factory ‚Äúin iron‚Äù is represented by plug-in intelligent shelves with drives. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Such a symmetrical structure unleashes the full potential of NVMe technology and guarantees high performance and reliability. </font><font style="vertical-align: inherit;">The I / O process is maximally parallelized across processors and cores, providing simultaneous reading and writing to multiple threads.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/ug/qb/qj/ugqbqj_nbohudbltl0frnroc0lw.jpeg"><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">... and what she gave us</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The maximum performance of Dorado V6 solutions is approximately three times higher than the performance of previous generation systems (of the same class) and can reach 20 million IOPS. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
This is due to the fact that in the previous generation of devices, NVMe support extended only to the shelves with drives. Now it is present at all stages, from the host to the SSD. The backend network has also undergone changes: SAS / PCIe gave way to RoCEv2 with a bandwidth of 100 Gb / s. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The SSD form factor has also changed. If previously there were 25 drives on the 2U shelf, now it has been brought up to 36 palm-sized physical disks. In addition, the shelves ‚Äúwiser‚Äù. Each of them now has a fault-tolerant system of two controllers based on ARM chips, similar to those installed in the central controllers.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/it/he/jw/ithejwh2y2-cqwxvedytjugaina.jpeg"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
So far they are only engaged in data reorganization, but with the release of new firmware, compression and erasure coding will be added to it, which will reduce the load on the main controllers from 15 to 5%. Transferring part of the tasks to the shelf at the same time frees up the bandwidth of the internal network. And all this significantly increases the scalability potential of the system. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Compression and deduplication in previous generation storage systems were performed with blocks of fixed length. Now, a mode of working with variable-length blocks has been added, which so far needs to be enabled forcibly. Subsequent firmware may change this fact.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Also briefly on failure tolerance. </font><font style="vertical-align: inherit;">Dorado V3 remained operational if one of the two failed. </font><font style="vertical-align: inherit;">Dorado V6 will ensure data availability even if seven out of eight controllers or four out of one ‚Äúengine‚Äù consecutively fail.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/n3/gg/ek/n3ggekdjpfwcronqvtppxsrej6g.jpeg"><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Economic Reliability</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Recently, among customers of Huawei, a survey was conducted on what kind of simple elements of IT-infrastructure the company considers acceptable. For the most part, respondents were tolerant of a hypothetical situation in which the application does not respond for several hundred seconds. For the operating system or host bus adapter, the critical downtime was tens of seconds (in fact, the reboot time). Customers make even higher demands on the network: its bandwidth should not disappear for more than 10-20 seconds. As you might guess, the respondents considered storage failures to be the most critical. From the point of view of business representatives, a simple storage system should not exceed ... a few seconds per year!</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In other words, if the client application of the bank does not respond for 100 seconds, this will most likely not cause catastrophic consequences. </font><font style="vertical-align: inherit;">But if the same number of storage systems does not work, a business stoppage and significant financial losses are likely.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/zp/wb/he/zpwbhehuqzmc56u8jcprlrtcyik.jpeg"><br>
<br>
<blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The chart above shows the cost of an hour of work for the ten largest banks (Forbes data for 2017). </font><font style="vertical-align: inherit;">Agree, if your company is approaching the size of Chinese banks, justifying the need to purchase storage for several million dollars will not be so difficult. </font><font style="vertical-align: inherit;">The converse is also true: if a business does not incur significant losses during a downtime, then it is unlikely to buy hi-end storage systems. </font><font style="vertical-align: inherit;">In any case, it is important to have an idea of ‚Äã‚Äãwhat size a hole threatens to form in your wallet, while the system administrator is dealing with a data storage system that has refused to work.</font></font></blockquote><br>
<br>
<img src="https://habrastorage.org/webt/ua/pv/zw/uapvzwvu-bniunuc_erursgxxts.jpeg"><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Failover Second</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In Solution A in the illustration above, you can recognize our previous generation Dorado V3 system. </font><font style="vertical-align: inherit;">Four of its controllers work in pairs, and only two controllers contain copies of the cache. </font><font style="vertical-align: inherit;">Controllers within a pair can redistribute the load. </font><font style="vertical-align: inherit;">At the same time, as you see, there are no "factories" of the frontend and backend, so each of the shelves with drives is connected to a specific controller pair. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The Solution B diagram shows the solution currently available on the market from another vendor (did you find out?). </font><font style="vertical-align: inherit;">There are already front-end and back-end factories, and drives are connected directly to four controllers. </font><font style="vertical-align: inherit;">True, in the work of internal system algorithms there are nuances that are not obvious in a first approximation.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
On the right is our current Dorado V6 storage architecture with all its internal elements. Consider how these systems survive a typical situation - the failure of one controller. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In classic systems, which include Dorado V3, the period required to redistribute the load in the event of failure reaches four seconds. At this time, I / O completely stops. In Solution B, from our colleagues, despite a more modern architecture, the downtime during a failure is even higher - six seconds.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dorado V6 storage restores its operation just one second after a failure. This result is achieved due to the homogeneous internal RDMA-environment, allowing the controller to access the "foreign" memory. The second important circumstance is the presence of a front-end factory, due to which the path for the host does not change. The port remains the same, and the load is simply sent to serviceable controllers by multipassing drivers. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The failure of the second controller in Dorado V6 is worked out in one second according to the same scheme. Dorado V3 takes about six seconds, while the solution of another vendor takes nine. For many DBMSs, such intervals can no longer be considered acceptable, since during this time the system goes into standby mode and stops working. This is the first thing that concerns a DBMS, consisting of many sections.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The failure of the third Solution A controller is not able to survive. </font><font style="vertical-align: inherit;">Just due to the fact that access to part of the data disks is lost. </font><font style="vertical-align: inherit;">In turn, Solution B in such a situation restores operability, which requires, as in the previous case, nine seconds. </font></font><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
What does Dorado V6 have? </font><font style="vertical-align: inherit;">One second.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/48/ty/eb/48tyebpvigm0nh0ybdzolr7yfei.jpeg"><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">What can be done in a second</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Almost nothing, but we do not need this. </font><font style="vertical-align: inherit;">Once again, in the Dorado V6 hi-end class, the front-end factory is untied from the controller factory. </font><font style="vertical-align: inherit;">This means that there are no hard-coded ports that belong to a specific controller. </font><font style="vertical-align: inherit;">Failover rebuilding does not imply finding alternative paths or reinitializing multipassing. </font><font style="vertical-align: inherit;">The system continues to work as it worked.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/j_/1t/os/j_1tos7de0oav_ndiy6pgwbeg1m.jpeg"><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Multiple Failure Resistance</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The older Dorado V6 models without any problems survive the simultaneous failure of any two (!) Controllers from any ‚Äúengines‚Äù. This is made possible because the solution now stores three copies of the cache. Therefore, even with a double failure, there will always be one complete copy. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The simultaneous failure of all four controllers in one of the ‚Äúengines‚Äù will also not cause fatal consequences, since all three copies of the cache at each moment in time are distributed between the ‚Äúengines‚Äù. The observance of such a logic of work is monitored by the system itself.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Finally, a very unlikely scenario is the sequential failure of seven out of eight controllers. </font><font style="vertical-align: inherit;">Moreover, the minimum acceptable interval for maintaining operability between individual failures is 15 minutes. </font><font style="vertical-align: inherit;">During this time, the storage system manages to perform the operations necessary for the migration of the cache. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The last surviving controller will provide data warehouse operation and maintain the cache for five days (the default value, which is easy to change in the settings). </font><font style="vertical-align: inherit;">After that, the cache will be disabled, but the storage will continue.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/ht/u2/08/htu208rwp-belahni-u6zebwqey.jpeg"><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Not disturbing updates</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The new Dorado V6 OS allows you to update the storage of the storage system without rebooting the controllers. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The OS, as in the case of previous solutions, is based on Linux, however, many operating processes are transferred from the kernel to user mode. Most functions, such as those responsible for deduplication and compression, are now regular daemons running in the background. Due to this, to update individual modules there is no need to change the entire operating system. Suppose, to add support for a new protocol, you only need to turn off the corresponding software module and start a new one.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
It is clear that the issues of updating the whole system still remain, after all, there may be elements in the kernel that need to be updated. </font><font style="vertical-align: inherit;">But such, according to our observations, less than 6% of the total. </font><font style="vertical-align: inherit;">This allows you to restart the controllers dozens of times less than before.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/ox/uc/oq/oxucoqctjppey9wxpvsyua9ymms.jpeg"><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Catastrophic and HA solutions (HA / DR)</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dorado V6 ‚Äúout of the box‚Äù is ready for integration into geo-distributed solutions, urban-level clusters (metro) and ‚Äútriple‚Äù data centers.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
On the left in the illustration above is a metro cluster already familiar to many. Two storage systems operate in active / active mode at a distance of up to 100 km from each other. A similar infrastructure with one or more quorum servers can be supported by solutions of different companies, including our cloud operating system FusionSphere. Of particular importance in such projects are the characteristics of the channel between the sites, all other tasks in our case are taken over by the HyperMetro function, which is available, again, out of the box. Fiber Channel integration as well as iSCSI integration in IP networks is possible if such a need arises. There is no longer a need for a dedicated ‚Äúdark‚Äù optics, as the system is able to communicate through existing channels.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
When building such systems, the only hardware requirement for storage is port allocation for replication. </font><font style="vertical-align: inherit;">It is enough to acquire a license, run quorum servers - physical or virtual - and provide IP connectivity to the controllers (10 Mbps, 50 ms). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
This architecture is easy to transfer to a system with three data centers (see the right side of the illustration). </font><font style="vertical-align: inherit;">For example, when two data centers operate in the metro cluster mode, and the third site, located at a distance of over 100 km, uses asynchronous replication. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The system technologically supports various business scenarios that will be implemented in the event of a large-scale excess.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/pk/au/ha/pkauhavrmru0ujfmbhllubnmp20.jpeg"><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Metro Cluster Survival with Multiple Failures</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The above and below also show the classic metro cluster, consisting of two storage systems and a quorum server. </font><font style="vertical-align: inherit;">As you can see, in six of the nine possible scenarios of multiple failures, our infrastructure will remain operational. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
For example, in the second scenario, if the quorum server fails and synchronization between sites occurs, the system remains productive, as the second site stops working. </font><font style="vertical-align: inherit;">Similar behavior is already embedded in the built-in algorithms. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Even after three failures, access to information can be maintained if the interval between them is at least 15 seconds.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/ut/om/_4/utom_4rdeq3gwrmqwao3uaib3hc.jpeg"><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Habitual trump card from the sleeve</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Recall that Huawei produces not only storage systems, but also a full range of network equipment. </font><font style="vertical-align: inherit;">Whatever storage provider you choose, if a WDM network is used between the sites, in 90% of cases it will be built on the solutions of our company. </font><font style="vertical-align: inherit;">A logical question arises: why assemble a zoo of systems when all guaranteed hardware compatible with each other can be obtained from one vendor?</font></font><br>
<br>
<img src="https://habrastorage.org/webt/wb/y3/wx/wby3wxmvfqzwbyivrtai4a6-uui.jpeg"><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">To the question of performance</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Probably, no one needs to be convinced that the transition to All-Flash storage can significantly reduce infrastructure maintenance costs, since all routine operations are performed many times faster. </font><font style="vertical-align: inherit;">This is evidenced by all suppliers of such equipment. </font><font style="vertical-align: inherit;">Meanwhile, many vendors begin to dissemble when it comes to a decline in performance when you turn on various storage modes.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In our industry, the issuance of storage systems for test operation for one to two days is widely practiced. The provider runs a 20-minute test on an empty system, receiving space-based performance indicators. And in actual operation, "underwater rakes" quickly come out. Already after a day, beautiful IOPS values ‚Äã‚Äãare reduced by half or three, and if the storage is filled by 80%, they are even less. When RAID 5 is turned on, instead of RAID 10, another 10-15% is lost, and in the metro cluster mode, performance is further halved. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
All of the above is not about Dorado V6. Our customers have the opportunity to run a performance test on the weekend or at least at night. Then garbage collection manifests itself, and it also becomes clear how the activation of various options - such as snapshots and replication - affects the amount of IOPS achieved.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In Dorado V6, snapshots and RAID with parity have virtually no effect on performance (3-5% instead of 10-15%). </font><font style="vertical-align: inherit;">Garbage collection (filling drive cells with zeros), compression, deduplication on storage systems that are 80% full will always affect the overall speed of request processing. </font><font style="vertical-align: inherit;">But it is Dorado V6 that is interesting in that no matter what combination of functions and protective mechanisms you activate, the total performance of the storage system will not fall below 80% of the figure obtained without load.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/al/ql/tf/alqltf2odmskrrwkumqpftokhho.jpeg"><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Load balancing</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
High performance Dorado V6 is achieved through balancing at every stage, namely:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">multipassing;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">use of several connections from one host;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">the presence of a front-end factory;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">parallelizing the operation of storage controllers;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">load balancing across all drives at RAID 2.0+.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In principle, this is a common practice. </font><font style="vertical-align: inherit;">Nowadays, few people keep all the data on one LUN: everyone is trying to have eight, even forty, or even more. </font><font style="vertical-align: inherit;">This is the obvious and correct approach that we share. </font><font style="vertical-align: inherit;">But if your task requires only one LUN, which is easier to maintain, our architectural solutions can achieve on it 80% of the performance available when using multiple LUNs.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/wm/rr/dh/wmrrdhkiap1jrxpfxgqkj9cs4vu.jpeg"><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dynamic processor load scheduling</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The load distribution on the processors when using one LUN is implemented as follows: tasks at the LUN level are split into separate small ‚Äúshards‚Äù, each of which is rigidly assigned to a specific controller in the ‚Äúengine‚Äù. This is done so that the system does not lose performance while "jumping" with this piece of data on different controllers. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Another mechanism for maintaining high performance is dynamic sheduling, in which processor cores can be allocated to different task pools. For example, if now the system is idle at the level of deduplication and compression, some of the cores may be included in the process of servicing I / O. Or vice versa. All this is done automatically and transparently to the user.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Data on the current load of each of the Dorado V6 cores is not displayed in the graphical interface, but through the command line you can access the controller OS and use the usual Linux </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">top</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> command </font><font style="vertical-align: inherit;">.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/4c/wl/a-/4cwla-nn1zh7bgwggnbto3_w4sq.jpeg"><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Support for NVMe and RoCE</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As already mentioned, at the moment Dorado V6 fully supports NVMe over Fiber Channel out of the box and does not require any licenses. </font><font style="vertical-align: inherit;">Mid-year support for NVMe over Ethernet. </font><font style="vertical-align: inherit;">For its full use, you will need Ethernet support with direct memory access (DMA) version v2.0 both from the storage system itself, and from the side of switches and network adapters. </font><font style="vertical-align: inherit;">For example, such as Mellanox ConnectX-4 or ConnectX-5. </font><font style="vertical-align: inherit;">You can use network cards made on the basis of our microcircuits. </font><font style="vertical-align: inherit;">RoCE support should also be implemented at the operating system level.</font></font><br>
<br>
<blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In general, we consider the Dorado V6 an NVMe-oriented system. </font><font style="vertical-align: inherit;">Despite the existing support for Fiber Channel and iSCSI, it is planned to switch to high-speed Ethernet with RDMA in the future.</font></font></blockquote><br>
<br>
<img src="https://habrastorage.org/webt/cr/zo/mw/crzomwjifeshb951eje2sn3tbwe.jpeg"><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pinch of marketing</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Due to the fact that the Dorado V6 system is highly resistant to failures, scales well, supports various migration technologies, etc., the economic effect of its acquisition is manifested with the beginning of intensive operation of storage systems. </font><font style="vertical-align: inherit;">We will continue to try to make the ownership of the system as profitable as possible, even if it is not striking at the first stage. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In particular, we have formed the FLASH EVER program related to the extension of the storage life cycle and designed to unload the customer as much as possible during upgrades. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/nz/nk/q6/nznkq6nq3lzjw9c4e99hvr7wqew.jpeg"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
This program includes a number of measures:</font></font><br>
<br>
<ul>
<li>               (  Dorado V6  hi-end);</li>
<li>   (   Dorado      );</li>
<li>  (        Dorado).</li>
</ul><br>
<img src="https://habrastorage.org/webt/z8/mb/h4/z8mbh4j2ndpjsqnlhrcspsfpxfm.jpeg"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
It remains to note that the difficult situation in the world had little effect on the commercial prospects of the new system. </font><font style="vertical-align: inherit;">Despite the fact that the official release of Dorado V6 took place only in January, we see significant demand for it in China, as well as great interest in it from Russian and international partners from the financial sphere and from government structures. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Among other things, in connection with the pandemic, no matter how long they last, the issue of providing remote employees with virtual desktops is especially acute. </font><font style="vertical-align: inherit;">In this process, Dorado V6 could also remove many questions. </font><font style="vertical-align: inherit;">To do this, we are making all the necessary efforts, including practically agreeing to include the new system in the VMware compatibility list.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">***</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
By the way, do not forget about our many webinars, held not only in the Russian-language segment, but also at the global level. </font><font style="vertical-align: inherit;">The list of webinars for April is available </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">here</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../en499476/index.html">Automation testing of turnkey web applications, without registration and SMS</a></li>
<li><a href="../en499482/index.html">How investors save finances in a crisis: approaches to diversification, selection of low-risk assets and cost optimization</a></li>
<li><a href="../en499486/index.html">Rhetoric as a safety tool</a></li>
<li><a href="../en499490/index.html">Random Algorithms</a></li>
<li><a href="../en499492/index.html">Is it possible for a developer in Silicon Valley to become a millionaire? Staff Engineer parsing at Lyft</a></li>
<li><a href="../en499504/index.html">Apple & Google Bluetooth Contact Tracking Protocol</a></li>
<li><a href="../en499506/index.html">Payment hourly, for the result or salary?</a></li>
<li><a href="../en499508/index.html">Making our product ready to scale with Laravel queues</a></li>
<li><a href="../en499512/index.html">AHK at the minimum. Binder</a></li>
<li><a href="../en499516/index.html">Beginning of beta testing of the plugin for Rider and PVS-Studio C # for Linux / macOS</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>