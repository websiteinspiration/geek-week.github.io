<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§¶üèæ üçì üëÇüèº ‚ÄúSorry, I recognized ...‚Äù or recognize raspberries and controllers using the Tensorflow Object Detection API üê¢ üëÜüèΩ üéä</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="At the end of last year, I wrote an article about how I was intrigued by the ability to recognize objects in images using neural networks. In that art...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>‚ÄúSorry, I recognized ...‚Äù or recognize raspberries and controllers using the Tensorflow Object Detection API</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/494804/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">At the end of last year, I wrote </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">an article</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> about how I was intrigued by the ability to recognize objects in images using neural networks. In that article, using PyTorch, we categorized either raspberries or an arduino-like controller on video. And despite the fact that I liked PyTorch, I turned to him because I couldn‚Äôt deal with TensorFlow right away. But I promised that I will return to the issue of recognition of objects in the video. It seems time has come to keep the promise. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In this article, we will try on our local machine to retrain the finished model in Tensorflow 1.13 and the Object Detection API on our own set of images, and then use it to recognize berries and controllers in the video stream of a web camera using OpenCV.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Want to improve your berry recognition skill by summer? </font><font style="vertical-align: inherit;">Then you are welcome under cat.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/fu/do/rd/fudordve5xz-8gwdnbvlnkkjusm.png"><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Contents: </font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Part I: introduction </font></font></a><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Part II: train the model in TenosrFlow </font></font></a><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Part III: apply the model in OpenCV </font></font></a><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Part IV: conclusion</font></font></a><br>
<a name="I"></a><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Part I: Introduction</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Those who have read the previous article about PyTorch already know that I am an amateur in questions of neural networks. Therefore, do not perceive this article as the ultimate truth. But anyway, I hope that I can help someone deal with the basics of video recognition using the Tensorflow Object Detection API. </font></font><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">This time I did not try to make a tutorial, so the article will be shorter than usual.</font></font></i><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
To begin with, the </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">official tutorial</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> on using the Object Detection API on a local machine, to put it mildly, is hardly exhaustive. As a novice, I was completely inadequate and had to focus on blog articles.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
To be honest, I would like to try TensorFlow 2.0, but in most publications, at the time of this writing, migration issues were not completely resolved. </font><font style="vertical-align: inherit;">Therefore, in the end, I settled on TF 1.13.2.</font></font><br>
<a name="II"></a><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Part II: teaching a model at TensorFlow </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
I drew instructions for teaching the model </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">from this article</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , or rather from its first half, until JavaScript was applied </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(If you do not speak English, you can see an article on the same topic </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">in Habr√©</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> )</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
True, in my case there are several differences:</font></font><br>
<br>
<ol>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I used Linux because Anaconda for Linux already has built protobuf and pycocoapi, so I didn't have to build them myself.</font></font></li>
<li>   TensorFlow 1.13.2,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow">Object Detection API 1.13</a> ,       TensorFlow 1.13.2.   master        TF 1.15,         1.13.</li>
<li>      numpy ‚Äî 1.17.5,  1.18    .</li>
<li>  faster_rcnn_inception_v2_coco    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow">ssd_mobilenet_v2_coco</a>,    ,     .</li>
</ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 Just in case, I‚Äôll say that I did not use a graphics accelerator. Training was carried out only on processor capacities. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A set of images, a configuration file, a saved graph, as well as a script for image recognition using OpenCV, as always, can be downloaded from </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GitHub</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A long 23 hours of model training have passed, all the tea in the house has already been drunk, ‚ÄúWhat? Where? When?" inspected and now my patience finally came to an end. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We stop training and save the model. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Install OpenCV in the same environment of ‚ÄúAnaconda‚Äù with the following command:</font></font><br>
<br>
<pre><code class="plaintext hljs">conda install -c conda-forge opencv</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
I eventually installed version 4.2. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Further, the instructions </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">from this article</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> we will no longer need. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
After saving the model, I made one mistake that was not obvious to me, namely, I immediately tried to substitute the graph.pbtxt file used earlier in the training / folder in the function:</font></font><br>
<br>
<pre><code class="python hljs">cv2.dnn.readNetFromTensorflow()</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Unfortunately, this does not work this way and we will have to do one more manipulation to get graph.pbtxt for OpenCV. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Most likely, the fact that I now advise is not a very good way, but for me it works. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Download </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">tf_text_graph_ssd.py</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , and also </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">tf_text_graph_common.py</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> put them in the folder where our saved graph is located (I have this inference_graph folder). </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Then go to the console in this folder and execute from it a command of approximately the following contents:</font></font><br>
<br>
<pre><code class="plaintext hljs">python tf_text_graph_ssd.py --input frozen_inference_graph.pb --config pipeline.config --output graph.pbtxt</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
And that‚Äôs all it remains to upload our model to OpenCV.</font></font><br>
<br>
<a name="III"></a><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Part III: apply the model in OpenCV </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As in the article about PyTorch regarding the work with OpenCV I took as a basis the program code from </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">this publication</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
I made small changes to simplify it a little more, but since I do not fully understand the code, I will not comment on it. </font><font style="vertical-align: inherit;">Works and nice. </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">It‚Äôs clear that the code could have been better, but I don‚Äôt have time yet to sit down for OpenCV tutorials</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<div class="spoiler" role="button" tabindex="0">
                        <b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">OpenCV code</font></font></b>
                        <div class="spoiler_text"><pre><code class="python hljs">
<span class="hljs-comment"># USAGE</span>
<span class="hljs-comment"># based on this code https://proglib.io/p/real-time-object-detection/</span>
<span class="hljs-comment"># import the necessary packages</span>
<span class="hljs-keyword">from</span> imutils.video <span class="hljs-keyword">import</span> VideoStream
<span class="hljs-keyword">from</span> imutils.video <span class="hljs-keyword">import</span> FPS
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> imutils
<span class="hljs-keyword">import</span> time
<span class="hljs-keyword">import</span> cv2<font></font>
<font></font>
prototxt=<span class="hljs-string">"graph.pbtxt"</span>
model=<span class="hljs-string">"frozen_inference_graph.pb"</span>
min_confidence = <span class="hljs-number">0.5</span><font></font>
<font></font>
<span class="hljs-comment"># initialize the list of class labels MobileNet SSD was trained to</span>
<span class="hljs-comment"># detect, then generate a set of bounding box colors for each class</span>
CLASSES = [<span class="hljs-string">"background"</span>, <span class="hljs-string">"duino"</span>,<span class="hljs-string">"raspb"</span>]<font></font>
COLORS = [(<span class="hljs-number">40</span>,<span class="hljs-number">50</span>,<span class="hljs-number">60</span>),((<span class="hljs-number">140</span>,<span class="hljs-number">55</span>,<span class="hljs-number">130</span>)),(<span class="hljs-number">240</span>,<span class="hljs-number">150</span>,<span class="hljs-number">25</span>)]<font></font>
<font></font>
<span class="hljs-comment"># load our serialized model from disk</span>
print(<span class="hljs-string">"[INFO] loading model..."</span>)<font></font>
<font></font>
net =cv2.dnn.readNetFromTensorflow(model,prototxt)<font></font>
<font></font>
<span class="hljs-comment"># initialize the video stream, allow the cammera sensor to warmup,</span>
<span class="hljs-comment"># and initialize the FPS counter</span>
print(<span class="hljs-string">"[INFO] starting video stream..."</span>)<font></font>
vs = VideoStream(src=<span class="hljs-number">0</span>).start()<font></font>
time.sleep(<span class="hljs-number">0.5</span>)<font></font>
fps = FPS().start()<font></font>
<font></font>
<span class="hljs-comment"># loop over the frames from the video stream</span>
<span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:
	<span class="hljs-comment"># grab the frame from the threaded video stream and resize it</span>
	<span class="hljs-comment"># to have a maximum width of 400 pixels</span><font></font>
	frame = vs.read()<font></font>
	frame = imutils.resize(frame, width=<span class="hljs-number">300</span>)<font></font>
<font></font>
	<span class="hljs-comment"># grab the frame dimensions and convert it to a blob</span>
	(h, w) = frame.shape[:<span class="hljs-number">2</span>]<font></font>
	blob = cv2.dnn.blobFromImage(frame, size=(<span class="hljs-number">300</span>, <span class="hljs-number">300</span>), swapRB=<span class="hljs-literal">True</span>)<font></font>
<font></font>
	<span class="hljs-comment"># pass the blob through the network and obtain the detections and</span>
	<span class="hljs-comment"># predictions</span><font></font>
	net.setInput(blob)<font></font>
	detections = net.forward()<font></font>
<font></font>
	<span class="hljs-comment"># loop over the detections</span>
	<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> np.arange(<span class="hljs-number">0</span>, detections.shape[<span class="hljs-number">2</span>]):
		<span class="hljs-comment"># extract the confidence (i.e., probability) associated with</span>
		<span class="hljs-comment"># the prediction</span>
		<span class="hljs-keyword">print</span> (detections)<font></font>
		confidence = detections[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, i, <span class="hljs-number">2</span>]<font></font>
<font></font>
		<span class="hljs-keyword">if</span> confidence &gt; min_confidence:
			<span class="hljs-comment"># extract the index of the class label from the</span>
			<span class="hljs-comment"># `detections`, then compute the (x, y)-coordinates of</span>
			<span class="hljs-comment"># the bounding box for the object</span>
			idx = int(detections[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, i, <span class="hljs-number">1</span>])<font></font>
			box = detections[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, i, <span class="hljs-number">3</span>:<span class="hljs-number">7</span>] * np.array([w, h, w, h])<font></font>
			(startX, startY, endX, endY) = box.astype(<span class="hljs-string">"int"</span>)<font></font>
<font></font>
			<span class="hljs-comment"># draw the prediction on the frame</span>
			label = <span class="hljs-string">"{}: {:.2f}%"</span>.format(CLASSES[idx],<font></font>
				confidence * <span class="hljs-number">100</span>)<font></font>
			cv2.rectangle(frame, (startX, startY), (endX, endY),<font></font>
				COLORS[idx], <span class="hljs-number">2</span>)<font></font>
			y = startY - <span class="hljs-number">15</span> <span class="hljs-keyword">if</span> startY - <span class="hljs-number">15</span> &gt; <span class="hljs-number">15</span> <span class="hljs-keyword">else</span> startY + <span class="hljs-number">15</span>
			cv2.putText(frame, label, (startX, y+<span class="hljs-number">3</span>),<font></font>
				cv2.FONT_HERSHEY_SIMPLEX, <span class="hljs-number">0.5</span>, COLORS[idx], <span class="hljs-number">1</span>)<font></font>
<font></font>
	<span class="hljs-comment"># show the output frame</span>
	cv2.imshow(<span class="hljs-string">"Frame"</span>, frame)<font></font>
	key = cv2.waitKey(<span class="hljs-number">1</span>) &amp; <span class="hljs-number">0xFF</span><font></font>
<font></font>
	<span class="hljs-comment"># if the `q` key was pressed, break from the loop</span>
	<span class="hljs-keyword">if</span> key == ord(<span class="hljs-string">"q"</span>):
		<span class="hljs-keyword">break</span><font></font>
<font></font>
	<span class="hljs-comment"># update the FPS counter</span><font></font>
	fps.update()<font></font>
<font></font>
<span class="hljs-comment"># stop the timer and display FPS information</span><font></font>
fps.stop()<font></font>
print(<span class="hljs-string">"[INFO] elapsed time: {:.2f}"</span>.format(fps.elapsed()))<font></font>
print(<span class="hljs-string">"[INFO] approx. FPS: {:.2f}"</span>.format(fps.fps()))<font></font>
<font></font>
<span class="hljs-comment"># do a bit of cleanup</span><font></font>
cv2.destroyAllWindows()<font></font>
vs.stop()<font></font>
</code></pre><br>
</div>
                    </div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
So, everything is ready. We launch the model, point the lens at my old CraftDuino and enjoy the result: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/bw/hj/yd/bwhjyd9pddoeop9yaz7fxbqozzo.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
At first glance, it‚Äôs not at all bad, but it‚Äôs only at first glance. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
It looks like in 23 hours, the model was retrained, therefore it gives serious errors when defining objects. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Here is a visual demonstration: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/1w/3y/gf/1w3ygfo-ufytpuyct1kaarpsgls.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As you can see, not only a knife, but even just a black background, this model defines it as an arduino-like controller. Perhaps this is because in the training data there were dark pictures with the Arduino and its analogs, on which the model managed to bump in 23 hours. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As a result, I had to load my computer for another 8 hours and train a new model. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Things are much better with her. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Here is an example with CraftDuino:</font></font><br>
<br>
<img src="https://habrastorage.org/webt/_c/8m/62/_c8m62y2q6as-l8sun5ah5ivppk.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Live raspberries are not at hand. </font><font style="vertical-align: inherit;">I had to print pictures. </font><font style="vertical-align: inherit;">From the screen of the phone or monitor, you can also recognize, but from paper it was more convenient. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/63/_k/ou/63_koujmchte7jor0ulqzxcvgcs.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Let's check how the model recognizes the Arduino nano, which in due time</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" class="user_link"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Drzugrik</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">for me, I soldered into my mega device with sensors: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ub/33/61/ub3361ozwkiwvl2sosx6yldvsou.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As you can see, it recognizes quite well, but with a very poor angle and in warm lighting, it can recognize some fragments like raspberries. But in fact, a frame with an error was hard to catch in the lens. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Now let's check how she classifies those objects on which she was not trained. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Again, an example with a knife and a black background: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/io/ja/6a/ioja6aexferclondu4228nsr06y.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
This time everything works as it should. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We will offer our model to recognize the Canny 3 tiny controller, which I wrote about in a </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">previous article</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br>
<img src="https://habrastorage.org/webt/xp/ay/14/xpay14o7clhp1y1twu4vyltiay4.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Since our model does not know anything except raspberries and arduino-like controllers, we can say that the model recognized the Canny controller quite successfully.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
True, as in the case of the Arduino nano, a lot depends on the angle and lighting. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
With the warm light of an incandescent lamp and with an unsuccessful angle, the controller may not only not be recognized, but even be defined as raspberry. </font><font style="vertical-align: inherit;">True, as in the past case, these angles still had to try to catch in the lens. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/01/ut/h_/01uth_-raiwnzasg7ypn-aoxezs.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Well, the last case is a kind of curtsy for the article about the </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">classification of images in PyTorch</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Like last time, the Raspberry Pi 2 single-board computer and its logo are compatible in one frame. </font><font style="vertical-align: inherit;">Unlike the previous article, in which we solved the classification problem and chose one most probable object for the image, in this case both the logo and the Raspberry itself are recognized.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/vx/fv/us/vxfvusfgitn6vk1pe6o4rvoen9i.png"><br>
<br>
<a name="IV"></a><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Part IV: Conclusion </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In conclusion, I want to say that despite the inexperience of this small example of working with the Tensorflow Object Detection API, it took both days off and part of Monday, I have no regrets. When at least a little understanding of how to use it all becomes insanely curious. In the learning process, you begin to regard the model as a living one, follow its successes and failures. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Therefore, I recommend to everyone who is not familiar with this one day try and recognize something of their own.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Moreover, as it has risen in the process, you do not even need to buy a real webcam. The fact is that during the preparation of the article, I managed to break my webcam (broke the focus mechanism) and already thought that I would have to abandon everything. But it turned out that with the help of Droidcam you can use a smartphone instead of a webcam (do not count for advertising). Moreover, the shooting quality turned out to be much better than that of a broken camera, and this greatly influenced the quality of recognition of objects in the image. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
By the way, since Anaconda has a normal </font><b><font style="vertical-align: inherit;">pycocotools</font></b><font style="vertical-align: inherit;"> build</font></font><b><font style="vertical-align: inherit;"></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I found only for Linux, and I was too lazy to switch between operating systems, I prepared this entire article only using open source software. </font><font style="vertical-align: inherit;">There were analogues of both Word and Photoshop and even a driver for the printer. </font><font style="vertical-align: inherit;">The first time in my life this happened. </font><font style="vertical-align: inherit;">It turned out that modern versions of Linux OS and application programs can be very convenient, even for a person using Microsoft OS for more than 25 years. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
PS If someone knows how to properly run the Object Detection API </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
for Tensorflow version 2 and higher, please unsubscribe in PM or in a comment. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Have a nice day and good health!</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../en488468/index.html">Introducing FastAPI</a></li>
<li><a href="../en488470/index.html">Kim Dotcom: Caught, the most wanted person online. Part 4</a></li>
<li><a href="../en488472/index.html">Weekend Reading: 10 materials about audio gadgets - from Soviet car radios to noise-canceling plugs</a></li>
<li><a href="../en488474/index.html">About color, sound and ‚Äúcrowd exploration‚Äù as a separate kind of beautiful</a></li>
<li><a href="../en494800/index.html">Reverse engineering of Chinese USB IR transceiver protocol</a></li>
<li><a href="../en494806/index.html">Cyber ‚Äã‚Äãtargets 2019 as trends 2020 - hackers have changed focus</a></li>
<li><a href="../en494808/index.html">Product analyst: what does it do, how much does it make, what benefits does the business bring</a></li>
<li><a href="../en494810/index.html">Introduction to 3D: Three.js Basics</a></li>
<li><a href="../en494814/index.html">Is Slurm useful?</a></li>
<li><a href="../en494818/index.html">How to choose a trading terminal to work on the exchange</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>