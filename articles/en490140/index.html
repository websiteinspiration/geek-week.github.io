<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü¶è üïñ üèÇüèº How we work on the quality and speed of selection of recommendations üõ©Ô∏è üö¥üèæ üå©Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="My name is Pavel Parkhomenko, I am an ML developer. In this article, I would like to talk about the design of the Yandex.Zen service and share technic...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>How we work on the quality and speed of selection of recommendations</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/yandex/blog/490140/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">My name is Pavel Parkhomenko, I am an ML developer. In this article, I would like to talk about the design of the Yandex.Zen service and share technical improvements, the introduction of which allowed to increase the quality of recommendations. From the post you will learn how to find the most relevant for the user among millions of documents in just a few milliseconds; how to do continuous decomposition of a large matrix (consisting of millions of columns and tens of millions of rows) so that new documents receive their vector in tens of minutes; how to reuse user-article matrix decomposition to get a good vector representation for video.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/nv/wk/lu/nvwklujlh3cigjss72fkrcjf6ie.png"><br>
<a name="habracut"></a> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Our recommendation database contains millions of documents of various formats: text articles created on our platform and taken from external sites, videos, narratives and short posts. </font><font style="vertical-align: inherit;">The development of such a service is associated with a large number of technical challenges. </font><font style="vertical-align: inherit;">Here are some of them:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Separate computational tasks: do all the heavy operations offline, and in real time only perform rapid application of models in order to be responsible for 100-200 ms.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Quickly consider user actions. </font><font style="vertical-align: inherit;">For this, it is necessary that all events are instantly delivered to the recommender and affect the outcome of the models.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Make the tape so that new users quickly adapt to their behavior. </font><font style="vertical-align: inherit;">People who have just entered the system should feel that their feedback influences recommendations.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Quickly understand who to recommend a new article.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Respond quickly to the constant emergence of new content. </font><font style="vertical-align: inherit;">Tens of thousands of articles are published every day, and many of them have a limited lifespan (say, news). </font><font style="vertical-align: inherit;">This is their difference from films, music and other long-lived and expensive content creation.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Transfer knowledge from one domain domain to another. </font><font style="vertical-align: inherit;">If the recommender system has trained models for text articles and we add video to it, you can reuse existing models so that the new type of content is better ranked.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
I will tell you how we solved these problems.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Candidate Selection</font></font></h2><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">How in a few milliseconds to reduce the set of documents under consideration a thousand times, without practically worsening the quality of the ranking?</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Suppose we trained many ML models, generated attributes based on them, and trained another model that ranks documents for the user. Everything would be fine, but you can‚Äôt just take and count all the signs for all documents in real time, if there are millions of these documents, and recommendations need to be built in 100-200 ms. The task is to choose from millions a certain subset that will be ranked for the user. This step is commonly called candidate selection. It has several requirements. Firstly, the selection must take place very quickly, so that as much time as possible remains on the ranking itself. Secondly, by greatly reducing the number of documents for ranking, we must keep the documents relevant to the user as fully as possible.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Our principle of candidate selection has evolved evolutionarily, and at the moment we have come to a multi-stage scheme: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/gj/yb/4c/gjyb4cr-st_hbkd0iopnaoxf_7c.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
First, all documents are divided into groups, and the most popular documents are taken from each group. Groups can be sites, topics, clusters. For each user, based on his story, the groups closest to him are selected and the best documents are already taken from them. We also use the kNN index to select the documents closest to the user in real time. There are several methods for constructing the kNN index, we have the best earned </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">HNSW</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(Hierarchical Navigable Small World graphs). This is a hierarchical model that allows you to find N nearest vectors for a user from a millionth database in a few milliseconds. Previously, we offline index our entire database of documents. Since the search in the index works quite quickly, if there are several strong embeddings, you can make several indexes (one index for each embedding) and access each of them in real time.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We still have tens of thousands of documents for each user. </font><font style="vertical-align: inherit;">This is still a lot to count all the attributes, so at this stage we apply easy ranking - a lightweight heavy ranking model with fewer attributes. </font><font style="vertical-align: inherit;">The task is to predict what documents the heavy model will have in the top. </font><font style="vertical-align: inherit;">The documents with the highest prediction will be used in the heavy model, that is, at the last stage of ranking. </font><font style="vertical-align: inherit;">This approach allows for tens of milliseconds to reduce the database of documents considered for the user from millions to thousands.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ALS step in runtime</font></font></h2><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">How to take into account the user's feedback immediately after a click?</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
An important factor in the recommendations is the response time to the user's feedback. This is especially important for new users: when a person is just starting to use the recommendation system, he receives a non-personalized stream of documents of various topics. As soon as he makes the first click, you must immediately take this into account and adapt to his interests. If all factors are calculated offline, a quick system response will become impossible due to the delay. So you need to process user actions in real time. For these purposes, we use the ALS step in runtime to build a vector representation of the user.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Suppose we have a vector representation for all documents. </font><font style="vertical-align: inherit;">For example, we can offline build on the basis of the article text embeddings using ELMo, BERT or other machine learning models. </font><font style="vertical-align: inherit;">How can one get a vector representation of users in the same space based on their interaction in the system?</font></font><br>
<br>
<div class="spoiler"><b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The general principle of the formation and decomposition of the user-document matrix</font></font></b><div class="spoiler_text">    m   n .         .         m x n:   ,   ‚Äî .      ,  ÃÅ     ,    .    (, , )    -  ‚Äî    ,     1,   ‚Äì1.<br>
<br>
   : P (m x d)  Q (d x n),  d ‚Äî    (   ).      d-  ( ‚Äî    P,  ‚Äî    Q).       .  ,    ,     .<br>
<br>
<img src="https://habrastorage.org/webt/co/13/pd/co13pdacvhi3axgly7ofjdkzmtq.png"></div></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
One of the possible ways of matrix decomposition is ALS (Alternating Least Squares). </font><font style="vertical-align: inherit;">We will optimize the following loss function:</font></font><br>
<p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><munder><mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;>min</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>q</mi><mo>&amp;#x2217;</mo><mo>,</mo><mi>p</mi><mo>&amp;#x2217;</mo></mrow></munder><munder><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>1</mn></mrow></munder><mo stretchy=&quot;false&quot;>(</mo><msub><mi>r</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>u</mi><mi>i</mi></mrow></msub><mo>&amp;#x2212;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msub><mi>q</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>T</mi></mrow></msup><msub><mi>p</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>u</mi></mrow></msub><msup><mo stretchy=&quot;false&quot;>)</mo><mn>2</mn></msup><mo>+</mo><mi>&amp;#x03BB;</mi><mo stretchy=&quot;false&quot;>(</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><msub><mi>q</mi><mi>i</mi></msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mn>2</mn></msup><mo>+</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><msub><mi>p</mi><mi>u</mi></msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mn>2</mn></msup><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="41.985ex" height="5.384ex" viewBox="0 -1024.6 18076.7 2318.2" role="img" focusable="false" style="vertical-align: -3.005ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/yandex/blog/490140/&amp;usg=ALkJrhjNxJBKYJ-Gij9OTHjKdt0yuPbtMQ#MJMAIN-6D"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/yandex/blog/490140/&amp;usg=ALkJrhjNxJBKYJ-Gij9OTHjKdt0yuPbtMQ#MJMAIN-69" x="833" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/yandex/blog/490140/&amp;usg=ALkJrhjNxJBKYJ-Gij9OTHjKdt0yuPbtMQ#MJMAIN-6E" x="1112" y="0"></use><g transform="translate(41,-601)"><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/yandex/blog/490140/&amp;usg=ALkJrhjNxJBKYJ-Gij9OTHjKdt0yuPbtMQ#MJMATHI-71" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/yandex/blog/490140/&amp;usg=ALkJrhjNxJBKYJ-Gij9OTHjKdt0yuPbtMQ#MJMAIN-2217" x="460" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/yandex/blog/490140/&amp;usg=ALkJrhjNxJBKYJ-Gij9OTHjKdt0yuPbtMQ#MJMAIN-2C" x="961" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/yandex/blog/490140/&amp;usg=ALkJrhjNxJBKYJ-Gij9OTHjKdt0yuPbtMQ#MJMATHI-70" x="1239" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/yandex/blog/490140/&amp;usg=ALkJrhjNxJBKYJ-Gij9OTHjKdt0yuPbtMQ#MJMAIN-2217" x="1743" y="0"></use></g><g transform="translate(1835,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/yandex/blog/490140/&amp;usg=ALkJrhjNxJBKYJ-Gij9OTHjKdt0yuPbtMQ#MJSZ2-2211" x="0" y="0"></use><g transform="translate(147,-1090)"><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/yandex/blog/490140/&amp;usg=ALkJrhjNxJBKYJ-Gij9OTHjKdt0yuPbtMQ#MJMATHI-69" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/yandex/blog/490140/&amp;usg=ALkJrhjNxJBKYJ-Gij9OTHjKdt0yuPbtMQ#MJMAIN-3D" x="345" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/yandex/blog/490140/&amp;usg=ALkJrhjNxJBKYJ-Gij9OTHjKdt0yuPbtMQ#MJMAIN-31" x="1124" y="0"></use></g></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/yandex/blog/490140/&amp;usg=ALkJrhjNxJBKYJ-Gij9OTHjKdt0yuPbtMQ#MJMAIN-28" x="3279" y="0"></use><g transform="translate(3669,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/yandex/blog/490140/&amp;usg=ALkJrhjNxJBKYJ-Gij9OTHjKdt0yuPbtMQ#MJMATHI-72" x="0" y="0"></use><g transform="translate(451,-150)"><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/yandex/blog/490140/&amp;usg=ALkJrhjNxJBKYJ-Gij9OTHjKdt0yuPbtMQ#MJMATHI-75" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/yandex/blog/490140/&amp;usg=ALkJrhjNxJBKYJ-Gij9OTHjKdt0yuPbtMQ#MJMATHI-69" x="572" y="0"></use></g></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/yandex/blog/490140/&amp;usg=ALkJrhjNxJBKYJ-Gij9OTHjKdt0yuPbtMQ#MJMAIN-2212" x="5092" y="0"></use><g transform="translate(6092,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/yandex/blog/490140/&amp;usg=ALkJrhjNxJBKYJ-Gij9OTHjKdt0yuPbtMQ#MJMATHI-71" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/yandex/blog/490140/&amp;usg=ALkJrhjNxJBKYJ-Gij9OTHjKdt0yuPbtMQ#MJMATHI-69" x="631" y="-213"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/yandex/blog/490140/&amp;usg=ALkJrhjNxJBKYJ-Gij9OTHjKdt0yuPbtMQ#MJMATHI-54" x="1118" y="583"></use></g><g transform="translate(7481,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/yandex/blog/490140/&amp;usg=ALkJrhjNxJBKYJ-Gij9OTHjKdt0yuPbtMQ#MJMATHI-70" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/yandex/blog/490140/&amp;usg=ALkJrhjNxJBKYJ-Gij9OTHjKdt0yuPbtMQ#MJMATHI-75" x="712" y="-213"></use></g><g transform="translate(8490,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/yandex/blog/490140/&amp;usg=ALkJrhjNxJBKYJ-Gij9OTHjKdt0yuPbtMQ#MJMAIN-29" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/yandex/blog/490140/&amp;usg=ALkJrhjNxJBKYJ-Gij9OTHjKdt0yuPbtMQ#MJMAIN-32" x="550" y="583"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/yandex/blog/490140/&amp;usg=ALkJrhjNxJBKYJ-Gij9OTHjKdt0yuPbtMQ#MJMAIN-2B" x="9555" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/yandex/blog/490140/&amp;usg=ALkJrhjNxJBKYJ-Gij9OTHjKdt0yuPbtMQ#MJMATHI-3BB" x="10556" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/yandex/blog/490140/&amp;usg=ALkJrhjNxJBKYJ-Gij9OTHjKdt0yuPbtMQ#MJMAIN-28" x="11139" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/yandex/blog/490140/&amp;usg=ALkJrhjNxJBKYJ-Gij9OTHjKdt0yuPbtMQ#MJMAIN-7C" x="11529" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/yandex/blog/490140/&amp;usg=ALkJrhjNxJBKYJ-Gij9OTHjKdt0yuPbtMQ#MJMAIN-7C" x="11807" y="0"></use><g transform="translate(12086,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/yandex/blog/490140/&amp;usg=ALkJrhjNxJBKYJ-Gij9OTHjKdt0yuPbtMQ#MJMATHI-71" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/yandex/blog/490140/&amp;usg=ALkJrhjNxJBKYJ-Gij9OTHjKdt0yuPbtMQ#MJMATHI-69" x="631" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/yandex/blog/490140/&amp;usg=ALkJrhjNxJBKYJ-Gij9OTHjKdt0yuPbtMQ#MJMAIN-7C" x="12877" y="0"></use><g transform="translate(13155,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/yandex/blog/490140/&amp;usg=ALkJrhjNxJBKYJ-Gij9OTHjKdt0yuPbtMQ#MJMAIN-7C" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/yandex/blog/490140/&amp;usg=ALkJrhjNxJBKYJ-Gij9OTHjKdt0yuPbtMQ#MJMAIN-32" x="393" y="675"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/yandex/blog/490140/&amp;usg=ALkJrhjNxJBKYJ-Gij9OTHjKdt0yuPbtMQ#MJMAIN-2B" x="14110" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/yandex/blog/490140/&amp;usg=ALkJrhjNxJBKYJ-Gij9OTHjKdt0yuPbtMQ#MJMAIN-7C" x="15111" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/yandex/blog/490140/&amp;usg=ALkJrhjNxJBKYJ-Gij9OTHjKdt0yuPbtMQ#MJMAIN-7C" x="15389" y="0"></use><g transform="translate(15668,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/yandex/blog/490140/&amp;usg=ALkJrhjNxJBKYJ-Gij9OTHjKdt0yuPbtMQ#MJMATHI-70" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/yandex/blog/490140/&amp;usg=ALkJrhjNxJBKYJ-Gij9OTHjKdt0yuPbtMQ#MJMATHI-75" x="712" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/yandex/blog/490140/&amp;usg=ALkJrhjNxJBKYJ-Gij9OTHjKdt0yuPbtMQ#MJMAIN-7C" x="16676" y="0"></use><g transform="translate(16954,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/yandex/blog/490140/&amp;usg=ALkJrhjNxJBKYJ-Gij9OTHjKdt0yuPbtMQ#MJMAIN-7C" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/yandex/blog/490140/&amp;usg=ALkJrhjNxJBKYJ-Gij9OTHjKdt0yuPbtMQ#MJMAIN-32" x="393" y="675"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/yandex/blog/490140/&amp;usg=ALkJrhjNxJBKYJ-Gij9OTHjKdt0yuPbtMQ#MJMAIN-29" x="17687" y="0"></use></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><munder><mo movablelimits="true" form="prefix">min</mo><mrow class="MJX-TeXAtom-ORD"><mi>q</mi><mo>‚àó</mo><mo>,</mo><mi>p</mi><mo>‚àó</mo></mrow></munder><munder><mo>‚àë</mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow></munder><mo stretchy="false">(</mo><msub><mi>r</mi><mrow class="MJX-TeXAtom-ORD"><mi>u</mi><mi>i</mi></mrow></msub><mo>‚àí</mo><msup><mrow class="MJX-TeXAtom-ORD"><msub><mi>q</mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi></mrow></msub></mrow><mrow class="MJX-TeXAtom-ORD"><mi>T</mi></mrow></msup><msub><mi>p</mi><mrow class="MJX-TeXAtom-ORD"><mi>u</mi></mrow></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo>+</mo><mi>Œª</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><msub><mi>q</mi><mi>i</mi></msub><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><msup><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mn>2</mn></msup><mo>+</mo><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><msub><mi>p</mi><mi>u</mi></msub><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><msup><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mn>2</mn></msup><mo stretchy="false">)</mo></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-1">\min_{q*,p*} \sum_{i=1}(r_{ui}-{q_{i}}^{T}p_{u})^2+\lambda(||q_i||^2+||p_u||^2)</script></p><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Here r </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ui</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> is the interaction of user u with document i, q </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">i</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> is the vector of document i, p </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">u</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> is the vector of user u. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Then the user vector that is optimal from the point of view of the mean square error (for fixed document vectors) is found analytically by solving the corresponding linear regression. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
This is called an ALS step. And the ALS algorithm itself consists in the fact that we alternately fix one of the matrices (users and articles) and update the other, finding the optimal solution.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Fortunately, finding a user's vector representation is a fairly quick operation that can be done in runtime using vector instructions. </font><font style="vertical-align: inherit;">This trick allows you to immediately take into account the user's feedback in the ranking. </font><font style="vertical-align: inherit;">The same embedding can be used in the kNN index to improve the selection of candidates.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Distributed Collaborative Filtering</font></font></h2><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">How to do incremental distributed matrix factorization and quickly find a vector representation of new articles?</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Content is not the only source of signals for recommendations. Collaborative information is another important source. Good signs in ranking can traditionally be obtained from the decomposition of the user-document matrix. But when trying to do this decomposition, we ran into problems: </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
1. We have millions of documents and tens of millions of users. The matrix does not fit entirely on one machine, and the decomposition will be very long. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
2. Most of the content in the system has a short lifetime: documents remain relevant for only a few hours. Therefore, it is necessary to construct their vector representation as quickly as possible.</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
3. If you build the decomposition immediately after the publication of the document, a sufficient number of users will not have time to evaluate it. Therefore, its vector representation is likely to be not very good. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
4. If the user likes or dislikes, we will not be able to immediately take this into account in the expansion. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
To solve these problems, we implemented a distributed decomposition of the user-document matrix with a frequent incremental update. How exactly does it work? </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Suppose we have a cluster of N machines (N in the hundreds) and we want to make a distributed decomposition of the matrix on them, which does not fit on one machine. The question is how to perform this decomposition so that, on the one hand, there is enough data on each machine and, on the other, so that the calculations are independent?</font></font><br>
<br>
<img src="https://habrastorage.org/webt/kg/sh/fy/kgshfyss_p2rj48ecnb3rn9scvc.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We will use the ALS decomposition algorithm described above. Consider how to perform one ALS step in a distributed manner - the rest of the steps will be similar. Suppose we have a fixed matrix of documents and we want to build a matrix of users. To do this, we divide it into N parts in rows, each part will contain approximately the same number of rows. We will send to each machine non-empty cells of the corresponding lines, as well as a matrix of document embeddings (in whole). Since it is not very large, and the user-document matrix is ‚Äã‚Äãusually very sparse, this data will fit on a regular machine.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Such a trick can be repeated for several eras until the model converges, alternately changing the fixed matrix. But even then, the decomposition of the matrix can last several hours. And this does not solve the problem of the need to quickly receive embeddings of new documents and update embeddings of those about which there was little information when building the model.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The introduction of a quick incremental update of the model helped us. Suppose we have a current trained model. Since her training, new articles have appeared with which our users have interacted, as well as articles that have had little interaction with the training. To quickly embed such articles, we use user embeddings obtained during the first large model training and take one ALS step to calculate the matrix of documents with a fixed matrix of users. This allows you to receive embeddings quite quickly - within a few minutes after the publication of a document - and often update embeddings of fresh documents.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In order to immediately take into account human actions for recommendations, in runtime we do not use user embeds received offline. </font><font style="vertical-align: inherit;">Instead, we take the ALS step and get the current user vector.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Transfer to another domain area</font></font></h2><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">How to use a user's feedback to text articles to build a vector representation of a video? </font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Initially, we recommended only text articles, so many of our algorithms are focused on this type of content. </font><font style="vertical-align: inherit;">But when adding content of a different type, we were faced with the need to adapt models. </font><font style="vertical-align: inherit;">How did we solve this problem using the example video? </font><font style="vertical-align: inherit;">One option is to retrain all models from scratch. </font><font style="vertical-align: inherit;">But this is a long time, besides, some of the algorithms are demanding on the volume of the training sample, which is not yet in the right quantity for a new type of content in the first moments of its life on the service.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We went the other way and reused text models for the video. </font><font style="vertical-align: inherit;">In creating vector representations of the video, the same trick with ALS helped us. </font><font style="vertical-align: inherit;">We took a vector representation of users based on text articles and took the ALS step using information about video views. </font><font style="vertical-align: inherit;">So we easily got a vector representation of the video. </font><font style="vertical-align: inherit;">And in runtime, we simply calculate the proximity between the user vector obtained from text articles and the video vector.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Conclusion</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The development of the core of a real-time recommender system is fraught with many tasks. </font><font style="vertical-align: inherit;">It is necessary to quickly process data and apply ML methods to effectively use this data; </font><font style="vertical-align: inherit;">Build complex distributed systems capable of processing user signals and new content units in a minimum amount of time; </font><font style="vertical-align: inherit;">and many other tasks.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In the current system, the device of which I described, the quality of recommendations for the user grows with its activity and the duration of the service. </font><font style="vertical-align: inherit;">But of course, here lies the main difficulty: it is difficult for the system to immediately understand the interests of a person who interacted little with content. </font><font style="vertical-align: inherit;">Improving recommendations for new users is our key concern. </font><font style="vertical-align: inherit;">We will continue to optimize the algorithms so that the content relevant to the person gets into his feed faster and the irrelevant does not appear.</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../en490120/index.html">Study in bit tones</a></li>
<li><a href="../en490126/index.html">Types of e-payments licenses in Singapore and how to get them?</a></li>
<li><a href="../en490130/index.html">Antiquities: digital cassette as an audiophile format</a></li>
<li><a href="../en490132/index.html">Rust 1.41.1 Release: Corrective Release</a></li>
<li><a href="../en490138/index.html">The use of R in the task of updating cash register software</a></li>
<li><a href="../en490142/index.html">AnalogBytes Conference: Section One, Technological Foundations of Democracy</a></li>
<li><a href="../en490152/index.html">Brad Templeton: how I see the future of robotic vehicles</a></li>
<li><a href="../en490154/index.html">Genetics of origin. Haplogroups</a></li>
<li><a href="../en490156/index.html">Disaster Recovery and migration using VMware vCloud Availability. Part 1</a></li>
<li><a href="../en490158/index.html">Robotic vehicles in numbers: 3 trillion miles, 50 billion hours, 8% of GDP</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>