<!doctype html>
<html class="no-js" lang="ja">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🎭 🥐 🖐️ 汎用人工知能。TK、現状、見通し 🤷🏽 🧒 🔏</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="現在、「人工知能」という言葉は、画像認識用のニューラルネットワークからQuakeをプレイするためのボットまで、さまざまなシステムを意味しています。ウィキペディアはAIの素晴らしい定義を提供します-これは「伝統的に人間の特権であると考えられている創造的な機能を実行するインテリジェントシステムの特性」で...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>汎用人工知能。TK、現状、見通し</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/468379/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">現在、「人工知能」という言葉は、画像認識用のニューラルネットワークからQuakeをプレイするためのボットまで、さまざまなシステムを意味しています。</font><font style="vertical-align: inherit;">ウィキペディアはAIの素晴らしい定義を提供します-これは「伝統的に人間の特権であると考えられている創造的な機能を実行するインテリジェントシステムの特性」です。</font><font style="vertical-align: inherit;">つまり、定義から明らかです。特定の機能が正常に自動化された場合、人工知能とは見なされなくなります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
しかし、「人工知能を作成する」というタスクが最初に設定されたとき、AIは別のことを意味しました。</font><font style="vertical-align: inherit;">この目標は現在、Strong AIまたはGeneral Purpose AIと呼ばれています。</font></font><br>
<a name="habracut"></a><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">問題の定式化</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
今、問題の2つのよく知られた公式があります。 1つ目は、Strong AIです。 2つ目は、汎用AI（別名Artifical General Intelligence、略してAGI）です。</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
更新。コメントで、彼らはこの違いが言語のレベルでよりありそうであると私に話します。ロシアでは、単語「知性」は英語で単語「知性」という意味ではありません</font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">強いAIは</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">人が行うことができることすべてを行うことができ架空のAIです。彼は通常、初期設定でチューリングテストに合格する必要があると言われています（うーん、人々はそれに合格しますか？）。自分自身を別人として認識し、自分の目標を達成できるようにしてください。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
つまり、人工的な人のようなものです。私の意見では、そのようなAIの有用性は主に研究です。なぜなら、強力なAIの定義は、その目的が何であるかをどこにも述べていないためです。</font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">AGIまたは汎用AI</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">-これは「結果の機械」です。入力で特定の目標設定を受け取り、モーター/レーザー/ネットワークカード/モニターにいくつかの制御アクションを提供します。そして、目標が達成されます。同時に、AGIは当初、環境に関する知識がなく、センサー、アクチュエーター、およびAGIが目標を設定するチャネルのみを認識しています。管理システムは、任意の環境で任意の目標を達成できる場合、AGIと見なされます。私たちは彼女に車を運転させて事故を避けます-彼女はそれを扱います。私たちは彼女を原子炉の制御下に置き、より多くのエネルギーがあるようにしますが、爆発しません-彼女はそれを処理できます。郵便受けを渡し、掃除機の販売を指導します-対応もいたします。 AGIは「逆問題」のソルバーです。販売されている掃除機の数を確認することは簡単です。しかし、この掃除機を買うように人を説得する方法を理解することはすでに知性の仕事です。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
この記事では、AGIについて説明します。</font><font style="vertical-align: inherit;">チューリングテスト、自己認識、人工パーソナリティはありません-非常に実用的なAIとそれほど実用的な演算子はありません。</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">現状</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
現在、強化学習や強化学習などのシステムのクラスがあります。これはAGIのようなもので、汎用性はありません。彼らは学ぶことができ、それによって多種多様な環境で目標を達成することができます。しかし、それでも彼らはどんな環境でも目標を達成するのにはほど遠い。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
一般に、強化学習システムはどのように配置され、それらの問題は何ですか？</font></font><br>
<br>
<img src="https://habrastorage.org/webt/oj/3-/dl/oj3-dl6vkgtbhaxeilj7rkl0jxe.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
どのRLもこのように配置されます。制御システムがあり、センサー（状態）を介して周囲の現実に関するいくつかの信号が入り、統治体（アクション）を介して周囲の現実に作用します。報酬は強化の合図です。 RLシステムでは、補強は制御ユニットの外側から形成され、AIが目標の達成にどれだけうまく対応しているかを示します。たとえば、直前に何台の掃除機が売れたか。</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
次に、テーブルは次のようなもので構成されます（これをSARテーブルと呼びます）。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/lp/oo/ek/lpooekbdpwktkkmvsilzsytsnbo.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
時間軸は下向きです。</font><font style="vertical-align: inherit;">テーブルには、AIが行ったすべて、彼が見たすべて、およびすべての強化信号が表示されます。</font><font style="vertical-align: inherit;">通常、RLが意味のあることを行うには、最初にしばらくランダムな動きをするか、他の人の動きを見る必要があります。</font><font style="vertical-align: inherit;">一般に、RLは、SARテーブルにすでに少なくとも数行あるときに開始します。</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
次は何が起こる？</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">サルサ</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
強化学習の最も単純な形。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ある種の機械学習モデルを採用し、SとA（状態とアクション）の組み合わせを使用して、次の数ティックの合計Rを予測します。たとえば、（上記の表に基づいて）女性に「男性になって、掃除機を買う！」と伝えると、報酬は低くなり、同じことを男性に言えば、高くなります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
使用できる特定のモデル-後で説明しますが、今のところ、これはニューラルネットワークだけではないということを述べておきます。決定木を使用したり、関数をテーブル形式で定義したりすることもできます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
そして、次のことが起こります。 AIは別のメッセージまたは別のクライアントへのリンクを受信します。すべての顧客データは外部からAIに入力されます-私たちは顧客ベースとメッセージカウンターをセンサーシステムの一部と見なします。つまり、一部のA（アクション）を割り当てて、増援を待つ必要があります。 AIはすべての可能なアクションを実行し、次に（同じ機械学習モデルを使用して）予測します-これを行うとどうなりますか？もしそうなら？そして、これにはどのくらいの補強がありますか？そして、RLは最大の報酬が期待されるアクションを実行します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
そのようなシンプルで不器用なシステムをゲームに取り入れました。 SARSAはゲーム内のユニットを雇用し、ゲームのルールが変更された場合に適応します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
さらに、強化されたトレーニングのすべてのタイプで、報酬の割引と探検/悪用のジレンマがあります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
報酬の割引は、RLが次のN移動の報酬量ではなく、「1年で100ルーブルは110よりも優れている」という原則に従って加重された金額を最大化しようとする場合のアプローチです。たとえば、割引係数が0.9で、計画期間が3の場合、次の3クロックサイクルの合計Rではなく、R1 * 0.9 + R2 * 0.81 + R3 * 0.729でモデルをトレーニングします。なぜこれが必要なのですか？次に、そのAIは、無限のどこかで利益を生み出すために必要ありません。今ここで利益を生み出すAIが必要です。</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ジレンマを探る/活用する。 RLがそのモデルが最適と考えるものを実行する場合、より良い戦略があったかどうかは決してわかりません。エクスプロイトは、RLが最大の報酬を約束することを行う戦略です。探索は、RLがより良い戦略を求めて環境を探索するために何かを行う戦略です。効果的なインテリジェンスを実装するには？たとえば、数小節ごとにランダムなアクションを実行できます。または、1つの予測モデルを作成するのではなく、わずかに異なる設定でいくつかを作成することもできます。それらは異なる結果を生成します。差が大きいほど、このオプションの不確実性の程度は大きくなります。アクションが最大値になるようにアクションを選択できます：M + k * std、ここでMはすべてのモデルの平均予測、stdは予測の標準偏差、そしてkは好奇心の係数です。</font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">欠点は何ですか？</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
オプションがあるとしましょう。ゴール（私たちから10 kmですが、道は良好です）に車で行くか、徒歩で行きます。そして、この選択の後、私たちは選択肢を持っています-注意深く移動するか、各柱にぶつかるようにしてください。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
その人はすぐに、通常は車を運転して注意深く振る舞うほうがよいと言います。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
しかし、SARSA ...彼は車で行くという決定が以前に何に導いたかについて調べます。</font><font style="vertical-align: inherit;">しかし、それはこれにつながりました。</font><font style="vertical-align: inherit;">最初の統計セットの段階で、AIは無謀に運転し、ケースの半分でどこかでクラッシュしました。</font><font style="vertical-align: inherit;">はい、彼はうまく運転できます。</font><font style="vertical-align: inherit;">しかし、彼が車で行くかどうかを選ぶとき、彼は彼が次の動きを何を選ぶかを知りません。</font><font style="vertical-align: inherit;">彼は統計を持っている-そして半分のケースで彼は適切なオプションを選び、そして半分-自殺的である。</font><font style="vertical-align: inherit;">したがって、平均して歩く方が良いです。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
SARSAは、エージェントがテーブルの作成に使用されたのと同じ戦略に従うと信じています。</font><font style="vertical-align: inherit;">そして、これに基づいて行動します。</font><font style="vertical-align: inherit;">しかし、それ以外の場合はどうなるでしょうか。エージェントが次の動きで最良の戦略を実行するとします。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Qラーニング</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
このモデルは、州ごとに、州から得られる最大の合計報酬を計算します。そして、彼はそれを特別な列Qに書き込みます。つまり、移動に応じて状態Sから2ポイントまたは1を取得できる場合、Q（S）は2に等しくなります（予測深度は1）。状態Sからどのような報酬を得ることができるか、予測モデルY（S、A）から学びます。 （S-状態、A-アクション）。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
次に、予測モデルQ（S、A）を作成します。つまり、SからアクションAを実行した場合にQがどのような状態になるかを示します。そして、テーブルの次の列Q2を作成します。つまり、状態Sから取得できる最大のQ（可能なすべてのAをソートします）。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
次に、回帰モデルQ3（S、A）を作成します。つまり、SからアクションAを実行した場合にQ2が移動する状態になります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
等。したがって、予測の深さを無制限に達成できます。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/ie/e_/gn/iee_gnm9ldz5uiv0dmyj4jni470.jpeg"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
写真のRは増援です。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
そして、すべての動きは、最大のQnを約束するアクションを選択します。このアルゴリズムをチェスに適用すると、理想的なミニマックスのようなものが得られます。計算を誤って計算するのとほとんど同じことは、非常に深くなります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
q学習動作の一般的な例。ハンターには槍があり、自分の主導で彼と一緒にクマに行きます。彼は彼の将来の動きの大部分が非常に大きな負の報酬（勝つ方法よりも失う方法がはるかに多い）があることを知っており、肯定的な報酬のある動きがあることを知っています。ハンターは将来彼が最高の動きをするだろうと信じており（そしてSARSAでどのような動きをするかは不明です）、彼が最高の動きをした場合、彼はクマを倒すでしょう。つまり、クマに行くためには、狩りに必要なすべての要素を実行できれば十分ですが、すぐに成功する経験は必要ありません。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ハンターがSARSAスタイルで行動した場合、彼は将来の行動は以前とほぼ同じであると想定し（現在、知識の手荷物が異なるという事実にもかかわらず）、彼はすでに行った場合にのみクマに行きますたとえば、彼は&gt; 50％のケースで勝利しました（まあ、他のハンターが経験から学んだ場合、半分以上のケースで勝利した場合）。</font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">欠点は何ですか？</font></font></b> <br>
<br>
<ol>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">モデルは変化する現実に対応していません。</font><font style="vertical-align: inherit;">私たちの人生全体が赤いボタンを押すことで授与され、今彼らは私たちを罰している、そして目に見える変化は起こらなかった場合... QLは非常に長い間このパターンを習得します。</font></font></li>
<li>Qn     . ,        N  —    .        —       ,           .</li>
<li>     . ,   ?   ,     ,       —    .     , ,       ,   - .  QL    — ,   -----    S,    .    RL,     .  ,       —           Qn.</li>
</ol><br>
<h3>Model-based </h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
しかし、RまたはQだけでなく、一般にすべての感覚データを予測するとどうなるでしょうか。私たちは常に現実のポケットコピーを手に入れ、その計画を確認することができます。この場合、Q関数の計算の難しさについてはそれほど心配していません。はい、それは計算するのに多くの時計を必要とします-とにかく、とにかく、それぞれの計画のために、我々は繰り返し予測モデルを実行します。計画10は前進しますか？モデルを10回起動し、そのたびに出力を入力にフィードします。</font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">欠点は何ですか？</font></font></b> <br>
<br>
<ol>
<li>. ,          .   10     2^10=1024  .   —  10  .     ,     ?        0.1 ?         ?       ,        .   -    —      ,   QL.</li>
<li> .    ,             .   ,      — - .     ,      ,        .</li>
</ol><br>
<h2>  </h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
AIのテスト環境にアクセスできる場合、実際には実行せずにシミュレーションで実行すると、エージェントの動作の戦略を何らかの形で書き留めることができます。次に、進化またはその他の方法で、最大の利益につながる戦略を選択します。</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
「戦略を選択する」とは、まず、戦略を進化アルゴリズムにプッシュできるように戦略を書き留める方法を学ぶ必要があることを意味します。つまり、戦略をプログラムコードで書き留めることができますが、一部の場所では係数を残し、進化させてそれらを取得します。または、ニューラルネットワークを使用して戦略を書き留め、進化によってその接続の重みをピックアップすることもできます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
つまり、ここでは予測はありません。 SARテーブルがありません。戦略を選択するだけで、すぐにアクションが実行されます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
これは強力で効果的な方法です。RLを試してみて、どこから始めればよいかわからない場合は、この方法をお勧めします。</font><font style="vertical-align: inherit;">これは「奇跡を見る」ための非常に安価な方法です。</font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">欠点は何ですか？</font></font></b><br>
<br>
<ol>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">同じ実験を何度も実行する機能が必要です。</font><font style="vertical-align: inherit;">つまり、現実を最初の数万回まで巻き戻すことができるはずです。</font><font style="vertical-align: inherit;">新しい戦略を試す。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
人生はめったにそのような機会を提供しません。</font><font style="vertical-align: inherit;">通常、関心のあるプロセスのモデルがある場合、狡猾な戦略を作成することはできません。モデルベースのアプローチのように、ダムの検索でも計画を立てることができます。</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">経験に対する不寛容。</font><font style="vertical-align: inherit;">長年の経験のSARチャートはありますか？</font><font style="vertical-align: inherit;">私たちはそれを忘れることができます、それは概念に適合しません。</font></font></li>
</ol><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">戦略を列挙する方法、ただし「ライブ」</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
戦略の同じ列挙ですが、生きている現実について。 1つの戦略の10のメジャーを試します。それから10は別のものを測定します。次に、3番目の10小節。次に、補強が強化されたものを選択します。</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
この方法で、人型歩行の最良の結果が得られました。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/zh/v_/sn/zhv_snutr8ma1aeenqr3itjojvc.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
私にとって、これはやや予想外に聞こえます。QL+モデルベースのアプローチが数学的に理想的であるように思われます。しかし、そのようなことは何もありません。このアプローチの利点は前のものとほぼ同じですが、戦略はあまり長くテストされていないため（つまり、進化には何千年もかかっていないため）、結果は不安定であるため、それほど明白ではありません。さらに、テストの数も無限に上げることはできません。つまり、それほど複雑ではないオプションの領域で戦略を模索する必要があります。彼女には、「ねじれる」ことができる「ペン」はほとんどありません。まあ、経験不寛容はキャンセルされていません。また、QLまたはモデルベースと比較して、これらのモデルは経験を非効率的に使用します。機械学習を使用するアプローチよりも、現実とのより多くの相互作用が必要です。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ご覧のとおり、理論的にAGIを作成するすべての試みには、アワードを予測するための機械学習、または戦略の何らかの形式のパラメトリック記録が含まれている必要があります。これにより、進化などの方法でこの戦略を選択できます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
これは、データベース、ロジック、および概念グラフに基づいてAIを作成することを提案する人々に対する強力な攻撃です。</font><font style="vertical-align: inherit;">シンボリックアプローチの支持者であるあなたがこれを読んだ場合、コメントを歓迎します。AGIが上記のメカニズムなしで何ができるかを知りたいと思います。</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">RLの機械学習モデル</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ほとんどすべてのMLモデルを強化学習に使用できます。もちろん、ニューラルネットワークは優れています。しかし、たとえば、KNNがあります。 SとAの各ペアについて、最も類似しているものを探しますが、過去のものです。そして、私たちはその後R.愚かなものを探していますか？はい、でも動作します。決定的な木があります。ここでは、「勾配の急増」と「決定的な森」というキーワードで散歩するのがよいでしょう。木は複雑な依存関係を捉えるのが苦手ですか？特徴生成を使用します。 AIを一般に近づけたいですか？自動FEを使用してください！一連のさまざまな数式を実行し、それらをブーストの機能として送信し、エラーを増加させる数式を破棄し、精度を向上させる数式を残します。次に、新しい数式の引数として最適な数式を送信します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
予測にシンボリック回帰を使用できます。つまり、QまたはRに近似するものを取得するために数式を並べ替えます。アルゴリズムを並べ替えることができます。次に、理論的に最適ですが、トレーニングするのがほとんど非常に難しいSolomonovの帰納法と呼ばれるものが得られます。関数の近似。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
しかし、ニューラルネットワークは通常、表現力と学習の複雑さの間の妥協点です。</font><font style="vertical-align: inherit;">アルゴリズム回帰は、理想的には何百年にもわたって依存関係を検出します。</font><font style="vertical-align: inherit;">決定木はすぐにうまくいきますが、y = a + bは外挿できません。</font><font style="vertical-align: inherit;">ニューラルネットワークはその中間にあります。</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">開発見通し</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
今正確にAGIを行う方法は何ですか？</font><font style="vertical-align: inherit;">少なくとも理論的には。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">進化</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
多くの異なるテスト環境を作成し、ニューラルネットワークの進化を開始できます。</font><font style="vertical-align: inherit;">すべてのトライアルで合計でより多くのポイントを獲得する構成は倍増します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ニューラルネットワークにはメモリが必要であり、メモリの少なくとも一部を、チューリングマシンなどのハードディスク上のテープの形で持つことが望ましいでしょう。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
問題は、進化の助けを借りて、もちろんRLのようなものを成長できることです。</font><font style="vertical-align: inherit;">しかし、RLがコンパクトに見えるように言語がどのように見えるべきか-進化がそれを見つけるために-と同時に、進化は「150層のニューロンを作成して、私が教える間、あなたはすべてナッツになるようにします！」などの解決策を見つけません。 。</font><font style="vertical-align: inherit;">進化は文盲のユーザーの群れのようなものです。コードの欠陥を見つけてシステム全体を捨ててしまいます。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">アイシー</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
多くのアルゴリズム回帰のパックに基づいてモデルベースシステムを作成できます。</font><font style="vertical-align: inherit;">アルゴリズムは完全なチューリングであることが保証されています。つまり、取得できないパターンはありません。</font><font style="vertical-align: inherit;">アルゴリズムはコードで記述されています。つまり、その複雑さは簡単に計算できます。</font><font style="vertical-align: inherit;">したがって、複雑さについて、世界のデバイスの仮説を数学的に正しく微調整することが可能です。</font><font style="vertical-align: inherit;">たとえば、ニューラルネットワークでは、このトリックは機能しません-複雑さのペナルティは非常に間接的で発見的です。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
アルゴリズム回帰をすばやくトレーニングする方法を学ぶだけです。</font><font style="vertical-align: inherit;">これまでのところ、これに最適なのは進化であり、許されないほど長いです。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">シードAI</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
自分自身を改善するAIを作成するのは素晴らしいことです。問題を解決する能力を向上させます。これは奇妙な考えのように思えるかもしれませんが、この問題</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">は、進化などの静的最適化システムではすでに解決されています</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。これをなんとか実現したら…出展者のことはすべて知っていますか？非常に強力なAIが非常に短時間で得られます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
どうやってするの？</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
RLでいくつかのアクションがRL自体の設定に影響するように調整することができます。</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
または、RLシステムに自分用の新しいデータ処理前後のプロセッサを作成するためのツールを提供します。 RLは馬鹿げているが、計算機、ノートブック、コンピューターを作成できるようになる。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
別のオプションは、アクションを使用してコードレベルでデバイスに影響を与える進化を使用して、ある種のAIを作成することです。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
しかし、現時点では、シードAIの実用的なオプションは見ていません。</font><font style="vertical-align: inherit;">開発者は隠れていますか？</font><font style="vertical-align: inherit;">または、これらのオプションはあまりにも弱いので、一般的な注意に値するものではなく、私を通り過ぎましたか？</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ただし、現在はGoogleとDeepMindの両方が主にニューラルネットワークアーキテクチャで動作しています。</font><font style="vertical-align: inherit;">どうやら、彼らは組み合わせ列挙に関与したくなく、自分の考えをエラーを逆伝播する方法に適したものにしようとはしていません。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
このレビュー記事がお役に立てば幸いです=）コメント、特に「AGIを改善する方法を知っている」などのコメントを歓迎します！</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../ja468363/index.html">モバイルゲームの世界からの私のマグナムオーパス</a></li>
<li><a href="../ja468367/index.html">アマゾンが地球温暖化計画を発表</a></li>
<li><a href="../ja468369/index.html">多くの8ビットゲームのパロディーである「WildMAN」を作成し、最近Androidに移植した方法</a></li>
<li><a href="../ja468371/index.html">人生にゲームデザイン。God of War 4へのシームレスなダウンロードまたは完全な没入</a></li>
<li><a href="../ja468377/index.html">中国内部についての8つの物語。外国人に見せられないもの</a></li>
<li><a href="../ja468381/index.html">バック・トゥ・ザ・フューチャー？量子遅延消しゴム</a></li>
<li><a href="../ja468383/index.html">言語への関心を引き付けるRubyミームジェネレータ</a></li>
<li><a href="../ja468385/index.html">デスクトップは死んでいます、デスクトップは長生きします！私は統計学を収集します</a></li>
<li><a href="../ja468387/index.html">モバイル＃316開発者向けの興味深い資料のダイジェスト（9月16〜22日）</a></li>
<li><a href="../ja468389/index.html">STO Bureau BureauのArtyom Galonsky：「私はDevOpsエンジニアのようなことに反対しています」</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>