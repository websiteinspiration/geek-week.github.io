<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚òÑÔ∏è üìê üèâ Multi-tagged classification üë©üèΩ‚Äçüè´ ü§ûüèº ü§õüèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello, habrozhiteli! We decided to cite an excerpt from the book by Andrei Burkov , Machine Learning Without Extra Words , dedicated to classification...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Multi-tagged classification</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/488362/"><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><img src="https://habrastorage.org/webt/hm/vf/c_/hmvfc_yyxepplv1mj0crw1vu7pw.jpeg" align="left" alt="image"></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hello, habrozhiteli! </font><font style="vertical-align: inherit;">We decided to cite an excerpt from the book by Andrei Burkov </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, Machine Learning Without Extra Words</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , dedicated to classification.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
To describe the image in the figure, several labels can be used simultaneously: ‚Äúconiferous forest‚Äù, ‚Äúmountains‚Äù, ‚Äúroad‚Äù. If the number of possible values ‚Äã‚Äãfor labels is large, but they all have the same nature as tags, each tagged sample can be converted to several tagged data, one for each tag. All this new data will have the same feature vectors and only one label. As a result, the task becomes a multiclass classification problem. It can be solved using the ‚Äúone against all‚Äù strategy. The only difference from the usual multiclass classification problem is the appearance of a new hyperparameter: the threshold. If the similarity score for a label is above a threshold value, this label is assigned to the input feature vector. In this scenario, multiple labels can be assigned to one characteristic vector.The threshold value is selected using the control set.</font></font><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
To solve the classification problem with many labels, one can similarly apply algorithms that are naturally converted to multiclass (decision trees, logistic regression, neural networks, etc.). They return an estimate for each class, so we can define a threshold and then assign several labels to one feature vector for which the proximity score exceeds this threshold. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Neural networks can naturally be trained in multi-label classifications using binary cross-entropy as a cost function. The output layer of the neural network in this case has one node per label. Each node in the output layer has a sigmoid activation function. Accordingly, each label l is binary</font></font><img src="https://habrastorage.org/webt/2n/_n/mg/2n_nmgvuciuadryomnq3urrg1xw.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">where l = 1, ..., L and i = 1, ..., N. Binary cross-entropy determines the probability </font></font><img src="https://habrastorage.org/webt/lb/le/cf/lblecfgyuy23k8zvlasuajncsds.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">that the sample xi has the label l, is defined as the </font></font><img src="https://habrastorage.org/webt/qd/1e/fx/qd1efxpmi6mmuhv_fhmsqpjgpi4.jpeg" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Minimization criterion - a simple average of all members of the binary cross-entropy in all training samples and all their tags. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In cases where the number of possible label values ‚Äã‚Äãis small, you can try to convert the classification problem with many labels into a multiclass classification problem. </font><font style="vertical-align: inherit;">Imagine the following problem. </font><font style="vertical-align: inherit;">You need to assign two types of labels to images. </font><font style="vertical-align: inherit;">Labels of the first type can have two possible meanings: { </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">photo, painting</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> }; </font><font style="vertical-align: inherit;">marks of the second type can have three possible meanings: { </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">portrait, landscape, other</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">}. </font><font style="vertical-align: inherit;">For each combination of two source classes, you can create a new dummy class, for example:</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/mz/px/zn/mzpxzn0rlrumwoql7gkk3no7ihk.jpeg" alt="image"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Now we have the same tagged data, but we replaced the set of true labels with one dummy label with values ‚Äã‚Äãfrom 1 to 6. In practice, this approach gives good results when there are not too many possible combinations of classes. Otherwise, much more training data needs to be used to compensate for the increase in the set of classes. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The main advantage of this latter approach is that the labels remain correlated, unlike the methods described above, which predict each label independently of each other. In many tasks, correlation between labels can be a significant factor. For example, imagine you want to classify email as </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">spam</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> and </font><i><font style="vertical-align: inherit;">non- </font></i><i><font style="vertical-align: inherit;">spam</font></i></font><i><font style="vertical-align: inherit;"></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, and at the same time as ordinary and important. </font><font style="vertical-align: inherit;">You would probably want to exclude forecasts such as [ </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">spam, important</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ].</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">7.5. </font><font style="vertical-align: inherit;">Ensemble training</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The fundamental algorithms that we covered in Chapter 3 have their limitations. Because of its simplicity, sometimes they cannot create a model that is effective enough for your task. In such cases, you can try to use deep neural networks. However, in practice, deep neural networks require a significant amount of labeled data, which you may not have. Another way to increase the effectiveness of simple learning algorithms is to use </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ensemble training</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ensemble training is a training paradigm that is based on training not just one super-correct model, but a large number of models with low accuracy and combining the forecasts given by these </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">weak</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> models to obtain a more correct </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">metamodel</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Models with low accuracy are usually trained by </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">weak learning algorithms</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> that are not able to train complex models and therefore show high speed at the training and forecasting stages. Most often, the decision tree learning algorithm is used as the weak algorithm, which usually stops breaking the training set after several iterations. The result is small and not very regular trees, but, as the idea of ‚Äã‚Äãlearning the ensemble says, if the trees are not identical and each tree is at least slightly better than random guessing, we can get high accuracy by combining a large number of such trees. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
To get the final forecast for entry </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, forecasts of all weak models are combined using some method of weighted voting. </font><font style="vertical-align: inherit;">The specific form of weighting the votes depends on the algorithm, but the essence does not depend on it: if, collectively, weak models predict that the email is spam, we assign </font><font style="vertical-align: inherit;">the </font><i><font style="vertical-align: inherit;">spam</font></i><font style="vertical-align: inherit;"> label </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> to the sample </font><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">
The two main methods for training ensembles are </font><b><font style="vertical-align: inherit;">boosting</font></b><font style="vertical-align: inherit;"> and </font><b><font style="vertical-align: inherit;">bagging</font></b><font style="vertical-align: inherit;"> (aggregation). </font><font style="vertical-align: inherit;">Translations of the terms boosting and bagging are inaccurate and not accustomed.</font></font><i><font style="vertical-align: inherit;"></font></i><font style="vertical-align: inherit;"></font><br>
<br><font style="vertical-align: inherit;"></font><b><font style="vertical-align: inherit;"></font></b><font style="vertical-align: inherit;"></font><b><font style="vertical-align: inherit;"></font></b><font style="vertical-align: inherit;"></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">7.5.1. </font><font style="vertical-align: inherit;">Boosting and Bagging</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The boosting method is to use the initial training data and iteratively create several models using a weak algorithm. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Each new model differs from the previous ones in that, constructing it, a weak algorithm tries to ‚Äúfix‚Äù the errors made by previous models. </font><font style="vertical-align: inherit;">The final ensemble model is a combination of these many weak iteratively constructed models. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The essence of bagging is to create a lot of ‚Äúcopies‚Äù of training data (each copy is slightly different from the others) and then apply a weak algorithm to each copy in order to obtain several weak models, and then combine them. </font><font style="vertical-align: inherit;">A widely used and efficient machine learning algorithm based on the idea of ‚Äã‚Äãbagging is a </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">random forest</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">7.5.2. </font><font style="vertical-align: inherit;">Random forest</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The ‚Äúclassic‚Äù bagging algorithm works as follows. </font><font style="vertical-align: inherit;">B random samples are created from the existing training set </font></font><img src="https://habrastorage.org/webt/-d/6r/gn/-d6rgnsn5wk-yzl3dbk0i7x2ole.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(for each b = 1, ..., B) and a </font><font style="vertical-align: inherit;">decision tree </font></font><img src="https://habrastorage.org/webt/-d/6r/gn/-d6rgnsn5wk-yzl3dbk0i7x2ole.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">model is built </font><font style="vertical-align: inherit;">on the basis of each sample </font></font><img src="https://habrastorage.org/webt/nd/ew/4z/ndew4zvx7r0jpeilfknakojmrbs.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">To get a sample </font></font><img src="https://habrastorage.org/webt/-d/6r/gn/-d6rgnsn5wk-yzl3dbk0i7x2ole.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">for some b, a </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">sample is made with replacement</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">That is, first an empty sample is created, and then a random sample is selected from the training set, and its exact copy is placed in </font></font><img src="https://habrastorage.org/webt/-d/6r/gn/-d6rgnsn5wk-yzl3dbk0i7x2ole.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, while the sample itself remains in the original training set. </font><font style="vertical-align: inherit;">The selection of data continues until the condition is fulfilled. </font></font><img src="https://habrastorage.org/webt/se/au/-5/seau-5gwous1c1cshmrx8rwubig.jpeg" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As a result of training, </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">B</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> decision trees are </font><font style="vertical-align: inherit;">obtained </font><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">The forecast for the new sample </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , in the case of regression, is determined as the average of </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">B</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> forecasts</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/re/zp/mj/rezpmjqa9lo7w4dxvqd6njwwqcm.jpeg" alt="image"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
or by a majority vote in case of classification.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Random forest has only one difference from classic bagging. It uses a modified tree learning algorithm that, with each splitting in the learning process, checks a random subset of features. This is done in order to eliminate the correlation between trees: if one or more features have a large predictive ability, many trees will choose them for splitting data. This will lead to the appearance in the "forest" of a large number of correlated trees. Sign correlation with high predictive ability prevents the prediction accuracy from increasing. The high efficiency of the ensemble of models is explained by the fact that good models are most likely to agree with the same forecast, and bad models are not likely to agree and will give different forecasts. Correlation will make poor models more likely to agree,which will distort the voting pattern or affect the average.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The most important hyperparameters for tuning are the number of trees B and the size of a random subset of features that must be considered for each splitting. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Random forest is one of the most widely used ensemble learning algorithms. What determines its effectiveness? The reason is that by using several samples from the original dataset, we reduce the </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">variance of the</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> final model. Remember that low variance means a weak predisposition to </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">retrain</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Retraining occurs when the model tries to explain small variations in the data set because the data set is just a small sample of all possible examples of the phenomenon that we are trying to simulate. </font><font style="vertical-align: inherit;">In the case of an unsuccessful approach to the formation of the training set, some undesirable (but inevitable) artifacts may fall into it: noise, abnormal and excessively or insufficiently representative data. </font><font style="vertical-align: inherit;">By creating several random samples with the replacement of the training set, we reduce the influence of these artifacts.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">7.5.3. </font><font style="vertical-align: inherit;">Gradient Boost</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Another effective ensemble training algorithm based on the idea of ‚Äã‚Äãboosting is gradient boosting. </font><font style="vertical-align: inherit;">First, consider the use of gradient boosting in regression. </font><font style="vertical-align: inherit;">We will start building an effective regression model with a constant model </font></font><img src="https://habrastorage.org/webt/7u/7x/jd/7u7xjdufljpsjwwu45j9_gkc3r0.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(as we did in ID3):</font></font><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/em/vj/bv/emvjbvtmptxl_d3bihzev4wbh7o.jpeg" alt="image"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Then change the labels in all samples i = 1, ..., N in the training set:</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/va/wc/ah/vawcahk0zsdpgnumh_bfjuozw_e.jpeg" alt="image"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
where it </font></font><img src="https://habrastorage.org/webt/rf/1x/pk/rf1xpkgfxmcivy-1tqpwv2vgroi.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">is called the </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">remainder</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> and is the new label of the sample </font></font><img src="https://habrastorage.org/webt/dk/ey/2r/dkey2rj3yf-zkei2029wfa2ujso.jpeg" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Now we use the modified training set with the residues instead of the original labels to build a new model of the decision tree. </font></font><img src="https://habrastorage.org/webt/mx/fw/um/mxfwumzpc5tq1wjpdate2rxra48.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The boosting model is now defined as </font></font><img src="https://habrastorage.org/webt/cy/c2/vz/cyc2vz0tmrrihcm6kta_7rnwmui.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">where Œ± is the learning speed (hyperparameter). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Then we recalculate the residuals using Equation 7.2, replace the labels in the training data again, teach a new model of the decision tree, </font></font><img src="https://habrastorage.org/webt/p4/wk/nl/p4wknlhlvwiqtr7zz7lx_y5oq3s.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">redefine the boosting model as </font></font><img src="https://habrastorage.org/webt/n-/mn/ht/n-mnhtck0rar7pz4anzlbdc-bmo.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">we repeat the process, until we combine the predetermined maximum number </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">M of</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> trees.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Let's intuitively understand what is happening here. By calculating the residuals, we determine how well (or poorly) the goal of each training sample is predicted by the current model f. Then we train another tree to correct the errors of the current model (which is why we use leftovers instead of actual labels) and add a new tree to the existing model with some weight Œ±. As a result, each new tree added to the model partially corrects the mistakes made by previous trees. The process continues until the maximum number M (another hyperparameter) of the trees is combined.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Now let's try to answer the question why this algorithm is called gradient boosting. In gradient boosting, we do not calculate the gradient, unlike what we did in chapter 4, solving the linear regression problem. To see the similarities between gradient boosting and gradient descent, remember why we computed the gradient in linear regression: to find out the direction of the parameter values ‚Äã‚Äãto minimize the MSE cost function. The gradient shows the direction, but does not show how far to go in this direction, so in each iteration we took a small step, and then again determined the direction. The same thing happens in gradient boosting, but instead of directly calculating the gradient, we use its estimate in the form of residuals: they show how the model should be adjusted to reduce the error (residual).</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In gradient boosting, three main hyperparameters are available for tuning: the number of trees, the speed of learning, and the depth of the trees. All three affect the accuracy of the model. The depth of trees also affects the speed of learning and forecasting: the smaller the depth, the faster. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
It can be shown that learning by residuals optimizes the overall model f for the standard error standard. Here you can see the difference from bagging: boosting reduces bias (or lack of education) instead of variance. As a result, boosting is subject to retraining. However, by adjusting the depth and number of trees, retraining can be largely avoided.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Gradient boosting is similar for grading tasks, but the steps are slightly different. Consider the case of binary classification. Suppose there are M regression decision trees. By analogy with the logistic regression, the forecast of the ensemble of decision trees is modeled using the sigmoid function:</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/wy/pd/gw/wypdgwjjgpzuojrelehnadtdggc.jpeg" alt="image"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
where </font></font><img src="https://habrastorage.org/webt/w6/3d/kb/w63dkbj3f9j-ik9hrhqbexyxtem.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">is the regression tree. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
And again, as in logistic regression, when trying to find a model f maximizing </font></font><img src="https://habrastorage.org/webt/c4/lv/tt/c4lvttexfzr8disbsv1ph_drzka.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, the principle of maximum likelihood is applied. Similarly, to avoid numerical overflow, we maximize the sum of the likelihood logarithms, rather than the product of the likelihood. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The algorithm starts with the initial constant model </font></font><img src="https://habrastorage.org/webt/hn/7j/ep/hn7jepdxhudnxvjjgbulvfwtpyw.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">where </font></font><img src="https://habrastorage.org/webt/gj/6s/f6/gj6sf6i874m3gq3_fbbxbkrudae.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(It can be shown that such initialization is optimal for the sigmoid function.) Then, at each iteration m, a new tree fm is added to the model. To find the best tree </font></font><img src="https://habrastorage.org/webt/b0/5w/rv/b05wrvcuk5hzvnrmkpebdnk_kxu.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">To find the best tree </font></font><img src="https://habrastorage.org/webt/xm/u4/dp/xmu4dpm1hi3-podtuiayxggydti.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, the partial derivative of the </font></font><img src="https://habrastorage.org/webt/ep/hx/gu/ephxgusen4ou8pc8atm4ksqathq.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">current model is </font><font style="vertical-align: inherit;">first calculated </font><font style="vertical-align: inherit;">for each i = 1, ..., N:</font></font><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/wg/v_/oo/wgv_oogvmupu4q5j6g3rq7dphvk.jpeg" alt="image"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
where f is the model of the ensemble classifier built on the previous iteration m - 1. To calculate </font></font><img src="https://habrastorage.org/webt/ep/hx/gu/ephxgusen4ou8pc8atm4ksqathq.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, we need to find the derivatives of with </font></font><img src="https://habrastorage.org/webt/pv/pe/lb/pvpelb2jplsmq_jmkbdfyl-pzom.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">respect to f for all i. </font><font style="vertical-align: inherit;">Note that the </font></font><img src="https://habrastorage.org/webt/nk/zn/0l/nkzn0lh0l0brnzl_vh0xxlmttrq.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">derivative with respect to f of the right term in the previous equation is</font></font><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/xx/en/iu/xxeniu2qr35ln17asbfk2fqb3sc.jpeg" alt="image"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Then, the training set is transformed by replacing the original label of the </font></font><img src="https://habrastorage.org/webt/a1/fv/cu/a1fvcukqqvsu5wpv3x8i1zj5smc.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">corresponding partial derivative </font></font><img src="https://habrastorage.org/webt/ep/hx/gu/ephxgusen4ou8pc8atm4ksqathq.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, and a new tree is built on the basis of the converted training set. </font></font><img src="https://habrastorage.org/webt/w1/8f/41/w18f41gto37doyvyakgs4np_fyy.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Next, the optimal update step is determined </font></font><img src="https://habrastorage.org/webt/1n/cf/sj/1ncfsjxfe-tao3ep_csuw-s-arw.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">as:</font></font><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/co/qc/hw/coqchwpctxgy2ukbdwaybkouxx0.jpeg" alt="image"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
At the end of iteration m, we update the ensemble model by </font></font><img src="https://habrastorage.org/webt/r7/ox/vy/r7oxvyc5mesbifpwfjtkcrtdr2q.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">adding a new tree</font></font><img src="https://habrastorage.org/webt/s2/vp/u0/s2vpu0-7pmzuzv0n55f1z1fgktu.jpeg" alt="image"><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/vg/ar/qn/vgarqnddik0vjhxxfsesr1t5qs8.jpeg" alt="image"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Iterations continue until the condition m = M is fulfilled, after which the training stops and the ensemble model f is obtained. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Gradient boosting is one of the most powerful machine learning algorithms. </font><font style="vertical-align: inherit;">Not only because it creates very accurate models, but also because it is capable of processing huge data sets with millions of data and features. </font><font style="vertical-align: inherit;">As a rule, it is superior in accuracy to a random forest, but due to the consistent nature it can learn much more slowly.</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../en488346/index.html">Installing or-tools with SCIP and GLPK in a Python 3.7 virtual environment on Linux</a></li>
<li><a href="../en488348/index.html">Webinar ‚ÄúTen Top Agile Challenges and Ways to Overcome Them in an Hour‚Äù February 17 at 20:00 Moscow time</a></li>
<li><a href="../en488352/index.html">VDI Cost Comparison: On-premise versus Public Cloud</a></li>
<li><a href="../en488356/index.html">Training at St. Petersburg State Marine Technical University for Dassault Syst√®mes products</a></li>
<li><a href="../en488360/index.html">Big Data Myths and Digital Culture</a></li>
<li><a href="../en488366/index.html">And again about ‚ÄúIncorrect time zone information for Russian time zones‚Äù [.Net bug, ID: 693286]</a></li>
<li><a href="../en488368/index.html">What I learned while working on my first large-scale project</a></li>
<li><a href="../en488370/index.html">TDD for microcontrollers. Part 2: How spies get rid of addictions</a></li>
<li><a href="../en488374/index.html">Telegram + 1C + Webhooks + Apache + Self-Signed Certificate</a></li>
<li><a href="../en488376/index.html">When the principle "to hell with everything, take it and do it!" not working: procrastinator notes</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>