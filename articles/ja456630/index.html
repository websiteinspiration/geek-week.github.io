<!doctype html>
<html class="no-js" lang="ja">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🤰 🦄 🧔🏻 Airflowを導入してiviでSparkジョブを管理する：希望と松葉杖 ⛵️ ♂️ ⚡️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="機械学習モデルを本番環境にデプロイする作業は常に苦痛を伴います。監視とフォールトトレランスの世界に居心地の良いjupyterノートブックから出ることは非常に不快だからです。リファクタリングの最初の反復
 
 についてはすでに書きましたオンライン映画推薦システムivi。過去1年間、アプリケーションアー...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Airflowを導入してiviでSparkジョブを管理する：希望と松葉杖</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ivi/blog/456630/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">機械学習モデルを本番環境にデプロイする作業は常に苦痛を伴います。監視とフォールトトレランスの世界に居心地の良いjupyterノートブックから出ることは非常に不快だからです。</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">リファクタリングの最初の反復</font></a></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
についてはすでに書きました</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">オンライン映画推薦システムivi。</font><font style="vertical-align: inherit;">過去1年間、アプリケーションアーキテクチャを変更することはほとんどありませんでした（グローバルから-廃止されたpython 2.7とpython 3.4から「新しい」python 3.6への移行のみ）。</font><font style="vertical-align: inherit;">この記事では、Apache Airflowなどのタスクフロー管理ツールの実装における私たちの経験について説明します。なぜチームにこの必要性があったのか、既存のソリューションに適さなかったのは何か、途中で松葉杖を切断しなければならなかったのです。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
→レポートのビデオ版は、</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">こちら</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">（03:00:00以降）のYouTubeでご覧いただけ</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">ます</font></a><font style="vertical-align: inherit;">。</font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><div style="text-align:center;"><img src="https://habrastorage.org/webt/qc/bw/tt/qcbwttjzivyzuedihk88u6nwn5o.png"></div></a><br>
<a name="habracut"></a><br>
<br>
<h2><font color="#fd004c"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ヒドラチーム</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
プロジェクトについて少しお話しします。iviは数万のコンテンツ単位であり、RuNetで最大の法的ディレクトリの1つを持っています。 ivi Webバージョンのメインページは、カタログからのパーソナライズされたカットであり、ユーザーのフィードバック（ビュー、評価など）に基づいて最もリッチで最も関連性の高いコンテンツをユーザーに提供するように設計されています。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/vj/-g/nh/vj-gnheaviq-z7tibuzawo-qdqs.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
レコメンデーションシステムのオンライン部分は、最大600 RPSの負荷を持つFlaskバックエンドアプリケーションです。</font><font style="vertical-align: inherit;">オフラインでは、モデルは1か月あたり2億5,000万を超えるコンテンツビューでトレーニングされます。</font><font style="vertical-align: inherit;">トレーニングデータパイプラインは、Hiveリポジトリの上で実行されるSparkに実装されます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
チームには現在、モデルの作成と本番環境への展開の両方に従事している7人の開発者がいます。これは、タスクフローを管理するための便利なツールを必要とするかなり大きなチームです。</font></font><br>
<br>
<h2><font color="#fd004c"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">オフラインアーキテクチャ</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
以下に、レコメンダーシステムのデータフローインフラストラクチャ図を示します。 </font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/xs/10/nv/xs10nvc8r3mz9bd8osjrn6sqn3u.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ここには2つのデータストレージが示されています。PostgresからHiveへの転送が調整されている間、ユーザーフィードバック（ビュー、評価）用のHiveとさまざまなビジネス情報（コンテンツの収益化のタイプなど）用のPostgresです。 Sparkアプリケーションのパックは、Hiveからデータを取り込み、このデータでモデルをトレーニングします（個人的な推奨についてはALS、コンテンツの類似性についてはさまざまな協調モデル）。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Sparkアプリケーションは従来、専用の仮想マシンから管理されてきました。専用の仮想マシンは、多数のcron +シェルスクリプトを使用してhydra-updaterと呼んでいます。このバンドルは、iviの運用部門で太古の昔に作成され、非常によく機能しました。シェルスクリプトは、sparkアプリケーションを起動するための単一のエントリポイントでした。つまり、管理者がこのスクリプトを完了した後にのみ、新しいモデルがそれぞれ製品で回転し始めました。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
モデルのトレーニングアーティファクトの一部は、HDFSに保存されて外部ストレージに保存され（誰かがそこからダウンロードしてオンラインパーツが回転しているサーバーに転送されるのを待っています）、一部はSparkドライバーからRedis高速ストレージに直接書き込まれます。オンライン部分の数十のpythonプロセスのメモリ。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
このようなアーキテクチャには、時間の経過とともに多くの欠点が蓄積されてきました。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/ri/x2/b0/rix2b0qjxi1qa0igp-bpl23bn04.png"></div> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
図は、データフローがかなり複雑で複雑な構造を持っていることを示しています。これを管理するためのシンプルで明確なツールがなければ、開発と運用は恐怖、衰退、苦しみに変わります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
管理アプリケーションスクリプトは、sparkアプリケーションの管理に加えて、戦闘でのサービスの再起動、Redisダンプ、その他のシステムなど、多くの便利な機能を実行します。</font><font style="vertical-align: inherit;">当然のことながら、私たちの新しいモデルがそれぞれ数十行生成したため、スクリプトは長い期間にわたって多くの機能を獲得しました。</font><font style="vertical-align: inherit;">スクリプトは機能面で過負荷になり始めたため、レコメンダーシステムのチームとして、Sparkアプリケーションの起動と管理に関係する機能の一部をどこかで取り除きたいと考えました。</font><font style="vertical-align: inherit;">これらの目的のために、エアフローを使用することにしました。</font></font><br>
<br>
<h2><font color="#fd004c"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">エアフロー用松葉杖</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
もちろん、これらすべての問題を解決することに加えて、私たちが自分で新しい問題を作成する途中で、Sparkアプリケーションを起動および監視するためにAirflowをデプロイすることは困難であることが判明しました。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
主な問題は、インフラストラクチャ全体を改造する人が誰もいないことでした。 devopsリソースは希少なものです。このため、Airflowを実装するだけでなく、Airflowを既存のシステムに統合する必要がありました。これは、一から見るのがはるかに困難です。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
私は、実装プロセス中に遭遇した痛みと、エアフローを取得するためにガッシングしなければならなかった松葉杖について話したいと思います。</font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">最初の主な問題</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">：Airflowを運用部門の大きなシェルスクリプトに統合する方法。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ここでの解決策は最も明白です。trigger_dagキーを持つairflowバイナリを使用して、シェルスクリプトから直接グラフをトリガーし始めました。このアプローチでは、Airflow shedulerを使用していません。実際、Sparkアプリケーションは同じクラウンで起動されます-これは非常に正確ではありません。しかし、既存のソリューションとのシームレスな統合が実現しました。これは、歴史的にハイドラマトリクスと呼ばれていたメインのSparkアプリケーションのシェルスクリプトの最初の部分です。</font></font><br>
<br>
<pre><code class="bash hljs">    <span class="hljs-built_in">log</span> <span class="hljs-string">"<span class="hljs-variable">$FUNCNAME</span> started"</span>
    <span class="hljs-built_in">local</span> RETVAL=0<font></font>
<font></font>
    <span class="hljs-built_in">export</span> AIRFLOW_CONFIG=/opt/airflow/airflow.cfg<font></font>
    AIRFLOW_API=api/dag_last_run/hydramatrices/all<font></font>
    <span class="hljs-built_in">log</span> <span class="hljs-string">"run /var/www/airflow/bin/airflow trigger_dag hydramatrices"</span>
    /var/www/airflow/bin/airflow trigger_dag hydramatrices 2&gt;&amp;1 | tee -a <span class="hljs-variable">$LOGFILE</span>
</code></pre><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">痛み：</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">運用部門</font><b><font style="vertical-align: inherit;">の</font></b><font style="vertical-align: inherit;">シェルスクリプトは、独自の実行フローを制御するために、何らかの形でAirflowグラフのステータスを決定する必要があります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Crutch：Airflow REST APIを拡張し、シェルスクリプト内でDAGを監視するためのエンドポイントを追加しました。現在、各グラフには3つの状態があります：実行中、成功、失敗。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
実際、Airflowで計算を開始した後、実行中のグラフを定期的にポーリングするだけです。GETリクエストを箇条書きにして、DAGが完了したかどうかを判断します。モニタリングエンドポイントがグラフの正常な実行について応答すると、シェルスクリプトは引き続きフローを実行します。</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Airflow REST APIは、パイプラインを柔軟に構成できるようにするものです。たとえば、POSTパラメータをグラフに転送できます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Airflow API拡張機能は、次のようなPythonクラスです。</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> os<font></font>
<font></font>
<span class="hljs-keyword">from</span> airflow <span class="hljs-keyword">import</span> settings
<span class="hljs-keyword">from</span> airflow.models <span class="hljs-keyword">import</span> DagBag, DagRun
<span class="hljs-keyword">from</span> flask <span class="hljs-keyword">import</span> Blueprint, request, Response<font></font>
<font></font>
airflow_api_blueprint = Blueprint(<span class="hljs-string">'airflow_api'</span>, __name__, url_prefix=<span class="hljs-string">'/api'</span>)<font></font>
AIRFLOW_DAGS = <span class="hljs-string">'{}/dags'</span>.format(<font></font>
    os.path.dirname(os.path.dirname(os.path.abspath(__file__)))<font></font>
)<font></font>
<font></font>
<font></font>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ApiResponse</span>:</span>
    <span class="hljs-string">"""    GET """</span><font></font>
<font></font>
    STATUS_OK = <span class="hljs-number">200</span>
    STATUS_NOT_FOUND = <span class="hljs-number">404</span><font></font>
<font></font>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span>
        <span class="hljs-keyword">pass</span><font></font>
<font></font>
<span class="hljs-meta">    @staticmethod</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">standard_response</span>(<span class="hljs-params">status: int, payload: dict</span>) -&gt; Response:</span><font></font>
        json_data = json.dumps(payload)<font></font>
        resp = Response(json_data, status=status, mimetype=<span class="hljs-string">'application/json'</span>)
        <span class="hljs-keyword">return</span> resp<font></font>
<font></font>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">success</span>(<span class="hljs-params">self, payload: dict</span>) -&gt; Response:</span>
        <span class="hljs-keyword">return</span> self.standard_response(self.STATUS_OK, payload)<font></font>
<font></font>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">error</span>(<span class="hljs-params">self, status: int, message: str</span>) -&gt; Response:</span>
        <span class="hljs-keyword">return</span> self.standard_response(status, {<span class="hljs-string">'error'</span>: message})<font></font>
<font></font>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">not_found</span>(<span class="hljs-params">self, message: str = <span class="hljs-string">'Resource not found'</span></span>) -&gt; Response:</span>
        <span class="hljs-keyword">return</span> self.error(self.STATUS_NOT_FOUND, message)
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
シェルスクリプトでAPIを使用します。10分ごとにエンドポイントをポーリングします。</font></font><br>
<br>
<pre><code class="bash hljs">    TRIGGER=$?<font></font>
    [ <span class="hljs-string">"<span class="hljs-variable">$TRIGGER</span>"</span> -eq <span class="hljs-string">"0"</span> ] &amp;&amp; <span class="hljs-built_in">log</span> <span class="hljs-string">"trigger airflow DAG succeeded"</span> || { <span class="hljs-built_in">log</span> <span class="hljs-string">"trigger airflow DAG failed"</span>; <span class="hljs-built_in">return</span> 1; }<font></font>
<font></font>
    CMD=<span class="hljs-string">"curl -s http://<span class="hljs-variable">$HYDRA_SERVER</span>/<span class="hljs-variable">$AIRFLOW_API</span> | jq .dag_last_run.state"</span>
    STATE=$(<span class="hljs-built_in">eval</span> <span class="hljs-variable">$CMD</span>)<font></font>
<font></font>
    <span class="hljs-keyword">while</span> [ <span class="hljs-variable">$STATE</span> == \"running\" ]; <span class="hljs-keyword">do</span>
        <span class="hljs-built_in">log</span> <span class="hljs-string">"Generating matrices in progress..."</span><font></font>
        sleep 600<font></font>
        STATE=$(<span class="hljs-built_in">eval</span> <span class="hljs-variable">$CMD</span>)
    <span class="hljs-keyword">done</span><font></font>
<font></font>
    [ <span class="hljs-variable">$STATE</span> == \"success\" ] &amp;&amp; RETVAL=0 || RETVAL=1<font></font>
    [ <span class="hljs-variable">$RETVAL</span> -eq 0 ] &amp;&amp; <span class="hljs-built_in">log</span> <span class="hljs-string">"<span class="hljs-variable">$FUNCNAME</span> succeeded"</span> || <span class="hljs-built_in">log</span> <span class="hljs-string">"<span class="hljs-variable">$FUNCNAME</span> failed"</span>
    <span class="hljs-built_in">return</span> <span class="hljs-variable">$RETVAL</span>
</code></pre><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">痛み</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">：クラスターモードでspark-submitを使用してSparkジョブを実行した場合、STDOUTのログは「SPARK APPLICATION_ID IS RUNNING」という行のある情報シートではないことがわかります。</font><font style="vertical-align: inherit;">Sparkアプリケーション自体のログは、たとえば、yarn logsコマンドを使用して表示できます。</font><font style="vertical-align: inherit;">シェルスクリプトでは、この問題は簡単に解決されました。クラスターマシンの1つに対してSSHトンネルが開かれ、このマシンのクライアントモードでspark-submitが実行されました。</font><font style="vertical-align: inherit;">この場合、STDOUTには読み取り可能で理解可能なログが含まれます。</font><font style="vertical-align: inherit;">Airflowでは、常にクラスター決定を使用することを決定しました。そのような数は機能しません。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Crutch：spark-submitが機能した後、application_idによってHDFSからドライバーログをプルし、Pythonのprint（）オペレーターを介してAirflowインターフェースに表示します。唯一の欠点-Airflowインターフェースでは、ログはspark-submitが機能した後にのみ表示され、YARN Webマズルなど、他の場所でリアルタイムで監視する必要があります。</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_logs</span>(<span class="hljs-params">config: BaseConfig, app_id: str</span>) -&gt; <span class="hljs-keyword">None</span>:</span>
    <span class="hljs-string">"""  

    :param config:
    :param app_id:
    """</span><font></font>
    hdfs = HDFSInteractor(config)<font></font>
<font></font>
    logs_path = <span class="hljs-string">'/tmp/logs/{username}/logs/{app_id}'</span>.format(username=config.CURRENT_USERNAME, app_id=app_id)<font></font>
    logs_files = hdfs.files_in_folder(logs_path)<font></font>
<font></font>
    logs_files = [file <span class="hljs-keyword">for</span> file <span class="hljs-keyword">in</span> logs_files <span class="hljs-keyword">if</span> file[<span class="hljs-number">-4</span>:] != <span class="hljs-string">'.tmp'</span>]<font></font>
<font></font>
    <span class="hljs-keyword">for</span> file <span class="hljs-keyword">in</span> logs_files:
        <span class="hljs-keyword">with</span> hdfs.hdfs_client.read(os.path.join(logs_path, file), encoding=<span class="hljs-string">'utf-8'</span>, delimiter=<span class="hljs-string">'\n'</span>) <span class="hljs-keyword">as</span> reader:<font></font>
            print_line = <span class="hljs-literal">False</span>
            <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> reader:
                <span class="hljs-keyword">if</span> re.search(<span class="hljs-string">'stdout'</span>, line) <span class="hljs-keyword">and</span> len(line) &gt; <span class="hljs-number">30</span>:<font></font>
                    print_line = <span class="hljs-literal">True</span><font></font>
<font></font>
                <span class="hljs-keyword">if</span> re.search(<span class="hljs-string">'stderr'</span>, line):<font></font>
                    print_line = <span class="hljs-literal">False</span><font></font>
<font></font>
                <span class="hljs-keyword">if</span> print_line:<font></font>
                    print(line)<font></font>
</code></pre><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">苦痛</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">：テスターや開発者にとっては、Airflowテストベンチがあればいいのですが、開発リソースを節約しているので、テスト環境を長期間展開する方法を考えました。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Crutch：AirflowをDockerコンテナーにパックし、DockerfileはSparkジョブを使用してそれをリポジトリに配置しました。したがって、各開発者またはテスターは、自分のAirflowをローカルマシンで発生させることができます。アプリケーションはクラスターモードで実行されるため、Dockerのローカルリソースはほとんど必要ありません。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Sparkのローカルインストールは、Dockerコンテナーとその全体の構成の中に、環境変数を介して隠されていました。環境のセットアップに数時間を費やす必要がなくなりました。</font><font style="vertical-align: inherit;">以下に、Airflowを使用するコンテナーのdockerファイルフラグメントの例を示します。環境変数を使用してAirflowがどのように構成されているかを確認できます。</font></font><br>
<br>
<pre><code class="bash hljs">FROM ubuntu:16.04<font></font>
<font></font>
ARG AIRFLOW_VERSION=1.9.0<font></font>
ARG AIRFLOW_HOME<font></font>
ARG USERNAME=airflow<font></font>
ARG USER_ID<font></font>
ARG GROUP_ID<font></font>
ARG LOCALHOST<font></font>
ARG AIRFLOW_PORT<font></font>
ARG PIPENV_PATH<font></font>
ARG PROJECT_HYDRAMATRICES_DOCKER_PATH<font></font>
<font></font>
RUN  apt-get update \<font></font>
    &amp;&amp; apt-get install -y \<font></font>
        python3.6 \<font></font>
        python3.6-dev \<font></font>
    &amp;&amp; update-alternatives --install /usr/bin/python3 python3.6 /usr/bin/python3.6 0 \<font></font>
    &amp;&amp; apt-get -y install python3-pip<font></font>
<font></font>
RUN mv /root/.pydistutils.cf /root/.pydistutils.cfg<font></font>
RUN pip3 install pandas==0.20.3 \<font></font>
        apache-airflow==<span class="hljs-variable">$AIRFLOW_VERSION</span> \<font></font>
        psycopg2==2.7.5 \<font></font>
        ldap3==2.5.1 \<font></font>
        cryptography<font></font>
<font></font>
<span class="hljs-comment">#   ,      </span>
ENV PROJECT_HYDRAMATRICES_DOCKER_PATH=<span class="hljs-variable">${PROJECT_HYDRAMATRICES_DOCKER_PATH}</span>
ENV PIPENV_PATH=<span class="hljs-variable">${PIPENV_PATH}</span><font></font>
ENV SPARK_HOME=/usr/lib/spark2<font></font>
ENV HADOOP_CONF_DIR=<span class="hljs-variable">$PROJECT_HYDRAMATRICES_DOCKER_PATH</span>/etc/hadoop-conf-preprod<font></font>
ENV PYTHONPATH=<span class="hljs-variable">${SPARK_HOME}</span>/python/lib/py4j-0.10.4-src.zip:<span class="hljs-variable">${SPARK_HOME}</span>/python/lib/pyspark.zip:<span class="hljs-variable">${SPARK_HOME}</span>/python/lib<font></font>
ENV PIP_NO_BINARY=numpy<font></font>
ENV AIRFLOW_HOME=<span class="hljs-variable">${AIRFLOW_HOME}</span>
ENV AIRFLOW_DAGS=<span class="hljs-variable">${AIRFLOW_HOME}</span>/dags<font></font>
ENV AIRFLOW_LOGS=<span class="hljs-variable">${AIRFLOW_HOME}</span>/logs<font></font>
ENV AIRFLOW_PLUGINS=<span class="hljs-variable">${AIRFLOW_HOME}</span>/plugins<font></font>
<font></font>
<span class="hljs-comment">#      Airflow (log url)</span>
BASE_URL=<span class="hljs-string">"http://<span class="hljs-variable">${AIRFLOW_CURRENT_HOST}</span>:<span class="hljs-variable">${AIRFLOW_PORT}</span>"</span> ;<font></font>
<font></font>
<span class="hljs-comment">#   Airflow</span>
ENV AIRFLOW__WEBSERVER__BASE_URL=<span class="hljs-variable">${BASE_URL}</span>
ENV AIRFLOW__WEBSERVER__ENDPOINT_URL=<span class="hljs-variable">${BASE_URL}</span>
ENV AIRFLOW__CORE__AIRFLOW_HOME=<span class="hljs-variable">${AIRFLOW_HOME}</span>
ENV AIRFLOW__CORE__DAGS_FOLDER=<span class="hljs-variable">${AIRFLOW_DAGS}</span>
ENV AIRFLOW__CORE__BASE_LOG_FOLDER=<span class="hljs-variable">${AIRFLOW_LOGS}</span>
ENV AIRFLOW__CORE__PLUGINS_FOLDER=<span class="hljs-variable">${AIRFLOW_PLUGINS}</span>
ENV AIRFLOW__SCHEDULER__CHILD_PROCESS_LOG_DIRECTORY=<span class="hljs-variable">${AIRFLOW_LOGS}</span>/scheduler
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Airflowを実装した結果、次の結果が得られました。</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">リリースサイクルの短縮：新しいモデル（またはデータ準備パイプライン）のロールアウトは、新しいAirflowグラフの作成につながります。グラフ自体はリポジトリに格納され、コードと共にデプロイされます。</font><font style="vertical-align: inherit;">このプロセスは完全に開発者の手に委ねられています。</font><font style="vertical-align: inherit;">管理者は満足しています。ささいなことで管理しなくなりました。</font></font></li>
<li> Spark-,          Aiflow    .          HDFS-.</li>
<li>       ,   ,   . </li>
<li>  -  ,     Spark   .   —      spark-submit    Dockerfile</li>
<li>  Aiflow — ,   ,   (,   ,     ).</li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
次はどこへ？</font><font style="vertical-align: inherit;">現在、膨大な数のデータソースとシンクがあり、その数は増加します。</font><font style="vertical-align: inherit;">hydramatricesリポジトリクラスの変更は、別のパイプライン（またはオンライン部分）でクラッシュする可能性があります。</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">クリックハウスがあふれ→ハイブ</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">データ前処理：Hive→Hive</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">c2cモデルのデプロイ：Hive→Redis</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ディレクトリの準備（コンテンツの収益化の種類など）：Postgres→Redis</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">モデルの準備：ローカルFS→HDFS</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
このような状況では、データ準備におけるパイプラインの自動テスト用のスタンドが本当に必要です。</font><font style="vertical-align: inherit;">これにより、リポジトリでの変更のテストのコストが大幅に削減され、本番環境での新しいモデルのロールアウトが高速化され、テスターのエンドルフィンのレベルが劇的に増加します。</font><font style="vertical-align: inherit;">しかし、Airflowなしでは、この種の自動テスト用のスタンドを配置することは不可能です。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
この記事は、Airflowの実装に関する私たちの経験について説明するために書きました。これは、同じような状況で他のチームに役立つ可能性があります。すでに大規模な作業システムがあり、何か新しい、ファッショナブルで若々しいものを試してみたいと思っています。</font><font style="vertical-align: inherit;">稼働中のシステムの更新を恐れる必要はありません。試してみる必要があります。このような実験は通常、さらなる発展のための新たな地平を開きます。</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../ja456614/index.html">疫病物語：イノセンスフレームのレンダリング方法</a></li>
<li><a href="../ja456616/index.html">コーディングできる人には300万ルーブル</a></li>
<li><a href="../ja456618/index.html">ララビールモスクワ-6月21日</a></li>
<li><a href="../ja456622/index.html">クラスI保護に従って認定されたOSを作成する方法</a></li>
<li><a href="../ja456624/index.html">便利なPythonツール</a></li>
<li><a href="../ja456632/index.html">RESTinioでC ++テンプレートの4階を構築しています。なぜ、どうやって？</a></li>
<li><a href="../ja456634/index.html">Nginxレシピ：CAS（中央承認サービス）</a></li>
<li><a href="../ja456638/index.html">Rust、Haskell、C ++、Python、Scala、OCamlで同じプロジェクトを比較する</a></li>
<li><a href="../ja456640/index.html">PHDays 9での競争情報コンテストの分析</a></li>
<li><a href="../ja456642/index.html">JetBrainsコーポレートマスタープログラムとITMO大学の最初の卒業</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>