<!doctype html>
<html class="no-js" lang="id">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🌨️ 🤵🏽 ⚛️ Melatih saingan gim pintar di Unity menggunakan metode "mainkan sendiri" menggunakan ML-Agents 🎙️ 🛡️ 🤓</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Halo, Habr! 
 
 Seperti yang diketahui oleh pembaca reguler kami, kami telah lama dan berhasil menerbitkan buku-buku tentang Persatuan . Sebagai bagia...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Melatih saingan gim pintar di Unity menggunakan metode "mainkan sendiri" menggunakan ML-Agents</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/507212/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Halo, Habr! </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Seperti yang diketahui oleh pembaca reguler kami, kami telah lama dan berhasil </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">menerbitkan buku-buku</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> tentang </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Persatuan</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Sebagai bagian dari studi topik, kami tertarik, khususnya, </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ML-Agents Toolkit</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Hari ini kami membawa kepada Anda terjemahan dari artikel dari blog Unity tentang cara melatih agen game secara efektif menggunakan metode "dengan diri sendiri"; khususnya, artikel ini membantu untuk memahami mengapa metode ini lebih efektif daripada pembelajaran tradisional yang diperkuat. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/vb/sz/5m/vbsz5musziyq5yighwu_iq0eggu.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Selamat membaca!</font></font><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Artikel ini kemudian memberikan gambaran umum tentang teknologi bermain sendiri (bermain dengan diri sendiri) dan menunjukkan bagaimana ini membantu untuk memberikan pelatihan yang stabil dan efektif dalam lingkungan demo Sepak Bola dari </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ML-Agents Toolkit</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dalam lingkungan demo Tenis dan Sepak Bola dari Unity ML-Agents Toolkit, agen saling mengadu seperti saingan. Agen pelatihan dalam skenario kompetitif semacam itu terkadang merupakan tugas yang sangat sepele. Bahkan, dalam rilis sebelumnya dari ML-Agents Toolkit, agar agen dapat belajar dengan percaya diri, diperlukan studi serius tentang penghargaan tersebut. Dalam </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">versi 0.14</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">sebuah kesempatan ditambahkan yang memungkinkan pengguna untuk melatih agen menggunakan reinforcement learning (RL) berdasarkan permainan mandiri, sebuah mekanisme yang sangat penting dalam mencapai beberapa hasil pembelajaran penguatan yang paling </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">canggih</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , seperti </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;">OpenAI Five</font></a><font style="vertical-align: inherit;"> dan </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">DeepSind's AlphaStar</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Bermain sendiri di lubang kerja satu sama lain dengan hipotesa agen saat ini dan di masa lalu. Dengan demikian, kami mendapatkan musuh untuk agen kami, yang dapat secara bertahap meningkatkan menggunakan algoritma pembelajaran penguatan tradisional. </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Agen yang terlatih sepenuhnya</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> dapat berhasil bersaing dengan pemain manusia tingkat lanjut.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bermain sendiri menyediakan lingkungan belajar yang dibangun di atas prinsip yang sama dengan kompetisi dari sudut pandang manusia. Sebagai contoh, seseorang yang belajar bermain tenis akan memilih untuk melawan lawan pada tingkat yang sama dengan dirinya, karena lawan yang terlalu kuat atau terlalu lemah tidak begitu nyaman untuk menguasai permainan. Dari sudut pandang mengembangkan keterampilan sendiri, bisa jauh lebih berharga bagi pemain tenis pemula untuk mengalahkan pemula yang sama, daripada, katakanlah, anak prasekolah atau Novak Djokovic. Yang pertama bahkan tidak akan bisa mengenai bola, dan yang kedua tidak akan memberi Anda servis yang bisa Anda kalahkan. Ketika seorang pemula mengembangkan kekuatan yang cukup, ia dapat pindah ke tingkat berikutnya atau melamar turnamen yang lebih serius untuk bermain melawan lawan yang lebih terampil.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dalam artikel ini, kami akan mempertimbangkan beberapa seluk-beluk teknis yang terkait dengan dinamika permainan dengan diri kita sendiri, dan juga mempertimbangkan contoh-contoh bekerja di lingkungan virtual Tennis and Soccer, yang diolah ulang sedemikian rupa untuk menggambarkan permainan itu sendiri.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Kisah gim dengan diri Anda sendiri dalam gim</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Fenomena bermain dengan diri sendiri memiliki sejarah panjang, tercermin dalam praktik pengembangan agen game buatan yang dirancang untuk bersaing dengan orang-orang dalam game. </font><font style="vertical-align: inherit;">Salah satu yang pertama menggunakan sistem ini adalah Arthur Samuel, yang mengembangkan simulator catur pada 1950-an dan menerbitkan </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">karya ini</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> pada 1959. </font><font style="vertical-align: inherit;">Sistem ini menjadi cikal bakal hasil tengara dalam pembelajaran penguatan yang dicapai oleh Gerald Tesauro di TD-Gammon; </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">total yang dipublikasikan</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">pada tahun 1995. TD-Gammon menggunakan algoritma time-difference TD (λ) dengan fungsi bermain dengan dirinya sendiri untuk melatih agen bermain backgammon sehingga ia dapat bersaing dengan orang yang profesional. Dalam beberapa kasus, telah diamati bahwa TD-Gammon memiliki visi posisi yang lebih percaya diri daripada pemain kelas dunia. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bermain dengan diri sendiri tercermin dalam banyak prestasi ikonik yang terkait dengan RL. Penting untuk dicatat bahwa bermain dengan diri sendiri membantu pengembangan agen untuk bermain </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">catur dan bermain</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> dengan kemampuan manusia super, agen </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">DOTA 2 yang</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> elit </font><font style="vertical-align: inherit;">, serta strategi yang kompleks dan strategi kontra dalam permainan seperti </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">gulat</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> dan </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">petak umpet</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Dalam hasil yang dicapai dengan bermain dengan diri sendiri, sering dicatat bahwa agen permainan memilih strategi yang mengejutkan orang yang ahli. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bermain dengan diri sendiri memberi agen kreativitas tertentu yang tidak tergantung pada kreativitas pemrogram. </font><font style="vertical-align: inherit;">Agen hanya menerima aturan permainan, dan kemudian - informasi tentang apakah dia menang atau kalah. </font><font style="vertical-align: inherit;">Selanjutnya, berdasarkan prinsip-prinsip dasar ini, agen harus mengembangkan perilaku yang kompeten. </font><font style="vertical-align: inherit;">Menurut pencipta TD-Gammon, pendekatan pembelajaran seperti itu membebaskan, "... dalam arti bahwa program tersebut tidak dibatasi oleh kecenderungan dan prasangka manusia, yang mungkin ternyata salah dan tidak dapat diandalkan." </font><font style="vertical-align: inherit;">Berkat kebebasan ini, agen menemukan strategi permainan brilian yang benar-benar mengubah cara para insinyur berpikir tentang permainan tertentu.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pelatihan penguatan kompetitif</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dalam kerangka tugas tradisional pembelajaran yang diperkuat, agen berusaha mengembangkan garis perilaku yang memaksimalkan total hadiah. Sinyal penghargaan mengkodekan tugas agen - tugas seperti itu mungkin, misalnya, merencanakan kursus atau mengumpulkan item. Perilaku agen tunduk pada batasan lingkungan. Seperti, misalnya, gravitasi, hambatan, serta pengaruh relatif dari tindakan yang diambil oleh agen itu sendiri - misalnya, penerapan kekuatan untuk gerakan seseorang. Faktor-faktor ini membatasi perilaku agen dan merupakan kekuatan eksternal yang harus dia pelajari untuk ditangani agar dapat menerima hadiah yang tinggi. Dengan demikian, agen bersaing dengan dinamika lingkungan, dan harus bergerak dari satu negara ke negara lain sehingga imbalan maksimum tercapai.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/if/jf/xi/ifjfxil_iqm9pznr61qbpxujs38.png"><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Skenario pelatihan penguatan khas ditunjukkan di sebelah kiri: agen bertindak di lingkungan, transfer ke negara berikutnya dan menerima hadiah. Skenario pelatihan ditunjukkan di sebelah kanan, di mana agen bersaing dengan saingan, yang, dari sudut pandang agen, sebenarnya merupakan elemen lingkungan.</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Dalam kasus permainan kompetitif, agen tersebut bersaing tidak hanya dengan dinamika lingkungan, tetapi juga dengan agen lain (mungkin intelektual). Kita dapat berasumsi bahwa lawan dibangun ke dalam lingkungan, dan tindakannya secara langsung mempengaruhi keadaan selanjutnya yang "dilihat" agen, serta hadiah yang akan ia terima. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/zl/kv/xo/zlkvxoa9_bt0cdsthmddwll-fvc.png"><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tenis Contoh dari ML-Agents Toolkit</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pertimbangkan demo Tenis ML-Agen. Racket biru (kiri) adalah agen pembelajaran, dan ungu (kanan) adalah lawannya. Untuk melempar bola melewati jaring, agen harus memperhitungkan lintasan bola yang terbang dari lawan, dan melakukan penyesuaian sudut dan kecepatan bola yang terbang, dengan mempertimbangkan kondisi lingkungan (gravitasi). Namun, dalam kompetisi dengan lawan, melempar bola melewati jaring hanya setengah pertempuran. Lawan yang kuat dapat merespons dengan pukulan yang tak tertahankan, dan sebagai hasilnya, agen akan kalah. Lawan yang lemah bisa memukul bola ke gawang. Lawan yang setara dapat mengembalikan servis, dan karenanya permainan akan berlanjut. Bagaimanapun, keadaan selanjutnya dan hadiah yang sesuai tergantung pada kondisi lingkungan dan lawan. Namun, dalam semua situasi ini, agen membuat nada yang sama. Karena itu, sebagai pelatihan dalam permainan kompetitif,dan memompa perilaku saingan oleh agen adalah masalah yang kompleks.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pertimbangan untuk lawan yang cocok tidak sepele. Seperti yang jelas dari penjelasan di atas, kekuatan relatif lawan secara signifikan memengaruhi hasil permainan tertentu. Jika lawan terlalu kuat, maka agen mungkin akan kesulitan mempelajari cara bermain dari awal. Di sisi lain, jika lawan terlalu lemah, maka agen dapat belajar untuk menang, tetapi keterampilan ini mungkin tidak berguna dalam persaingan dengan lawan yang lebih kuat atau berbeda. Karena itu, kita membutuhkan lawan yang kekuatannya kira-kira setara dengan agen (pantang menyerah, tetapi tidak dapat diatasi). Selain itu, karena keterampilan agen kami meningkat dengan setiap pertandingan selesai, kami harus meningkatkan kekuatan lawannya pada tingkat yang sama. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/fn/vk/pt/fnvkptgsipqopi2iiyflsa5ec_a.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Saat bermain dengan diri Anda sendiri, snapshot dari masa lalu atau agen dalam kondisi saat ini adalah lawan yang ada di lingkungan.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Di sinilah permainan dengan diri kita berguna! Agen itu sendiri memenuhi kedua persyaratan untuk lawan yang diinginkan. Dia jelas memiliki kekuatan yang hampir sama untuk dirinya sendiri, dan keterampilannya meningkat seiring waktu. Dalam hal ini, kebijakan agen sendiri dibuat di lingkungan (lihat gambar). Mereka yang akrab dengan </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">pendidikan kompleksitas yang meningkat secara bertahap</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (pembelajaran kurikulum), menunjukkan kepada Anda bahwa kami dapat berasumsi bahwa sistem secara alami mengembangkan kurikulum, setelah itu, agen tersebut belajar untuk melawan lawan yang semakin kuat. Dengan demikian, bermain dengan diri sendiri memungkinkan Anda untuk menggunakan lingkungan itu sendiri untuk melatih agen kompetitif untuk permainan kompetitif!</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dalam dua bagian berikutnya, kami akan mempertimbangkan lebih banyak detail teknis dari pelatihan agen kompetitif, khususnya, mengenai implementasi dan penggunaan game dengan diri sendiri dalam ML-Agents Toolkit.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pertimbangan praktis</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Beberapa masalah praktis muncul sehubungan dengan kerangka untuk bermain dengan diri sendiri. </font><font style="vertical-align: inherit;">Secara khusus, pelatihan ulang dimungkinkan, di mana agen belajar untuk menang hanya dengan gaya permainan tertentu, serta ketidakstabilan yang melekat dalam proses pembelajaran, yang dapat timbul karena ketidakstabilan fungsi transisi (yaitu, karena lawan yang terus berubah). </font><font style="vertical-align: inherit;">Masalah pertama muncul karena kami ingin agen kami memiliki pengetahuan umum dan kemampuan untuk melawan lawan dari berbagai jenis.</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Masalah kedua dapat diilustrasikan dalam lingkungan Tenis: lawan yang berbeda akan memukul bola dengan kecepatan dan sudut yang berbeda. Dari sudut pandang agen pembelajaran, ini berarti bahwa, ketika Anda belajar, keputusan yang sama akan mengarah pada hasil yang berbeda dan, karenanya, agen tersebut akan berada dalam situasi berikutnya yang berbeda. Dalam pembelajaran penguatan tradisional, fungsi transisi stasioner tersirat. Sayangnya, setelah menyiapkan pilihan berbagai lawan untuk agen untuk menyelesaikan masalah pertama, kami, karena ceroboh, dapat memperburuk yang kedua.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Untuk mengatasinya, kami akan menjaga buffer dengan kebijakan agen masa lalu, dari mana kami akan memilih saingan potensial untuk "siswa" kami untuk jangka panjang. </font><font style="vertical-align: inherit;">Memilih agen dari kebijakan masa lalu, kami memberinya beragam pilihan lawan. </font><font style="vertical-align: inherit;">Selain itu, memungkinkan agen untuk berlatih dengan lawan tetap untuk waktu yang lama, kami menstabilkan fungsi transisi dan menciptakan lingkungan belajar yang lebih konsisten. </font><font style="vertical-align: inherit;">Akhirnya, aspek algoritmik ini dapat dikontrol menggunakan hiperparameter, yang dibahas pada bagian selanjutnya.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Detail Implementasi dan Penggunaan</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Memilih hyperparameter untuk bermain dengan diri kita sendiri, kita, pertama-tama, ingatlah kompromi antara tingkat lawan, universalitas kebijakan akhir dan stabilitas pelatihan. Pelatihan dalam kompetisi dengan sekelompok lawan yang berubah perlahan atau tidak berubah sama sekali, yang berarti mereka memberikan hasil yang lebih sedikit, adalah proses yang lebih stabil daripada pelatihan dalam kompetisi dengan banyak lawan yang berbeda yang berubah dengan cepat. Hyperparameters yang tersedia memungkinkan Anda untuk mengontrol seberapa sering kebijakan agen saat ini akan disimpan untuk digunakan nanti sebagai salah satu lawan dalam sampel, seberapa sering lawan baru akan diselamatkan, kemudian dipilih untuk sparring, seberapa sering lawan baru akan dipilih, jumlah lawan yang diselamatkan, serta kemungkinannyabahwa dalam kasus ini, siswa harus bermain melawan alter egonya sendiri, dan bukan melawan lawan yang dipilih dari grup.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dalam permainan kompetitif, penghargaan "kumulatif" yang dikeluarkan oleh lingkungan mungkin bukan metrik paling informatif untuk melacak kemajuan belajar. Faktanya adalah bahwa penghargaan akumulatif sepenuhnya tergantung pada level lawan. Agen dengan keterampilan permainan tertentu akan menerima hadiah yang lebih besar atau lebih kecil, tergantung pada lawan yang kurang terampil atau lebih terampil. Kami mengusulkan penerapan </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">sistem peringkat ELO</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , yang memungkinkan Anda menghitung keterampilan permainan relatif dari dua pemain dari populasi tertentu saat bermain dengan jumlah nol. Selama satu latihan tunggal, nilai ini harus terus meningkat. Anda dapat melacaknya, bersama dengan metrik pembelajaran lainnya, misalnya, keseluruhan penghargaan, menggunakan </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">TensorBoard</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bermain sendiri di Soccer</font></font></h3> <br>
<img src="https://habrastorage.org/webt/gl/m_/rh/glm_rhop1whkyve0i-wpubxfpqw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Rilis terbaru dari ML-Agent Toolkit tidak termasuk kebijakan agen untuk lingkungan pembelajaran Soccer, karena proses pelatihan yang andal tidak dibangun di dalamnya. </font><font style="vertical-align: inherit;">Namun, menggunakan permainan dengan diri kita sendiri dan beberapa refactoring, kita dapat melatih agen dalam perilaku non-sepele. </font><font style="vertical-align: inherit;">Perubahan paling signifikan adalah penghapusan "posisi permainan" dari karakteristik agen. </font><font style="vertical-align: inherit;">Sebelumnya di lingkungan Soccer, "penjaga gawang" dan "striker" jelas menonjol, sehingga keseluruhan permainan tampak lebih logis. </font><font style="vertical-align: inherit;">Dalam </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">video ini</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">sebuah lingkungan baru disajikan di mana seseorang dapat melihat bagaimana perilaku peran terbentuk secara spontan, di mana beberapa agen mulai bertindak sebagai penyerang dan yang lainnya sebagai penjaga gawang. Sekarang para agen sendiri sedang belajar memainkan posisi-posisi ini! Fungsi hadiah untuk keempat agen didefinisikan sebagai +1.0 untuk gol yang dicetak dan -1.0 untuk kebobolan gol, dengan penalti tambahan sebesar -0.0003 per langkah - penalti ini diberikan untuk merangsang agen untuk menyerang.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Di sini kami menekankan sekali lagi bahwa agen dalam lingkungan pembelajaran Soccer sendiri mempelajari perilaku kooperatif, dan untuk ini, tidak ada algoritma eksplisit yang digunakan terkait dengan perilaku multi-agen atau tugas peran. </font><font style="vertical-align: inherit;">Hasil ini menunjukkan bahwa agen dapat dilatih dalam perilaku kompleks menggunakan algoritma yang relatif sederhana - asalkan tugas tersebut diformulasikan dengan baik. </font><font style="vertical-align: inherit;">Kondisi yang paling penting untuk ini adalah bahwa agen dapat mengamati rekan satu tim mereka, yaitu, mereka menerima informasi tentang posisi relatif rekan satu tim. </font><font style="vertical-align: inherit;">Memaksa pertarungan agresif untuk mendapatkan bola, agen tersebut secara tidak langsung memberi tahu rekan setimnya bahwa ia harus bergerak dalam pertahanan. </font><font style="vertical-align: inherit;">Sebaliknya, bergerak menjauh dalam pertahanan, agen itu memprovokasi rekan satu tim untuk menyerang.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Apa berikutnya</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Jika Anda pernah menggunakan salah satu fitur baru dari rilis ini - beri tahu kami tentang mereka. </font><font style="vertical-align: inherit;">Kami menarik perhatian Anda ke halaman </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">masalah ML-Agents GitHub</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , di mana Anda dapat berbicara tentang bug yang ditemukan, serta ke halaman </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">forum Unity ML-Agents</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , di mana pertanyaan umum dan masalah dibahas.</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id507200/index.html">7 Cara Data Ilmuwan Membodohi Anda</a></li>
<li><a href="../id507202/index.html">Pertemuan Avito Analytics</a></li>
<li><a href="../id507204/index.html">Desain interior dapur: mulai dari sketsa hingga produk dalam kotak</a></li>
<li><a href="../id507206/index.html">Arsitektur Y messenger</a></li>
<li><a href="../id507210/index.html">Kinerja Java modern saat bekerja dengan sejumlah besar data, bagian 2</a></li>
<li><a href="../id507214/index.html">Cara membuat dan memodifikasi formulir PDF interaktif, atau skill baru ABBYY FineReader PDF</a></li>
<li><a href="../id507218/index.html">Baca saya, atau mengapa teks tidak dibaca sampai akhir</a></li>
<li><a href="../id507222/index.html">Kenapa semua orang harus memakai topeng</a></li>
<li><a href="../id507224/index.html">Cara menghilangkan bintik-bintik buta dengan pengujian visual</a></li>
<li><a href="../id507226/index.html">OCR untuk PDF di .NET - Cara mengekstrak teks dari dokumen PDF yang tidak dapat diakses</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>