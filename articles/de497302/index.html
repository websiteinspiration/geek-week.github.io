<!doctype html>
<html class="no-js" lang="de">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>💇🏽 👩🏽‍✈️ ✌🏿 Autonome Navigation eines mobilen Roboters 💪🏻 👉🏽 🛢️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Es gibt eine Vielzahl von Möglichkeiten, wie ein Roboter Informationen von der Außenwelt empfangen kann, um mit ihm zu interagieren. Abhängig von den ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Autonome Navigation eines mobilen Roboters</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/497302/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Es gibt eine Vielzahl von Möglichkeiten, wie ein Roboter Informationen von der Außenwelt empfangen kann, um mit ihm zu interagieren. </font><font style="vertical-align: inherit;">Abhängig von den ihm zugewiesenen Aufgaben unterscheiden sich auch die Methoden zur Verarbeitung dieser Informationen. </font><font style="vertical-align: inherit;">In diesem Artikel werde ich die Hauptphasen der im Rahmen des Schulprojekts durchgeführten Arbeiten beschreiben, deren Ziel es ist, Informationen zu verschiedenen Methoden der autonomen Roboternavigation zu systematisieren und das bei der Erstellung des Roboters für die Wettbewerbe „RTK Cup“ gewonnene Wissen anzuwenden.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/qb/xw/dw/qbxwdwc_cwrypc3c8dthahudzmk.jpeg"><br>
<a name="habracut"></a><br>
<h2><font color="#4d7f95"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Einführung</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bei den Wettbewerben „RTK Cup“ gibt es einen Aufgabenblock, der ohne Eingreifen des Bedieners erledigt werden muss. </font><font style="vertical-align: inherit;">Ich glaube, dass viele Teilnehmer diese Aufgaben zu Unrecht vermeiden, weil die scheinbar Komplexität der Erstellung eines Roboterdesigns und des Schreibens eines Programms weitgehend vereinfachte Aufgaben aus anderen Wettbewerbsdisziplinen verbirgt, die auf einem Trainingsgelände zusammengefasst sind. </font><font style="vertical-align: inherit;">Mit meinem Projekt möchte ich mögliche Lösungen für solche Probleme aufzeigen, wobei ich als Beispiel das Folgende entlang der Linie betrachte. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Um das Projektziel zu erreichen, wurden folgende Zwischenaufgaben formuliert:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Analyse </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">der</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Wettbewerbsregeln „RTK Cup“</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Analyse bestehender Algorithmen zur autonomen Orientierung eines mobilen Roboters</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Softwareerstellung</font></font></li>
</ul><br>
<h2><font color="#4d7f95"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Analyse der Wettbewerbsregeln „RTK Cup“</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bei den „RTK Cup“ -Wettbewerben wird den Teilnehmern ein Trainingsgelände präsentiert, auf dem Abschnitte unterschiedlicher Komplexität modelliert werden. </font><font style="vertical-align: inherit;">Der Wettbewerb soll die junge Robotik dazu anregen, Geräte zu entwickeln, die unter extremen Bedingungen arbeiten, Hindernisse überwinden, unter menschlicher Kontrolle oder autonom arbeiten können.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/6a/sr/6w/6asr6wzvlhnqgkmz0b8dfyf2h9o.jpeg"><br>
<br>
<div class="spoiler"><b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">kurz über die Elemente, aus denen das Polygon besteht</font></font></b><div class="spoiler_text"> «»          ,    .    ,       ,     (), ,      (),   ..<br>
<br>
:<br>
<br>
<img src="https://habrastorage.org/webt/mk/ks/d3/mkksd313rprmlsilxcrq5xdytg4.png" width="300"><br>
<br>
:<br>
<br>
<img src="https://habrastorage.org/webt/rv/fp/cu/rvfpcu-6qtvdfrclsqjzlr2xok4.jpeg" width="300"><br>
<br>
  –  ,     «»  ( )  ,        . ,         ,    ,      ,         .<br>
<br>
     .    ,       ,     ,   ,     ,    ,    .<br>
</div></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die Wettbewerbe sind in zwei grundlegend unterschiedliche Nominierungen unterteilt: "Seeker" und "Extreme". Damit soll sichergestellt werden, dass der Wettbewerb zwischen Teilnehmern mit einem Mindestunterschied in Bezug auf Alter und Erfahrung bei der Entwicklung von Robotersystemen durchgeführt wurde: Sucher für jüngere und Extreme für Teilnehmer ab 14 Jahren. Bei der Seeker-Nominierung kann sich der Bediener frei in der Reichweite bewegen und direkten Blickkontakt mit der Maschine haben, während bei der Extreme-Nominierung davon ausgegangen wird, dass der Roboter über Videokommunikationssysteme und / oder Computer Vision verfügt, da der Bediener im Labyrinth navigieren muss und sich nur auf das Labyrinth verlassen muss Die Kamera und die Sensoren sind in den Roboter eingebaut, während sie sich hinter einem speziellen Bildschirm befinden.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Um sich für Wettbewerbe zu qualifizieren, muss der Roboter entweder die Aufgabe zur Fernsteuerung des Manipulators bestehen oder eines der Elemente der Autonomie ausführen. </font><font style="vertical-align: inherit;">Im Rahmen des Projekts wurde die Aufgabe festgelegt, Autonomieaufgaben zu erfüllen, da sie die meisten Punkte zu den niedrigsten Kosten des Betreibers liefern. </font><font style="vertical-align: inherit;">Die Elemente der Autonomie umfassen:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fahren entlang einer Linie mit einem Umgebungslichtsensor oder einem Sichtliniensystem</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Standalone-Beacon-Erfassung mit Entfernungssensor oder Bildverarbeitungssystemen</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bewegung entlang einer komplexen Flugbahn (z. B. Auf- / Abstieg von Treppen) entlang einer Linie mit einem Kompass, Gyroskop, Beschleunigungsmesser, Sichtsystem oder kombinierten Methoden</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Außerdem werden Punkte zur Überwindung von Hindernissen verdoppelt, wenn der Roboter sie autonom passiert.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Im Rahmen dieses Projekts wird die Lösung der ersten Aufgabe in Betracht gezogen - Bewegung entlang der Linie. Die gebräuchlichsten Methoden, um sich entlang der Linie zu bewegen, sind Lichtsensoren und eine Kamera. Zu den Pluspunkten der Sensoren gehört die einfache Erstellung eines Programms - viele von ihnen sind mit einem Abstimmwiderstand ausgestattet, sodass beim Einstellen des Sensors für die Hintergrundbeleuchtung 0 oder 1 ausgegeben wird, je nachdem, ob er in der Leitung ist oder nicht. Aus dem gleichen Grund stellen Lichtsensoren keine Anforderungen an die Verarbeitungsleistung der verwendeten Steuerung. Aus diesem Grund ist die Lösung des Problems mit Hilfe von Lichtsensoren am kostengünstigsten - die Kosten für den einfachsten Sensor betragen 35 Rubel, und für eine relativ stabile Fahrt entlang der Linie reichen drei Sensoren aus (einer ist an der Linie und zwei an den Seiten installiert). Jedoch,Einer der Hauptnachteile solcher Sensoren sind Installationsbeschränkungen. Idealerweise sollte der Sensor genau in der Mitte in geringem Abstand vom Boden installiert werden, da sonst falsche Werte angezeigt werden. Dies ist kein Problem bei speziellen Wettbewerben, bei denen der Roboter so schnell wie möglich auf der Strecke fahren muss. Unter den Bedingungen des „RTK Cup“ -Wettbewerbs können jedoch alle oben genannten Sensorfehler kritisch sein - ihre Installation erfordert in erster Linie das Vorhandensein zusätzlicher mechanischer Teile am Roboter, die anheben und Das Absenken der Sensoren erfordert zusätzlichen Platz auf dem Roboter, einen separaten Motor, der die Sensoren bewegt, und ist auch ein Ort für potenzielle Schäden und erhöht die Masse des Roboters.Andernfalls werden falsche Werte angezeigt. Dies ist kein Problem bei speziellen Wettbewerben, bei denen der Roboter so schnell wie möglich auf der Strecke fahren muss. Unter den Bedingungen des „RTK Cup“ -Wettbewerbs können jedoch alle oben genannten Sensorfehler kritisch sein - ihre Installation erfordert in erster Linie das Vorhandensein zusätzlicher mechanischer Teile am Roboter, die anheben und Das Absenken der Sensoren erfordert zusätzlichen Platz auf dem Roboter, einen separaten Motor, der die Sensoren bewegt, und ist auch ein Ort für potenzielle Schäden und erhöht die Masse des Roboters.Andernfalls werden falsche Werte angezeigt. Dies ist kein Problem bei speziellen Wettbewerben, bei denen der Roboter so schnell wie möglich auf der Strecke fahren muss. Unter den Bedingungen des „RTK Cup“ -Wettbewerbs können jedoch alle oben genannten Sensorfehler kritisch sein - ihre Installation erfordert in erster Linie das Vorhandensein zusätzlicher mechanischer Teile am Roboter, die anheben und Das Absenken der Sensoren erfordert zusätzlichen Platz auf dem Roboter, einen separaten Motor, der die Sensoren bewegt, und ist auch ein Ort für potenzielle Schäden und erhöht die Masse des Roboters.Alle oben genannten Sensorfehler können kritisch sein - ihre Installation erfordert in erster Linie das Vorhandensein zusätzlicher mechanischer Teile am Roboter, die die Sensoren anheben und absenken. Dies erfordert zusätzlichen Platz auf dem Roboter, einen separaten Motor, der die Sensoren bewegt, und ist auch ein Ort für potenzielle Schäden und erhöht die Masse des Roboters .Alle oben genannten Sensorfehler können kritisch sein - ihre Installation erfordert in erster Linie das Vorhandensein zusätzlicher mechanischer Teile am Roboter, die die Sensoren anheben und absenken. Dies erfordert zusätzlichen Platz auf dem Roboter, einen separaten Motor, der die Sensoren bewegt, und ist auch ein Ort für potenzielle Schäden und erhöht die Masse des Roboters .</font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><img src="https://habrastorage.org/webt/_g/pt/zz/_gptzzip0rdk6ocgg67ttumux2k.jpeg" width="300" align="right"></a><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die Kamera hat wiederum die folgenden Vorteile: Sie hat einen praktisch unbegrenzten (im Vergleich zu Sensoren) Messradius, d.h. Nur ein Kameramodul kann gleichzeitig die Linie sehen, sowohl direkt unter dem Roboter als auch in ausreichendem Abstand davon, wodurch beispielsweise seine Krümmung ausgewertet und eine proportionale Steueraktion ausgewählt werden kann. Gleichzeitig stört das Kameramodul nicht die Weiterentwicklung des Roboters in anderen Teilen der Deponie, die keine Autonomie erfordern, da die Kamera in einem Abstand vom Boden befestigt ist. Der Hauptnachteil der Kamera besteht darin, dass die Videoverarbeitung einen leistungsstarken Rechenkomplex an Bord des Roboters erfordert und die zu entwickelnde Software eine genauere Abstimmung erfordert, da die Kamera eine Größenordnung mehr Informationen von der Außenwelt empfängt als drei Lichtsensoren, während Kamera und ComputerIn der Lage sind, die empfangenen Informationen zu verarbeiten, sind um ein Vielfaches mehr als drei Sensoren und "Arduine".</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Für mich persönlich liegt die Antwort auf der Hand - in der Nominierung „extremal“ muss der Roboter über eine Richtkamera verfügen, mit der der Bediener navigieren kann. </font><font style="vertical-align: inherit;">Wenn Sie vorgefertigte FPV-Lösungen verwenden, können die Gesamtkosten für „Sensoren“ sogar noch höher sein, während zusätzliche Geräte installiert werden müssen. </font><font style="vertical-align: inherit;">Darüber hinaus hat ein Roboter mit Himbeer-Pi und einer Kamera ein größeres Potenzial für die Entwicklung einer autonomen Bewegung, da die Kamera eine Vielzahl von Problemen lösen kann und nicht nur bei Linienbewegungen verwendet werden kann, ohne das Design wesentlich zu komplizieren.</font></font><br>
<br>
<h2><font color="#4d7f95"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Analyse bestehender Computer-Vision-Algorithmen</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Computer Vision ist die Theorie der Erstellung von Geräten, die Bilder von Objekten der realen Welt empfangen, die erhaltenen Daten verarbeiten und verwenden können, um verschiedene Arten von angewandten Problemen ohne menschliches Eingreifen zu lösen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Computer Vision Systeme bestehen aus:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">eine oder mehrere Kameras </font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Computerkomplex </font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Software, die Bildverarbeitungswerkzeuge bereitstellt</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Kommunikationskanäle zur Übertragung von Ziel- und Telemetrieinformationen. </font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wie bereits geschrieben, gibt es viele Möglichkeiten, Objekte zu identifizieren, die für uns von Interesse sind. Wenn Sie entlang einer Linie fahren, müssen Sie die Linie selbst vom kontrastierenden Hintergrund trennen (eine schwarze Linie auf weißem Hintergrund oder eine weiße Linie auf schwarzem Hintergrund für eine umgekehrte Linie). Algorithmen, die ein Computer-Vision-System verwenden, können in mehrere „Schritte“ zur Verarbeitung des Originalbilds unterteilt werden: </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bildaufnahme</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : Digitale Bilder werden direkt von der Kamera, von einem an das Gerät übertragenen Videostream oder als separate Bilder erhalten. Pixelwerte entsprechen normalerweise der Lichtintensität (Farb- oder Graustufenbilder), können jedoch mit verschiedenen physikalischen Messungen in Verbindung gebracht werden, beispielsweise mit der Temperatur einer Wärmebildkamera. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Vorbearbeitung</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">: Bevor Computer-Vision-Methoden auf Videodaten angewendet werden können, ist abhängig von der verwendeten Methode eine Vorverarbeitung erforderlich, um bestimmte Bedingungen einzuführen. </font><font style="vertical-align: inherit;">Beispiele sind:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Entfernen von Rauschen oder Verzerrungen durch den verwendeten Sensor</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bildunschärfe wird verwendet, um kleine Artefakte zu entfernen, die während des Kamerabetriebs, von Dekomprimierungselementen, Rauschen usw. auftreten.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Verbesserung des Kontrasts, damit die richtigen Informationen wahrscheinlicher erkannt werden können</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ändern Sie die Belichtung von Schatten oder Lichtern</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Skalieren oder Zuschneiden, um die Strukturen im Bild besser unterscheiden zu können.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Konvertieren eines Bilds in Schwarzweiß oder Ändern der Auflösung für eine schnellere Systemleistung</font></font></li>
</ul><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hervorheben von Details</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : Bilddetails verschiedener Schwierigkeitsgrade werden aus den Videodaten extrahiert. </font><font style="vertical-align: inherit;">Typische Beispiele für solche Details sind Linien, Ränder, Kanten, einzelne Punkte und Bereiche, die für jedes Merkmal charakteristisch sind. </font></font><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Erkennung</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : In einem bestimmten Stadium der Programmarbeit werden programmrelevante Informationen vom Rest des Bildes getrennt. </font><font style="vertical-align: inherit;">Beispiele sind:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Auswahl eines bestimmten Satzes von Punkten von Interesse für die Farbe, die Anzahl der isolierten Pixel, die in irgendeiner Weise ähnlich sind (Krümmung der Figur, Farbe, Helligkeit usw.)</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Segmentierung eines oder mehrerer Bildabschnitte, die ein charakteristisches Objekt enthalten.</font></font></li>
</ul><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Verarbeitung</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> auf </font><b><font style="vertical-align: inherit;">hoher Ebene</font></b><font style="vertical-align: inherit;"> : In diesem Schritt wird die Informationsfülle aus dem Bild auf eine Größe reduziert, die leicht verarbeitet werden kann, z. B. ein Satz bestimmter Pixel oder die Koordinaten des Teils des Bildes, in dem sich das interessierende Objekt angeblich befindet. </font><font style="vertical-align: inherit;">Beispiele sind:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Filtern von Werten nach einem beliebigen Kriterium</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bewertung von Parametern wie den physischen Abmessungen des Objekts, der Form, seiner Position im Rahmen oder relativ zu anderen charakteristischen Objekten</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Einstufung</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Als nächstes musste die Bibliothek ausgewählt werden, auf deren Grundlage das Programm erstellt wird. </font><font style="vertical-align: inherit;">Die Schlüsselfaktoren bei meiner Wahl waren:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Unterstützung der Bibliothek für die Python-Oberfläche aufgrund der relativ einfachen Lernfähigkeit dieser Sprache durch Anfänger ist eine einfache Syntax, die sich positiv auf die Lesbarkeit des Programms auswirkt.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Portabilität, d.h. </font><font style="vertical-align: inherit;">die Möglichkeit, ein Programm mit dieser Bibliothek auf Himbeer-Pi3 auszuführen.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Verbreitung der Bibliothek, die eine gut entwickelte Community von Programmierern garantiert, die möglicherweise bereits auf Probleme gestoßen sind, die während Ihrer Arbeit auftreten können.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Unter den Optionen, die ich untersucht habe, habe ich die OpenCV Open Computer Vision-Bibliothek hervorgehoben, da sie Python unterstützt und über eine umfangreiche Online-Dokumentation verfügt. </font><font style="vertical-align: inherit;">Es gibt viele Artikel und Anweisungen im Internet, die alle Feinheiten der Arbeit mit dieser Bibliothek beschreiben. </font><font style="vertical-align: inherit;">Es gibt ein offizielles Forum von Entwicklern, in dem jeder eine Frage dazu stellen kann. </font><font style="vertical-align: inherit;">Außerdem ist diese Bibliothek in C / C ++ - Sprachen implementiert, was die Systemleistung garantiert, und ihre Struktur unterstützt verschiedene Module, die deaktiviert werden können, um die Leistung zu steigern.</font></font><br>
<br>
<h2><font color="#4d7f95"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Software-Entwicklung</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nach der Installation des Betriebssystems und der Erstkonfiguration von Raspberry pi müssen Sie jedoch alle dafür erforderlichen Pakete installieren, bevor Sie mit der Erstellung des Programms beginnen. </font><font style="vertical-align: inherit;">Die meisten dieser Pakete werden wiederum mit dem Pip-Paketmanager installiert (im Fall von Python 3, pip3).</font></font><br>
<br>
<pre><code class="bash hljs">$ sudo apt install python3-pip</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die folgenden Bibliotheken sind installiert, z.</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">picamera - Bibliothek für die Arbeit mit Himbeer-Pi-Kamera</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">numpy - eine Bibliothek zum Arbeiten mit mehrdimensionalen Datenarrays als Bilder</font></font></li>
</ul><br>
<pre><code class="bash hljs">$ sudo pip3 install picamera<font></font>
$ sudo pip3 install numpy<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
cmake - Dienstprogramm zum automatischen Erstellen eines Programms aus dem Quellcode </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
cmake-curses-gui - GUI-Paket (grafische Oberfläche) für cmake</font></font><br>
<br>
<pre><code class="bash hljs">$ sudo apt-get install cmake cmake-curses-gui libgtk2.0-dev<font></font>
$ sudo apt-get install cmake cmake-curses-gui libgtk2.0-dev<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bibliotheken für die Arbeit mit verschiedenen Bild- und Videoformaten und mehr</font></font><br>
<br>
<pre><code class="bash hljs">$ sudo apt-get install libavcodec-dev libavformat-dev libswscale-dev libv4l-dev libx264-dev libxvidcore-dev<font></font>
$ sudo apt-get install libjpeg-dev libpng12-dev libtiff5-dev libjasper-dev<font></font>
$ sudo apt-get install gfortran libatlas-base-dev<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Zum Übertragen von Videodaten vom Roboter zum Computer wird GStreamer verwendet - ein Framework zum Empfangen, Verarbeiten und Übertragen von Multimediadaten:</font></font><br>
<br>
<pre><code class="bash hljs">$ sudo apt install libgstreamer1.0-0 gstreamer1.0-plugins-base gstreamer1.0-plugins-good gstreamer1.0-plugins-bad gstreamer1.0-plugins-ugly gstreamer1.0-libav gstreamer1.0-doc gstreamer1.0-tools gstreamer1.0-x gstreamer1.0-alsa gstreamer1.0-gl gstreamer1.0-gtk3 gstreamer1.0-qt5 gstreamer1.0-pulseaudio
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Der nächste Schritt besteht darin, die openCV-Bibliothek selbst aus Quellen zu installieren, zu konfigurieren und zu erstellen. </font><font style="vertical-align: inherit;">Dazu wird ein opencv-Arbeitsordner erstellt.</font></font><br>
<br>
<pre><code class="bash hljs">$ mkdir opencv<font></font>
$ <span class="hljs-built_in">cd</span> opencv
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Zum Herunterladen der neuesten Versionen der Bibliothek wird wget verwendet - ein Konsolenprogramm zum Herunterladen von Dateien aus dem Netzwerk. </font><font style="vertical-align: inherit;">Zum Zeitpunkt der Erstellung des Programms ist die neueste stabile Version von openCV 4.1.0. Laden Sie also die Quellen herunter und entpacken Sie sie:</font></font><br>
<br>
<pre><code class="bash hljs">$ wget https://github.com/opencv/opencv/archive/4.1.0.zip -O opencv_source.zip<font></font>
$ unzip opencv_source.zip<font></font>
$ wget https://github.com/opencv/opencv_contrib/archive/4.1.0.zip -O opencv_contrib.zip<font></font>
$ unzip opencv_contrib.zip<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nach Abschluss des Entpackvorgangs können die Quellarchive gelöscht werden.</font></font><br>
<br>
<pre><code class="bash hljs">$ rm opencv_source.zip<font></font>
$ rm opencv_contrib.zip<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Für die Montage und Konfiguration wird ein Verzeichnis erstellt.</font></font><br>
<br>
<pre><code class="bash hljs">$ <span class="hljs-built_in">cd</span> /home/pi/opencv/opencv-4.1.0<font></font>
$ mkdir build<font></font>
$ <span class="hljs-built_in">cd</span> build
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Build-Parameter werden mit dem Dienstprogramm cmake konfiguriert. </font><font style="vertical-align: inherit;">Zu diesem Zweck werden alle wichtigen Parameter zusammen mit den zugewiesenen Werten als Dienstprogrammvariablen übergeben:</font></font><br>
<br>
<pre><code class="cmake hljs">cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local -D INSTALL_PYTHON_EXAMPLES=<span class="hljs-keyword">ON</span> -D INSTALL_C_EXAMPLES=<span class="hljs-keyword">OFF</span> -D BUILD_opencv_python2=<span class="hljs-keyword">OFF</span> -D WITH_GSTREAMER=<span class="hljs-keyword">ON</span> -D BUILD_EXAMPLES=<span class="hljs-keyword">ON</span> -DENABLE_VFPV3=<span class="hljs-keyword">ON</span> -DENABLE_NEON=<span class="hljs-keyword">ON</span> -DCPU_BASELINE=NEON ..
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nach dem Einrichten der Konfiguration zeigt das Dienstprogramm alle Parameter an. Als nächstes müssen Sie die Bibliothek kompilieren. Verwenden Sie dazu den Konsolenbefehl make –jN, wobei N die Anzahl der Kerne ist, die am Kompilierungsprozess beteiligt sind. Für Himbeer-Pi 3 beträgt die Anzahl der Kerne 4, aber Sie können diese Anzahl definitiv herausfinden, indem Sie den Befehl nproc in die Konsole schreiben.</font></font><br>
<br>
<pre><code class="bash hljs">$ make –j4</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Aufgrund der begrenzten Ressourcen von Himbeeren kann die Kompilierung eine Weile dauern. </font><font style="vertical-align: inherit;">In einigen Fällen können Himbeeren sogar einfrieren, aber wenn Sie später in den Build-Ordner gehen und make neu registrieren, wird die Arbeit fortgesetzt. </font><font style="vertical-align: inherit;">In diesem Fall lohnt es sich, die Anzahl der beteiligten Kerne zu reduzieren. Meine Kompilierung verlief jedoch ohne Probleme. </font><font style="vertical-align: inherit;">In diesem Stadium lohnt es sich auch, über die aktive Abkühlung von Himbeeren nachzudenken, da selbst damit die Prozessortemperatur etwa 75 Grad erreichte. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wenn die Kompilierung erfolgreich war, muss die Bibliothek installiert werden. </font><font style="vertical-align: inherit;">Dies erfolgt auch mit dem Dienstprogramm make. </font><font style="vertical-align: inherit;">Dann werden wir alle notwendigen Verbindungen mit dem Dienstprogramm ldconfig herstellen:</font></font><br>
<br>
<pre><code class="bash hljs">$ sudo make install<font></font>
$ sudo ldconfig<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wir überprüfen die Installation, indem wir die folgenden Befehle im interaktiven Python-Modus schreiben:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">import</span> cv2<font></font>
print(cv2.getBuildInformation())<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die folgende Schlussfolgerung des Programms ist ein Beweis für die korrekte Installation. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/np/vi/ng/npving2rmncveg11-8qxvhvco1q.jpeg"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Es ist zu beachten, dass das obige Verfahren zum Kompilieren der Bibliothek sowohl auf dem Roboter als auch auf dem PC ausgeführt werden muss, von dem aus die Steuerung des Roboters geplant ist und auf dem die Sendung vom Roboter empfangen wird. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Erstellen eines Videoverteilungsschemas</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bevor Sie mit dem Schreiben von Code beginnen, müssen Sie ein Schema entwickeln, nach dem der Algorithmus funktioniert. Im betrachteten Fall der Softwareentwicklung für einen Roboter, der für die Teilnahme an den RTK Cup-Wettbewerben in der Extreme-Nominierung erstellt wurde, wird das gesamte Programm in zwei Teile unterteilt: einen Roboter und eine Fernbedienung, die von einem Computer mit installiertem Linux gespielt werden. Eine der wichtigsten Aufgaben hierbei ist die Erstellung eines ungefähren Schemas für die Übertragung von Videodaten zwischen verschiedenen Teilen des Algorithmus. Wi-Fi wird als Kommunikationskanal zwischen den beiden Geräten verwendet. Datenpakete, die die Steuerung des Roboters und Rückmeldedaten bereitstellen, werden unter Verwendung des in der Socket-Bibliothek implementierten UDP-Protokolls von einem Gerät zu einem anderen übertragen. VideodatenAufgrund von Einschränkungen in der Größe des UDP-Pakets wird mit GStreamer übertragen. Zum bequemen Debuggen werden zwei Videostreams implementiert:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hauptvideostream - Überträgt Videodaten direkt von der Kamera des Roboters auf einen Computer, um eine minimale Steuerverzögerung zu gewährleisten.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hilfsvideostream - überträgt die vom Roboter verarbeiteten Videodaten, die zum Einrichten und Debuggen eines Programms erforderlich sind, das Computer Vision implementiert.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Auf dem Roboter sind gleichzeitig zwei Videostreams aktiv, und der Computer zeigt je nach aktiviertem Antriebsmodus das gewünschte Bild an. </font><font style="vertical-align: inherit;">Der Roboter verwendet wiederum abhängig davon, ob der Autonomiemodus ein- oder ausgeschaltet ist, entweder Steuerdaten, die von einem Computer empfangen oder von einem Bildprozessor generiert wurden. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/zw/0l/uw/zw0luwrmjjesbm1ygsra6gxtkm4.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die Fernsteuerung des Roboters erfolgt aufgrund der Arbeit von zwei parallelen Flüssen am Roboter und am Computer:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die „Konsole“ in einem Zyklus fragt alle verfügbaren Eingabegeräte ab und bildet ein Steuerdatenpaket, das aus den Daten selbst und der Prüfsumme besteht (zum Zeitpunkt der endgültigen Änderungen am Artikel habe ich mich geweigert, Prüfsummen zu erstellen, um die Verzögerung zu verringern, aber in der Quelle, die ich Am Ende wird dieser Code verlassen) - von einem bestimmten Wert, der aus einem Datensatz berechnet wird, indem ein Algorithmus verwendet wird, der zur Bestimmung der Integrität von Daten während der Übertragung verwendet wird</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Roboter - Wartet auf den Datenzugriff vom Computer. </font><font style="vertical-align: inherit;">Entpackt die Daten, berechnet die Prüfsumme neu und vergleicht sie mit der auf der Computerseite gesendeten und berechneten. </font><font style="vertical-align: inherit;">Wenn die Prüfsummen übereinstimmen, werden die Daten an das Hauptprogramm übertragen.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bevor Sie den Linienerkennungsalgorithmus analysieren, sollten Sie sich mit den Konstruktionsmerkmalen des Roboters vertraut machen:</font></font><br>
<br>
<div class="spoiler"><b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">über den Roboter</font></font></b><div class="spoiler_text">          .<br>
<br>
<img src="https://habrastorage.org/webt/vw/ao/ex/vwaoexwehb49titxcgdis_ryonc.jpeg" width="200" align="right"><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"></a> —       .      (3  )        .                 ,      .      6 ,        .           .      .          .     ,    -  .     «»   rasberry pi 3 b —      .<br>
<br>
<img src="https://habrastorage.org/webt/ho/zw/bi/hozwbiptp_fihoiqncxq2zsbpim.png" width="200" align="left"> ,       ,   ,   ,   Solidworks    petg .    ,     raspberry        .<br>
<br>
<img src="https://habrastorage.org/webt/mh/po/bd/mhpobduedmyoxzrdbhhac2ewdpq.png" width="200" align="left">          ubiquiti bullet M5 hp.     (   )      ,          .   ,   «»  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"> </a> . <br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/ti/h_/7l/tih_7l74vjx8leso89cwynfpb3o.jpeg" width="400"></div><br>
:     «»     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow">  thingiverse</a>.    ,  ,   ,      ,          .<br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/wd/df/q_/wddfq_nyi7-xcqmdiil5glkybqe.gif" width="300"></div><br>
   ,     ,   .       ,     .              ,  ,        ,     ,    .     ,       ,        ,     .<br>
<br>
<img src="https://habrastorage.org/webt/sl/ju/f9/sljuf9jwaqm2kdadelgsgubyf5o.gif" width="450"><br>
<br>
<img src="https://habrastorage.org/webt/sg/xk/_k/sgxk_kt1f0xdxkg4igwgzmbudk0.png" width="250"><br>
<br>
-     (   -  200 )    ,       ,     90       70   (     ),          ,     « ». ,            VL53L0X        ,      .<br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/jq/wh/hc/jqwhhc7et4crpin64qkaw6txgk0.png" width="250"></div><br>
 «»     ,     ,    (<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow">rds3115</a>).    — ,     ,  ,     ,     .<br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/4p/ot/ic/4poticuqt_itsiasls1of3927ma.jpeg" width="250"></div><br>
      ,      ,    ,   :<br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/qc/w5/ol/qcw5olk3klaxq75typuz41hdt18.png" width="250"></div><br>
- ,       ,          ,      .           . <img src="https://habrastorage.org/webt/he/4o/kp/he4okpaqyd5pof9x1cjwc1aalwi.jpeg" width="200" align="left">        raspberry,      ,     .       ,      .<br>
<br>
     ,   USB.            ,            ,     .<br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/rq/wv/yr/rqwvyr8kv5dtvpgz6x7rahoyfho.gif" width="200"></div><br>
<i>        </i><br>
</div></div><br>
<h3><font color="#4d7f95"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Erstellung eines Zeilenerkennungsalgorithmus mit OpenCV-Bibliotheksmethoden</font></font></font></h3><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I. Empfangen von Daten</font></font></b> <br>
<br>
<img src="https://habrastorage.org/webt/ua/q7/zo/uaq7zojtflqtezqkiq2meem5mam.png" width="300" align="right"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Aufgrund der Tatsache, dass der Bildprozessor keine Videodaten direkt von der Kamera, sondern vom Hauptstrom empfängt, ist es notwendig, sie von dem für die Übersetzung verwendeten Format in das für die Bildverarbeitung verwendete Format zu übertragen, nämlich ein aus roten Werten bestehendes Numpy-Array , grün und blau für jedes der Pixel. </font><font style="vertical-align: inherit;">Dazu benötigen Sie die Anfangsdaten - einen Frame, der vom Himbeer-Pi-Kameramodul empfangen wird. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Der einfachste Weg, Bilder von Kamera c zur weiteren Verarbeitung abzurufen, ist die Verwendung der Picamera-Bibliothek. </font><font style="vertical-align: inherit;">Bevor Sie beginnen, müssen Sie den Zugriff auf die Kamera über raspi-config -&gt; Schnittstellenoptionen Kamera -&gt; Ja auswählen zulassen.</font></font><br>
<br>
<pre><code class="bash hljs">sudo raspi-config</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Der nächste Codeabschnitt ist mit der Himbeerkamera verbunden und empfängt in einem Zyklus mit einer bestimmten Frequenz Frames in Form eines Arrays, das von der opencv-Bibliothek verwendet werden kann.</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> picamera.array <span class="hljs-keyword">import</span> PiRGBArray
<span class="hljs-keyword">from</span> picamera <span class="hljs-keyword">import</span> PiCamera
<span class="hljs-keyword">import</span> cv2
<span class="hljs-comment">#   </span><font></font>
camera = PiCamera()<font></font>
camera.resolution = (<span class="hljs-number">640</span>, <span class="hljs-number">480</span>) <font></font>
camera.framerate = <span class="hljs-number">30</span>
cap = PiRGBArray(camera, size=(<span class="hljs-number">640</span>, <span class="hljs-number">480</span>))<font></font>
<font></font>
<span class="hljs-keyword">for</span> frame <span class="hljs-keyword">in</span> camera.capture_continuous(cap , format=<span class="hljs-string">"bgr"</span>, use_video_port=<span class="hljs-literal">True</span>):<font></font>
	new_frame = frame.array<font></font>
	cap.truncate(<span class="hljs-number">0</span>)
	<span class="hljs-keyword">if</span> <span class="hljs-literal">False</span>: <span class="hljs-comment">#   -   </span>
		<span class="hljs-keyword">break</span>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Es ist auch erwähnenswert, dass diese Methode zum Erfassen von Frames zwar die einfachste ist, jedoch einen schwerwiegenden Nachteil aufweist: Sie ist nicht sehr effektiv, wenn Sie Frames über GStreamer senden müssen, da dies mehrere Male erfordert, um das Video neu zu codieren, was die Geschwindigkeit des Programms verringert. </font><font style="vertical-align: inherit;">Ein viel schnellerer Weg, um Bilder zu erhalten, besteht darin, auf Anfrage des Bildprozessors Bilder aus dem Videostream auszugeben. Die weiteren Stufen der Bildverarbeitung hängen jedoch nicht von der verwendeten Methode ab. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ein Beispiel für ein Bild von einer Roboterkamera ohne Verarbeitung:</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/bo/yi/ez/boyiezf6vfa1nqrlcdllhwbmsgg.png" width="350"></div><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">II. Vorverarbeitung</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Wenn Sie auf einer Linie fahren, ist es am einfachsten, den Bereich der Punkte zu trennen, die am stärksten von der Hintergrundfarbe abweichen. Diese Methode ist ideal für den RTK Cup-Wettbewerb geeignet, da sie eine schwarze Linie auf weißem Hintergrund (oder eine weiße Linie auf schwarzem Hintergrund für umgekehrte Abschnitte) verwendet. Um die Menge an Informationen zu reduzieren, die verarbeitet werden müssen, können Sie einen Binärisierungsalgorithmus anwenden, dh das Bild in ein Schwarzweißformat konvertieren, in dem es nur zwei Arten von Pixeln gibt - Dunkel und Hell. Zuvor sollte das Bild in Graustufen übersetzt und verwischt werden, um kleine Fehler und Rauschen zu vermeiden, die während des Betriebs der Kamera unvermeidlich sind. Um das Bild zu verwischen, wird ein Gauß-Filter verwendet.</font></font><br>
<br>
<pre><code class="python hljs">gray = cv2.cvtColor(self._frame, cv2.COLOR_RGB2GRAY)<font></font>
blur = cv2.GaussianBlur(gray, (ksize, ksize), <span class="hljs-number">0</span>)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dabei ist ksize die Größe des Gaußschen Kerns. Wenn Sie diese Größe erhöhen, können Sie den Grad der Unschärfe erhöhen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Beispielbild nach Übersetzung in Graustufen und Unschärfe:</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/ye/5h/_d/ye5h_d7dqttbxo_af3hhkxnllce.png" width="350"></div><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">III. </font><font style="vertical-align: inherit;">Auswählen von Details</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Nachdem das Bild in Graustufen übersetzt wurde, muss es bei einem bestimmten Schwellenwert binärisiert werden. </font><font style="vertical-align: inherit;">Mit dieser Aktion können Sie die Datenmenge weiter reduzieren. Dieser Schwellenwert wird vor jedem Abflug des Roboters an einem neuen Ort oder wenn sich die Lichtverhältnisse ändern, angepasst. </font><font style="vertical-align: inherit;">Im Idealfall besteht die Aufgabe der Kalibrierung darin, sicherzustellen, dass der Umriss der Linie auf dem Bild definiert ist. Gleichzeitig sollten jedoch keine weiteren Details auf dem Bild vorhanden sein, die keine Linie sind:</font></font><br>
<br>
<pre><code class="python hljs">thresh = cv2.threshold(blur, self._limit, <span class="hljs-number">255</span>, cv2.THRESH_BINARY_INV)[<span class="hljs-number">1</span>]</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Hier werden alle Pixel, die dunkler als der Schwellenwert (self._limit) sind, durch 0 (schwarz), heller - durch 255 (weiß) ersetzt. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nach der Verarbeitung sieht das Bild wie folgt aus:</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/ne/rq/tl/nerqtlzt7p-k0q-4quw6nhsbfau.png" width="350"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wie Sie sehen können, hat das Programm mehrere der dunkelsten Teile des Bildes identifiziert. </font><font style="vertical-align: inherit;">Nachdem Sie den Schwellenwert so kalibriert haben, dass die Kopfhörer vollständig „erfasst“ werden, werden neben ihnen andere weiße Elemente auf dem Bildschirm angezeigt. </font><font style="vertical-align: inherit;">Natürlich können Sie die Schwelle fein einstellen, und auf dem Trainingsgelände schaut die Kamera nach unten, sodass keine unnötigen Elemente in den Rahmen gelangen. Ich halte es jedoch für erforderlich, die Linie von allem anderen zu trennen. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">IV. Erkennung</font></font></b> <br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Im binärisierten Bild habe ich einen Rahmensuchalgorithmus angewendet. </font><font style="vertical-align: inherit;">Es wird benötigt, um freistehende Punkte zu bestimmen und sie in eine bequeme Anordnung von Koordinatenwerten von Punkten umzuwandeln, aus denen der Rand besteht. </font><font style="vertical-align: inherit;">Im Fall von opencv, wie in der Dokumentation beschrieben, verwendet der Standardalgorithmus zum Auffinden von Schleifen den Suzuki85-Algorithmus (ich konnte mit Ausnahme der opencv-Dokumentation nirgendwo Verweise auf den Algorithmus mit diesem Namen finden, aber ich gehe davon aus, dass dies der </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Suzuki-Abe-</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Algorithmus ist </font><font style="vertical-align: inherit;">).</font></font><br>
<br>
<pre><code class="python hljs">contours = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)[<span class="hljs-number">0</span>]</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Und hier ist der Rahmen, der zu diesem Zeitpunkt erhalten wurde:</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/ex/3r/gx/ex3rgxc7bmefqdwhetn5wfxrtko.png" width="350"></div><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">V. Verarbeitung auf hoher Ebene</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nachdem alle Konturen im Rahmen gefunden wurden, wird die Kontur mit der größten Fläche ausgewählt und als Kontur der Linie verwendet. Wenn man die Koordinaten aller Punkte dieser Kontur kennt, wird die Koordinate ihres Mittelpunkts gefunden. Hierzu werden die sogenannten "Bildmomente" verwendet. Der Moment ist die Gesamtcharakteristik der Kontur, berechnet durch Summieren der Koordinaten aller Pixel der Kontur. Es gibt verschiedene Arten von Momenten - bis zur dritten Ordnung. Für dieses Problem wird nur das Moment nullter Ordnung (m00) benötigt - die Anzahl aller Punkte, aus denen die Kontur besteht (Konturumfang), das Moment erster Ordnung (m10), das die Summe der X-Koordinaten aller Punkte ist, und m01 ist die Summe der Y-Koordinaten aller Punkte. Durch Teilen der Summe der Koordinaten der Punkte entlang einer der Achsen durch ihre Anzahl wird das arithmetische Mittel erhalten - die ungefähre Koordinate des Mittelpunkts der Kontur. Als nächstes wird die Abweichung des Roboters vom Kurs berechnet:Der Kurs „direkt“ entspricht der Koordinate des Mittelpunkts entlang X nahe der durch zwei geteilten Rahmenbreite. Wenn die Koordinate des Linienmittelpunkts nahe an der Mitte des Rahmens liegt, ist die Steueraktion minimal, und dementsprechend behält der Roboter seinen aktuellen Kurs bei. Wenn der Roboter von einer der Seiten abweicht, wird eine der Abweichung proportionale Steueraktion eingeleitet, bis er zurückkehrt.</font></font><br>
<br>
<pre><code class="python hljs">mainContour = max(contours, key = cv2.contourArea)<font></font>
M = cv2.moments(mainContour)<font></font>
<span class="hljs-keyword">if</span> M[<span class="hljs-string">'m00'</span>] != <span class="hljs-number">0</span>:<span class="hljs-comment">#     (..   -  )</span>
    cx = int(M[<span class="hljs-string">'m10'</span>]/M[<span class="hljs-string">'m00'</span>])<font></font>
    cy = int(M[<span class="hljs-string">'m01'</span>]/M[<span class="hljs-string">'m00'</span>])
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Unten sehen Sie eine schematische Zeichnung der Position des Roboters relativ zur Linie und den Rahmen, wobei die Ergebnisse des Programms überlagert sind: die „Hauptkontur“, die Linien, die durch die Mitte der Kontur verlaufen, sowie der Punkt in der Mitte, um die Abweichung abzuschätzen. </font><font style="vertical-align: inherit;">Diese Elemente werden mit dem folgenden Code hinzugefügt:</font></font><br>
<br>
<pre><code class="python hljs">cv2.line(frame, (cx, <span class="hljs-number">0</span>), (cx, self.height), (<span class="hljs-number">255</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>), <span class="hljs-number">1</span>)    <span class="hljs-comment">#    </span>
cv2.line(frame, (<span class="hljs-number">0</span>, cy), (self.width, cy), (<span class="hljs-number">255</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>), <span class="hljs-number">1</span>)                  <font></font>
cv2.circle(frame, (self.width//<span class="hljs-number">2</span>, self.height//<span class="hljs-number">2</span>), <span class="hljs-number">3</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">255</span>), <span class="hljs-number">-1</span>) <span class="hljs-comment">#  </span>
cv2.drawContours(frame, mainContour, <span class="hljs-number">-1</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">255</span>, <span class="hljs-number">0</span>), <span class="hljs-number">2</span>, cv2.FILLED) <span class="hljs-comment">#   </span>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Zur Vereinfachung des Debuggens werden alle zuvor beschriebenen Elemente zum Roh-Frame hinzugefügt: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/f4/fl/os/f4flos522ouvlu-vi2b9rr_17lg.png" width="350"><br>
<br>
<img src="https://habrastorage.org/webt/uc/r1/vx/ucr1vxcecjqdv5qdswcbjsltw9w.png" width="350"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nachdem wir den Frame durch den Verarbeitungsalgorithmus gefahren haben, erhalten wir die X- und Y-Koordinaten des Zentrums des für uns interessanten Objekts sowie das Debug-Bild. </font><font style="vertical-align: inherit;">Als nächstes wird die Position des Roboters relativ zur Linie schematisch gezeigt, sowie das Bild, das den Verarbeitungsalgorithmus bestanden hat.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/4r/co/fy/4rcofyknawjnesluhuvoyp9zomu.jpeg" width="500"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Der nächste Schritt im Programm besteht darin, die im vorherigen Schritt erhaltenen Informationen in die Leistungswerte von zwei Motoren umzuwandeln. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/io/s3/gn/ios3gnw-mt2xsmvh_hsfrpsdkdu.jpeg" width="250" align="right"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Der einfachste Weg, um die Differenz zwischen der Verschiebung der Mitte des Farbflecks relativ zur Mitte des Rahmens umzurechnen, ist der Proportionalregler (es gibt auch einen Relaisregler, der jedoch aufgrund seiner Funktionsweise nicht sehr gut zum Fahren entlang der Linie geeignet ist). Das Funktionsprinzip eines solchen Algorithmus besteht darin, dass die Steuerung eine Steueraktion auf das Objekt proportional zur Größe des Fehlers erzeugt. Neben dem Proportionalregler gibt es auch einen Integralregler, bei dem die Integralkomponente im Laufe der Zeit den Fehler und das Differential „akkumuliert“, dessen Prinzip auf der Anwendung der Regelungsmaßnahme nur bei ausreichender Änderung der Regelgröße beruht. In der Praxis werden diese einfachsten P, I, D-Regler zu Reglern vom Typ PI, PD, PID kombiniert.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Es ist erwähnenswert, dass ich bei meinem Roboter versucht habe, den PID-Regler zu „starten“, aber seine Verwendung brachte keine ernsthaften Vorteile gegenüber dem üblichen Proportionalregler. Ich gebe zu, dass ich den Regler nicht richtig einstellen konnte, aber es ist auch möglich, dass seine Vorteile bei einem schweren Roboter, der keine hohen Geschwindigkeiten entwickeln kann, nicht so deutlich sichtbar sind. In der neuesten Version des Programms zum Zeitpunkt des Schreibens wird ein einfacher Proportionalregler verwendet, jedoch mit einer kleinen Funktion, mit der Sie mehr Informationen von der Kamera verwenden können: Bei der Generierung des Fehlerwerts wurde nicht nur die horizontale Position der Mitte des Spots berücksichtigt, sondern auch die vertikale, was verschiedene Möglichkeiten ermöglichte auf Linienelemente reagierenBefindet sich „in der Ferne“ und unmittelbar vor oder unter dem Roboter (die Steuerkamera des Roboters hat einen großen Betrachtungswinkel, sodass Sie bereits einen signifikanten Teil des Feldes unter dem Roboter sehen können, wenn Sie ihn nur um 45 Grad nach unten drehen).</font></font><br>
<br>
<pre><code class="python hljs">error= cx / (self.width/<span class="hljs-number">2</span>) - <span class="hljs-number">1</span>  
<span class="hljs-comment">#  ( 0   )  [-1; 1]</span>
error*= cy / self.height + self.gain <span class="hljs-comment">#</span>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Unter den Bedingungen des "RTK Cup" -Wettbewerbs verwenden die Teilnehmer meistens den sogenannten "Tankkreis" - ein oder mehrere Motoren steuern eine Seite des Roboters und funktionieren sowohl mit Ketten als auch mit Rädern. Mit diesem Schema können Sie komplexe Getriebeelemente entfernen, die die Bruchgefahr erhöhen (Differentiale oder Kardanwellen), und den kleinstmöglichen Wenderadius erzielen, was bei einem begrenzten Polygon von Vorteil ist. Dieses Schema beinhaltet die parallele Steuerung von zwei "Seiten" für die Bewegung entlang eines komplexen Pfades. Zu diesem Zweck verwendet das Programm zwei Variablen - die Leistung des rechten und des linken Motors. Diese Leistung hängt von der Grundgeschwindigkeit (BASE_SPEED) ab und variiert im Bereich von 0 bis 100.Fehler (Fehler) - die Differenz zwischen der Mitte des Rahmens und der Koordinate der Linienmitte und dem vom Bediener kalibrierten Proportional-Effekt-Koeffizienten (self._koof). Sein absoluter Wert beeinflusst, wie schnell der Roboter versucht, sich an der Linie auszurichten. Aufgrund der Tatsache, dass bei einem Motor die Regelwirkung von der Grundgeschwindigkeit abgezogen wird und bei dem anderen - es wird hinzugefügt, wird eine Abweichung ausgeführt, wenn von der Strecke abgewichen wird. Die Richtung, in der die Umkehrung durchgeführt wird, kann durch Ändern des Vorzeichens der Variablen self._koof angepasst werden. Möglicherweise stellen Sie auch fest, dass im nächsten Codeabschnitt möglicherweise ein Leistungswert von mehr als 100 angezeigt wird. In meinem Programm werden solche Fälle jedoch später zusätzlich verarbeitet.Sein absoluter Wert beeinflusst, wie schnell der Roboter versucht, sich an der Linie auszurichten. Aufgrund der Tatsache, dass bei einem Motor die Regelwirkung von der Grundgeschwindigkeit abgezogen wird und bei dem anderen - es wird hinzugefügt, wird eine Abweichung ausgeführt, wenn von der Strecke abgewichen wird. Die Richtung, in der die Umkehrung durchgeführt wird, kann durch Ändern des Vorzeichens der Variablen self._koof angepasst werden. Möglicherweise stellen Sie auch fest, dass im nächsten Codeabschnitt möglicherweise ein Leistungswert von mehr als 100 angezeigt wird. In meinem Programm werden solche Fälle jedoch später weiter verarbeitet.Sein absoluter Wert beeinflusst, wie schnell der Roboter versucht, sich an der Linie auszurichten. Aufgrund der Tatsache, dass bei einem Motor die Regelwirkung von der Grundgeschwindigkeit abgezogen wird und bei dem anderen - es wird hinzugefügt, wird eine Abweichung ausgeführt, wenn von der Strecke abgewichen wird. Die Richtung, in der die Umkehrung durchgeführt wird, kann durch Ändern des Vorzeichens der Variablen self._koof angepasst werden. Möglicherweise stellen Sie auch fest, dass im nächsten Codeabschnitt möglicherweise ein Leistungswert von mehr als 100 angezeigt wird. In meinem Programm werden solche Fälle jedoch später zusätzlich verarbeitet.Bei der Umkehrung können Sie die Einstellung vornehmen, indem Sie das Vorzeichen der Variablen self._koof ändern. Möglicherweise stellen Sie auch fest, dass im nächsten Codeabschnitt möglicherweise ein Leistungswert von mehr als 100 angezeigt wird. In meinem Programm werden solche Fälle jedoch später weiter verarbeitet.Bei der Umkehrung können Sie die Einstellung vornehmen, indem Sie das Vorzeichen der Variablen self._koof ändern. Möglicherweise stellen Sie auch fest, dass im nächsten Codeabschnitt möglicherweise ein Leistungswert von mehr als 100 angezeigt wird. In meinem Programm werden solche Fälle jedoch später weiter verarbeitet.</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-comment">#if lineFound:</span><font></font>
leftSpeed = round(self.base_speed + error*self.koof)<font></font>
rightSpeed = round(self.base_speed - error*self.koof)<font></font>
</code></pre><br>
<h2><font color="#4d7f95"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fazit</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nachdem ich das resultierende Programm getestet habe, kann ich sagen, dass der wichtigste schwierige Moment beim Einrichten des Programms die Kalibrierung des Algorithmus auf die Beleuchtungsmerkmale ist. </font><font style="vertical-align: inherit;">Da die Phase der Erstellung des Artikels mit der erklärten Selbstisolierung zusammenfiel, musste ich ein Video mit einer Demonstration der Arbeit in einem kleinen Raum erstellen. </font><font style="vertical-align: inherit;">Dies bereitete mir folgende Schwierigkeiten:</font></font><br>
<br>
<ul>
<li> -,    ,    (   ,     ),        .        ,    ,         ,      .      ,     , ,            ,              </li>
<li> -,       —    ,   ,         </li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Trotz der Tatsache, dass diese beiden Probleme unter den Bedingungen realer Wettbewerbe nicht vorhanden sind, werde ich Maßnahmen ergreifen, um sicherzustellen, dass die Arbeit des Programms nur minimal von externen Faktoren abhängt. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Außerdem ist geplant, in Zukunft die Arbeit an der Implementierung von Algorithmen mithilfe von Computer-Vision-Methoden fortzusetzen und Software zu erstellen, die die verbleibenden im ersten Teil des Artikels beschriebenen Elemente der Autonomie (autonome Beacon-Erfassung, Bewegung auf einem komplexen Pfad) durchlaufen kann. Es ist geplant, die Funktionalität des Roboters um zusätzliche Sensoren zu erweitern: Entfernungsmesser, Gyroskop-Beschleunigungsmesser, Kompass. Trotz der Tatsache, dass die Veröffentlichung dieses Artikels meine Arbeit an dem Projekt als Pflichtschulfach beenden wird, plane ich, hier die weiteren Entwicklungsstadien weiter zu beschreiben. Daher möchte ich Kommentare zu dieser Arbeit erhalten.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nach der Implementierung aller Schritte zur Lösung der Probleme des Projekts kann man mit Sicherheit sagen, dass die Verwendung von Computer-Vision-Algorithmen mit all ihrer relativen Komplexität beim Programmieren und Debuggen in der Phase der Wettbewerbe selbst den größten Gewinn bringt. Aufgrund der geringen Abmessungen der Kamera bietet sie ein enormes Potenzial für die Softwareentwicklung, da Sie mit der Kamera mehrere "herkömmliche" Sensoren gleichzeitig austauschen und dabei unglaublich mehr Informationen von der Außenwelt erhalten können. Es war möglich, das Ziel des Projekts zu verwirklichen - ein Programm zu erstellen, das mithilfe von Computer Vision das Problem der autonomen Navigation des Roboters unter den Bedingungen des Wettbewerbs „RTK Cup“ löst und den Prozess der Programmerstellung und die wichtigsten Phasen der Bildverarbeitung beschreibt.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wie ich bereits sagte, war es jedoch nicht möglich, die komplexe Trajektorie der Hauslinie nachzubilden, und dieses Beispiel zeigt, wie der Algorithmus Kurven erfüllt. </font><font style="vertical-align: inherit;">Die Dicke der Linie entspricht hier der gemäß der Vorschrift, und die Kurve selbst aus den Windungen spiegelt ungefähr die Rotationskrümmung um 90 Grad am Polygon wider:</font></font><br>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/YmHk3f-qQ5E" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Sie können den Programmcode sehen und weitere Arbeiten am Projekt, auf </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">meinem Github</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> oder hier </font><font style="vertical-align: inherit;">überwachen </font><font style="vertical-align: inherit;">, wenn ich fortfahre.</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de497286/index.html">Ludum Dare: Checkliste eine Woche vor dem Start</a></li>
<li><a href="../de497288/index.html">Dekorative Deckenleuchte Feron AL5000</a></li>
<li><a href="../de497290/index.html">Verbessern der Leistung mithilfe des UOP-Cache auf Sandy Bridge +</a></li>
<li><a href="../de497292/index.html">Technologie-Stapel-Shiro-Spiele</a></li>
<li><a href="../de497296/index.html">Beliebte Fehler in Englisch bei IT-Fachleuten. Teil 2: Aussprache</a></li>
<li><a href="../de497304/index.html">Intercepter-NG 2.5 für Android veröffentlicht</a></li>
<li><a href="../de497306/index.html">DLL-Spoofing (DLL-Hijacking)</a></li>
<li><a href="../de497308/index.html">Kann künstliche Intelligenz Kunst machen?</a></li>
<li><a href="../de497310/index.html">Bipolare morphologische Netzwerke: ein Neuron ohne Multiplikation</a></li>
<li><a href="../de497312/index.html">Frage zu CAN FD</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>