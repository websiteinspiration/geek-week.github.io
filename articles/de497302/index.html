<!doctype html>
<html class="no-js" lang="de">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üíáüèΩ üë©üèΩ‚Äç‚úàÔ∏è ‚úåüèø Autonome Navigation eines mobilen Roboters üí™üèª üëâüèΩ üõ¢Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Es gibt eine Vielzahl von M√∂glichkeiten, wie ein Roboter Informationen von der Au√üenwelt empfangen kann, um mit ihm zu interagieren. Abh√§ngig von den ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Autonome Navigation eines mobilen Roboters</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/497302/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Es gibt eine Vielzahl von M√∂glichkeiten, wie ein Roboter Informationen von der Au√üenwelt empfangen kann, um mit ihm zu interagieren. </font><font style="vertical-align: inherit;">Abh√§ngig von den ihm zugewiesenen Aufgaben unterscheiden sich auch die Methoden zur Verarbeitung dieser Informationen. </font><font style="vertical-align: inherit;">In diesem Artikel werde ich die Hauptphasen der im Rahmen des Schulprojekts durchgef√ºhrten Arbeiten beschreiben, deren Ziel es ist, Informationen zu verschiedenen Methoden der autonomen Roboternavigation zu systematisieren und das bei der Erstellung des Roboters f√ºr die Wettbewerbe ‚ÄûRTK Cup‚Äú gewonnene Wissen anzuwenden.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/qb/xw/dw/qbxwdwc_cwrypc3c8dthahudzmk.jpeg"><br>
<a name="habracut"></a><br>
<h2><font color="#4d7f95"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Einf√ºhrung</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bei den Wettbewerben ‚ÄûRTK Cup‚Äú gibt es einen Aufgabenblock, der ohne Eingreifen des Bedieners erledigt werden muss. </font><font style="vertical-align: inherit;">Ich glaube, dass viele Teilnehmer diese Aufgaben zu Unrecht vermeiden, weil die scheinbar Komplexit√§t der Erstellung eines Roboterdesigns und des Schreibens eines Programms weitgehend vereinfachte Aufgaben aus anderen Wettbewerbsdisziplinen verbirgt, die auf einem Trainingsgel√§nde zusammengefasst sind. </font><font style="vertical-align: inherit;">Mit meinem Projekt m√∂chte ich m√∂gliche L√∂sungen f√ºr solche Probleme aufzeigen, wobei ich als Beispiel das Folgende entlang der Linie betrachte. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Um das Projektziel zu erreichen, wurden folgende Zwischenaufgaben formuliert:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Analyse </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">der</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Wettbewerbsregeln ‚ÄûRTK Cup‚Äú</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Analyse bestehender Algorithmen zur autonomen Orientierung eines mobilen Roboters</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Softwareerstellung</font></font></li>
</ul><br>
<h2><font color="#4d7f95"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Analyse der Wettbewerbsregeln ‚ÄûRTK Cup‚Äú</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bei den ‚ÄûRTK Cup‚Äú -Wettbewerben wird den Teilnehmern ein Trainingsgel√§nde pr√§sentiert, auf dem Abschnitte unterschiedlicher Komplexit√§t modelliert werden. </font><font style="vertical-align: inherit;">Der Wettbewerb soll die junge Robotik dazu anregen, Ger√§te zu entwickeln, die unter extremen Bedingungen arbeiten, Hindernisse √ºberwinden, unter menschlicher Kontrolle oder autonom arbeiten k√∂nnen.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/6a/sr/6w/6asr6wzvlhnqgkmz0b8dfyf2h9o.jpeg"><br>
<br>
<div class="spoiler"><b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">kurz √ºber die Elemente, aus denen das Polygon besteht</font></font></b><div class="spoiler_text"> ¬´¬ª          ,    .    ,       ,     (), ,      (),   ..<br>
<br>
:<br>
<br>
<img src="https://habrastorage.org/webt/mk/ks/d3/mkksd313rprmlsilxcrq5xdytg4.png" width="300"><br>
<br>
:<br>
<br>
<img src="https://habrastorage.org/webt/rv/fp/cu/rvfpcu-6qtvdfrclsqjzlr2xok4.jpeg" width="300"><br>
<br>
  ‚Äì  ,     ¬´¬ª  ( )  ,        . ,         ,    ,      ,         .<br>
<br>
     .    ,       ,     ,   ,     ,    ,    .<br>
</div></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die Wettbewerbe sind in zwei grundlegend unterschiedliche Nominierungen unterteilt: "Seeker" und "Extreme". Damit soll sichergestellt werden, dass der Wettbewerb zwischen Teilnehmern mit einem Mindestunterschied in Bezug auf Alter und Erfahrung bei der Entwicklung von Robotersystemen durchgef√ºhrt wurde: Sucher f√ºr j√ºngere und Extreme f√ºr Teilnehmer ab 14 Jahren. Bei der Seeker-Nominierung kann sich der Bediener frei in der Reichweite bewegen und direkten Blickkontakt mit der Maschine haben, w√§hrend bei der Extreme-Nominierung davon ausgegangen wird, dass der Roboter √ºber Videokommunikationssysteme und / oder Computer Vision verf√ºgt, da der Bediener im Labyrinth navigieren muss und sich nur auf das Labyrinth verlassen muss Die Kamera und die Sensoren sind in den Roboter eingebaut, w√§hrend sie sich hinter einem speziellen Bildschirm befinden.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Um sich f√ºr Wettbewerbe zu qualifizieren, muss der Roboter entweder die Aufgabe zur Fernsteuerung des Manipulators bestehen oder eines der Elemente der Autonomie ausf√ºhren. </font><font style="vertical-align: inherit;">Im Rahmen des Projekts wurde die Aufgabe festgelegt, Autonomieaufgaben zu erf√ºllen, da sie die meisten Punkte zu den niedrigsten Kosten des Betreibers liefern. </font><font style="vertical-align: inherit;">Die Elemente der Autonomie umfassen:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fahren entlang einer Linie mit einem Umgebungslichtsensor oder einem Sichtliniensystem</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Standalone-Beacon-Erfassung mit Entfernungssensor oder Bildverarbeitungssystemen</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bewegung entlang einer komplexen Flugbahn (z. B. Auf- / Abstieg von Treppen) entlang einer Linie mit einem Kompass, Gyroskop, Beschleunigungsmesser, Sichtsystem oder kombinierten Methoden</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Au√üerdem werden Punkte zur √úberwindung von Hindernissen verdoppelt, wenn der Roboter sie autonom passiert.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Im Rahmen dieses Projekts wird die L√∂sung der ersten Aufgabe in Betracht gezogen - Bewegung entlang der Linie. Die gebr√§uchlichsten Methoden, um sich entlang der Linie zu bewegen, sind Lichtsensoren und eine Kamera. Zu den Pluspunkten der Sensoren geh√∂rt die einfache Erstellung eines Programms - viele von ihnen sind mit einem Abstimmwiderstand ausgestattet, sodass beim Einstellen des Sensors f√ºr die Hintergrundbeleuchtung 0 oder 1 ausgegeben wird, je nachdem, ob er in der Leitung ist oder nicht. Aus dem gleichen Grund stellen Lichtsensoren keine Anforderungen an die Verarbeitungsleistung der verwendeten Steuerung. Aus diesem Grund ist die L√∂sung des Problems mit Hilfe von Lichtsensoren am kosteng√ºnstigsten - die Kosten f√ºr den einfachsten Sensor betragen 35 Rubel, und f√ºr eine relativ stabile Fahrt entlang der Linie reichen drei Sensoren aus (einer ist an der Linie und zwei an den Seiten installiert). Jedoch,Einer der Hauptnachteile solcher Sensoren sind Installationsbeschr√§nkungen. Idealerweise sollte der Sensor genau in der Mitte in geringem Abstand vom Boden installiert werden, da sonst falsche Werte angezeigt werden. Dies ist kein Problem bei speziellen Wettbewerben, bei denen der Roboter so schnell wie m√∂glich auf der Strecke fahren muss. Unter den Bedingungen des ‚ÄûRTK Cup‚Äú -Wettbewerbs k√∂nnen jedoch alle oben genannten Sensorfehler kritisch sein - ihre Installation erfordert in erster Linie das Vorhandensein zus√§tzlicher mechanischer Teile am Roboter, die anheben und Das Absenken der Sensoren erfordert zus√§tzlichen Platz auf dem Roboter, einen separaten Motor, der die Sensoren bewegt, und ist auch ein Ort f√ºr potenzielle Sch√§den und erh√∂ht die Masse des Roboters.Andernfalls werden falsche Werte angezeigt. Dies ist kein Problem bei speziellen Wettbewerben, bei denen der Roboter so schnell wie m√∂glich auf der Strecke fahren muss. Unter den Bedingungen des ‚ÄûRTK Cup‚Äú -Wettbewerbs k√∂nnen jedoch alle oben genannten Sensorfehler kritisch sein - ihre Installation erfordert in erster Linie das Vorhandensein zus√§tzlicher mechanischer Teile am Roboter, die anheben und Das Absenken der Sensoren erfordert zus√§tzlichen Platz auf dem Roboter, einen separaten Motor, der die Sensoren bewegt, und ist auch ein Ort f√ºr potenzielle Sch√§den und erh√∂ht die Masse des Roboters.Andernfalls werden falsche Werte angezeigt. Dies ist kein Problem bei speziellen Wettbewerben, bei denen der Roboter so schnell wie m√∂glich auf der Strecke fahren muss. Unter den Bedingungen des ‚ÄûRTK Cup‚Äú -Wettbewerbs k√∂nnen jedoch alle oben genannten Sensorfehler kritisch sein - ihre Installation erfordert in erster Linie das Vorhandensein zus√§tzlicher mechanischer Teile am Roboter, die anheben und Das Absenken der Sensoren erfordert zus√§tzlichen Platz auf dem Roboter, einen separaten Motor, der die Sensoren bewegt, und ist auch ein Ort f√ºr potenzielle Sch√§den und erh√∂ht die Masse des Roboters.Alle oben genannten Sensorfehler k√∂nnen kritisch sein - ihre Installation erfordert in erster Linie das Vorhandensein zus√§tzlicher mechanischer Teile am Roboter, die die Sensoren anheben und absenken. Dies erfordert zus√§tzlichen Platz auf dem Roboter, einen separaten Motor, der die Sensoren bewegt, und ist auch ein Ort f√ºr potenzielle Sch√§den und erh√∂ht die Masse des Roboters .Alle oben genannten Sensorfehler k√∂nnen kritisch sein - ihre Installation erfordert in erster Linie das Vorhandensein zus√§tzlicher mechanischer Teile am Roboter, die die Sensoren anheben und absenken. Dies erfordert zus√§tzlichen Platz auf dem Roboter, einen separaten Motor, der die Sensoren bewegt, und ist auch ein Ort f√ºr potenzielle Sch√§den und erh√∂ht die Masse des Roboters .</font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><img src="https://habrastorage.org/webt/_g/pt/zz/_gptzzip0rdk6ocgg67ttumux2k.jpeg" width="300" align="right"></a><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die Kamera hat wiederum die folgenden Vorteile: Sie hat einen praktisch unbegrenzten (im Vergleich zu Sensoren) Messradius, d.h. Nur ein Kameramodul kann gleichzeitig die Linie sehen, sowohl direkt unter dem Roboter als auch in ausreichendem Abstand davon, wodurch beispielsweise seine Kr√ºmmung ausgewertet und eine proportionale Steueraktion ausgew√§hlt werden kann. Gleichzeitig st√∂rt das Kameramodul nicht die Weiterentwicklung des Roboters in anderen Teilen der Deponie, die keine Autonomie erfordern, da die Kamera in einem Abstand vom Boden befestigt ist. Der Hauptnachteil der Kamera besteht darin, dass die Videoverarbeitung einen leistungsstarken Rechenkomplex an Bord des Roboters erfordert und die zu entwickelnde Software eine genauere Abstimmung erfordert, da die Kamera eine Gr√∂√üenordnung mehr Informationen von der Au√üenwelt empf√§ngt als drei Lichtsensoren, w√§hrend Kamera und ComputerIn der Lage sind, die empfangenen Informationen zu verarbeiten, sind um ein Vielfaches mehr als drei Sensoren und "Arduine".</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
F√ºr mich pers√∂nlich liegt die Antwort auf der Hand - in der Nominierung ‚Äûextremal‚Äú muss der Roboter √ºber eine Richtkamera verf√ºgen, mit der der Bediener navigieren kann. </font><font style="vertical-align: inherit;">Wenn Sie vorgefertigte FPV-L√∂sungen verwenden, k√∂nnen die Gesamtkosten f√ºr ‚ÄûSensoren‚Äú sogar noch h√∂her sein, w√§hrend zus√§tzliche Ger√§te installiert werden m√ºssen. </font><font style="vertical-align: inherit;">Dar√ºber hinaus hat ein Roboter mit Himbeer-Pi und einer Kamera ein gr√∂√üeres Potenzial f√ºr die Entwicklung einer autonomen Bewegung, da die Kamera eine Vielzahl von Problemen l√∂sen kann und nicht nur bei Linienbewegungen verwendet werden kann, ohne das Design wesentlich zu komplizieren.</font></font><br>
<br>
<h2><font color="#4d7f95"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Analyse bestehender Computer-Vision-Algorithmen</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Computer Vision ist die Theorie der Erstellung von Ger√§ten, die Bilder von Objekten der realen Welt empfangen, die erhaltenen Daten verarbeiten und verwenden k√∂nnen, um verschiedene Arten von angewandten Problemen ohne menschliches Eingreifen zu l√∂sen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Computer Vision Systeme bestehen aus:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">eine oder mehrere Kameras </font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Computerkomplex </font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Software, die Bildverarbeitungswerkzeuge bereitstellt</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Kommunikationskan√§le zur √úbertragung von Ziel- und Telemetrieinformationen. </font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wie bereits geschrieben, gibt es viele M√∂glichkeiten, Objekte zu identifizieren, die f√ºr uns von Interesse sind. Wenn Sie entlang einer Linie fahren, m√ºssen Sie die Linie selbst vom kontrastierenden Hintergrund trennen (eine schwarze Linie auf wei√üem Hintergrund oder eine wei√üe Linie auf schwarzem Hintergrund f√ºr eine umgekehrte Linie). Algorithmen, die ein Computer-Vision-System verwenden, k√∂nnen in mehrere ‚ÄûSchritte‚Äú zur Verarbeitung des Originalbilds unterteilt werden: </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bildaufnahme</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : Digitale Bilder werden direkt von der Kamera, von einem an das Ger√§t √ºbertragenen Videostream oder als separate Bilder erhalten. Pixelwerte entsprechen normalerweise der Lichtintensit√§t (Farb- oder Graustufenbilder), k√∂nnen jedoch mit verschiedenen physikalischen Messungen in Verbindung gebracht werden, beispielsweise mit der Temperatur einer W√§rmebildkamera. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Vorbearbeitung</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">: Bevor Computer-Vision-Methoden auf Videodaten angewendet werden k√∂nnen, ist abh√§ngig von der verwendeten Methode eine Vorverarbeitung erforderlich, um bestimmte Bedingungen einzuf√ºhren. </font><font style="vertical-align: inherit;">Beispiele sind:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Entfernen von Rauschen oder Verzerrungen durch den verwendeten Sensor</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bildunsch√§rfe wird verwendet, um kleine Artefakte zu entfernen, die w√§hrend des Kamerabetriebs, von Dekomprimierungselementen, Rauschen usw. auftreten.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Verbesserung des Kontrasts, damit die richtigen Informationen wahrscheinlicher erkannt werden k√∂nnen</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">√Ñndern Sie die Belichtung von Schatten oder Lichtern</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Skalieren oder Zuschneiden, um die Strukturen im Bild besser unterscheiden zu k√∂nnen.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Konvertieren eines Bilds in Schwarzwei√ü oder √Ñndern der Aufl√∂sung f√ºr eine schnellere Systemleistung</font></font></li>
</ul><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hervorheben von Details</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : Bilddetails verschiedener Schwierigkeitsgrade werden aus den Videodaten extrahiert. </font><font style="vertical-align: inherit;">Typische Beispiele f√ºr solche Details sind Linien, R√§nder, Kanten, einzelne Punkte und Bereiche, die f√ºr jedes Merkmal charakteristisch sind. </font></font><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Erkennung</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : In einem bestimmten Stadium der Programmarbeit werden programmrelevante Informationen vom Rest des Bildes getrennt. </font><font style="vertical-align: inherit;">Beispiele sind:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Auswahl eines bestimmten Satzes von Punkten von Interesse f√ºr die Farbe, die Anzahl der isolierten Pixel, die in irgendeiner Weise √§hnlich sind (Kr√ºmmung der Figur, Farbe, Helligkeit usw.)</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Segmentierung eines oder mehrerer Bildabschnitte, die ein charakteristisches Objekt enthalten.</font></font></li>
</ul><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Verarbeitung</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> auf </font><b><font style="vertical-align: inherit;">hoher Ebene</font></b><font style="vertical-align: inherit;"> : In diesem Schritt wird die Informationsf√ºlle aus dem Bild auf eine Gr√∂√üe reduziert, die leicht verarbeitet werden kann, z. B. ein Satz bestimmter Pixel oder die Koordinaten des Teils des Bildes, in dem sich das interessierende Objekt angeblich befindet. </font><font style="vertical-align: inherit;">Beispiele sind:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Filtern von Werten nach einem beliebigen Kriterium</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bewertung von Parametern wie den physischen Abmessungen des Objekts, der Form, seiner Position im Rahmen oder relativ zu anderen charakteristischen Objekten</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Einstufung</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Als n√§chstes musste die Bibliothek ausgew√§hlt werden, auf deren Grundlage das Programm erstellt wird. </font><font style="vertical-align: inherit;">Die Schl√ºsselfaktoren bei meiner Wahl waren:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Unterst√ºtzung der Bibliothek f√ºr die Python-Oberfl√§che aufgrund der relativ einfachen Lernf√§higkeit dieser Sprache durch Anf√§nger ist eine einfache Syntax, die sich positiv auf die Lesbarkeit des Programms auswirkt.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Portabilit√§t, d.h. </font><font style="vertical-align: inherit;">die M√∂glichkeit, ein Programm mit dieser Bibliothek auf Himbeer-Pi3 auszuf√ºhren.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Verbreitung der Bibliothek, die eine gut entwickelte Community von Programmierern garantiert, die m√∂glicherweise bereits auf Probleme gesto√üen sind, die w√§hrend Ihrer Arbeit auftreten k√∂nnen.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Unter den Optionen, die ich untersucht habe, habe ich die OpenCV Open Computer Vision-Bibliothek hervorgehoben, da sie Python unterst√ºtzt und √ºber eine umfangreiche Online-Dokumentation verf√ºgt. </font><font style="vertical-align: inherit;">Es gibt viele Artikel und Anweisungen im Internet, die alle Feinheiten der Arbeit mit dieser Bibliothek beschreiben. </font><font style="vertical-align: inherit;">Es gibt ein offizielles Forum von Entwicklern, in dem jeder eine Frage dazu stellen kann. </font><font style="vertical-align: inherit;">Au√üerdem ist diese Bibliothek in C / C ++ - Sprachen implementiert, was die Systemleistung garantiert, und ihre Struktur unterst√ºtzt verschiedene Module, die deaktiviert werden k√∂nnen, um die Leistung zu steigern.</font></font><br>
<br>
<h2><font color="#4d7f95"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Software-Entwicklung</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nach der Installation des Betriebssystems und der Erstkonfiguration von Raspberry pi m√ºssen Sie jedoch alle daf√ºr erforderlichen Pakete installieren, bevor Sie mit der Erstellung des Programms beginnen. </font><font style="vertical-align: inherit;">Die meisten dieser Pakete werden wiederum mit dem Pip-Paketmanager installiert (im Fall von Python 3, pip3).</font></font><br>
<br>
<pre><code class="bash hljs">$ sudo apt install python3-pip</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die folgenden Bibliotheken sind installiert, z.</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">picamera - Bibliothek f√ºr die Arbeit mit Himbeer-Pi-Kamera</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">numpy - eine Bibliothek zum Arbeiten mit mehrdimensionalen Datenarrays als Bilder</font></font></li>
</ul><br>
<pre><code class="bash hljs">$ sudo pip3 install picamera<font></font>
$ sudo pip3 install numpy<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
cmake - Dienstprogramm zum automatischen Erstellen eines Programms aus dem Quellcode </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
cmake-curses-gui - GUI-Paket (grafische Oberfl√§che) f√ºr cmake</font></font><br>
<br>
<pre><code class="bash hljs">$ sudo apt-get install cmake cmake-curses-gui libgtk2.0-dev<font></font>
$ sudo apt-get install cmake cmake-curses-gui libgtk2.0-dev<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bibliotheken f√ºr die Arbeit mit verschiedenen Bild- und Videoformaten und mehr</font></font><br>
<br>
<pre><code class="bash hljs">$ sudo apt-get install libavcodec-dev libavformat-dev libswscale-dev libv4l-dev libx264-dev libxvidcore-dev<font></font>
$ sudo apt-get install libjpeg-dev libpng12-dev libtiff5-dev libjasper-dev<font></font>
$ sudo apt-get install gfortran libatlas-base-dev<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Zum √úbertragen von Videodaten vom Roboter zum Computer wird GStreamer verwendet - ein Framework zum Empfangen, Verarbeiten und √úbertragen von Multimediadaten:</font></font><br>
<br>
<pre><code class="bash hljs">$ sudo apt install libgstreamer1.0-0 gstreamer1.0-plugins-base gstreamer1.0-plugins-good gstreamer1.0-plugins-bad gstreamer1.0-plugins-ugly gstreamer1.0-libav gstreamer1.0-doc gstreamer1.0-tools gstreamer1.0-x gstreamer1.0-alsa gstreamer1.0-gl gstreamer1.0-gtk3 gstreamer1.0-qt5 gstreamer1.0-pulseaudio
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Der n√§chste Schritt besteht darin, die openCV-Bibliothek selbst aus Quellen zu installieren, zu konfigurieren und zu erstellen. </font><font style="vertical-align: inherit;">Dazu wird ein opencv-Arbeitsordner erstellt.</font></font><br>
<br>
<pre><code class="bash hljs">$ mkdir opencv<font></font>
$ <span class="hljs-built_in">cd</span> opencv
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Zum Herunterladen der neuesten Versionen der Bibliothek wird wget verwendet - ein Konsolenprogramm zum Herunterladen von Dateien aus dem Netzwerk. </font><font style="vertical-align: inherit;">Zum Zeitpunkt der Erstellung des Programms ist die neueste stabile Version von openCV 4.1.0. Laden Sie also die Quellen herunter und entpacken Sie sie:</font></font><br>
<br>
<pre><code class="bash hljs">$ wget https://github.com/opencv/opencv/archive/4.1.0.zip -O opencv_source.zip<font></font>
$ unzip opencv_source.zip<font></font>
$ wget https://github.com/opencv/opencv_contrib/archive/4.1.0.zip -O opencv_contrib.zip<font></font>
$ unzip opencv_contrib.zip<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nach Abschluss des Entpackvorgangs k√∂nnen die Quellarchive gel√∂scht werden.</font></font><br>
<br>
<pre><code class="bash hljs">$ rm opencv_source.zip<font></font>
$ rm opencv_contrib.zip<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
F√ºr die Montage und Konfiguration wird ein Verzeichnis erstellt.</font></font><br>
<br>
<pre><code class="bash hljs">$ <span class="hljs-built_in">cd</span> /home/pi/opencv/opencv-4.1.0<font></font>
$ mkdir build<font></font>
$ <span class="hljs-built_in">cd</span> build
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Build-Parameter werden mit dem Dienstprogramm cmake konfiguriert. </font><font style="vertical-align: inherit;">Zu diesem Zweck werden alle wichtigen Parameter zusammen mit den zugewiesenen Werten als Dienstprogrammvariablen √ºbergeben:</font></font><br>
<br>
<pre><code class="cmake hljs">cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local -D INSTALL_PYTHON_EXAMPLES=<span class="hljs-keyword">ON</span> -D INSTALL_C_EXAMPLES=<span class="hljs-keyword">OFF</span> -D BUILD_opencv_python2=<span class="hljs-keyword">OFF</span> -D WITH_GSTREAMER=<span class="hljs-keyword">ON</span> -D BUILD_EXAMPLES=<span class="hljs-keyword">ON</span> -DENABLE_VFPV3=<span class="hljs-keyword">ON</span> -DENABLE_NEON=<span class="hljs-keyword">ON</span> -DCPU_BASELINE=NEON ..
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nach dem Einrichten der Konfiguration zeigt das Dienstprogramm alle Parameter an. Als n√§chstes m√ºssen Sie die Bibliothek kompilieren. Verwenden Sie dazu den Konsolenbefehl make ‚ÄìjN, wobei N die Anzahl der Kerne ist, die am Kompilierungsprozess beteiligt sind. F√ºr Himbeer-Pi 3 betr√§gt die Anzahl der Kerne 4, aber Sie k√∂nnen diese Anzahl definitiv herausfinden, indem Sie den Befehl nproc in die Konsole schreiben.</font></font><br>
<br>
<pre><code class="bash hljs">$ make ‚Äìj4</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Aufgrund der begrenzten Ressourcen von Himbeeren kann die Kompilierung eine Weile dauern. </font><font style="vertical-align: inherit;">In einigen F√§llen k√∂nnen Himbeeren sogar einfrieren, aber wenn Sie sp√§ter in den Build-Ordner gehen und make neu registrieren, wird die Arbeit fortgesetzt. </font><font style="vertical-align: inherit;">In diesem Fall lohnt es sich, die Anzahl der beteiligten Kerne zu reduzieren. Meine Kompilierung verlief jedoch ohne Probleme. </font><font style="vertical-align: inherit;">In diesem Stadium lohnt es sich auch, √ºber die aktive Abk√ºhlung von Himbeeren nachzudenken, da selbst damit die Prozessortemperatur etwa 75 Grad erreichte. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wenn die Kompilierung erfolgreich war, muss die Bibliothek installiert werden. </font><font style="vertical-align: inherit;">Dies erfolgt auch mit dem Dienstprogramm make. </font><font style="vertical-align: inherit;">Dann werden wir alle notwendigen Verbindungen mit dem Dienstprogramm ldconfig herstellen:</font></font><br>
<br>
<pre><code class="bash hljs">$ sudo make install<font></font>
$ sudo ldconfig<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wir √ºberpr√ºfen die Installation, indem wir die folgenden Befehle im interaktiven Python-Modus schreiben:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">import</span> cv2<font></font>
print(cv2.getBuildInformation())<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die folgende Schlussfolgerung des Programms ist ein Beweis f√ºr die korrekte Installation. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/np/vi/ng/npving2rmncveg11-8qxvhvco1q.jpeg"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Es ist zu beachten, dass das obige Verfahren zum Kompilieren der Bibliothek sowohl auf dem Roboter als auch auf dem PC ausgef√ºhrt werden muss, von dem aus die Steuerung des Roboters geplant ist und auf dem die Sendung vom Roboter empfangen wird. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Erstellen eines Videoverteilungsschemas</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bevor Sie mit dem Schreiben von Code beginnen, m√ºssen Sie ein Schema entwickeln, nach dem der Algorithmus funktioniert. Im betrachteten Fall der Softwareentwicklung f√ºr einen Roboter, der f√ºr die Teilnahme an den RTK Cup-Wettbewerben in der Extreme-Nominierung erstellt wurde, wird das gesamte Programm in zwei Teile unterteilt: einen Roboter und eine Fernbedienung, die von einem Computer mit installiertem Linux gespielt werden. Eine der wichtigsten Aufgaben hierbei ist die Erstellung eines ungef√§hren Schemas f√ºr die √úbertragung von Videodaten zwischen verschiedenen Teilen des Algorithmus. Wi-Fi wird als Kommunikationskanal zwischen den beiden Ger√§ten verwendet. Datenpakete, die die Steuerung des Roboters und R√ºckmeldedaten bereitstellen, werden unter Verwendung des in der Socket-Bibliothek implementierten UDP-Protokolls von einem Ger√§t zu einem anderen √ºbertragen. VideodatenAufgrund von Einschr√§nkungen in der Gr√∂√üe des UDP-Pakets wird mit GStreamer √ºbertragen. Zum bequemen Debuggen werden zwei Videostreams implementiert:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hauptvideostream - √úbertr√§gt Videodaten direkt von der Kamera des Roboters auf einen Computer, um eine minimale Steuerverz√∂gerung zu gew√§hrleisten.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hilfsvideostream - √ºbertr√§gt die vom Roboter verarbeiteten Videodaten, die zum Einrichten und Debuggen eines Programms erforderlich sind, das Computer Vision implementiert.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Auf dem Roboter sind gleichzeitig zwei Videostreams aktiv, und der Computer zeigt je nach aktiviertem Antriebsmodus das gew√ºnschte Bild an. </font><font style="vertical-align: inherit;">Der Roboter verwendet wiederum abh√§ngig davon, ob der Autonomiemodus ein- oder ausgeschaltet ist, entweder Steuerdaten, die von einem Computer empfangen oder von einem Bildprozessor generiert wurden. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/zw/0l/uw/zw0luwrmjjesbm1ygsra6gxtkm4.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die Fernsteuerung des Roboters erfolgt aufgrund der Arbeit von zwei parallelen Fl√ºssen am Roboter und am Computer:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die ‚ÄûKonsole‚Äú in einem Zyklus fragt alle verf√ºgbaren Eingabeger√§te ab und bildet ein Steuerdatenpaket, das aus den Daten selbst und der Pr√ºfsumme besteht (zum Zeitpunkt der endg√ºltigen √Ñnderungen am Artikel habe ich mich geweigert, Pr√ºfsummen zu erstellen, um die Verz√∂gerung zu verringern, aber in der Quelle, die ich Am Ende wird dieser Code verlassen) - von einem bestimmten Wert, der aus einem Datensatz berechnet wird, indem ein Algorithmus verwendet wird, der zur Bestimmung der Integrit√§t von Daten w√§hrend der √úbertragung verwendet wird</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Roboter - Wartet auf den Datenzugriff vom Computer. </font><font style="vertical-align: inherit;">Entpackt die Daten, berechnet die Pr√ºfsumme neu und vergleicht sie mit der auf der Computerseite gesendeten und berechneten. </font><font style="vertical-align: inherit;">Wenn die Pr√ºfsummen √ºbereinstimmen, werden die Daten an das Hauptprogramm √ºbertragen.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bevor Sie den Linienerkennungsalgorithmus analysieren, sollten Sie sich mit den Konstruktionsmerkmalen des Roboters vertraut machen:</font></font><br>
<br>
<div class="spoiler"><b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">√ºber den Roboter</font></font></b><div class="spoiler_text">          .<br>
<br>
<img src="https://habrastorage.org/webt/vw/ao/ex/vwaoexwehb49titxcgdis_ryonc.jpeg" width="200" align="right"><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"></a> ‚Äî       .      (3  )        .                 ,      .      6 ,        .           .      .          .     ,    -  .     ¬´¬ª   rasberry pi 3 b ‚Äî      .<br>
<br>
<img src="https://habrastorage.org/webt/ho/zw/bi/hozwbiptp_fihoiqncxq2zsbpim.png" width="200" align="left"> ,       ,   ,   ,   Solidworks    petg .    ,     raspberry        .<br>
<br>
<img src="https://habrastorage.org/webt/mh/po/bd/mhpobduedmyoxzrdbhhac2ewdpq.png" width="200" align="left">          ubiquiti bullet M5 hp.     (   )      ,          .   ,   ¬´¬ª  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"> </a> . <br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/ti/h_/7l/tih_7l74vjx8leso89cwynfpb3o.jpeg" width="400"></div><br>
:     ¬´¬ª     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow">  thingiverse</a>.    ,  ,   ,      ,          .<br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/wd/df/q_/wddfq_nyi7-xcqmdiil5glkybqe.gif" width="300"></div><br>
   ,     ,   .       ,     .              ,  ,        ,     ,    .     ,       ,        ,     .<br>
<br>
<img src="https://habrastorage.org/webt/sl/ju/f9/sljuf9jwaqm2kdadelgsgubyf5o.gif" width="450"><br>
<br>
<img src="https://habrastorage.org/webt/sg/xk/_k/sgxk_kt1f0xdxkg4igwgzmbudk0.png" width="250"><br>
<br>
-     (   -  200 )    ,       ,     90       70   (     ),          ,     ¬´ ¬ª. ,            VL53L0X        ,      .<br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/jq/wh/hc/jqwhhc7et4crpin64qkaw6txgk0.png" width="250"></div><br>
 ¬´¬ª     ,     ,    (<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow">rds3115</a>).    ‚Äî ,     ,  ,     ,     .<br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/4p/ot/ic/4poticuqt_itsiasls1of3927ma.jpeg" width="250"></div><br>
      ,      ,    ,   :<br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/qc/w5/ol/qcw5olk3klaxq75typuz41hdt18.png" width="250"></div><br>
- ,       ,          ,      .           . <img src="https://habrastorage.org/webt/he/4o/kp/he4okpaqyd5pof9x1cjwc1aalwi.jpeg" width="200" align="left">        raspberry,      ,     .       ,      .<br>
<br>
     ,   USB.            ,            ,     .<br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/rq/wv/yr/rqwvyr8kv5dtvpgz6x7rahoyfho.gif" width="200"></div><br>
<i>        </i><br>
</div></div><br>
<h3><font color="#4d7f95"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Erstellung eines Zeilenerkennungsalgorithmus mit OpenCV-Bibliotheksmethoden</font></font></font></h3><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I. Empfangen von Daten</font></font></b> <br>
<br>
<img src="https://habrastorage.org/webt/ua/q7/zo/uaq7zojtflqtezqkiq2meem5mam.png" width="300" align="right"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Aufgrund der Tatsache, dass der Bildprozessor keine Videodaten direkt von der Kamera, sondern vom Hauptstrom empf√§ngt, ist es notwendig, sie von dem f√ºr die √úbersetzung verwendeten Format in das f√ºr die Bildverarbeitung verwendete Format zu √ºbertragen, n√§mlich ein aus roten Werten bestehendes Numpy-Array , gr√ºn und blau f√ºr jedes der Pixel. </font><font style="vertical-align: inherit;">Dazu ben√∂tigen Sie die Anfangsdaten - einen Frame, der vom Himbeer-Pi-Kameramodul empfangen wird. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Der einfachste Weg, Bilder von Kamera c zur weiteren Verarbeitung abzurufen, ist die Verwendung der Picamera-Bibliothek. </font><font style="vertical-align: inherit;">Bevor Sie beginnen, m√ºssen Sie den Zugriff auf die Kamera √ºber raspi-config -&gt; Schnittstellenoptionen Kamera -&gt; Ja ausw√§hlen zulassen.</font></font><br>
<br>
<pre><code class="bash hljs">sudo raspi-config</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Der n√§chste Codeabschnitt ist mit der Himbeerkamera verbunden und empf√§ngt in einem Zyklus mit einer bestimmten Frequenz Frames in Form eines Arrays, das von der opencv-Bibliothek verwendet werden kann.</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> picamera.array <span class="hljs-keyword">import</span> PiRGBArray
<span class="hljs-keyword">from</span> picamera <span class="hljs-keyword">import</span> PiCamera
<span class="hljs-keyword">import</span> cv2
<span class="hljs-comment">#   </span><font></font>
camera = PiCamera()<font></font>
camera.resolution = (<span class="hljs-number">640</span>, <span class="hljs-number">480</span>) <font></font>
camera.framerate = <span class="hljs-number">30</span>
cap = PiRGBArray(camera, size=(<span class="hljs-number">640</span>, <span class="hljs-number">480</span>))<font></font>
<font></font>
<span class="hljs-keyword">for</span> frame <span class="hljs-keyword">in</span> camera.capture_continuous(cap , format=<span class="hljs-string">"bgr"</span>, use_video_port=<span class="hljs-literal">True</span>):<font></font>
	new_frame = frame.array<font></font>
	cap.truncate(<span class="hljs-number">0</span>)
	<span class="hljs-keyword">if</span> <span class="hljs-literal">False</span>: <span class="hljs-comment">#   -   </span>
		<span class="hljs-keyword">break</span>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Es ist auch erw√§hnenswert, dass diese Methode zum Erfassen von Frames zwar die einfachste ist, jedoch einen schwerwiegenden Nachteil aufweist: Sie ist nicht sehr effektiv, wenn Sie Frames √ºber GStreamer senden m√ºssen, da dies mehrere Male erfordert, um das Video neu zu codieren, was die Geschwindigkeit des Programms verringert. </font><font style="vertical-align: inherit;">Ein viel schnellerer Weg, um Bilder zu erhalten, besteht darin, auf Anfrage des Bildprozessors Bilder aus dem Videostream auszugeben. Die weiteren Stufen der Bildverarbeitung h√§ngen jedoch nicht von der verwendeten Methode ab. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ein Beispiel f√ºr ein Bild von einer Roboterkamera ohne Verarbeitung:</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/bo/yi/ez/boyiezf6vfa1nqrlcdllhwbmsgg.png" width="350"></div><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">II. Vorverarbeitung</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Wenn Sie auf einer Linie fahren, ist es am einfachsten, den Bereich der Punkte zu trennen, die am st√§rksten von der Hintergrundfarbe abweichen. Diese Methode ist ideal f√ºr den RTK Cup-Wettbewerb geeignet, da sie eine schwarze Linie auf wei√üem Hintergrund (oder eine wei√üe Linie auf schwarzem Hintergrund f√ºr umgekehrte Abschnitte) verwendet. Um die Menge an Informationen zu reduzieren, die verarbeitet werden m√ºssen, k√∂nnen Sie einen Bin√§risierungsalgorithmus anwenden, dh das Bild in ein Schwarzwei√üformat konvertieren, in dem es nur zwei Arten von Pixeln gibt - Dunkel und Hell. Zuvor sollte das Bild in Graustufen √ºbersetzt und verwischt werden, um kleine Fehler und Rauschen zu vermeiden, die w√§hrend des Betriebs der Kamera unvermeidlich sind. Um das Bild zu verwischen, wird ein Gau√ü-Filter verwendet.</font></font><br>
<br>
<pre><code class="python hljs">gray = cv2.cvtColor(self._frame, cv2.COLOR_RGB2GRAY)<font></font>
blur = cv2.GaussianBlur(gray, (ksize, ksize), <span class="hljs-number">0</span>)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dabei ist ksize die Gr√∂√üe des Gau√üschen Kerns. Wenn Sie diese Gr√∂√üe erh√∂hen, k√∂nnen Sie den Grad der Unsch√§rfe erh√∂hen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Beispielbild nach √úbersetzung in Graustufen und Unsch√§rfe:</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/ye/5h/_d/ye5h_d7dqttbxo_af3hhkxnllce.png" width="350"></div><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">III. </font><font style="vertical-align: inherit;">Ausw√§hlen von Details</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Nachdem das Bild in Graustufen √ºbersetzt wurde, muss es bei einem bestimmten Schwellenwert bin√§risiert werden. </font><font style="vertical-align: inherit;">Mit dieser Aktion k√∂nnen Sie die Datenmenge weiter reduzieren. Dieser Schwellenwert wird vor jedem Abflug des Roboters an einem neuen Ort oder wenn sich die Lichtverh√§ltnisse √§ndern, angepasst. </font><font style="vertical-align: inherit;">Im Idealfall besteht die Aufgabe der Kalibrierung darin, sicherzustellen, dass der Umriss der Linie auf dem Bild definiert ist. Gleichzeitig sollten jedoch keine weiteren Details auf dem Bild vorhanden sein, die keine Linie sind:</font></font><br>
<br>
<pre><code class="python hljs">thresh = cv2.threshold(blur, self._limit, <span class="hljs-number">255</span>, cv2.THRESH_BINARY_INV)[<span class="hljs-number">1</span>]</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Hier werden alle Pixel, die dunkler als der Schwellenwert (self._limit) sind, durch 0 (schwarz), heller - durch 255 (wei√ü) ersetzt. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nach der Verarbeitung sieht das Bild wie folgt aus:</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/ne/rq/tl/nerqtlzt7p-k0q-4quw6nhsbfau.png" width="350"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wie Sie sehen k√∂nnen, hat das Programm mehrere der dunkelsten Teile des Bildes identifiziert. </font><font style="vertical-align: inherit;">Nachdem Sie den Schwellenwert so kalibriert haben, dass die Kopfh√∂rer vollst√§ndig ‚Äûerfasst‚Äú werden, werden neben ihnen andere wei√üe Elemente auf dem Bildschirm angezeigt. </font><font style="vertical-align: inherit;">Nat√ºrlich k√∂nnen Sie die Schwelle fein einstellen, und auf dem Trainingsgel√§nde schaut die Kamera nach unten, sodass keine unn√∂tigen Elemente in den Rahmen gelangen. Ich halte es jedoch f√ºr erforderlich, die Linie von allem anderen zu trennen. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">IV. Erkennung</font></font></b> <br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Im bin√§risierten Bild habe ich einen Rahmensuchalgorithmus angewendet. </font><font style="vertical-align: inherit;">Es wird ben√∂tigt, um freistehende Punkte zu bestimmen und sie in eine bequeme Anordnung von Koordinatenwerten von Punkten umzuwandeln, aus denen der Rand besteht. </font><font style="vertical-align: inherit;">Im Fall von opencv, wie in der Dokumentation beschrieben, verwendet der Standardalgorithmus zum Auffinden von Schleifen den Suzuki85-Algorithmus (ich konnte mit Ausnahme der opencv-Dokumentation nirgendwo Verweise auf den Algorithmus mit diesem Namen finden, aber ich gehe davon aus, dass dies der </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Suzuki-Abe-</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Algorithmus ist </font><font style="vertical-align: inherit;">).</font></font><br>
<br>
<pre><code class="python hljs">contours = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)[<span class="hljs-number">0</span>]</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Und hier ist der Rahmen, der zu diesem Zeitpunkt erhalten wurde:</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/ex/3r/gx/ex3rgxc7bmefqdwhetn5wfxrtko.png" width="350"></div><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">V. Verarbeitung auf hoher Ebene</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nachdem alle Konturen im Rahmen gefunden wurden, wird die Kontur mit der gr√∂√üten Fl√§che ausgew√§hlt und als Kontur der Linie verwendet. Wenn man die Koordinaten aller Punkte dieser Kontur kennt, wird die Koordinate ihres Mittelpunkts gefunden. Hierzu werden die sogenannten "Bildmomente" verwendet. Der Moment ist die Gesamtcharakteristik der Kontur, berechnet durch Summieren der Koordinaten aller Pixel der Kontur. Es gibt verschiedene Arten von Momenten - bis zur dritten Ordnung. F√ºr dieses Problem wird nur das Moment nullter Ordnung (m00) ben√∂tigt - die Anzahl aller Punkte, aus denen die Kontur besteht (Konturumfang), das Moment erster Ordnung (m10), das die Summe der X-Koordinaten aller Punkte ist, und m01 ist die Summe der Y-Koordinaten aller Punkte. Durch Teilen der Summe der Koordinaten der Punkte entlang einer der Achsen durch ihre Anzahl wird das arithmetische Mittel erhalten - die ungef√§hre Koordinate des Mittelpunkts der Kontur. Als n√§chstes wird die Abweichung des Roboters vom Kurs berechnet:Der Kurs ‚Äûdirekt‚Äú entspricht der Koordinate des Mittelpunkts entlang X nahe der durch zwei geteilten Rahmenbreite. Wenn die Koordinate des Linienmittelpunkts nahe an der Mitte des Rahmens liegt, ist die Steueraktion minimal, und dementsprechend beh√§lt der Roboter seinen aktuellen Kurs bei. Wenn der Roboter von einer der Seiten abweicht, wird eine der Abweichung proportionale Steueraktion eingeleitet, bis er zur√ºckkehrt.</font></font><br>
<br>
<pre><code class="python hljs">mainContour = max(contours, key = cv2.contourArea)<font></font>
M = cv2.moments(mainContour)<font></font>
<span class="hljs-keyword">if</span> M[<span class="hljs-string">'m00'</span>] != <span class="hljs-number">0</span>:<span class="hljs-comment">#     (..   -  )</span>
    cx = int(M[<span class="hljs-string">'m10'</span>]/M[<span class="hljs-string">'m00'</span>])<font></font>
    cy = int(M[<span class="hljs-string">'m01'</span>]/M[<span class="hljs-string">'m00'</span>])
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Unten sehen Sie eine schematische Zeichnung der Position des Roboters relativ zur Linie und den Rahmen, wobei die Ergebnisse des Programms √ºberlagert sind: die ‚ÄûHauptkontur‚Äú, die Linien, die durch die Mitte der Kontur verlaufen, sowie der Punkt in der Mitte, um die Abweichung abzusch√§tzen. </font><font style="vertical-align: inherit;">Diese Elemente werden mit dem folgenden Code hinzugef√ºgt:</font></font><br>
<br>
<pre><code class="python hljs">cv2.line(frame, (cx, <span class="hljs-number">0</span>), (cx, self.height), (<span class="hljs-number">255</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>), <span class="hljs-number">1</span>)    <span class="hljs-comment">#    </span>
cv2.line(frame, (<span class="hljs-number">0</span>, cy), (self.width, cy), (<span class="hljs-number">255</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>), <span class="hljs-number">1</span>)                  <font></font>
cv2.circle(frame, (self.width//<span class="hljs-number">2</span>, self.height//<span class="hljs-number">2</span>), <span class="hljs-number">3</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">255</span>), <span class="hljs-number">-1</span>) <span class="hljs-comment">#  </span>
cv2.drawContours(frame, mainContour, <span class="hljs-number">-1</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">255</span>, <span class="hljs-number">0</span>), <span class="hljs-number">2</span>, cv2.FILLED) <span class="hljs-comment">#   </span>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Zur Vereinfachung des Debuggens werden alle zuvor beschriebenen Elemente zum Roh-Frame hinzugef√ºgt: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/f4/fl/os/f4flos522ouvlu-vi2b9rr_17lg.png" width="350"><br>
<br>
<img src="https://habrastorage.org/webt/uc/r1/vx/ucr1vxcecjqdv5qdswcbjsltw9w.png" width="350"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nachdem wir den Frame durch den Verarbeitungsalgorithmus gefahren haben, erhalten wir die X- und Y-Koordinaten des Zentrums des f√ºr uns interessanten Objekts sowie das Debug-Bild. </font><font style="vertical-align: inherit;">Als n√§chstes wird die Position des Roboters relativ zur Linie schematisch gezeigt, sowie das Bild, das den Verarbeitungsalgorithmus bestanden hat.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/4r/co/fy/4rcofyknawjnesluhuvoyp9zomu.jpeg" width="500"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Der n√§chste Schritt im Programm besteht darin, die im vorherigen Schritt erhaltenen Informationen in die Leistungswerte von zwei Motoren umzuwandeln. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/io/s3/gn/ios3gnw-mt2xsmvh_hsfrpsdkdu.jpeg" width="250" align="right"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Der einfachste Weg, um die Differenz zwischen der Verschiebung der Mitte des Farbflecks relativ zur Mitte des Rahmens umzurechnen, ist der Proportionalregler (es gibt auch einen Relaisregler, der jedoch aufgrund seiner Funktionsweise nicht sehr gut zum Fahren entlang der Linie geeignet ist). Das Funktionsprinzip eines solchen Algorithmus besteht darin, dass die Steuerung eine Steueraktion auf das Objekt proportional zur Gr√∂√üe des Fehlers erzeugt. Neben dem Proportionalregler gibt es auch einen Integralregler, bei dem die Integralkomponente im Laufe der Zeit den Fehler und das Differential ‚Äûakkumuliert‚Äú, dessen Prinzip auf der Anwendung der Regelungsma√ünahme nur bei ausreichender √Ñnderung der Regelgr√∂√üe beruht. In der Praxis werden diese einfachsten P, I, D-Regler zu Reglern vom Typ PI, PD, PID kombiniert.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Es ist erw√§hnenswert, dass ich bei meinem Roboter versucht habe, den PID-Regler zu ‚Äûstarten‚Äú, aber seine Verwendung brachte keine ernsthaften Vorteile gegen√ºber dem √ºblichen Proportionalregler. Ich gebe zu, dass ich den Regler nicht richtig einstellen konnte, aber es ist auch m√∂glich, dass seine Vorteile bei einem schweren Roboter, der keine hohen Geschwindigkeiten entwickeln kann, nicht so deutlich sichtbar sind. In der neuesten Version des Programms zum Zeitpunkt des Schreibens wird ein einfacher Proportionalregler verwendet, jedoch mit einer kleinen Funktion, mit der Sie mehr Informationen von der Kamera verwenden k√∂nnen: Bei der Generierung des Fehlerwerts wurde nicht nur die horizontale Position der Mitte des Spots ber√ºcksichtigt, sondern auch die vertikale, was verschiedene M√∂glichkeiten erm√∂glichte auf Linienelemente reagierenBefindet sich ‚Äûin der Ferne‚Äú und unmittelbar vor oder unter dem Roboter (die Steuerkamera des Roboters hat einen gro√üen Betrachtungswinkel, sodass Sie bereits einen signifikanten Teil des Feldes unter dem Roboter sehen k√∂nnen, wenn Sie ihn nur um 45 Grad nach unten drehen).</font></font><br>
<br>
<pre><code class="python hljs">error= cx / (self.width/<span class="hljs-number">2</span>) - <span class="hljs-number">1</span>  
<span class="hljs-comment">#  ( 0   )  [-1; 1]</span>
error*= cy / self.height + self.gain <span class="hljs-comment">#</span>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Unter den Bedingungen des "RTK Cup" -Wettbewerbs verwenden die Teilnehmer meistens den sogenannten "Tankkreis" - ein oder mehrere Motoren steuern eine Seite des Roboters und funktionieren sowohl mit Ketten als auch mit R√§dern. Mit diesem Schema k√∂nnen Sie komplexe Getriebeelemente entfernen, die die Bruchgefahr erh√∂hen (Differentiale oder Kardanwellen), und den kleinstm√∂glichen Wenderadius erzielen, was bei einem begrenzten Polygon von Vorteil ist. Dieses Schema beinhaltet die parallele Steuerung von zwei "Seiten" f√ºr die Bewegung entlang eines komplexen Pfades. Zu diesem Zweck verwendet das Programm zwei Variablen - die Leistung des rechten und des linken Motors. Diese Leistung h√§ngt von der Grundgeschwindigkeit (BASE_SPEED) ab und variiert im Bereich von 0 bis 100.Fehler (Fehler) - die Differenz zwischen der Mitte des Rahmens und der Koordinate der Linienmitte und dem vom Bediener kalibrierten Proportional-Effekt-Koeffizienten (self._koof). Sein absoluter Wert beeinflusst, wie schnell der Roboter versucht, sich an der Linie auszurichten. Aufgrund der Tatsache, dass bei einem Motor die Regelwirkung von der Grundgeschwindigkeit abgezogen wird und bei dem anderen - es wird hinzugef√ºgt, wird eine Abweichung ausgef√ºhrt, wenn von der Strecke abgewichen wird. Die Richtung, in der die Umkehrung durchgef√ºhrt wird, kann durch √Ñndern des Vorzeichens der Variablen self._koof angepasst werden. M√∂glicherweise stellen Sie auch fest, dass im n√§chsten Codeabschnitt m√∂glicherweise ein Leistungswert von mehr als 100 angezeigt wird. In meinem Programm werden solche F√§lle jedoch sp√§ter zus√§tzlich verarbeitet.Sein absoluter Wert beeinflusst, wie schnell der Roboter versucht, sich an der Linie auszurichten. Aufgrund der Tatsache, dass bei einem Motor die Regelwirkung von der Grundgeschwindigkeit abgezogen wird und bei dem anderen - es wird hinzugef√ºgt, wird eine Abweichung ausgef√ºhrt, wenn von der Strecke abgewichen wird. Die Richtung, in der die Umkehrung durchgef√ºhrt wird, kann durch √Ñndern des Vorzeichens der Variablen self._koof angepasst werden. M√∂glicherweise stellen Sie auch fest, dass im n√§chsten Codeabschnitt m√∂glicherweise ein Leistungswert von mehr als 100 angezeigt wird. In meinem Programm werden solche F√§lle jedoch sp√§ter weiter verarbeitet.Sein absoluter Wert beeinflusst, wie schnell der Roboter versucht, sich an der Linie auszurichten. Aufgrund der Tatsache, dass bei einem Motor die Regelwirkung von der Grundgeschwindigkeit abgezogen wird und bei dem anderen - es wird hinzugef√ºgt, wird eine Abweichung ausgef√ºhrt, wenn von der Strecke abgewichen wird. Die Richtung, in der die Umkehrung durchgef√ºhrt wird, kann durch √Ñndern des Vorzeichens der Variablen self._koof angepasst werden. M√∂glicherweise stellen Sie auch fest, dass im n√§chsten Codeabschnitt m√∂glicherweise ein Leistungswert von mehr als 100 angezeigt wird. In meinem Programm werden solche F√§lle jedoch sp√§ter zus√§tzlich verarbeitet.Bei der Umkehrung k√∂nnen Sie die Einstellung vornehmen, indem Sie das Vorzeichen der Variablen self._koof √§ndern. M√∂glicherweise stellen Sie auch fest, dass im n√§chsten Codeabschnitt m√∂glicherweise ein Leistungswert von mehr als 100 angezeigt wird. In meinem Programm werden solche F√§lle jedoch sp√§ter weiter verarbeitet.Bei der Umkehrung k√∂nnen Sie die Einstellung vornehmen, indem Sie das Vorzeichen der Variablen self._koof √§ndern. M√∂glicherweise stellen Sie auch fest, dass im n√§chsten Codeabschnitt m√∂glicherweise ein Leistungswert von mehr als 100 angezeigt wird. In meinem Programm werden solche F√§lle jedoch sp√§ter weiter verarbeitet.</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-comment">#if lineFound:</span><font></font>
leftSpeed = round(self.base_speed + error*self.koof)<font></font>
rightSpeed = round(self.base_speed - error*self.koof)<font></font>
</code></pre><br>
<h2><font color="#4d7f95"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fazit</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nachdem ich das resultierende Programm getestet habe, kann ich sagen, dass der wichtigste schwierige Moment beim Einrichten des Programms die Kalibrierung des Algorithmus auf die Beleuchtungsmerkmale ist. </font><font style="vertical-align: inherit;">Da die Phase der Erstellung des Artikels mit der erkl√§rten Selbstisolierung zusammenfiel, musste ich ein Video mit einer Demonstration der Arbeit in einem kleinen Raum erstellen. </font><font style="vertical-align: inherit;">Dies bereitete mir folgende Schwierigkeiten:</font></font><br>
<br>
<ul>
<li> -,    ,    (   ,     ),        .        ,    ,         ,      .      ,     , ,            ,              </li>
<li> -,       ‚Äî    ,   ,         </li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Trotz der Tatsache, dass diese beiden Probleme unter den Bedingungen realer Wettbewerbe nicht vorhanden sind, werde ich Ma√ünahmen ergreifen, um sicherzustellen, dass die Arbeit des Programms nur minimal von externen Faktoren abh√§ngt. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Au√üerdem ist geplant, in Zukunft die Arbeit an der Implementierung von Algorithmen mithilfe von Computer-Vision-Methoden fortzusetzen und Software zu erstellen, die die verbleibenden im ersten Teil des Artikels beschriebenen Elemente der Autonomie (autonome Beacon-Erfassung, Bewegung auf einem komplexen Pfad) durchlaufen kann. Es ist geplant, die Funktionalit√§t des Roboters um zus√§tzliche Sensoren zu erweitern: Entfernungsmesser, Gyroskop-Beschleunigungsmesser, Kompass. Trotz der Tatsache, dass die Ver√∂ffentlichung dieses Artikels meine Arbeit an dem Projekt als Pflichtschulfach beenden wird, plane ich, hier die weiteren Entwicklungsstadien weiter zu beschreiben. Daher m√∂chte ich Kommentare zu dieser Arbeit erhalten.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nach der Implementierung aller Schritte zur L√∂sung der Probleme des Projekts kann man mit Sicherheit sagen, dass die Verwendung von Computer-Vision-Algorithmen mit all ihrer relativen Komplexit√§t beim Programmieren und Debuggen in der Phase der Wettbewerbe selbst den gr√∂√üten Gewinn bringt. Aufgrund der geringen Abmessungen der Kamera bietet sie ein enormes Potenzial f√ºr die Softwareentwicklung, da Sie mit der Kamera mehrere "herk√∂mmliche" Sensoren gleichzeitig austauschen und dabei unglaublich mehr Informationen von der Au√üenwelt erhalten k√∂nnen. Es war m√∂glich, das Ziel des Projekts zu verwirklichen - ein Programm zu erstellen, das mithilfe von Computer Vision das Problem der autonomen Navigation des Roboters unter den Bedingungen des Wettbewerbs ‚ÄûRTK Cup‚Äú l√∂st und den Prozess der Programmerstellung und die wichtigsten Phasen der Bildverarbeitung beschreibt.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wie ich bereits sagte, war es jedoch nicht m√∂glich, die komplexe Trajektorie der Hauslinie nachzubilden, und dieses Beispiel zeigt, wie der Algorithmus Kurven erf√ºllt. </font><font style="vertical-align: inherit;">Die Dicke der Linie entspricht hier der gem√§√ü der Vorschrift, und die Kurve selbst aus den Windungen spiegelt ungef√§hr die Rotationskr√ºmmung um 90 Grad am Polygon wider:</font></font><br>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/YmHk3f-qQ5E" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Sie k√∂nnen den Programmcode sehen und weitere Arbeiten am Projekt, auf </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">meinem Github</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> oder hier </font><font style="vertical-align: inherit;">√ºberwachen </font><font style="vertical-align: inherit;">, wenn ich fortfahre.</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de497286/index.html">Ludum Dare: Checkliste eine Woche vor dem Start</a></li>
<li><a href="../de497288/index.html">Dekorative Deckenleuchte Feron AL5000</a></li>
<li><a href="../de497290/index.html">Verbessern der Leistung mithilfe des UOP-Cache auf Sandy Bridge +</a></li>
<li><a href="../de497292/index.html">Technologie-Stapel-Shiro-Spiele</a></li>
<li><a href="../de497296/index.html">Beliebte Fehler in Englisch bei IT-Fachleuten. Teil 2: Aussprache</a></li>
<li><a href="../de497304/index.html">Intercepter-NG 2.5 f√ºr Android ver√∂ffentlicht</a></li>
<li><a href="../de497306/index.html">DLL-Spoofing (DLL-Hijacking)</a></li>
<li><a href="../de497308/index.html">Kann k√ºnstliche Intelligenz Kunst machen?</a></li>
<li><a href="../de497310/index.html">Bipolare morphologische Netzwerke: ein Neuron ohne Multiplikation</a></li>
<li><a href="../de497312/index.html">Frage zu CAN FD</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>