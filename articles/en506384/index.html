<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üå∑ üêøÔ∏è üìç Neural network - training without a teacher. Policy Gradient Method üë• ‚óæÔ∏è üî∏</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Good day, Habr
 This article opens a series of articles on how to train neural networks without a teacher. 
 (Reinforcement Learning for Neuron Networ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Neural network - training without a teacher. Policy Gradient Method</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/506384/"><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Good day, Habr</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
This article opens a series of articles on how to train neural networks without a teacher. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 (Reinforcement Learning for Neuron Networks) </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In the cycle I plan to make three articles on the theory and implementation in the code of three learning algorithms for neural networks without a teacher. </font><font style="vertical-align: inherit;">The first article will be on Policy Gradient, the second on Q-learning, the third article will be final according to the Actor-Critic method. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Enjoy reading.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Article One - Policy Gradient Learning Without Teacher </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 (Policy Gradient for Reinforcement Learning)</font></font></h4><br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Introduction</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Among machine learning algorithms, a special place is occupied by machine learning algorithms where the algorithm learns to solve the problem on its own without human intervention, directly interacting with the environment in which he learns. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Such algorithms have received the general name - learning algorithms without a teacher, for such algorithms, you do not need to collect databases, you do not need to classify or mark them up. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
It is enough for the student without the teacher to only give a backward response to his actions or decisions - they were good or not.</font></font><br>
<a name="habracut"></a><br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Chapter 1. Teacher Training</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
So what is it - Learning with or without a teacher. We will examine in more detail this with examples from modern machine learning and the tasks that it solves. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Most modern machine learning algorithms for problems of classification, regression, segmentation are essentially training algorithms with a teacher in which the person is the teacher. Because it is the person marking up the data that tells the algorithm what the correct answer should be and thereby the algorithm tries to find a solution so that the answer that the algorithm gives when solving the problem matches the answer that the person indicated for the given task as the correct answer. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Using the example of the classification problem for the Mnist dataset, the correct answer that the person gives to the algorithm is the label of the class of the digit in the training set.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/za/kq/b6/zakqb6ob1_xce07caywnurtsl7k.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In the Mnist dataset, for each image that the machine algorithm has to learn to classify, people pre-set the correct labels to which class this image belongs. In the learning process, the algorithm predicting the image class compares its obtained class for a particular image with the true class for the same image and gradually adjusts its parameters in the learning process so that the class predicted by the algorithm tends to correspond to the class specified by the person. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Thus, the following idea can be summarized - the learning algorithm with the teacher is any machine learning algorithm, where we give the algorithm how it needs to be done correctly from our point of view.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
And it doesn‚Äôt matter how to proceed - indicate to which class this image should be assigned if it is a classification task, or draw the contours of an object if it is a segmentation task or which direction to turn the car‚Äôs steering wheel if the algorithm is autopilot, it‚Äôs important that for each specific situation we We explicitly indicate to the algorithm where the correct answer is, how to do it correctly. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
This is the key to understanding how fundamentally the learning algorithm with a teacher differs from the learning algorithm without a teacher.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Chapter 2. Learning without a teacher</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Having figured out what it is - teaching with a teacher, now we will understand what it is - teaching without a teacher. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As we found in the last chapter, when teaching with a teacher, for each teaching situation, we give the algorithm an understanding of which answer is right from our point of view, then going from the opposite - in learning without a teacher, for each specific situation we don‚Äôt give such an answer to the algorithm will be. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
But then the question arises, if we do not give the algorithm an explicit indication of what to do correctly, then what will the algorithm learn? How the algorithm will learn without knowing where to adjust its internal parameters in order to do the right thing and ultimately solve the problem as we would like.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Let's think about this topic. But it‚Äôs important for us that the algorithm solves the problem as a whole, and how exactly it will act in the process of solving this problem and which way it will go to solve it does not concern us, we will give it up to the algorithm itself, we expect only the final result from it . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Therefore, we will let the algorithm understand the final result whether it solved our problem well or not. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Thus, summarizing all of the above, we come to the conclusion that we call such learning algorithms without a teacher where there is no explicit indication for the algorithm how to do it, but there is only a general assessment of all its actions in the process of solving the problem.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/qi/ci/nn/qicinnqzow8o5b7lacir_mrs4aa.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In the example of a game where a racket tries to catch the cubes falling from above, we don‚Äôt tell the algorithm to control the racket at what specific time point where to move the racket. </font><font style="vertical-align: inherit;">We will tell the algorithm only the result of its actions - did he catch a cube with a racket or not. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/-f/dp/df/-fdpdf5nbjpawt1mutegmxzw9iu.gif"><br>
<br>
<img src="https://habrastorage.org/webt/ve/jk/wz/vejkwz-fezjvbjcjydzmx7q4hk8.gif"><br>
<br>
<img src="https://habrastorage.org/webt/ws/gv/nb/wsgvnb34jwzwk--6iehpg3y3lqw.gif"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
This is the essence of learning without a teacher. </font><font style="vertical-align: inherit;">The algorithm itself must learn to decide what to do in each specific case based on the final assessment of the totality of all its actions.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Chapter 3. Agent, Environment and Reward</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Having figured out what training without a teacher is, we will delve into the algorithms that can learn how to solve a problem without our tips on how to do it right. </font></font><br>
 <br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
It is time to introduce us to the terminology which we will use in the future. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We will call an agent our algorithm that can analyze the state of the environment and perform some actions in it. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Environment is the virtual world in which our Agent exists and through its actions can change its state ... </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Reward - feedback from the Environment to the Agent as a response to its actions.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/23/dp/gw/23dpgwboqw80sv_1uj5_ubifms8.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The environment in which our agent lives may be arbitrarily complex, the agent may not even know how it is structured to make its decisions and perform actions. </font><font style="vertical-align: inherit;">For the Agent, only feedback in the form of a reward that he receives from the environment is important. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
If we consider in more detail the process of interaction of the agent with the environment, then it can be expressed by the following scheme </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ml/f5/eq/mlf5eqvivcvcwodu66mfyddfpdo.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
St - state of the environment in step t </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
at - action of the agent in step t </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
rt - reward in step t</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
At each moment of time t, our agent observes the state of the medium - St, performs the action - at, for which it receives a reward - rt from the medium, the field of which the medium goes to the state St + 1, which our agent observes, performs the action - at + 1, for that receives a reward from the environment - rt + 1 and such states t, we can have an infinite set - n.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Chapter 4. Parameterization of the task of learning without a teacher</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In order to train an agent, we need to somehow parameterize the task of learning without a teacher, in other words, to understand what functions we are going to optimize. </font><font style="vertical-align: inherit;">In reinforcement learning - in what follows we will call training without a teacher, there are three such basic functions: </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
1) p (a | s) - policy function </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The probability function of the optimality of an action is a, depending on the state of the environment -s. </font><font style="vertical-align: inherit;">It shows us how the action of a is optimal under the state of the medium s. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
2) v (s) - value function </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The state </font><font style="vertical-align: inherit;">value function </font><font style="vertical-align: inherit;">is s. </font><font style="vertical-align: inherit;">It shows us how much the state s is generally valuable for us in terms of rewards </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
3) Q (s, a) - Q-function</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Q is an optimal strategy function. It allows us, according to this optimal strategy, in the state - s to choose the optimal action for this state - a </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
First we consider the function - policy function, as the most simple and intuitive to understand reinforcement learning function. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Since we are going to solve the problems of reinforcement learning through neural networks. Then, schematically, we can parameterize a policy function through a neural network as follows.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/oa/fv/1x/oafv1xdki7zeejvxvy-tsgd5oeu.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We will supply states - s to the input of the neural network, and construct the output of the neural network so that the output layer of the neural network is the SoftMax layer, with the number of outputs equal to the number of actions possible for the agent in our environment. </font><font style="vertical-align: inherit;">Thus, passing state s at the output through the layers of the neural network, we obtain the probability distribution for the actions of the agent in state s. </font><font style="vertical-align: inherit;">Which, in fact, is what we need in order to begin to train our neural network and iteratively improve the policy function, which is now essentially our neural network, through the back-propagation error algorithm.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Chapter 5. Improving policy function through neural network training</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
To train the neural network, we use the gradient descent method. Since the last layer of our neural network is a SoftMax layer, its Loss function is: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/29/y9/u2/29y9u2wizl5w2yf5yeyw08nk1ic.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
where: </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
1 - true labels * log (predicted labels) </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
2 - the sum of all examples </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
However, how can we train a neural network if we do not yet have the correct labels for agent actions in states S0-Sj? And we don‚Äôt need them, instead of the correct labels, we will use the reward that the agent received from the medium by performing the action that the neural network predicted for him.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/46/jp/or/46jporp-h9pj-bujcmytwhnfmze.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We have every right to do this because, for Cross Entropy Loss, yj are true labels for the correct class and they are equal to one, and for Policy Function Loss, rj is the reward that the environment accrued to the agent for the action that he performed. </font><font style="vertical-align: inherit;">That is, rj serves as a weight for the gradients in the back propagation of the error when we train the neural network. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Received a positive reward - it means you need to increase the weight of the neural network where the gradient is directed. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/2e/ge/bq/2egebqgdxjutx0tzxvwawunwffq.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
If the reward is negative, then the corresponding weights in the neural network, according to the direction of the gradient where the error is directed, we reduce.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Chapter 6. Building a Data Set for Training</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In order to train our Agent - the neural network using the classic Machine Learning - through the method of back propagation of errors, we need to assemble a dataset. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
From the statement of the problem it is clear that at the input of the neural network we want to submit the state of the medium S - an image of a well with falling cubes and a racket that catches them. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/4h/pv/hr/4hpvhry6ju1kcckfjj4vxu_7eps.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
and Y - which we will collect in accordance with state S, this will be the predicted action of the neural network - and the reward that the environment accrued to the agent for this action - r</font></font><br>
<br>
<img src="https://habrastorage.org/webt/an/gh/wo/anghwor5xj9mqdbgnnjrjgvzbta.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
But after all, Wednesday, during the course of the game, the agent can not award a reward for every action, for example, in our case, the agent will receive a positive reward only when the racket catches a falling die. If the racquet does not catch the cube and it falls to the bottom, then Wednesday will charge the agent a negative reward. The rest of the time, regardless of how the agent moves the racket, until the cube either hits the racket or falls to the bottom, Wednesday will charge the agent a reward equal to zero.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/7f/rh/5u/7frh5uda0syl7dbcutr8ez8fcq8.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As can be seen from the description of the process of our game, a positive or negative reward in an agent is extremely rare, but basically a reward regardless of its actions is zero. How to train the Code Agent most of the time he does not receive a response from the environment to his actions. From the point of view that our agent is a neural network and the reward from the environment is zero, then the gradients for the reverse propagation of the error through the neural network in most cases in the dataset will be zero, the weights of the neural network will have nowhere to change, which means that our agent will not learn anything. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
How to solve the problem with zero reward in most of the dataset that is going to train the agent? </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
There are two ways to get out of this situation:</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
the first is to assign all the actions of the agent that he took during the time the cube fell one and the same final reward of the episode +1 or -1, depending on whether the agent caught the cube or did not catch it. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Thus, we will consider all actions of the Agent if he caught the cube correct and will consolidate such behavior of the agent during training, assigning them a positive reward. If the Agent did not catch the cube, then we will assign a negative reward to all actions of the Agent in the episode and will train him to avoid such a sequence of actions in the future. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
the second - the final award with a certain discount step in descending order to apply to all agent actions in this episode. In other words, the closer the agent‚Äôs action to the final, the closer to +1 or -1 the reward for this action.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
By introducing such a discounted reward for an action in decreasing order as the action moves away from the episode ending, we let the Agent understand that the last actions he took are more important for the outcome of the episode of the Game than the actions that he took at the beginning. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Usually, the discounted reward is calculated by the formula - The final reward of the episode is multiplied by the discount coefficient in the power of the step number minus one for all agent actions in the episode (for the time that the cube fell). </font></font><br>
<br>
<img src="https://habrastorage.org/webt/4r/cf/w6/4rcfw67kmlniyfhczs_p2v2kb-c.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Gamma - discount coefficient (decrease in reward). It is always in the range from 0 to 1. Typically, the gamut is taken in the region of 0.95. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
After we have decided what data we collect in DataSet, run the environment simulator and play a game with several episodes several times in a row, collect data about:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">state of the environment, </font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> actions taken by the Agent </font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">the reward that the Agent received.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
For ease of understanding, let us call the fall of one cube through the well - an episode of the game, we also assume that the game itself will consist of several episodes. </font><font style="vertical-align: inherit;">This means that in one game we will drop several cubes in turn into the well, and the racket will try to catch them. </font><font style="vertical-align: inherit;">For each dice caught, the agent will be awarded +1 point, for each dice that fell to the bottom and the racket did not catch him, the agent will be awarded -1 point.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Chapter 7. The internal structure of the agent and the environment</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Environment - since we have an environment in which the agent is to exist, it essentially consists of a well matrix inside of which cubes fall down at a speed of one line per clock, and the agent goes in one direction also to one cell. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We will write a simulator of the environment that can drop a cube from the top line on an arbitrary column at a random moment, knows how to receive a command from an agent to move the racket one cell in one of the directions, after which it checks whether the falling cube was caught by the racket or if it fell on bottom of the well. Depending on this, the simulator returns to the agent the reward that he received for his action.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Agent - the main element of which is our neural network, capable of returning the probabilities of all actions for a given state of the environment as the state of the environment devoted to it at the input. </font><font style="vertical-align: inherit;">From the probability of actions received from the neural network, the agent selects the best, sends it to the environment, receives feedback from the environment in the form of a reward from the environment. </font><font style="vertical-align: inherit;">Also, the Agent must have an internal algorithm based on which he will be able to learn to maximize the reward received from the environment.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Chapter 8. Agent Training</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In order to train the Agent, we need to accumulate statistics according to the data from our simulator and the actions taken by the agent. </font><font style="vertical-align: inherit;">We will collect statistical data for training in triples of values ‚Äã‚Äã- the state of the environment, the agent‚Äôs action and the reward for this action.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/yo/qq/od/yoqqodgrfzatgri5dljps6uqnvw.png"><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A bit of code on how to collect statistics</font></font></h4><br>
<img src="https://habrastorage.org/webt/ph/hr/6x/phhr6xb35udm3hyc3mr9rwv5yno.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We put each such three values ‚Äã‚Äãin a special memory buffer, where we store them all the time while we simulate the game and accumulate statistics on it.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Code for organizing a memory buffer:</font></font></h4><br>
<img src="https://habrastorage.org/webt/u2/55/ty/u255ty2pzaoeagitk_osyxzxirg.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
After a series of games of our Agent with Wednesday and accumulating statistics, we can proceed to the training of the Agent. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
To do this, we get a batch of our data in the form of triples of values ‚Äã‚Äãfrom the memory buffer with accumulated statistics, unpack it and convert it into Pytorch tensors. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/jo/se/6t/jose6tkxmmtzsttgrnc-q0xkrf0.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 The environmental state batch in the form of the Pytorch tensor is applied to the input on the agent‚Äôs neural network, we obtain the probability distribution for each agent move for each environment state in the match, logarithm these probabilities, multiply the reward received by the agent per move by the logarithm of these probabilities, then take the average over the products and make this mean negative: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/hx/om/3i/hxom3ikgq9n13pb6brfzsfsbmkw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Stage 1 </font></font><br>
<br>
<img src="https://habrastorage.org/webt/4j/gh/8l/4jgh8layvkcwg0dhrylxuwul2ri.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Stage 2</font></font><br>
<br>
<img src="https://habrastorage.org/webt/pn/ac/pj/pnacpjuzovqe4-zql7lxlarerly.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
After we get the value of the Loss function, we make a reverse pass through the neural network to get gradients and make the step an optimizer to adjust the weights. </font><font style="vertical-align: inherit;">On this, the training cycle of our agent is being downloaded. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Since after the optimizer shifted the weights of the neural network, our data in the statistics collected are no longer relevant, because a neural network with shifted weights will give completely different probabilities for actions on the same environmental conditions and agent training will go wrong again. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Therefore, we clear our buffer with memory, lose again a certain number of games to collect statistics, and restart the agent training process. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
This is the learning loop when learning without a teacher using the Policy Gradient method.</font></font><br>
<br>
<ol>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Statistics accumulation</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Agent Training</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Reset Statistics</font></font></li>
</ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 We repeat the learning process so many times until our Agent learns to receive from the system the reward that suits us.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Chapter 9. Experiments with Agent and Environment</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Let's start a series of experiments to train our Agent. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
For experiments, we select the following environmental parameters: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/xi/ym/2_/xiym2_kitmamvf52hhuff6tkpqs.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
For the Agent, we choose - we construct a neural network - it will be convolutional (as we work with the image), it will have 9 outputs (1-right, 2-left, 3-up, 4-down, on the output) 5-right-up, 6-left-up, 7-right-down, 8-left-down, 9-do nothing) and SoftMax to get the probability for each action.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Neural network architecture</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
First Conv2d layer 32 neuron image size 1 * 32 * 16 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
MaxPool2d layer image size 32 * 16 * 8 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Second Conv2d layer 32 neuron image size 32 * 16 * 8 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
MaxPool2d layer image size 32 * 8 * 4 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Flatten - straighten the image to a size of 1024 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Linear layer on 1024 neurons </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dropout (0.25) layer </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Linear layer on 512 neurons </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Linear layer on 256 neurons </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Linear layer on 9 neurons and SoftMax</font></font><br>
<br>
<img src="https://habrastorage.org/webt/2d/tz/ne/2dtzneqr2lsxdkz-yvu9xonh0uw.png"><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pytorch neural network creation code</font></font></h4><br>
<img src="https://habrastorage.org/webt/wq/mx/yw/wqmxyw67mbjtmnpnbj86zry8y4a.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Alternately, we will launch three Agent training cycles in the Environment, the parameters of which were discussed above:</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Experiment No. 1 - The agent learned to solve the problem in 13,600 game cycles</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Initial state of the Agent. Agent </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ws/gv/nb/wsgvnb34jwzwk--6iehpg3y3lqw.gif"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
training schedule. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/jt/kd/3q/jtkd3qmdl7xixi-bwpkdp-tk0uy.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Trained state of the Agent.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/4a/v5/9l/4av59lvev4oget2gotgg-xsfy2w.gif"><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Experiment No. 2 - The agent learned to solve the problem in 8250 cycles of the game</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Initial state of the Agent. Agent </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ve/jk/wz/vejkwz-fezjvbjcjydzmx7q4hk8.gif"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
training schedule. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/9e/zc/1x/9ezc1x4gfv7bilgb8sx-r3iemgy.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Trained state of the Agent.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/l9/r7/bn/l9r7bnstfbaqszje4npotjrze4o.gif"><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Experiment No. 3 - The agent learned to solve the problem in 19800 game cycles</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Initial state of the Agent. Agent </font></font><br>
<br>
<img src="https://habrastorage.org/webt/hz/wm/ss/hzwmssjogsxwzv0wrik-1nb8loc.gif"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
training schedule. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/sh/xv/zd/shxvzdpumy7fnoctkb6bqnhie2o.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Trained state of the Agent.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/2h/ga/yh/2hgayhvtgypaq1fvu-i54d4l4ok.gif"><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Chapter 10. Conclusions</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Looking at the charts, it can be said that the training of the Agent is of course slow. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The agent initially has been looking for at least some reasonable policy for his actions for a long time to start receiving a positive reward. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
At this time, at the first stage of the schedule, the reward for the game is slowly growing, then suddenly the Agent finds a good option for his moves, and the reward received by him for the game increases steeply and goes up, and then approaching the maximum reward, the agent goes again a slow increase in efficiency when he improves his policy of moves he has already learned, but striving, like any greedy algorithm, to take the reward to himself completely.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
I would also like to note the great need for calculations for training the Agent using the Policy Gradient method, because The main time the algorithm works is the collection of statistics on the Agent‚Äôs moves and not its training. Having collected statistics on moves from the entire array, we use only one data batch for training the Agent, and we discard all other data as already unsuitable for training. And again we collect new data. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
You can still experiment a lot with this algorithm and the environment - by changing the depth and width of the well, increasing or decreasing the number of dice falling during the game, making these dices of different colors. To observe what this effect will have on the effectiveness and speed of training of the Agent.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Also, an extensive field for experiments is the parameters of the neural network, in essence of which we are training our Agent, you can change layers, convolution kernels, enable and fine-tune regularization. </font><font style="vertical-align: inherit;">Yes, and much more you can try to increase the effectiveness of training the Agent. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Thus, having launched practical experiments with teaching without a teacher using the Policy Gradient method, we were convinced that teaching without a teacher has a place to be, and it really works. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The agent independently trained to maximize his reward in the game. </font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Link to GitHub with code adapted for working in a Google Colab laptop</font></font></a></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../en506358/index.html">3 options for court behavior when they receive cases of prosecutorial locks</a></li>
<li><a href="../en506362/index.html">How we did a dance ball online</a></li>
<li><a href="../en506370/index.html">Compare the performance of Check Constraint and Foreign Key in SQL Server</a></li>
<li><a href="../en506372/index.html">–£ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –Ω–∞–π–¥–µ–Ω –∏–Ω—Å—Ç–∏–Ω–∫—Ç —Ä–∞–∑–º–Ω–æ–∂–µ–Ω–∏—è</a></li>
<li><a href="../en506380/index.html">Free or domestic software. Standard or free tuition</a></li>
<li><a href="../en506386/index.html">–ö–∞–∫ –±—Ä–æ—Å–∏—Ç—å —à–∫–æ–ª—É –∏ –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ —Ä–µ–±–µ–Ω–∫–∞ –Ω–∞ —É–¥–∞–ª—ë–Ω–∫—É</a></li>
<li><a href="../en506392/index.html">Notification of Roskomnadzor on the processing of personal data in 2020</a></li>
<li><a href="../en506394/index.html">Anycubic Mega X 3D Printer: A Great Printer at a Modest Price</a></li>
<li><a href="../en506396/index.html">Elon Musk: ‚ÄúLidar is a waste of time. All who rely on lidar are doomed ‚Äù</a></li>
<li><a href="../en506398/index.html">Joel Spolsky: ‚ÄúNot Usability Alone‚Äù</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>