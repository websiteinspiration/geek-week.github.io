<!doctype html>
<html class="no-js" lang="fr">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸˆ ğŸ“ ğŸ‘©ğŸ¿â€ğŸš€ Former des rivaux de jeux intelligents dans Unity en utilisant la mÃ©thode "jouer avec soi-mÃªme" en utilisant ML-Agents â›¹ğŸ» ğŸ¤³ğŸ¾ ğŸº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bonjour, Habr! 
 
 Comme nos lecteurs rÃ©guliers le savent, nous avons publiÃ© avec succÃ¨s des livres sur Unity . Dans le cadre de l'Ã©tude du sujet, nou...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Former des rivaux de jeux intelligents dans Unity en utilisant la mÃ©thode "jouer avec soi-mÃªme" en utilisant ML-Agents</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/507212/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bonjour, Habr! </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Comme nos lecteurs rÃ©guliers le savent, nous avons </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">publiÃ©</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> avec succÃ¨s des </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;">livres</font></a><font style="vertical-align: inherit;"> sur </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Unity</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Dans le cadre de l'Ã©tude du sujet, nous nous sommes intÃ©ressÃ©s en particulier au </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ML-Agents Toolkit</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Aujourd'hui, nous portons Ã  votre attention une traduction d'un article du blog Unity sur la faÃ§on de former efficacement les agents de jeu en utilisant la mÃ©thode Â«avec soi-mÃªmeÂ»; en particulier, l'article aide Ã  comprendre pourquoi cette mÃ©thode est plus efficace que l'apprentissage renforcÃ© traditionnel. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/vb/sz/5m/vbsz5musziyq5yighwu_iq0eggu.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bonne lecture!</font></font><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Cet article donne ensuite un aperÃ§u de la technologie d'auto-jeu (jouer avec soi-mÃªme) et montre comment elle permet de fournir un entraÃ®nement stable et efficace dans l'environnement de dÃ©monstration de football Ã  partir de la </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">boÃ®te Ã  outils ML-Agents</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dans les environnements de dÃ©monstration de tennis et de football de la boÃ®te Ã  outils Unity ML-Agents, les agents s'affrontent comme des rivaux. Former des agents dans un tel scÃ©nario compÃ©titif est parfois une tÃ¢che trÃ¨s simple. En fait, dans les versions prÃ©cÃ©dentes de la boÃ®te Ã  outils ML-Agents, pour que l'agent apprenne en toute confiance, une Ã©tude sÃ©rieuse du prix Ã©tait nÃ©cessaire. Dans la </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">version 0.14</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">une opportunitÃ© a Ã©tÃ© ajoutÃ©e qui permet Ã  l'utilisateur de former des agents en utilisant l'apprentissage par renforcement (RL) basÃ© sur l'auto-jeu, un mÃ©canisme qui est crucial pour obtenir certains des rÃ©sultats d'apprentissage par renforcement les plus haut de gamme, tels que </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">OpenAI Five</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> et </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">AlphaStar de DeepMind</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . L'auto-jeu au travail oppose les hypostases actuelles et passÃ©es de l'agent. Ainsi, nous obtenons un adversaire pour notre agent, qui peut progressivement s'amÃ©liorer en utilisant des algorithmes d'apprentissage par renforcement traditionnels. </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Un agent parfaitement formÃ©</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> peut rivaliser avec succÃ¨s avec des joueurs humains avancÃ©s.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Le jeu libre fournit un environnement d'apprentissage qui est construit sur les mÃªmes principes que la compÃ©tition d'un point de vue humain. Par exemple, une personne qui apprend Ã  jouer au tennis choisira d'entraÃ®ner des adversaires Ã  peu prÃ¨s au mÃªme niveau que lui, car un adversaire trop fort ou trop faible n'est pas si pratique pour maÃ®triser le jeu. Du point de vue du dÃ©veloppement de leurs propres compÃ©tences, il peut Ãªtre beaucoup plus utile pour un joueur de tennis dÃ©butant de battre les mÃªmes dÃ©butants, plutÃ´t que, par exemple, un enfant d'Ã¢ge prÃ©scolaire ou Novak Djokovic. Le premier ne pourra mÃªme pas frapper la balle, et le second ne vous donnera pas un tel service que vous pourrez battre. Lorsqu'un dÃ©butant dÃ©veloppe une force suffisante, il peut passer au niveau suivant ou postuler Ã  un tournoi plus sÃ©rieux pour jouer contre des adversaires plus qualifiÃ©s.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dans cet article, nous considÃ©rerons quelques subtilitÃ©s techniques associÃ©es Ã  la dynamique du jeu avec nous-mÃªmes, et considÃ©rerons Ã©galement des exemples de travail dans des environnements virtuels Tennis et Soccer, refactorisÃ©s de maniÃ¨re Ã  illustrer le jeu avec lui-mÃªme.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">L'histoire d'un jeu avec vous-mÃªme dans les jeux</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Le phÃ©nomÃ¨ne du jeu avec soi a une longue histoire, qui se reflÃ¨te dans la pratique du dÃ©veloppement d'agents de jeux artificiels conÃ§us pour rivaliser avec les gens dans les jeux. </font><font style="vertical-align: inherit;">Arthur Samuel, l'un des premiers Ã  utiliser ce systÃ¨me, a dÃ©veloppÃ© un simulateur d'Ã©checs dans les annÃ©es 1950 et a publiÃ© </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ce travail</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> en 1959. </font><font style="vertical-align: inherit;">Ce systÃ¨me est devenu le prÃ©curseur d'un rÃ©sultat historique dans l'apprentissage par renforcement rÃ©alisÃ© par Gerald Tesauro dans TD-Gammon; </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">totaux publiÃ©s</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">en 1995. TD-Gammon a utilisÃ© l'algorithme de diffÃ©rence de temps TD (Î») avec la fonction de jouer avec lui-mÃªme pour entraÃ®ner l'agent Ã  jouer au backgammon afin qu'il puisse rivaliser avec un professionnel. Dans certains cas, il a Ã©tÃ© observÃ© que TD-Gammon a une vision des positions plus confiante que les joueurs de classe mondiale. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Jouer avec vous-mÃªme se reflÃ¨te dans de nombreuses rÃ©alisations emblÃ©matiques associÃ©es Ã  RL. Il est important de noter que jouer avec soi-mÃªme a aidÃ© au dÃ©veloppement d'agents pour jouer aux </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ã©checs et aller</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> avec des capacitÃ©s surhumaines, des agents </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">DOTA 2 d'</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Ã©lite </font><font style="vertical-align: inherit;">, ainsi que des stratÃ©gies et contre-stratÃ©gies complexes dans des jeux tels que la </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">lutte</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> et la </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;">cache</font></a><font style="vertical-align: inherit;"> - </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;">cache</font></a><font style="vertical-align: inherit;"> .</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Dans les rÃ©sultats obtenus en jouant avec soi-mÃªme, il est souvent notÃ© que les agents de jeu choisissent des stratÃ©gies qui surprennent les experts. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Jouer avec soi donne aux agents une certaine crÃ©ativitÃ© indÃ©pendante de la crÃ©ativitÃ© des programmeurs. </font><font style="vertical-align: inherit;">L'agent ne reÃ§oit que les rÃ¨gles du jeu, puis - des informations sur s'il a gagnÃ© ou perdu. </font><font style="vertical-align: inherit;">En outre, sur la base de ces principes de base, l'agent doit dÃ©velopper un comportement compÃ©tent. </font><font style="vertical-align: inherit;">Selon le crÃ©ateur de TD-Gammon, une telle approche de l'apprentissage libÃ¨re ", en ce sens que le programme n'est pas contraint par des inclinations et des prÃ©jugÃ©s humains, qui peuvent s'avÃ©rer erronÃ©s et peu fiables". </font><font style="vertical-align: inherit;">GrÃ¢ce Ã  cette libertÃ©, les agents dÃ©couvrent des stratÃ©gies de jeu gÃ©niales qui changent complÃ¨tement la faÃ§on dont les ingÃ©nieurs pensent Ã  certains jeux.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Formation de renforcement compÃ©titif</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dans le cadre de la tÃ¢che traditionnelle d'apprentissage renforcÃ©, l'agent essaie de dÃ©velopper une ligne de comportement qui maximise la rÃ©compense totale. Le signal de rÃ©compense encode la tÃ¢che de l'agent - une telle tÃ¢che peut Ãªtre, par exemple, le tracÃ© d'un cours ou la collecte d'articles. Le comportement de l'agent est soumis Ã  des restrictions environnementales. Tels, par exemple, la gravitÃ©, les obstacles, ainsi que lâ€™influence relative des mesures prises par lâ€™agent lui-mÃªme - par exemple, lâ€™application de la force Ã  son propre mouvement. Ces facteurs limitent le comportement de l'agent et sont des forces externes qu'il doit apprendre Ã  gÃ©rer afin de recevoir une rÃ©compense Ã©levÃ©e. Ainsi, l'agent est en concurrence avec la dynamique de l'environnement et doit passer d'un Ã©tat Ã  l'autre prÃ©cisÃ©ment pour que la rÃ©compense maximale soit atteinte.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/if/jf/xi/ifjfxil_iqm9pznr61qbpxujs38.png"><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Le scÃ©nario typique de formation de renforcement est illustrÃ© Ã  gauche: l'agent agit dans l'environnement, passe Ã  l'Ã©tat suivant et reÃ§oit une rÃ©compense. Le scÃ©nario de formation est illustrÃ© Ã  droite, oÃ¹ l'agent est en concurrence avec un rival, qui, du point de vue de l'agent, est en fait un Ã©lÃ©ment de l'environnement.</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Dans le cas des jeux de compÃ©tition, l'agent est en concurrence non seulement avec la dynamique de l'environnement, mais aussi avec un autre agent (Ã©ventuellement intellectuel). Nous pouvons supposer que l'adversaire est intÃ©grÃ© Ã  l'environnement, et ses actions affectent directement l'Ã©tat suivant que l'agent Â«voitÂ», ainsi que la rÃ©compense qu'il recevra. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/zl/kv/xo/zlkvxoa9_bt0cdsthmddwll-fvc.png"><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Exemple de tennis de ML-Agents Toolkit</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ConsidÃ©rez la dÃ©mo de ML-Agents Tennis. La raquette bleue (Ã  gauche) est l'agent d'apprentissage et la violette (Ã  droite) est son adversaire. Pour lancer le ballon au-dessus du filet, l'agent doit tenir compte de la trajectoire du ballon volant de l'adversaire, et effectuer un ajustement de l'angle et de la vitesse du ballon volant, en tenant compte des conditions environnementales (gravitÃ©). Cependant, dans une compÃ©tition avec un adversaire, lancer le ballon sur le filet n'est que la moitiÃ© de la bataille. Un adversaire fort peut rÃ©pondre d'un coup irrÃ©sistible et, par consÃ©quent, l'agent perdra. Un adversaire faible peut frapper la balle dans le filet. Un adversaire Ã©gal peut retourner le service, et donc le jeu continuera. Dans tous les cas, l'Ã©tat suivant et la rÃ©compense correspondante dÃ©pendent Ã  la fois des conditions environnementales et de l'adversaire. Cependant, dans toutes ces situations, l'agent fait le mÃªme pitch. Par consÃ©quent, en tant qu'entraÃ®nement Ã  des jeux compÃ©titifs,et le pompage des comportements rivaux par un agent est un problÃ¨me complexe.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Les considÃ©rations pour un adversaire appropriÃ© ne sont pas anodines. Comme il ressort clairement de ce qui prÃ©cÃ¨de, la force relative de l'adversaire affecte de maniÃ¨re significative le rÃ©sultat d'un match particulier. Si l'adversaire est trop fort, l'agent peut avoir du mal Ã  apprendre Ã  jouer Ã  partir de zÃ©ro. D'un autre cÃ´tÃ©, si l'adversaire est trop faible, alors l'agent peut apprendre Ã  gagner, mais ces compÃ©tences peuvent Ãªtre inutiles en concurrence avec un adversaire plus fort ou simplement diffÃ©rent. Par consÃ©quent, nous avons besoin d'un adversaire qui sera Ã  peu prÃ¨s Ã©gal en force Ã  l'agent (inflexible, mais pas insurmontable). De plus, comme les compÃ©tences de notre agent s'amÃ©liorent Ã  chaque match terminÃ©, nous devons augmenter la force de son adversaire dans la mÃªme mesure. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/fn/vk/pt/fnvkptgsipqopi2iiyflsa5ec_a.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Lorsque vous jouez avec vous-mÃªme, un instantanÃ© du passÃ© ou un agent dans son Ã©tat actuel est l'adversaire intÃ©grÃ© Ã  l'environnement.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
C'est lÃ  que le jeu avec nous est utile! L'agent lui-mÃªme satisfait aux deux exigences de l'adversaire souhaitÃ©. Il est certainement Ã  peu prÃ¨s Ã©gal en force Ã  lui-mÃªme et ses compÃ©tences s'amÃ©liorent avec le temps. Dans ce cas, la propre stratÃ©gie de l'agent est intÃ©grÃ©e Ã  l'environnement (voir la figure). Ceux qui connaissent bien </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">la complexitÃ© croissante de l'Ã©ducation</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (apprentissage du curriculum), vous montrent que nous pouvons supposer que le systÃ¨me dÃ©veloppe naturellement le curriculum, aprÃ¨s quoi l'agent apprend Ã  lutter contre des adversaires de plus en plus puissants. En consÃ©quence, jouer avec vous-mÃªme vous permet d'utiliser l'environnement lui-mÃªme pour former des agents compÃ©titifs pour des jeux compÃ©titifs!</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dans les deux sections suivantes, nous examinerons des dÃ©tails plus techniques de la formation d'agents compÃ©titifs, en particulier, concernant la mise en Å“uvre et l'utilisation du jeu avec soi-mÃªme dans la boÃ®te Ã  outils ML-Agents.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ConsidÃ©rations pratiques</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Certains problÃ¨mes pratiques se posent concernant le cadre de jeu avec vous-mÃªme. </font><font style="vertical-align: inherit;">En particulier, la reconversion est possible, dans laquelle l'agent apprend Ã  gagner uniquement avec un certain style de jeu, ainsi que l'instabilitÃ© inhÃ©rente au processus d'apprentissage, qui peut survenir en raison de l'instabilitÃ© de la fonction de transition (c'est-Ã -dire en raison d'adversaires en constante Ã©volution). </font><font style="vertical-align: inherit;">Le premier problÃ¨me se pose parce que nous voulons que nos agents aient une comprÃ©hension gÃ©nÃ©rale et une capacitÃ© Ã  combattre des adversaires de diffÃ©rents types.</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Le deuxiÃ¨me problÃ¨me peut Ãªtre illustrÃ© dans l'environnement du tennis: diffÃ©rents adversaires vont frapper la balle Ã  diffÃ©rentes vitesses et Ã  diffÃ©rents angles. Du point de vue de l'agent d'apprentissage, cela signifie que, au fur et Ã  mesure que vous apprenez, les mÃªmes dÃ©cisions conduiront Ã  des rÃ©sultats diffÃ©rents et, par consÃ©quent, l'agent sera dans diffÃ©rentes situations ultÃ©rieures. Dans l'apprentissage par renforcement traditionnel, les fonctions de transition stationnaires sont implicites. Malheureusement, aprÃ¨s avoir prÃ©parÃ© une sÃ©lection de divers adversaires pour l'agent afin de rÃ©soudre le premier problÃ¨me, nous, Ã©tant imprudents, pouvons aggraver le second.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pour y faire face, nous maintiendrons un tampon avec les politiques d'agent passÃ©es, parmi lesquelles nous choisirons des rivaux potentiels pour notre "Ã©tudiant" Ã  long terme. </font><font style="vertical-align: inherit;">En choisissant un agent parmi les politiques passÃ©es, nous obtenons pour lui une sÃ©lection d'adversaires divers. </font><font style="vertical-align: inherit;">De plus, permettant Ã  l'agent de s'entraÃ®ner avec un adversaire fixe pendant une longue pÃ©riode, nous stabilisons la fonction de transition et crÃ©ons un environnement d'apprentissage plus cohÃ©rent. </font><font style="vertical-align: inherit;">Enfin, ces aspects algorithmiques peuvent Ãªtre contrÃ´lÃ©s Ã  l'aide d'hyperparamÃ¨tres, qui sont abordÃ©s dans la section suivante.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">DÃ©tails d'implÃ©mentation et d'utilisation</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En choisissant des hyperparamÃ¨tres pour jouer avec nous-mÃªmes, nous gardons tout d'abord Ã  l'esprit un compromis entre le niveau de l'adversaire, l'universalitÃ© de la politique finale et la stabilitÃ© de l'entraÃ®nement. S'entraÃ®ner Ã  la rivalitÃ© avec un groupe d'adversaires qui changent lentement ou ne changent pas du tout et, par consÃ©quent, donnent une plus petite dispersion des rÃ©sultats, est un processus plus stable que l'entraÃ®nement Ã  la rivalitÃ© avec de nombreux rivaux divers qui changent rapidement. Les hyperparamÃ¨tres disponibles vous permettent de contrÃ´ler la frÃ©quence Ã  laquelle la politique actuelle de l'agent sera enregistrÃ©e pour une utilisation ultÃ©rieure en tant qu'un des opposants dans l'Ã©chantillon, la frÃ©quence Ã  laquelle le nouvel adversaire sera enregistrÃ©, puis sÃ©lectionnÃ© pour le combat, la frÃ©quence Ã  laquelle le nouvel adversaire sera sÃ©lectionnÃ©, le nombre d'adversaires enregistrÃ©s, ainsi que la probabilitÃ©que dans ce cas, l'Ã©tudiant devra jouer contre son propre alter ego, et non contre un adversaire sÃ©lectionnÃ© dans la poule.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dans les jeux compÃ©titifs, le prix Â«cumulatifÂ» dÃ©livrÃ© par l'environnement n'est peut-Ãªtre pas la mesure la plus informative pour suivre les progrÃ¨s de l'apprentissage. Le fait est que la rÃ©compense cumulative dÃ©pend entiÃ¨rement du niveau de l'adversaire. Un agent avec une certaine compÃ©tence de jeu recevra une rÃ©compense plus ou moins grande, selon un adversaire moins qualifiÃ© ou plus habile, respectivement. Nous proposons la mise en Å“uvre </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">du systÃ¨me de classement ELO</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , qui vous permet de calculer la compÃ©tence de jeu relative de deux joueurs d'une certaine population lorsqu'ils jouent avec un montant nul. Au cours d'une seule sÃ©ance d'entraÃ®nement, cette valeur devrait augmenter rÃ©guliÃ¨rement. Vous pouvez le suivre, ainsi que d'autres mesures d'apprentissage, par exemple, le prix global, Ã  l'aide de </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">TensorBoard</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Jouer avec vous-mÃªme dans le football</font></font></h3> <br>
<img src="https://habrastorage.org/webt/gl/m_/rh/glm_rhop1whkyve0i-wpubxfpqw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Les derniÃ¨res versions de ML-Agent Toolkit n'incluent pas de stratÃ©gies d'agent pour l'environnement d'apprentissage Soccer, car le processus de formation fiable n'y a pas Ã©tÃ© intÃ©grÃ©. </font><font style="vertical-align: inherit;">Cependant, en utilisant le jeu avec nous-mÃªmes et certains refactoring, nous pouvons former l'agent Ã  des comportements non triviaux. </font><font style="vertical-align: inherit;">Le changement le plus important est la suppression des Â«positions de jeuÂ» des caractÃ©ristiques de l'agent. </font><font style="vertical-align: inherit;">Plus tÃ´t dans l'environnement du football, le Â«gardien de butÂ» et Â«l'attaquantÂ» se sont clairement dÃ©marquÃ©s, de sorte que l'ensemble du gameplay semblait plus logique. </font><font style="vertical-align: inherit;">Dans </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">cette vidÃ©o</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">un nouvel environnement est prÃ©sentÃ© dans lequel on peut voir comment le comportement de rÃ´le se forme spontanÃ©ment, dans lequel certains agents commencent Ã  agir comme des attaquants et d'autres comme des gardiens de but. Maintenant, les agents eux-mÃªmes apprennent Ã  jouer ces positions! La fonction de rÃ©compense pour les quatre agents est dÃ©finie comme +1,0 pour un but marquÃ© et -1,0 pour un but encaissÃ©, avec une pÃ©nalitÃ© supplÃ©mentaire de -0,0003 par pas - cette pÃ©nalitÃ© est prÃ©vue pour encourager les agents Ã  attaquer.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ici, nous soulignons une fois de plus que les agents dans l'environnement d'apprentissage du football apprennent eux-mÃªmes le comportement coopÃ©ratif, et pour cela, aucun algorithme explicite n'est utilisÃ© concernant le comportement multi-agents ou l'attribution de rÃ´le. </font><font style="vertical-align: inherit;">Ce rÃ©sultat dÃ©montre qu'un agent peut Ãªtre formÃ© Ã  des comportements complexes Ã  l'aide d'algorithmes relativement simples - Ã  condition que la tÃ¢che soit bien formulÃ©e. </font><font style="vertical-align: inherit;">La condition la plus importante pour cela est que les agents puissent observer leurs coÃ©quipiers, c'est-Ã -dire qu'ils reÃ§oivent des informations sur la position relative du coÃ©quipier. </font><font style="vertical-align: inherit;">ForÃ§ant un combat agressif pour le ballon, l'agent dit indirectement au coÃ©quipier qu'il doit se dÃ©placer en dÃ©fense. </font><font style="vertical-align: inherit;">Au contraire, s'Ã©loignant en dÃ©fense, l'agent provoque l'attaque d'un coÃ©quipier.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Et aprÃ¨s</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Si vous avez dÃ©jÃ  utilisÃ© l'une des nouvelles fonctionnalitÃ©s de cette version, parlez-nous-en. </font><font style="vertical-align: inherit;">Nous attirons votre attention sur la page des </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">problÃ¨mes de ML-Agents GitHub</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , oÃ¹ vous pouvez parler des bogues trouvÃ©s, ainsi que sur la page des </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">forums Unity ML-Agents</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , oÃ¹ les questions et problÃ¨mes gÃ©nÃ©raux sont discutÃ©s.</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr507200/index.html">7 faÃ§ons dont les scientifiques des donnÃ©es vous trompent</a></li>
<li><a href="../fr507202/index.html">Meetup Avito Analytics</a></li>
<li><a href="../fr507204/index.html">Cuisine d'intÃ©rieur design industriel: du croquis au produit en boÃ®te</a></li>
<li><a href="../fr507206/index.html">Architecture Y messenger</a></li>
<li><a href="../fr507210/index.html">Les performances de Java moderne lorsque vous travaillez avec de grandes quantitÃ©s de donnÃ©es, partie 2</a></li>
<li><a href="../fr507214/index.html">Comment crÃ©er et modifier des formulaires PDF interactifs, ou la nouvelle compÃ©tence ABBYY FineReader PDF</a></li>
<li><a href="../fr507218/index.html">Lisez-moi, ou pourquoi le texte n'est pas lu jusqu'Ã  la fin</a></li>
<li><a href="../fr507222/index.html">Pourquoi tout le monde devrait porter des masques</a></li>
<li><a href="../fr507224/index.html">Comment Ã©liminer les angles morts avec des tests visuels</a></li>
<li><a href="../fr507226/index.html">OCR pour PDF dans .NET - Comment extraire du texte de documents PDF inaccessibles</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>