<!doctype html>
<html class="no-js" lang="fr">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🐈 📏 👩🏿‍🚀 Former des rivaux de jeux intelligents dans Unity en utilisant la méthode "jouer avec soi-même" en utilisant ML-Agents ⛹🏻 🤳🏾 🏺</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bonjour, Habr! 
 
 Comme nos lecteurs réguliers le savent, nous avons publié avec succès des livres sur Unity . Dans le cadre de l'étude du sujet, nou...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Former des rivaux de jeux intelligents dans Unity en utilisant la méthode "jouer avec soi-même" en utilisant ML-Agents</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/507212/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bonjour, Habr! </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Comme nos lecteurs réguliers le savent, nous avons </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">publié</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> avec succès des </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;">livres</font></a><font style="vertical-align: inherit;"> sur </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Unity</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Dans le cadre de l'étude du sujet, nous nous sommes intéressés en particulier au </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ML-Agents Toolkit</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Aujourd'hui, nous portons à votre attention une traduction d'un article du blog Unity sur la façon de former efficacement les agents de jeu en utilisant la méthode «avec soi-même»; en particulier, l'article aide à comprendre pourquoi cette méthode est plus efficace que l'apprentissage renforcé traditionnel. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/vb/sz/5m/vbsz5musziyq5yighwu_iq0eggu.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bonne lecture!</font></font><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Cet article donne ensuite un aperçu de la technologie d'auto-jeu (jouer avec soi-même) et montre comment elle permet de fournir un entraînement stable et efficace dans l'environnement de démonstration de football à partir de la </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">boîte à outils ML-Agents</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dans les environnements de démonstration de tennis et de football de la boîte à outils Unity ML-Agents, les agents s'affrontent comme des rivaux. Former des agents dans un tel scénario compétitif est parfois une tâche très simple. En fait, dans les versions précédentes de la boîte à outils ML-Agents, pour que l'agent apprenne en toute confiance, une étude sérieuse du prix était nécessaire. Dans la </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">version 0.14</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">une opportunité a été ajoutée qui permet à l'utilisateur de former des agents en utilisant l'apprentissage par renforcement (RL) basé sur l'auto-jeu, un mécanisme qui est crucial pour obtenir certains des résultats d'apprentissage par renforcement les plus haut de gamme, tels que </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">OpenAI Five</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> et </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">AlphaStar de DeepMind</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . L'auto-jeu au travail oppose les hypostases actuelles et passées de l'agent. Ainsi, nous obtenons un adversaire pour notre agent, qui peut progressivement s'améliorer en utilisant des algorithmes d'apprentissage par renforcement traditionnels. </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Un agent parfaitement formé</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> peut rivaliser avec succès avec des joueurs humains avancés.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Le jeu libre fournit un environnement d'apprentissage qui est construit sur les mêmes principes que la compétition d'un point de vue humain. Par exemple, une personne qui apprend à jouer au tennis choisira d'entraîner des adversaires à peu près au même niveau que lui, car un adversaire trop fort ou trop faible n'est pas si pratique pour maîtriser le jeu. Du point de vue du développement de leurs propres compétences, il peut être beaucoup plus utile pour un joueur de tennis débutant de battre les mêmes débutants, plutôt que, par exemple, un enfant d'âge préscolaire ou Novak Djokovic. Le premier ne pourra même pas frapper la balle, et le second ne vous donnera pas un tel service que vous pourrez battre. Lorsqu'un débutant développe une force suffisante, il peut passer au niveau suivant ou postuler à un tournoi plus sérieux pour jouer contre des adversaires plus qualifiés.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dans cet article, nous considérerons quelques subtilités techniques associées à la dynamique du jeu avec nous-mêmes, et considérerons également des exemples de travail dans des environnements virtuels Tennis et Soccer, refactorisés de manière à illustrer le jeu avec lui-même.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">L'histoire d'un jeu avec vous-même dans les jeux</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Le phénomène du jeu avec soi a une longue histoire, qui se reflète dans la pratique du développement d'agents de jeux artificiels conçus pour rivaliser avec les gens dans les jeux. </font><font style="vertical-align: inherit;">Arthur Samuel, l'un des premiers à utiliser ce système, a développé un simulateur d'échecs dans les années 1950 et a publié </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ce travail</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> en 1959. </font><font style="vertical-align: inherit;">Ce système est devenu le précurseur d'un résultat historique dans l'apprentissage par renforcement réalisé par Gerald Tesauro dans TD-Gammon; </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">totaux publiés</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">en 1995. TD-Gammon a utilisé l'algorithme de différence de temps TD (λ) avec la fonction de jouer avec lui-même pour entraîner l'agent à jouer au backgammon afin qu'il puisse rivaliser avec un professionnel. Dans certains cas, il a été observé que TD-Gammon a une vision des positions plus confiante que les joueurs de classe mondiale. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Jouer avec vous-même se reflète dans de nombreuses réalisations emblématiques associées à RL. Il est important de noter que jouer avec soi-même a aidé au développement d'agents pour jouer aux </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">échecs et aller</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> avec des capacités surhumaines, des agents </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">DOTA 2 d'</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> élite </font><font style="vertical-align: inherit;">, ainsi que des stratégies et contre-stratégies complexes dans des jeux tels que la </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">lutte</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> et la </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;">cache</font></a><font style="vertical-align: inherit;"> - </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;">cache</font></a><font style="vertical-align: inherit;"> .</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Dans les résultats obtenus en jouant avec soi-même, il est souvent noté que les agents de jeu choisissent des stratégies qui surprennent les experts. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Jouer avec soi donne aux agents une certaine créativité indépendante de la créativité des programmeurs. </font><font style="vertical-align: inherit;">L'agent ne reçoit que les règles du jeu, puis - des informations sur s'il a gagné ou perdu. </font><font style="vertical-align: inherit;">En outre, sur la base de ces principes de base, l'agent doit développer un comportement compétent. </font><font style="vertical-align: inherit;">Selon le créateur de TD-Gammon, une telle approche de l'apprentissage libère ", en ce sens que le programme n'est pas contraint par des inclinations et des préjugés humains, qui peuvent s'avérer erronés et peu fiables". </font><font style="vertical-align: inherit;">Grâce à cette liberté, les agents découvrent des stratégies de jeu géniales qui changent complètement la façon dont les ingénieurs pensent à certains jeux.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Formation de renforcement compétitif</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dans le cadre de la tâche traditionnelle d'apprentissage renforcé, l'agent essaie de développer une ligne de comportement qui maximise la récompense totale. Le signal de récompense encode la tâche de l'agent - une telle tâche peut être, par exemple, le tracé d'un cours ou la collecte d'articles. Le comportement de l'agent est soumis à des restrictions environnementales. Tels, par exemple, la gravité, les obstacles, ainsi que l’influence relative des mesures prises par l’agent lui-même - par exemple, l’application de la force à son propre mouvement. Ces facteurs limitent le comportement de l'agent et sont des forces externes qu'il doit apprendre à gérer afin de recevoir une récompense élevée. Ainsi, l'agent est en concurrence avec la dynamique de l'environnement et doit passer d'un état à l'autre précisément pour que la récompense maximale soit atteinte.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/if/jf/xi/ifjfxil_iqm9pznr61qbpxujs38.png"><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Le scénario typique de formation de renforcement est illustré à gauche: l'agent agit dans l'environnement, passe à l'état suivant et reçoit une récompense. Le scénario de formation est illustré à droite, où l'agent est en concurrence avec un rival, qui, du point de vue de l'agent, est en fait un élément de l'environnement.</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Dans le cas des jeux de compétition, l'agent est en concurrence non seulement avec la dynamique de l'environnement, mais aussi avec un autre agent (éventuellement intellectuel). Nous pouvons supposer que l'adversaire est intégré à l'environnement, et ses actions affectent directement l'état suivant que l'agent «voit», ainsi que la récompense qu'il recevra. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/zl/kv/xo/zlkvxoa9_bt0cdsthmddwll-fvc.png"><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Exemple de tennis de ML-Agents Toolkit</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Considérez la démo de ML-Agents Tennis. La raquette bleue (à gauche) est l'agent d'apprentissage et la violette (à droite) est son adversaire. Pour lancer le ballon au-dessus du filet, l'agent doit tenir compte de la trajectoire du ballon volant de l'adversaire, et effectuer un ajustement de l'angle et de la vitesse du ballon volant, en tenant compte des conditions environnementales (gravité). Cependant, dans une compétition avec un adversaire, lancer le ballon sur le filet n'est que la moitié de la bataille. Un adversaire fort peut répondre d'un coup irrésistible et, par conséquent, l'agent perdra. Un adversaire faible peut frapper la balle dans le filet. Un adversaire égal peut retourner le service, et donc le jeu continuera. Dans tous les cas, l'état suivant et la récompense correspondante dépendent à la fois des conditions environnementales et de l'adversaire. Cependant, dans toutes ces situations, l'agent fait le même pitch. Par conséquent, en tant qu'entraînement à des jeux compétitifs,et le pompage des comportements rivaux par un agent est un problème complexe.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Les considérations pour un adversaire approprié ne sont pas anodines. Comme il ressort clairement de ce qui précède, la force relative de l'adversaire affecte de manière significative le résultat d'un match particulier. Si l'adversaire est trop fort, l'agent peut avoir du mal à apprendre à jouer à partir de zéro. D'un autre côté, si l'adversaire est trop faible, alors l'agent peut apprendre à gagner, mais ces compétences peuvent être inutiles en concurrence avec un adversaire plus fort ou simplement différent. Par conséquent, nous avons besoin d'un adversaire qui sera à peu près égal en force à l'agent (inflexible, mais pas insurmontable). De plus, comme les compétences de notre agent s'améliorent à chaque match terminé, nous devons augmenter la force de son adversaire dans la même mesure. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/fn/vk/pt/fnvkptgsipqopi2iiyflsa5ec_a.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Lorsque vous jouez avec vous-même, un instantané du passé ou un agent dans son état actuel est l'adversaire intégré à l'environnement.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
C'est là que le jeu avec nous est utile! L'agent lui-même satisfait aux deux exigences de l'adversaire souhaité. Il est certainement à peu près égal en force à lui-même et ses compétences s'améliorent avec le temps. Dans ce cas, la propre stratégie de l'agent est intégrée à l'environnement (voir la figure). Ceux qui connaissent bien </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">la complexité croissante de l'éducation</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (apprentissage du curriculum), vous montrent que nous pouvons supposer que le système développe naturellement le curriculum, après quoi l'agent apprend à lutter contre des adversaires de plus en plus puissants. En conséquence, jouer avec vous-même vous permet d'utiliser l'environnement lui-même pour former des agents compétitifs pour des jeux compétitifs!</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dans les deux sections suivantes, nous examinerons des détails plus techniques de la formation d'agents compétitifs, en particulier, concernant la mise en œuvre et l'utilisation du jeu avec soi-même dans la boîte à outils ML-Agents.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Considérations pratiques</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Certains problèmes pratiques se posent concernant le cadre de jeu avec vous-même. </font><font style="vertical-align: inherit;">En particulier, la reconversion est possible, dans laquelle l'agent apprend à gagner uniquement avec un certain style de jeu, ainsi que l'instabilité inhérente au processus d'apprentissage, qui peut survenir en raison de l'instabilité de la fonction de transition (c'est-à-dire en raison d'adversaires en constante évolution). </font><font style="vertical-align: inherit;">Le premier problème se pose parce que nous voulons que nos agents aient une compréhension générale et une capacité à combattre des adversaires de différents types.</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Le deuxième problème peut être illustré dans l'environnement du tennis: différents adversaires vont frapper la balle à différentes vitesses et à différents angles. Du point de vue de l'agent d'apprentissage, cela signifie que, au fur et à mesure que vous apprenez, les mêmes décisions conduiront à des résultats différents et, par conséquent, l'agent sera dans différentes situations ultérieures. Dans l'apprentissage par renforcement traditionnel, les fonctions de transition stationnaires sont implicites. Malheureusement, après avoir préparé une sélection de divers adversaires pour l'agent afin de résoudre le premier problème, nous, étant imprudents, pouvons aggraver le second.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pour y faire face, nous maintiendrons un tampon avec les politiques d'agent passées, parmi lesquelles nous choisirons des rivaux potentiels pour notre "étudiant" à long terme. </font><font style="vertical-align: inherit;">En choisissant un agent parmi les politiques passées, nous obtenons pour lui une sélection d'adversaires divers. </font><font style="vertical-align: inherit;">De plus, permettant à l'agent de s'entraîner avec un adversaire fixe pendant une longue période, nous stabilisons la fonction de transition et créons un environnement d'apprentissage plus cohérent. </font><font style="vertical-align: inherit;">Enfin, ces aspects algorithmiques peuvent être contrôlés à l'aide d'hyperparamètres, qui sont abordés dans la section suivante.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Détails d'implémentation et d'utilisation</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En choisissant des hyperparamètres pour jouer avec nous-mêmes, nous gardons tout d'abord à l'esprit un compromis entre le niveau de l'adversaire, l'universalité de la politique finale et la stabilité de l'entraînement. S'entraîner à la rivalité avec un groupe d'adversaires qui changent lentement ou ne changent pas du tout et, par conséquent, donnent une plus petite dispersion des résultats, est un processus plus stable que l'entraînement à la rivalité avec de nombreux rivaux divers qui changent rapidement. Les hyperparamètres disponibles vous permettent de contrôler la fréquence à laquelle la politique actuelle de l'agent sera enregistrée pour une utilisation ultérieure en tant qu'un des opposants dans l'échantillon, la fréquence à laquelle le nouvel adversaire sera enregistré, puis sélectionné pour le combat, la fréquence à laquelle le nouvel adversaire sera sélectionné, le nombre d'adversaires enregistrés, ainsi que la probabilitéque dans ce cas, l'étudiant devra jouer contre son propre alter ego, et non contre un adversaire sélectionné dans la poule.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dans les jeux compétitifs, le prix «cumulatif» délivré par l'environnement n'est peut-être pas la mesure la plus informative pour suivre les progrès de l'apprentissage. Le fait est que la récompense cumulative dépend entièrement du niveau de l'adversaire. Un agent avec une certaine compétence de jeu recevra une récompense plus ou moins grande, selon un adversaire moins qualifié ou plus habile, respectivement. Nous proposons la mise en œuvre </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">du système de classement ELO</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , qui vous permet de calculer la compétence de jeu relative de deux joueurs d'une certaine population lorsqu'ils jouent avec un montant nul. Au cours d'une seule séance d'entraînement, cette valeur devrait augmenter régulièrement. Vous pouvez le suivre, ainsi que d'autres mesures d'apprentissage, par exemple, le prix global, à l'aide de </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">TensorBoard</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Jouer avec vous-même dans le football</font></font></h3> <br>
<img src="https://habrastorage.org/webt/gl/m_/rh/glm_rhop1whkyve0i-wpubxfpqw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Les dernières versions de ML-Agent Toolkit n'incluent pas de stratégies d'agent pour l'environnement d'apprentissage Soccer, car le processus de formation fiable n'y a pas été intégré. </font><font style="vertical-align: inherit;">Cependant, en utilisant le jeu avec nous-mêmes et certains refactoring, nous pouvons former l'agent à des comportements non triviaux. </font><font style="vertical-align: inherit;">Le changement le plus important est la suppression des «positions de jeu» des caractéristiques de l'agent. </font><font style="vertical-align: inherit;">Plus tôt dans l'environnement du football, le «gardien de but» et «l'attaquant» se sont clairement démarqués, de sorte que l'ensemble du gameplay semblait plus logique. </font><font style="vertical-align: inherit;">Dans </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">cette vidéo</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">un nouvel environnement est présenté dans lequel on peut voir comment le comportement de rôle se forme spontanément, dans lequel certains agents commencent à agir comme des attaquants et d'autres comme des gardiens de but. Maintenant, les agents eux-mêmes apprennent à jouer ces positions! La fonction de récompense pour les quatre agents est définie comme +1,0 pour un but marqué et -1,0 pour un but encaissé, avec une pénalité supplémentaire de -0,0003 par pas - cette pénalité est prévue pour encourager les agents à attaquer.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ici, nous soulignons une fois de plus que les agents dans l'environnement d'apprentissage du football apprennent eux-mêmes le comportement coopératif, et pour cela, aucun algorithme explicite n'est utilisé concernant le comportement multi-agents ou l'attribution de rôle. </font><font style="vertical-align: inherit;">Ce résultat démontre qu'un agent peut être formé à des comportements complexes à l'aide d'algorithmes relativement simples - à condition que la tâche soit bien formulée. </font><font style="vertical-align: inherit;">La condition la plus importante pour cela est que les agents puissent observer leurs coéquipiers, c'est-à-dire qu'ils reçoivent des informations sur la position relative du coéquipier. </font><font style="vertical-align: inherit;">Forçant un combat agressif pour le ballon, l'agent dit indirectement au coéquipier qu'il doit se déplacer en défense. </font><font style="vertical-align: inherit;">Au contraire, s'éloignant en défense, l'agent provoque l'attaque d'un coéquipier.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Et après</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Si vous avez déjà utilisé l'une des nouvelles fonctionnalités de cette version, parlez-nous-en. </font><font style="vertical-align: inherit;">Nous attirons votre attention sur la page des </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">problèmes de ML-Agents GitHub</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , où vous pouvez parler des bogues trouvés, ainsi que sur la page des </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">forums Unity ML-Agents</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , où les questions et problèmes généraux sont discutés.</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr507200/index.html">7 façons dont les scientifiques des données vous trompent</a></li>
<li><a href="../fr507202/index.html">Meetup Avito Analytics</a></li>
<li><a href="../fr507204/index.html">Cuisine d'intérieur design industriel: du croquis au produit en boîte</a></li>
<li><a href="../fr507206/index.html">Architecture Y messenger</a></li>
<li><a href="../fr507210/index.html">Les performances de Java moderne lorsque vous travaillez avec de grandes quantités de données, partie 2</a></li>
<li><a href="../fr507214/index.html">Comment créer et modifier des formulaires PDF interactifs, ou la nouvelle compétence ABBYY FineReader PDF</a></li>
<li><a href="../fr507218/index.html">Lisez-moi, ou pourquoi le texte n'est pas lu jusqu'à la fin</a></li>
<li><a href="../fr507222/index.html">Pourquoi tout le monde devrait porter des masques</a></li>
<li><a href="../fr507224/index.html">Comment éliminer les angles morts avec des tests visuels</a></li>
<li><a href="../fr507226/index.html">OCR pour PDF dans .NET - Comment extraire du texte de documents PDF inaccessibles</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>