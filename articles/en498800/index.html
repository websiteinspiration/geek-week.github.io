<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèº‚Äçü§ù‚Äçüë©üèª üëÜüèø üåî How I taught my computer to play Dobble with OpenCV and Deep Learning ‚ö±Ô∏è üë©üèΩ‚Äçüéì üò©</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello dear subscribers! You probably already know that we have launched a new course "Computer Vision" , classes on which will start in the coming day...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>How I taught my computer to play Dobble with OpenCV and Deep Learning</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/498800/"><b><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hello dear subscribers! </font><font style="vertical-align: inherit;">You probably already know that we have launched a new course </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Computer Vision"</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , classes on which will start in the coming days. </font><font style="vertical-align: inherit;">In anticipation of the start of classes, we prepared another interesting translation for immersion in the world of CV.</font></font></i></b><br>
<br>
<img src="https://habrastorage.org/webt/q9/rm/5j/q9rm5jisb3ecqro7lnwivk2wwqc.png"><br>
<hr><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
My hobby is playing board games, and since I am a little familiar with convolutional neural networks, I decided to create an application that can beat a person in a card game. </font><font style="vertical-align: inherit;">I wanted to build a model from scratch using my own dataset and see how well it works with a small dataset. </font><font style="vertical-align: inherit;">I decided to start with the simple Dobble game (also known as Spot it!).</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
If you don‚Äôt know what Dobble is, I‚Äôll briefly recall the rules of the game: Dobble is a simple pattern recognition game in which players try to find a picture depicted simultaneously on two cards. </font><font style="vertical-align: inherit;">Each card in the original Dobble game contains eight different characters, and on different cards they are of different sizes. </font><font style="vertical-align: inherit;">Any two cards have only one common symbol. </font><font style="vertical-align: inherit;">If you find the symbol first, then pick up a card. </font><font style="vertical-align: inherit;">When the deck of 55 cards ends, the one with the most cards wins. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/qt/b7/sw/qtb7swvb0bghekmkzqswukrkwoy.jpeg"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Try it for yourself: What symbol is common for these two cards?</font></font></i><br>
<a name="habracut"></a><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Where to begin?</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The first step in solving any data analysis task is to collect data. </font><font style="vertical-align: inherit;">I took six photos of each card on the phone. </font><font style="vertical-align: inherit;">In total 330 photos turned out. </font><font style="vertical-align: inherit;">Four of them you see below. </font><font style="vertical-align: inherit;">You may ask, is this enough to create a good convolutional neural network? </font><font style="vertical-align: inherit;">We will come back to this!</font></font><br>
<br>
<img src="https://habrastorage.org/webt/cf/8s/c5/cf8sc5csfihv8q9ngt4r5wipzom.jpeg"><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Image processing</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
OK, the data we have, what's next? </font><font style="vertical-align: inherit;">Probably the most important part on the path to success: image processing. </font><font style="vertical-align: inherit;">We need to get characters from each image. </font><font style="vertical-align: inherit;">Some difficulties await us here. </font><font style="vertical-align: inherit;">In the photos above, it is noticeable that some characters are more difficult to distinguish than others: the snowman and the ghost (in the third photo) and the needle (in the fourth) of light colors, and the blots (in the second photo) and the exclamation mark (in the fourth photo) consist of several parts . </font><font style="vertical-align: inherit;">To process light characters we will add contrast. </font><font style="vertical-align: inherit;">After that we will resize and save the image.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Add contrast</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
To add contrast, we use the </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Lab</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> color space </font><i><font style="vertical-align: inherit;">. </font></i><i><font style="vertical-align: inherit;">L</font></i><font style="vertical-align: inherit;"> is lightness, </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">a</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> is the chromatic component in the range from green to magenta, and </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">b</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> is the chromatic component in the range from blue to yellow. </font><font style="vertical-align: inherit;">We can easily extract these components using </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">OpenCV</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> :</font></font><br>
<br>
<pre><code class="plaintext hljs">import cv2<font></font>
import imutils<font></font>
imgname = 'picture1'<font></font>
image = cv2.imread(f‚Äô{imgname}.jpg‚Äô)<font></font>
lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)<font></font>
l, a, b = cv2.split(lab)</code></pre><br>
<img src="https://habrastorage.org/webt/2u/rf/xs/2urfxs4kcjdhokrytfnpggg_l3s.jpeg"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">From left to right: the original image, the lightness component, component a and component b</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Now we add contrast to the lightness component, again combine all the components together and convert to a normal image:</font></font><br>
<br>
<pre><code class="plaintext hljs">clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))<font></font>
cl = clahe.apply(l)<font></font>
limg = cv2.merge((cl,a,b))<font></font>
final = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)</code></pre><br>
<img src="https://habrastorage.org/webt/6i/fc/wy/6ifcwytem84rmka2rwsn8e2iicu.jpeg"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">From left to right: the original image, the lightness component, the image with high contrast and the image converted back to RGB</font></font></i><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Change of size</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Now resize and save the image:</font></font><br>
<br>
<pre><code class="plaintext hljs">resized = cv2.resize(final, (800, 800))<font></font>
# save the image<font></font>
cv2.imwrite(f'{imgname}processed.jpg', blurred)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Done!</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Card and character recognition</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Now that the image is processed, we can detect a card in the image. </font><font style="vertical-align: inherit;">Using OpenCV, we are looking for external contours. </font><font style="vertical-align: inherit;">Then we convert the image into halftones, select the </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">threshold</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> value </font><font style="vertical-align: inherit;">(in our case, 190) to create a black-and-white image and search for a path. </font><font style="vertical-align: inherit;">The code:</font></font><br>
<br>
<pre><code class="plaintext hljs">image = cv2.imread(f‚Äô{imgname}processed.jpg‚Äô)<font></font>
gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)<font></font>
thresh = cv2.threshold(gray, 190, 255, cv2.THRESH_BINARY)[1]<font></font>
# find contours<font></font>
cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)<font></font>
cnts = imutils.grab_contours(cnts)<font></font>
output = image.copy()<font></font>
# draw contours on image<font></font>
for c in cnts:<font></font>
    cv2.drawContours(output, [c], -1, (255, 0, 0), 3)</code></pre><br>
<img src="https://habrastorage.org/webt/bh/y6/24/bhy624atzopnzxchn5gz0navg0e.jpeg"><br>
 <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Processed image converted into halftones using threshold and selecting external contours</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
If we sort the external contours by area, we will find the contour with the largest area - this will be our card. </font><font style="vertical-align: inherit;">To extract the characters we can create a white background.</font></font><br>
<br>
<pre><code class="plaintext hljs"># sort by area, grab the biggest one<font></font>
cnts = sorted(cnts, key=cv2.contourArea, reverse=True)[0]<font></font>
# create mask with the biggest contour<font></font>
mask = np.zeros(gray.shape,np.uint8)<font></font>
mask = cv2.drawContours(mask, [cnts], -1, 255, cv2.FILLED)<font></font>
# card in foreground<font></font>
fg_masked = cv2.bitwise_and(image, image, mask=mask)<font></font>
# white background (use inverted mask)<font></font>
mask = cv2.bitwise_not(mask)<font></font>
bk = np.full(image.shape, 255, dtype=np.uint8)<font></font>
bk_masked = cv2.bitwise_and(bk, bk, mask=mask)<font></font>
# combine back- and foreground<font></font>
final = cv2.bitwise_or(fg_masked, bk_masked)</code></pre><br>
<img src="https://habrastorage.org/webt/j3/lh/dn/j3lhdnzj07y84s5uqiiuphysety.jpeg"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mask, background, foreground image, final image</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Now it's time for character recognition! </font><font style="vertical-align: inherit;">We can use the resulting image to detect external contours on it again, these contours will be symbols. </font><font style="vertical-align: inherit;">If we create a square around each symbol, we can extract this area. </font><font style="vertical-align: inherit;">Here the code is a little longer:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-comment"># just like before (with detecting the card)</span><font></font>
gray = cv2.cvtColor(final, cv2.COLOR_RGB2GRAY)<font></font>
thresh = cv2.threshold(gray, <span class="hljs-number">195</span>, <span class="hljs-number">255</span>, cv2.THRESH_BINARY)[<span class="hljs-number">1</span>]<font></font>
thresh = cv2.bitwise_not(thresh)<font></font>
cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)<font></font>
cnts = imutils.grab_contours(cnts)<font></font>
cnts = sorted(cnts, key=cv2.contourArea, reverse=<span class="hljs-literal">True</span>)[:<span class="hljs-number">10</span>]
<span class="hljs-comment"># handle each contour</span>
i = <span class="hljs-number">0</span>
<span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> cnts:
    <span class="hljs-keyword">if</span> cv2.contourArea(c) &gt; <span class="hljs-number">1000</span>:
        <span class="hljs-comment"># draw mask, keep contour</span><font></font>
        mask = np.zeros(gray.shape, np.uint8)<font></font>
        mask = cv2.drawContours(mask, [c], <span class="hljs-number">-1</span>, <span class="hljs-number">255</span>, cv2.FILLED)
        <span class="hljs-comment"># white background</span><font></font>
        fg_masked = cv2.bitwise_and(image, image, mask=mask)<font></font>
        mask = cv2.bitwise_not(mask)<font></font>
        bk = np.full(image.shape, <span class="hljs-number">255</span>, dtype=np.uint8)<font></font>
        bk_masked = cv2.bitwise_and(bk, bk, mask=mask)<font></font>
        finalcont = cv2.bitwise_or(fg_masked, bk_masked)<font></font>
        <span class="hljs-comment"># bounding rectangle around contour</span><font></font>
        output = finalcont.copy()<font></font>
        x,y,w,h = cv2.boundingRect(c)<font></font>
        <span class="hljs-comment"># squares io rectangles</span>
        <span class="hljs-keyword">if</span> w &lt; h:<font></font>
            x += int((w-h)/<span class="hljs-number">2</span>)<font></font>
            w = h<font></font>
        <span class="hljs-keyword">else</span>:<font></font>
            y += int((h-w)/<span class="hljs-number">2</span>)<font></font>
            h = w<font></font>
        <span class="hljs-comment"># take out the square with the symbol</span><font></font>
        roi = finalcont[y:y+h, x:x+w]<font></font>
        roi = cv2.resize(roi, (<span class="hljs-number">400</span>,<span class="hljs-number">400</span>))
        <span class="hljs-comment"># save the symbol</span>
        cv2.imwrite(<span class="hljs-string">f"<span class="hljs-subst">{imgname}</span>_icon<span class="hljs-subst">{i}</span>.jpg"</span>, roi)<font></font>
        i += <span class="hljs-number">1</span></code></pre><br>
<img src="https://habrastorage.org/webt/qf/pf/vk/qfpfvkmhh5u674jgxpbiecjcovg.jpeg"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Black and white image (thresholded), detected outlines, a ghost symbol and a heart symbol (characters extracted with masks)</font></font></i><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Character sort</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
And now the most boring! </font><font style="vertical-align: inherit;">You need to sort the characters. </font><font style="vertical-align: inherit;">You will need the train, test, and validation directories, 57 directories each (we have 57 different characters in total). </font><font style="vertical-align: inherit;">The folder structure is as follows:</font></font><br>
<br>
<pre><code class="python hljs">symbols<font></font>
 ‚îú‚îÄ‚îÄ test<font></font>
 ‚îÇ   ‚îú‚îÄ‚îÄ anchor<font></font>
 ‚îÇ   ‚îú‚îÄ‚îÄ apple<font></font>
 ‚îÇ   ‚îÇ   ...<font></font>
 ‚îÇ   ‚îî‚îÄ‚îÄ zebra<font></font>
 ‚îú‚îÄ‚îÄ train<font></font>
 ‚îÇ   ‚îú‚îÄ‚îÄ anchor<font></font>
 ‚îÇ   ‚îú‚îÄ‚îÄ apple<font></font>
 ‚îÇ   ‚îÇ   ...<font></font>
 ‚îÇ   ‚îî‚îÄ‚îÄ zebra<font></font>
 ‚îî‚îÄ‚îÄ validation<font></font>
     ‚îú‚îÄ‚îÄ anchor<font></font>
     ‚îú‚îÄ‚îÄ apple<font></font>
     ‚îÇ   ...<font></font>
     ‚îî‚îÄ‚îÄ zebra</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
It will take some time to put the extracted characters (more than 2500 pieces) in the necessary directories! </font><font style="vertical-align: inherit;">I have code for creating subfolders, a test suite and a validation kit on </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GitHub</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Maybe next time it‚Äôs better to do the sorting based on the clustering algorithm ...</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Convolutional neural network training</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
After the boring part, the fun comes again! </font><font style="vertical-align: inherit;">It's time to create and train a convolutional neural network. </font><font style="vertical-align: inherit;">You can </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">find</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> information about convolutional neural networks </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;">here</font></a><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Model architecture</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We have the task of multi-class classification with one label. </font><font style="vertical-align: inherit;">For each character we need one label. </font><font style="vertical-align: inherit;">That is why we will need a function to activate the output </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">softmax</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> layer </font><font style="vertical-align: inherit;">with 57 nodes and categorical cross-entropy as a loss function. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The architecture of the final model is as follows:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-comment"># imports</span>
<span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> layers
<span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> models
<span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> optimizers
<span class="hljs-keyword">from</span> keras.preprocessing.image <span class="hljs-keyword">import</span> ImageDataGenerator
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-comment"># layers, activation layer with 57 nodes (one for every symbol)</span><font></font>
model = models.Sequential()<font></font>
model.add(layers.Conv2D(<span class="hljs-number">32</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">'relu'</span>, input_shape=(<span class="hljs-number">400</span>, <span class="hljs-number">400</span>, <span class="hljs-number">3</span>)))<font></font>
model.add(layers.MaxPooling2D((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))  <font></font>
model.add(layers.Conv2D(<span class="hljs-number">64</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">'relu'</span>))<font></font>
model.add(layers.MaxPooling2D((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))<font></font>
model.add(layers.Conv2D(<span class="hljs-number">128</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">'relu'</span>))<font></font>
model.add(layers.MaxPooling2D((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))<font></font>
model.add(layers.Conv2D(<span class="hljs-number">256</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">'relu'</span>))<font></font>
model.add(layers.MaxPooling2D((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))<font></font>
model.add(layers.Conv2D(<span class="hljs-number">256</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">'relu'</span>))<font></font>
model.add(layers.MaxPooling2D((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))<font></font>
model.add(layers.Conv2D(<span class="hljs-number">128</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">'relu'</span>))<font></font>
model.add(layers.Flatten())<font></font>
model.add(layers.Dropout(<span class="hljs-number">0.5</span>)) <font></font>
model.add(layers.Dense(<span class="hljs-number">512</span>, activation=<span class="hljs-string">'relu'</span>))<font></font>
model.add(layers.Dense(<span class="hljs-number">57</span>, activation=<span class="hljs-string">'softmax'</span>))<font></font>
model.compile(loss=<span class="hljs-string">'categorical_crossentropy'</span>,       optimizer=optimizers.RMSprop(lr=<span class="hljs-number">1e-4</span>), metrics=[<span class="hljs-string">'acc'</span>])</code></pre><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Data Augmentation</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
To improve performance, I used data augmentation. </font><font style="vertical-align: inherit;">Data augmentation is the process of increasing the volume and variety of input data. </font><font style="vertical-align: inherit;">This can be done by rotating, shifting, scaling, cropping and flipping existing images. </font><font style="vertical-align: inherit;">Keras can easily augment data:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-comment"># specify the directories</span>
train_dir = <span class="hljs-string">'symbols/train'</span>
validation_dir = <span class="hljs-string">'symbols/validation'</span>
test_dir = <span class="hljs-string">'symbols/test'</span>
<span class="hljs-comment"># data augmentation with ImageDataGenerator from Keras (only train)</span>
train_datagen = ImageDataGenerator(rescale=<span class="hljs-number">1.</span>/<span class="hljs-number">255</span>, rotation_range=<span class="hljs-number">40</span>, width_shift_range=<span class="hljs-number">0.1</span>, height_shift_range=<span class="hljs-number">0.1</span>, shear_range=<span class="hljs-number">0.1</span>, zoom_range=<span class="hljs-number">0.1</span>, horizontal_flip=<span class="hljs-literal">True</span>, vertical_flip=<span class="hljs-literal">True</span>)<font></font>
test_datagen = ImageDataGenerator(rescale=<span class="hljs-number">1.</span>/<span class="hljs-number">255</span>)<font></font>
train_generator = train_datagen.flow_from_directory(train_dir, target_size=(<span class="hljs-number">400</span>,<span class="hljs-number">400</span>), batch_size=<span class="hljs-number">20</span>, class_mode=<span class="hljs-string">'categorical'</span>)<font></font>
validation_generator = test_datagen.flow_from_directory(validation_dir, target_size=(<span class="hljs-number">400</span>,<span class="hljs-number">400</span>), batch_size=<span class="hljs-number">20</span>, class_mode=<span class="hljs-string">'categorical'</span>)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
If you were interested, the augmented ghost looks like this: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/hj/wq/_c/hjwq_ckmzjkvmyr9xu9fr5pyq0w.jpeg"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The original image of the ghost on the left, augmented ghosts in all the other pictures</font></font></i><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Model training</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Let's train the model, save it to use for predictions, and check the results.</font></font><br>
<br>
<pre><code class="python hljs">history = model.fit_generator(train_generator, steps_per_epoch=<span class="hljs-number">100</span>, epochs=<span class="hljs-number">100</span>, validation_data=validation_generator, validation_steps=<span class="hljs-number">50</span>)
<span class="hljs-comment"># don't forget to save your model!</span>
model.save(<span class="hljs-string">'models/model.h5'</span>)</code></pre><br>
<img src="https://habrastorage.org/webt/uz/c2/qm/uzc2qml9v4-zezvickpgjezm8ly.gif"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Perfect predictions!</font></font></i><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">results</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The basic model that I trained without data augmentation, dropouts and with fewer layers. </font><font style="vertical-align: inherit;">This model gave the following results: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/lc/tg/u3/lctgu3nfsj3felc31tuhtg68neg.jpeg"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The results of the basic model</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
With the naked eye, it is clear that this model is retrained. </font><font style="vertical-align: inherit;">The results of the final version of the model (its code is presented in the previous sections) are much better. </font><font style="vertical-align: inherit;">On the graph below you can see the accuracy and losses during training and on the validation set. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/bw/tn/s2/bwtns2xrfdgnlzuxmwppzdqhyom.jpeg"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Results of the final model.</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
On the test set, this model made only one mistake, it recognized the bomb as a drop. </font><font style="vertical-align: inherit;">I decided to stay on this model, the accuracy on the test set was 0.995.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Recognition of a common symbol on two cards</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Now you can start looking for common symbols on two cards. </font><font style="vertical-align: inherit;">We use two photographs, we will make predictions for each image separately and use the intersection of sets to find out which symbol is on both cards. </font><font style="vertical-align: inherit;">We have 3 work options:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Something went wrong during the prediction: no common characters were found.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">There is one symbol at the intersection (prediction can be true or false).</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">There is more than one character at the intersection. </font><font style="vertical-align: inherit;">In this case, I choose the symbol with the highest probability (the average of both predictions).</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The code for predicting all the combination on the two images in the catalog lies with </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GitHub</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 's </font></font><code>main.py</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
And here are the results:</font></font><br>
<br>
<img src="https://habrastorage.org/webt/kz/6h/em/kz6hemmqhp7bhdq25ywzipipdou.gif"><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Conclusion</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 Isn't that the perfect model? Unfortunately no. When I took new photos of the cards and gave them the models for prediction, there were some problems with the snowman. Sometimes he recognized the eye or zebra as a snowman! As a result, sometimes the results were strange: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/b0/y4/e8/b0y4e8nw6kq7juigw_oekx2jjtk.jpeg"> <br>
 <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Well, where is the snowman here?</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Is this model better than man? Depending on what we need: people recognize perfectly, but the model does it faster! I noticed the time for which the computer is coping: I gave a deck of 55 cards and I had to get a common symbol for each combination of two cards. In total, these are 1485 combinations. The computer did it in less than 140 seconds. He made a few mistakes, but he will definitely beat any person when it comes to speed!</font></font><br>
<br>
<img src="https://habrastorage.org/webt/t2/pn/ce/t2pncemklqifymf39ixawz9ejsg.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 I don‚Äôt think that creating a working 100% model is difficult. </font><font style="vertical-align: inherit;">This can be achieved through transfer training. </font><font style="vertical-align: inherit;">To understand what the model does, we could visualize layers for the test image. </font><font style="vertical-align: inherit;">You can do it next time!</font></font><br>
<br>
<hr><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Learn more about the course and pass the entrance test</font></font></a><br>
<br>
<hr></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../en498774/index.html">Computer vision tasks - finding objects of the right color</a></li>
<li><a href="../en498782/index.html">Battle Experimenters - Sand and Stick Science - Inoculation of Scientific Thinking into the Public Mind</a></li>
<li><a href="../en498788/index.html">Learning a Mini Stepper Motor</a></li>
<li><a href="../en498796/index.html">Content Security Policy in Magento 2</a></li>
<li><a href="../en498798/index.html">Keep learning and learning with JetBrains</a></li>
<li><a href="../en498808/index.html">Vim with YAML Support for Kubernetes</a></li>
<li><a href="../en498814/index.html">You failed one theoretical question on the Social Security, and they put an end to you. This is normal? // We Are Doomed # 3</a></li>
<li><a href="../en498816/index.html">The truth first of all, or why the system needs to be designed based on the database device</a></li>
<li><a href="../en498820/index.html">Some more tricky questions on .NET and C #</a></li>
<li><a href="../en498822/index.html">Development and creation from scratch of an arcade machine for four players</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>