<!doctype html>
<html class="no-js" lang="pt">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚ö±Ô∏è üïú üìÅ Redes Neurais Recorrentes (RNN) com Keras üçü üöÇ üî°</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Tradu√ß√£o do Guia da Rede Neural Recursiva de Tensorflow.org. O material discute os recursos internos do Keras / Tensorflow 2.0 para malhas r√°pidas, be...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Redes Neurais Recorrentes (RNN) com Keras</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/487808/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tradu√ß√£o do Guia da Rede Neural Recursiva de Tensorflow.org. </font><font style="vertical-align: inherit;">O material discute os recursos internos do Keras / Tensorflow 2.0 para malhas r√°pidas, bem como a possibilidade de personalizar camadas e c√©lulas. </font><font style="vertical-align: inherit;">Casos e limita√ß√µes do uso do n√∫cleo CuDNN tamb√©m s√£o considerados, o que permite acelerar o processo de aprendizado da rede neural.</font></font><br>
 <br>
<img src="https://habrastorage.org/webt/xt/_q/nj/xt_qnjgfjengqoqd4gizkq4j_wk.png"><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As redes neurais recursivas (RNNs) s√£o uma classe de redes neurais que s√£o boas para modelar dados seriais, como s√©ries temporais ou linguagem natural. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Se esquematicamente, a camada RNN usa um loop </font></font><code>for</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">para iterar em uma sequ√™ncia ordenada pelo tempo, enquanto armazena em um estado interno, informa√ß√µes codificadas sobre as etapas que ele j√° viu. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Keras RNN API √© projetado com foco em: </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Facilidade de uso</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : built-in camadas </font></font><code>tf.keras.layers.RNN</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, </font></font><code>tf.keras.layers.LSTM</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, </font></font><code>tf.keras.layers.GRU</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">permitem construir rapidamente um modelo recursivo sem ter que fazer configura√ß√µes complexas. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">F√°cil personaliza√ß√£o</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : voc√™ tamb√©m pode definir sua pr√≥pria camada de c√©lulas RNN (parte interna do loop</font></font><code>for</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">) com comportamento personalizado e use-o com uma camada comum de `tf.keras.layers.RNN` (o pr√≥prio loop` for`). </font><font style="vertical-align: inherit;">Isso permitir√° que voc√™ prot√≥tipo rapidamente de v√°rias id√©ias de pesquisa de maneira flex√≠vel, com um m√≠nimo de c√≥digo.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Instala√ß√£o</font></font></h2><br>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> __future__ <span class="hljs-keyword">import</span> absolute_import, division, print_function, unicode_literals<font></font>
<font></font>
<span class="hljs-keyword">import</span> collections
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<font></font>
<font></font>
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<font></font>
<font></font>
<span class="hljs-keyword">from</span> tensorflow.keras <span class="hljs-keyword">import</span> layers</code></pre><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Construindo um modelo simples</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Keras possui tr√™s camadas RNN integradas:</font></font><br>
<br>
<ol>
<li><code>tf.keras.layers.SimpleRNN</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, uma RNN totalmente conectada na qual a sa√≠da da etapa anterior deve ser passada para a pr√≥xima etapa.</font></font></li>
<li><code>tf.keras.layers.GRU</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, proposto pela primeira vez no artigo </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Estudo de frases usando o codec RNN para tradu√ß√£o autom√°tica estat√≠stica</font></font></a></li>
<li><code>tf.keras.layers.LSTM</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, proposto pela primeira vez no artigo </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mem√≥ria de curto prazo a longo prazo</font></font></a></li>
</ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
No in√≠cio de 2015, Keras apresentou as primeiras implementa√ß√µes reutiliz√°veis ‚Äã‚Äãde c√≥digo aberto Python e LSTM e GRU. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A seguir, √© apresentado um exemplo de </font></font><code>Sequential</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">modelo que processa seq√º√™ncias de n√∫meros inteiros, aninhando cada n√∫mero inteiro em um vetor de 64 dimens√µes e processando sequ√™ncias de vetores usando uma camada </font></font><code>LSTM</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font><br>
<br>
<pre><code class="python hljs">model = tf.keras.Sequential()
<span class="hljs-comment">#   Embedding      1000, </span>
<span class="hljs-comment">#     64.</span>
model.add(layers.Embedding(input_dim=<span class="hljs-number">1000</span>, output_dim=<span class="hljs-number">64</span>))<font></font>
<font></font>
<span class="hljs-comment">#   LSTM  128  .</span>
model.add(layers.LSTM(<span class="hljs-number">128</span>))<font></font>
<font></font>
<span class="hljs-comment">#   Dense  10    softmax.</span>
model.add(layers.Dense(<span class="hljs-number">10</span>))<font></font>
<font></font>
model.summary()</code></pre><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sa√≠das e status</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Por padr√£o, a sa√≠da da camada RNN cont√©m um vetor por elemento. Esse vetor √© a sa√≠da da √∫ltima c√©lula RNN que cont√©m informa√ß√µes sobre toda a sequ√™ncia de entrada. A dimens√£o desta sa√≠da </font></font><code>(batch_size, units)</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, em que </font></font><code>units</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">corresponde ao argumento </font></font><code>units</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">passado ao construtor da camada. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A camada RNN tamb√©m pode retornar a sequ√™ncia de sa√≠da inteira para cada elemento (um vetor para cada etapa), se voc√™ especificar </font></font><code>return_sequences=True</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. A dimens√£o desta sa√≠da √© </font></font><code>(batch_size, timesteps, units)</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font><br>
<br>
<pre><code class="python hljs">model = tf.keras.Sequential()<font></font>
model.add(layers.Embedding(input_dim=<span class="hljs-number">1000</span>, output_dim=<span class="hljs-number">64</span>))<font></font>
<font></font>
<span class="hljs-comment">#  GRU  3D   (batch_size, timesteps, 256)</span>
model.add(layers.GRU(<span class="hljs-number">256</span>, return_sequences=<span class="hljs-literal">True</span>))<font></font>
<font></font>
<span class="hljs-comment">#  SimpleRNN  2D   (batch_size, 128)</span>
model.add(layers.SimpleRNN(<span class="hljs-number">128</span>))<font></font>
<font></font>
model.add(layers.Dense(<span class="hljs-number">10</span>))<font></font>
<font></font>
model.summary()</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Al√©m disso, a camada RNN pode retornar seus estados internos finais. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Os estados retornados podem ser usados ‚Äã‚Äãposteriormente para retomar a execu√ß√£o do RNN ou </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">para inicializar outro RNN</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Essa configura√ß√£o geralmente √© usada no modelo codificador-decodificador, sequ√™ncia a sequ√™ncia, em que o estado final do codificador √© usado para o estado inicial do decodificador. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para que a camada RNN retorne seu estado interno, defina o par√¢metro </font></font><code>return_state</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">como valor </font></font><code>True</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ao criar a camada. Observe que existem </font></font><code>LSTM</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2 tensores de estado e </font></font><code>GRU</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">apenas um. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para ajustar o estado inicial de uma camada, basta chamar a camada com um argumento adicional </font></font><code>initial_state</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Observe que a dimens√£o deve corresponder √† dimens√£o do elemento da camada, como no exemplo a seguir.</font></font><br>
<br>
<pre><code class="python hljs">encoder_vocab = <span class="hljs-number">1000</span>
decoder_vocab = <span class="hljs-number">2000</span><font></font>
<font></font>
encoder_input = layers.Input(shape=(<span class="hljs-literal">None</span>, ))<font></font>
encoder_embedded = layers.Embedding(input_dim=encoder_vocab, output_dim=<span class="hljs-number">64</span>)(encoder_input)<font></font>
<font></font>
<span class="hljs-comment">#       </span><font></font>
output, state_h, state_c = layers.LSTM(<font></font>
    <span class="hljs-number">64</span>, return_state=<span class="hljs-literal">True</span>, name=<span class="hljs-string">'encoder'</span>)(encoder_embedded)<font></font>
encoder_state = [state_h, state_c]<font></font>
<font></font>
decoder_input = layers.Input(shape=(<span class="hljs-literal">None</span>, ))<font></font>
decoder_embedded = layers.Embedding(input_dim=decoder_vocab, output_dim=<span class="hljs-number">64</span>)(decoder_input)<font></font>
<font></font>
<span class="hljs-comment">#  2     LSTM    </span><font></font>
decoder_output = layers.LSTM(<font></font>
    <span class="hljs-number">64</span>, name=<span class="hljs-string">'decoder'</span>)(decoder_embedded, initial_state=encoder_state)<font></font>
output = layers.Dense(<span class="hljs-number">10</span>)(decoder_output)<font></font>
<font></font>
model = tf.keras.Model([encoder_input, decoder_input], output)<font></font>
model.summary()<font></font>
</code></pre><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Camadas RNN e c√©lulas RNN</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A API RNN, al√©m das camadas RNN internas, tamb√©m fornece APIs no n√≠vel da c√©lula. </font><font style="vertical-align: inherit;">Diferentemente das camadas RNN, que processam pacotes inteiros de seq√º√™ncias de entrada, uma c√©lula RNN processa apenas uma etapa. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A c√©lula est√° dentro do ciclo da </font></font><code>for</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">camada RNN. </font><font style="vertical-align: inherit;">A quebra de uma c√©lula com uma camada </font></font><code>tf.keras.layers.RNN</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">fornece uma camada capaz de processar pacotes de sequ√™ncia, por exemplo </font></font><code>RNN(LSTMCell(10))</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Matematicamente, </font></font><code>RNN(LSTMCell(10))</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">fornece o mesmo resultado que </font></font><code>LSTM(10)</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">De fato, a implementa√ß√£o dessa camada no TF v1.x foi apenas para criar a c√©lula RNN correspondente e envolv√™-la na camada RNN. </font><font style="vertical-align: inherit;">No entanto, o uso de camadas incorporadas </font></font><code>GRU</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">e </font></font><code>LSTM</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">permite o uso de CuDNN que pode oferecer melhor desempenho.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Existem tr√™s c√©lulas RNN internas, cada uma das quais corresponde √† sua pr√≥pria camada RNN.</font></font><br>
<br>
<ul>
<li><code>tf.keras.layers.SimpleRNNCell</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">corresponde √† camada </font></font><code>SimpleRNN</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font></li>
<li><code>tf.keras.layers.GRUCell</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">corresponde √† camada </font></font><code>GRU</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font></li>
<li><code>tf.keras.layers.LSTMCell</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">corresponde √† camada </font></font><code>LSTM</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A abstra√ß√£o de uma c√©lula junto com uma classe comum </font></font><code>tf.keras.layers.RNN</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">facilita muito a implementa√ß√£o de arquiteturas RNN personalizadas para sua pesquisa.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Estado de salvamento em lotes cruzados</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ao processar sequ√™ncias longas (possivelmente intermin√°veis), conv√©m usar o padr√£o de </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">estado de lote cruzado</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Geralmente, o estado interno da camada RNN √© redefinido a cada novo pacote de dados (ou seja, cada exemplo que v√™ a camada √© considerado independente do passado). A camada manter√° o estado apenas pela dura√ß√£o do processamento desse elemento. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
No entanto, se voc√™ tiver sequ√™ncias muito longas, √© √∫til dividi-las em sequ√™ncias mais curtas e transferi-las para a camada RNN, por sua vez, sem redefinir o estado da camada. Assim, uma camada pode armazenar informa√ß√µes sobre toda a sequ√™ncia, embora apenas veja uma subsequ√™ncia por vez. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Voc√™ pode fazer isso definindo `stateful = True` no construtor.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Se voc√™ tiver a sequ√™ncia `s = [t0, t1, ... t1546, t1547]`, poder√° dividi-la, por exemplo, em:</font></font><br>
<br>
<pre><code class="python hljs">s1 = [t0, t1, ... t100]<font></font>
s2 = [t101, ... t201]<font></font>
...<font></font>
s16 = [t1501, ... t1547]</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ent√£o voc√™ pode process√°-lo com:</font></font><br>
<br>
<pre><code class="python hljs">lstm_layer = layers.LSTM(<span class="hljs-number">64</span>, stateful=<span class="hljs-literal">True</span>)
<span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> sub_sequences:<font></font>
  output = lstm_layer(s)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Quando voc√™ quiser limpar a condi√ß√£o, use </font></font><code>layer.reset_states()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font><br>
<blockquote><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nota:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Nesse caso, sup√µe-se que o exemplo </font></font><code>i</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">neste pacote seja uma continua√ß√£o do exemplo do </font></font><code>i</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">pacote anterior. </font><font style="vertical-align: inherit;">Isso significa que todos os pacotes cont√™m o mesmo n√∫mero de elementos (tamanho do pacote). </font><font style="vertical-align: inherit;">Por exemplo, se o pacote contiver </font></font><code>[sequence_A_from_t0_to_t100, sequence_B_from_t0_to_t100]</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, o pr√≥ximo pacote dever√° conter </font></font><code>[sequence_A_from_t101_to_t200, sequence_B_from_t101_to_t200]</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font></blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aqui est√° um exemplo completo:</font></font><br>
<br>
<pre><code class="python hljs">paragraph1 = np.random.random((<span class="hljs-number">20</span>, <span class="hljs-number">10</span>, <span class="hljs-number">50</span>)).astype(np.float32)<font></font>
paragraph2 = np.random.random((<span class="hljs-number">20</span>, <span class="hljs-number">10</span>, <span class="hljs-number">50</span>)).astype(np.float32)<font></font>
paragraph3 = np.random.random((<span class="hljs-number">20</span>, <span class="hljs-number">10</span>, <span class="hljs-number">50</span>)).astype(np.float32)<font></font>
<font></font>
lstm_layer = layers.LSTM(<span class="hljs-number">64</span>, stateful=<span class="hljs-literal">True</span>)<font></font>
output = lstm_layer(paragraph1)<font></font>
output = lstm_layer(paragraph2)<font></font>
output = lstm_layer(paragraph3)<font></font>
<font></font>
<span class="hljs-comment"># reset_states()      initial_state.</span>
<span class="hljs-comment">#  initial_state   ,      .</span>
lstm_layer.reset_states()</code></pre><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">RNN bidirecional</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para sequ√™ncias que n√£o sejam s√©ries temporais (por exemplo, textos), muitas vezes acontece que o modelo RNN funciona melhor se processar a sequ√™ncia n√£o apenas do in√≠cio ao fim, mas vice-versa. Por exemplo, para prever a pr√≥xima palavra em uma frase, geralmente √© √∫til conhecer o contexto em torno da palavra, e n√£o apenas as palavras √† sua frente. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Keras fornece uma API simples para criar esses RNNs bidirecionais: um wrapper </font></font><code>tf.keras.layers.Bidirectional</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font><br>
<br>
<pre><code class="python hljs">model = tf.keras.Sequential()<font></font>
<font></font>
model.add(layers.Bidirectional(layers.LSTM(<span class="hljs-number">64</span>, return_sequences=<span class="hljs-literal">True</span>), <font></font>
                               input_shape=(<span class="hljs-number">5</span>, <span class="hljs-number">10</span>)))<font></font>
model.add(layers.Bidirectional(layers.LSTM(<span class="hljs-number">32</span>)))<font></font>
model.add(layers.Dense(<span class="hljs-number">10</span>))<font></font>
<font></font>
model.summary()</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Sob o cap√¥, a </font></font><code>Bidirectional</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">camada RNN transferida </font></font><code>go_backwards</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ser√° copiada </font><font style="vertical-align: inherit;">e o campo da </font><font style="vertical-align: inherit;">nova camada copiada ser√° </font><font style="vertical-align: inherit;">revertido </font><font style="vertical-align: inherit;">e, assim, os dados de entrada ser√£o processados ‚Äã‚Äãna ordem inversa. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A sa√≠da de ` </font></font><code>Bidirectional</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">RNN por padr√£o ser√° a soma da sa√≠da da camada direta e a sa√≠da da camada reversa. </font><font style="vertical-align: inherit;">Se voc√™ precisar de outro comportamento de mesclagem, por exemplo, </font><font style="vertical-align: inherit;">concatena√ß√£o, altere o par√¢metro `merge_mode` no construtor do wrapper` Bidirecional`.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Otimiza√ß√£o de desempenho e n√∫cleo CuDNN no TensorFlow 2.0</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
No TensorFlow 2.0, as camadas LSTM e GRU incorporadas s√£o utiliz√°veis ‚Äã‚Äãpor n√∫cleos CuDNN padr√£o, se um processador gr√°fico estiver dispon√≠vel. </font><font style="vertical-align: inherit;">Com essa altera√ß√£o, as camadas anteriores </font></font><code>keras.layers.CuDNNLSTM/CuDNNGRU</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">est√£o desatualizadas e voc√™ pode construir seu modelo sem se preocupar com o equipamento no qual ele trabalhar√°. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Como o kernel CuDNN √© constru√≠do com algumas suposi√ß√µes, isso significa que a camada </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">n√£o poder√° usar a camada CuDNN se voc√™ alterar as configura√ß√µes padr√£o das camadas LSTM ou GRU internas</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Por exemplo.</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Alterando uma fun√ß√£o </font></font><code>activation</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">de </font></font><code>tanh</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">para outra coisa.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Alterando uma fun√ß√£o </font></font><code>recurrent_activation</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">de </font></font><code>sigmoid</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">para outra coisa.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Uso </font></font><code>recurrent_dropout</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">&gt; 0.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Configurando-o </font></font><code>unroll</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">como True, o que faz com que o LSTM / GRU decomponha o interno </font></font><code>tf.while_loop</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">em um loop implementado </font></font><code>for</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Defina </font></font><code>use_bias</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">como Falso.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Usando m√°scaras quando os dados de entrada n√£o est√£o corretamente justificados (se a m√°scara corresponder aos dados estritamente alinhados, o CuDNN ainda poder√° ser usado. Esse √© o caso mais comum).</font></font></li>
</ul><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Quando poss√≠vel, use kernels CuDNN</font></font></h3><br>
<pre><code class="python hljs">batch_size = <span class="hljs-number">64</span>
<span class="hljs-comment">#    MNIST    (batch_size, 28, 28).</span>
<span class="hljs-comment">#     (28, 28) (   ).</span>
input_dim = <span class="hljs-number">28</span><font></font>
<font></font>
units = <span class="hljs-number">64</span>
output_size = <span class="hljs-number">10</span>  <span class="hljs-comment">#   0  9</span><font></font>
<font></font>
<span class="hljs-comment">#  RNN </span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">build_model</span>(<span class="hljs-params">allow_cudnn_kernel=True</span>):</span>
  <span class="hljs-comment"># CuDNN     ,     .</span>
  <span class="hljs-comment">#   `LSTM(units)`    CuDNN,</span>
  <span class="hljs-comment">#   RNN(LSTMCell(units))   non-CuDNN .</span>
  <span class="hljs-keyword">if</span> allow_cudnn_kernel:
    <span class="hljs-comment">#  LSTM      CuDNN.</span>
    lstm_layer = tf.keras.layers.LSTM(units, input_shape=(<span class="hljs-literal">None</span>, input_dim))
  <span class="hljs-keyword">else</span>:
    <span class="hljs-comment">#  LSTMCell  RNN    CuDNN.</span><font></font>
    lstm_layer = tf.keras.layers.RNN(<font></font>
        tf.keras.layers.LSTMCell(units),<font></font>
        input_shape=(<span class="hljs-literal">None</span>, input_dim))<font></font>
  model = tf.keras.models.Sequential([<font></font>
      lstm_layer,<font></font>
      tf.keras.layers.BatchNormalization(),<font></font>
      tf.keras.layers.Dense(output_size)]<font></font>
  )<font></font>
  <span class="hljs-keyword">return</span> model
</code></pre><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Carregando o conjunto de dados MNIST</font></font></h3><br>
<pre><code class="python hljs">mnist = tf.keras.datasets.mnist<font></font>
<font></font>
(x_train, y_train), (x_test, y_test) = mnist.load_data()<font></font>
x_train, x_test = x_train / <span class="hljs-number">255.0</span>, x_test / <span class="hljs-number">255.0</span>
sample, sample_label = x_train[<span class="hljs-number">0</span>], y_train[<span class="hljs-number">0</span>]</code></pre><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Crie uma inst√¢ncia do modelo e compile-a</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
N√≥s escolhemos </font></font><code>sparse_categorical_crossentropy</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">em fun√ß√£o das perdas. </font><font style="vertical-align: inherit;">A sa√≠da do modelo tem uma dimens√£o </font></font><code>[batch_size, 10]</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">A resposta do modelo √© um vetor inteiro, cada um dos n√∫meros est√° no intervalo de 0 a 9.</font></font><br>
<br>
<pre><code class="python hljs">model = build_model(allow_cudnn_kernel=<span class="hljs-literal">True</span>)<font></font>
<font></font>
model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="hljs-literal">True</span>), <font></font>
              optimizer=<span class="hljs-string">'sgd'</span>,<font></font>
              metrics=[<span class="hljs-string">'accuracy'</span>])
</code></pre><br>
<pre><code class="python hljs">model.fit(x_train, y_train,<font></font>
          validation_data=(x_test, y_test),<font></font>
          batch_size=batch_size,<font></font>
          epochs=<span class="hljs-number">5</span>)</code></pre><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Crie um novo modelo sem o n√∫cleo CuDNN</font></font></h3><br>
<pre><code class="python hljs">slow_model = build_model(allow_cudnn_kernel=<span class="hljs-literal">False</span>)<font></font>
slow_model.set_weights(model.get_weights())<font></font>
slow_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="hljs-literal">True</span>), <font></font>
                   optimizer=<span class="hljs-string">'sgd'</span>, <font></font>
                   metrics=[<span class="hljs-string">'accuracy'</span>])<font></font>
slow_model.fit(x_train, y_train, <font></font>
               validation_data=(x_test, y_test), <font></font>
               batch_size=batch_size,<font></font>
               epochs=<span class="hljs-number">1</span>)  <span class="hljs-comment">#         .</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Como voc√™ pode ver, o modelo criado com o CuDNN √© muito mais r√°pido para o treinamento do que o modelo usando o n√∫cleo habitual do TensorFlow. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O mesmo modelo com suporte CuDNN pode ser usado para sa√≠da em um ambiente de processador √∫nico. </font><font style="vertical-align: inherit;">A anota√ß√£o </font></font><code>tf.device</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">indica simplesmente o dispositivo usado. </font><font style="vertical-align: inherit;">O modelo ser√° executado por padr√£o na CPU se a GPU n√£o estiver dispon√≠vel. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Voc√™ simplesmente n√£o precisa se preocupar com o hardware em que est√° trabalhando. </font><font style="vertical-align: inherit;">Isso n√£o √© legal?</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">with</span> tf.device(<span class="hljs-string">'CPU:0'</span>):<font></font>
  cpu_model = build_model(allow_cudnn_kernel=<span class="hljs-literal">True</span>)<font></font>
  cpu_model.set_weights(model.get_weights())<font></font>
  result = tf.argmax(cpu_model.predict_on_batch(tf.expand_dims(sample, <span class="hljs-number">0</span>)), axis=<span class="hljs-number">1</span>)<font></font>
  print(<span class="hljs-string">'Predicted result is: %s, target result is: %s'</span> % (result.numpy(), sample_label))<font></font>
  plt.imshow(sample, cmap=plt.get_cmap(<span class="hljs-string">'gray'</span>))
</code></pre><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">RNN com entrada de lista / dicion√°rio ou entrada aninhada</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Estruturas aninhadas permitem incluir mais informa√ß√µes em uma √∫nica etapa. </font><font style="vertical-align: inherit;">Por exemplo, um quadro de v√≠deo pode conter entrada de √°udio e v√≠deo simultaneamente. </font><font style="vertical-align: inherit;">A dimens√£o dos dados neste caso pode ser:</font></font><br>
<br>
<pre><code class="plaintext hljs">[batch, timestep, {\"video\": [height, width, channel], \"audio\": [frequency]}]</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Em outro exemplo, os dados manuscritos podem ter coordenadas xey para a posi√ß√£o atual da caneta, al√©m de informa√ß√µes de press√£o. </font><font style="vertical-align: inherit;">Portanto, os dados podem ser representados da seguinte maneira:</font></font><br>
<br>
<pre><code class="plaintext hljs">[batch, timestep, {\"location\": [x, y], \"pressure\": [force]}]</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O c√≥digo a seguir cria um exemplo de uma c√©lula RNN personalizada que funciona com essa entrada estruturada.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Definir uma c√©lula do usu√°rio que suporte entrada / sa√≠da aninhada</font></font></h3><br>
<pre><code class="python hljs">NestedInput = collections.namedtuple(<span class="hljs-string">'NestedInput'</span>, [<span class="hljs-string">'feature1'</span>, <span class="hljs-string">'feature2'</span>])<font></font>
NestedState = collections.namedtuple(<span class="hljs-string">'NestedState'</span>, [<span class="hljs-string">'state1'</span>, <span class="hljs-string">'state2'</span>])<font></font>
<font></font>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">NestedCell</span>(<span class="hljs-params">tf.keras.layers.Layer</span>):</span><font></font>
<font></font>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, unit_1, unit_2, unit_3, **kwargs</span>):</span><font></font>
    self.unit_1 = unit_1<font></font>
    self.unit_2 = unit_2<font></font>
    self.unit_3 = unit_3<font></font>
    self.state_size = NestedState(state1=unit_1, <font></font>
                                  state2=tf.TensorShape([unit_2, unit_3]))<font></font>
    self.output_size = (unit_1, tf.TensorShape([unit_2, unit_3]))<font></font>
    super(NestedCell, self).__init__(**kwargs)<font></font>
<font></font>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">build</span>(<span class="hljs-params">self, input_shapes</span>):</span>
    <span class="hljs-comment"># #  input_shape  2 , [(batch, i1), (batch, i2, i3)]</span>
    input_1 = input_shapes.feature1[<span class="hljs-number">1</span>]<font></font>
    input_2, input_3 = input_shapes.feature2[<span class="hljs-number">1</span>:]<font></font>
<font></font>
    self.kernel_1 = self.add_weight(<font></font>
        shape=(input_1, self.unit_1), initializer=<span class="hljs-string">'uniform'</span>, name=<span class="hljs-string">'kernel_1'</span>)<font></font>
    self.kernel_2_3 = self.add_weight(<font></font>
        shape=(input_2, input_3, self.unit_2, self.unit_3),<font></font>
        initializer=<span class="hljs-string">'uniform'</span>,<font></font>
        name=<span class="hljs-string">'kernel_2_3'</span>)<font></font>
<font></font>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">call</span>(<span class="hljs-params">self, inputs, states</span>):</span>
    <span class="hljs-comment">#     [(batch, input_1), (batch, input_2, input_3)]</span>
    <span class="hljs-comment">#     [(batch, unit_1), (batch, unit_2, unit_3)]</span><font></font>
    input_1, input_2 = tf.nest.flatten(inputs)<font></font>
    s1, s2 = states<font></font>
<font></font>
    output_1 = tf.matmul(input_1, self.kernel_1)<font></font>
    output_2_3 = tf.einsum(<span class="hljs-string">'bij,ijkl-&gt;bkl'</span>, input_2, self.kernel_2_3)<font></font>
    state_1 = s1 + output_1<font></font>
    state_2_3 = s2 + output_2_3<font></font>
<font></font>
    output = [output_1, output_2_3]<font></font>
    new_states = NestedState(state1=state_1, state2=state_2_3)<font></font>
<font></font>
    <span class="hljs-keyword">return</span> output, new_states
</code></pre><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Construir um modelo RNN com entrada / sa√≠da aninhada</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Vamos construir um modelo Keras que usa uma camada </font></font><code>tf.keras.layers.RNN</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">e uma c√©lula personalizada que acabamos de definir.</font></font><br>
<br>
<pre><code class="python hljs">unit_1 = <span class="hljs-number">10</span>
unit_2 = <span class="hljs-number">20</span>
unit_3 = <span class="hljs-number">30</span><font></font>
<font></font>
input_1 = <span class="hljs-number">32</span>
input_2 = <span class="hljs-number">64</span>
input_3 = <span class="hljs-number">32</span>
batch_size = <span class="hljs-number">64</span>
num_batch = <span class="hljs-number">100</span>
timestep = <span class="hljs-number">50</span><font></font>
<font></font>
cell = NestedCell(unit_1, unit_2, unit_3)<font></font>
rnn = tf.keras.layers.RNN(cell)<font></font>
<font></font>
inp_1 = tf.keras.Input((<span class="hljs-literal">None</span>, input_1))<font></font>
inp_2 = tf.keras.Input((<span class="hljs-literal">None</span>, input_2, input_3))<font></font>
<font></font>
outputs = rnn(NestedInput(feature1=inp_1, feature2=inp_2))<font></font>
<font></font>
model = tf.keras.models.Model([inp_1, inp_2], outputs)<font></font>
<font></font>
model.compile(optimizer=<span class="hljs-string">'adam'</span>, loss=<span class="hljs-string">'mse'</span>, metrics=[<span class="hljs-string">'accuracy'</span>])unit_1 = <span class="hljs-number">10</span>
unit_2 = <span class="hljs-number">20</span>
unit_3 = <span class="hljs-number">30</span><font></font>
<font></font>
input_1 = <span class="hljs-number">32</span>
input_2 = <span class="hljs-number">64</span>
input_3 = <span class="hljs-number">32</span>
batch_size = <span class="hljs-number">64</span>
num_batch = <span class="hljs-number">100</span>
timestep = <span class="hljs-number">50</span><font></font>
<font></font>
cell = NestedCell(unit_1, unit_2, unit_3)<font></font>
rnn = tf.keras.layers.RNN(cell)<font></font>
<font></font>
inp_1 = tf.keras.Input((<span class="hljs-literal">None</span>, input_1))<font></font>
inp_2 = tf.keras.Input((<span class="hljs-literal">None</span>, input_2, input_3))<font></font>
<font></font>
outputs = rnn(NestedInput(feature1=inp_1, feature2=inp_2))<font></font>
<font></font>
model = tf.keras.models.Model([inp_1, inp_2], outputs)<font></font>
<font></font>
model.compile(optimizer=<span class="hljs-string">'adam'</span>, loss=<span class="hljs-string">'mse'</span>, metrics=[<span class="hljs-string">'accuracy'</span>])
</code></pre><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Treine o modelo em dados gerados aleatoriamente</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Como n√£o temos um bom conjunto de dados para esse modelo, usamos dados aleat√≥rios gerados pela biblioteca Numpy para demonstra√ß√£o.</font></font><br>
<br>
<pre><code class="python hljs">input_1_data = np.random.random((batch_size * num_batch, timestep, input_1))<font></font>
input_2_data = np.random.random((batch_size * num_batch, timestep, input_2, input_3))<font></font>
target_1_data = np.random.random((batch_size * num_batch, unit_1))<font></font>
target_2_data = np.random.random((batch_size * num_batch, unit_2, unit_3))<font></font>
input_data = [input_1_data, input_2_data]<font></font>
target_data = [target_1_data, target_2_data]<font></font>
<font></font>
model.fit(input_data, target_data, batch_size=batch_size)<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Com uma camada, </font></font><code>tf.keras.layers.RNN</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">voc√™ s√≥ precisa determinar a l√≥gica matem√°tica de uma √∫nica etapa na sequ√™ncia, e a camada </font></font><code>tf.keras.layers.RNN</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">manipular√° a itera√ß√£o da sequ√™ncia para voc√™. </font><font style="vertical-align: inherit;">Essa √© uma maneira incrivelmente poderosa de prototipar rapidamente novos tipos de RNNs (por exemplo, a variante LSTM). </font></font><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ap√≥s a verifica√ß√£o, a tradu√ß√£o tamb√©m aparecer√° no Tensorflow.org. </font><font style="vertical-align: inherit;">Se voc√™ deseja participar da tradu√ß√£o da documenta√ß√£o do site Tensorflow.org para o russo, entre em contato com um coment√°rio ou pessoal. </font><font style="vertical-align: inherit;">Quaisquer corre√ß√µes e coment√°rios s√£o apreciados.</font></font></i></div>
      
    </div><p class="reference-to-source js-reference-to-source">Source: https://habr.com/ru/post/undefined/</p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt487798/index.html">Como migramos do Oracle JDK e Java Web Start para AdoptOpenJDK e OpenWebStart</a></li>
<li><a href="../pt487800/index.html">Por que √© importante dizer ao solicitante o que deu errado durante a entrevista (e como faz√™-lo corretamente)</a></li>
<li><a href="../pt487802/index.html">APC Smart UPS ininterrupta e como cozinh√°-los</a></li>
<li><a href="../pt487804/index.html">Meetups de Crescimento de equipes em Raiffeisenbank</a></li>
<li><a href="../pt487806/index.html">Criando uma pequena API Deno</a></li>
<li><a href="../pt487812/index.html">O teste de espectro LED polon√™s levou E27</a></li>
<li><a href="../pt487814/index.html">Velocidade e confiabilidade s√£o mais altas e o pre√ßo √© mais baixo. Novas unidades de estado s√≥lido Kingston KC2000</a></li>
<li><a href="../pt487822/index.html">AvitoTech On Tour: encontro com o Android em Nizhny Novgorod</a></li>
<li><a href="../pt487824/index.html">Vis√£o geral das l√¢mpadas LED Spectrum Led GU10 da Europa</a></li>
<li><a href="../pt487826/index.html">Vis√£o geral das l√¢mpadas LED da Pol√¥nia Spectrum Led E14</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>