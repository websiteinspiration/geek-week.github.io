<!doctype html>
<html class="no-js" lang="ar">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👐🏻 🧓🏽 🧕🏼 الشبكات العصبية المتكررة (RNN) مع Keras 🍊 🤳🏼 🏴󠁧󠁢󠁳󠁣󠁴󠁿</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="ترجمة دليل الشبكة العصبية العودية من Tensorflow.org. تناقش المادة كلاً من القدرات المدمجة لـ Keras / Tensorflow 2.0 للتشبيك السريع ، بالإضافة إلى إمكا...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>الشبكات العصبية المتكررة (RNN) مع Keras</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/487808/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ترجمة دليل الشبكة العصبية العودية من Tensorflow.org. </font><font style="vertical-align: inherit;">تناقش المادة كلاً من القدرات المدمجة لـ Keras / Tensorflow 2.0 للتشبيك السريع ، بالإضافة إلى إمكانية تخصيص الطبقات والخلايا. </font><font style="vertical-align: inherit;">كما يتم النظر في حالات وحدود استخدام نواة CuDNN ، مما يسمح بتسريع عملية تعلم الشبكة العصبية.</font></font><br>
 <br>
<img src="https://habrastorage.org/webt/xt/_q/nj/xt_qnjgfjengqoqd4gizkq4j_wk.png"><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
الشبكات العصبية العودية (RNNs) هي فئة من الشبكات العصبية جيدة لنمذجة البيانات التسلسلية ، مثل السلاسل الزمنية أو اللغة الطبيعية. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
إذا كانت الطبقة RNN تستخدم بشكل تخطيطي حلقة </font></font><code>for</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">للتكرار عبر تسلسل مرتب حسب الوقت ، أثناء التخزين في حالة داخلية ، يتم ترميز المعلومات حول الخطوات التي شاهدها بالفعل. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
تم تصميم Keras RNN API مع التركيز على: </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">سهولة الاستخدام</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : المدمج في طبقات </font></font><code>tf.keras.layers.RNN</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">، </font></font><code>tf.keras.layers.LSTM</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">، </font></font><code>tf.keras.layers.GRU</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">تسمح لك لبناء بسرعة نموذجا متكررة دون الحاجة إلى إجراء إعدادات التكوين معقدة. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">التخصيص السهل</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : يمكنك أيضًا تحديد الطبقة الخاصة بك من خلايا RNN (الجزء الداخلي من الحلقة</font></font><code>for</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">) مع السلوك المخصص واستخدامه مع طبقة مشتركة من `tf.keras.layers.RNN` (حلقة` for` نفسها). </font><font style="vertical-align: inherit;">سيتيح لك ذلك وضع نماذج لأفكار البحث المختلفة بسرعة وبأسلوب مرن ، مع الحد الأدنى من التعليمات البرمجية.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">التركيب</font></font></h2><br>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> __future__ <span class="hljs-keyword">import</span> absolute_import, division, print_function, unicode_literals<font></font>
<font></font>
<span class="hljs-keyword">import</span> collections
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<font></font>
<font></font>
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<font></font>
<font></font>
<span class="hljs-keyword">from</span> tensorflow.keras <span class="hljs-keyword">import</span> layers</code></pre><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">بناء نموذج بسيط</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
تحتوي Keras على ثلاث طبقات RNN مضمنة:</font></font><br>
<br>
<ol>
<li><code>tf.keras.layers.SimpleRNN</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">، RNN متصل بالكامل بحيث يتم تمرير ناتج الخطوة الزمنية السابقة إلى الخطوة التالية.</font></font></li>
<li><code>tf.keras.layers.GRU</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">، اقترح لأول مرة في المقالة </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">دراسة العبارات باستخدام برنامج الترميز RNN للترجمة الآلية الإحصائية</font></font></a></li>
<li><code>tf.keras.layers.LSTM</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">، اقترح لأول مرة في مقال </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">طويل الأمد الذاكرة قصيرة المدى</font></font></a></li>
</ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
في أوائل عام 2015 ، قدمت Keras أول تطبيقات مفتوحة المصدر قابلة لإعادة الاستخدام Python و LSTM و GRU. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
فيما يلي مثال على </font></font><code>Sequential</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">نموذج يعالج تسلسل الأعداد الصحيحة عن طريق تداخل كل عدد صحيح في ناقل 64-الأبعاد ، ثم معالجة تسلسلات المتجهات باستخدام طبقة </font></font><code>LSTM</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font><br>
<br>
<pre><code class="python hljs">model = tf.keras.Sequential()
<span class="hljs-comment">#   Embedding      1000, </span>
<span class="hljs-comment">#     64.</span>
model.add(layers.Embedding(input_dim=<span class="hljs-number">1000</span>, output_dim=<span class="hljs-number">64</span>))<font></font>
<font></font>
<span class="hljs-comment">#   LSTM  128  .</span>
model.add(layers.LSTM(<span class="hljs-number">128</span>))<font></font>
<font></font>
<span class="hljs-comment">#   Dense  10    softmax.</span>
model.add(layers.Dense(<span class="hljs-number">10</span>))<font></font>
<font></font>
model.summary()</code></pre><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">المخرجات والحالات</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
بشكل افتراضي ، يحتوي إخراج طبقة RNN على متجه واحد لكل عنصر. هذا المتجه هو إخراج آخر خلية RNN تحتوي على معلومات حول تسلسل الإدخال بأكمله. بُعد هذا الإخراج </font></font><code>(batch_size, units)</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">، حيث </font></font><code>units</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">يتوافق مع الوسيطة التي </font></font><code>units</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">تم تمريرها إلى مُنشئ الطبقة. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
يمكن لطبقة RNN أيضًا إرجاع تسلسل الإخراج الكامل لكل عنصر (متجه واحد لكل خطوة) ، إذا حددت </font></font><code>return_sequences=True</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. البعد من هذا الناتج </font></font><code>(batch_size, timesteps, units)</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font><br>
<br>
<pre><code class="python hljs">model = tf.keras.Sequential()<font></font>
model.add(layers.Embedding(input_dim=<span class="hljs-number">1000</span>, output_dim=<span class="hljs-number">64</span>))<font></font>
<font></font>
<span class="hljs-comment">#  GRU  3D   (batch_size, timesteps, 256)</span>
model.add(layers.GRU(<span class="hljs-number">256</span>, return_sequences=<span class="hljs-literal">True</span>))<font></font>
<font></font>
<span class="hljs-comment">#  SimpleRNN  2D   (batch_size, 128)</span>
model.add(layers.SimpleRNN(<span class="hljs-number">128</span>))<font></font>
<font></font>
model.add(layers.Dense(<span class="hljs-number">10</span>))<font></font>
<font></font>
model.summary()</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
بالإضافة إلى ذلك ، يمكن لطبقة RNN إرجاع حالتها (حالاتها) الداخلية النهائية. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
يمكن استخدام الحالات المرتجعة لاحقًا لاستئناف تنفيذ RNN أو </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">لتهيئة RNN أخرى</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . يُستخدم هذا الإعداد عادةً في نموذج وحدة التشفير - التشفير ، بالتسلسل إلى التسلسل ، حيث يتم استخدام الحالة النهائية للمشفرة للحالة الأولية لمفكك الشفرة. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
لكي تقوم طبقة RNN بإعادة حالتها الداخلية ، قم بتعيين المعلمة </font></font><code>return_state</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">على القيمة </font></font><code>True</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">عند إنشاء الطبقة. لاحظ أن هناك </font></font><code>LSTM</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">موتران لحالة الولاية </font></font><code>GRU</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">وواحد فقط. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
لضبط الحالة الأولية للطبقة ، ما عليك سوى استدعاء الطبقة بحجة إضافية </font></font><code>initial_state</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
لاحظ أن البعد يجب أن يتطابق مع بُعد عنصر الطبقة ، كما في المثال التالي.</font></font><br>
<br>
<pre><code class="python hljs">encoder_vocab = <span class="hljs-number">1000</span>
decoder_vocab = <span class="hljs-number">2000</span><font></font>
<font></font>
encoder_input = layers.Input(shape=(<span class="hljs-literal">None</span>, ))<font></font>
encoder_embedded = layers.Embedding(input_dim=encoder_vocab, output_dim=<span class="hljs-number">64</span>)(encoder_input)<font></font>
<font></font>
<span class="hljs-comment">#       </span><font></font>
output, state_h, state_c = layers.LSTM(<font></font>
    <span class="hljs-number">64</span>, return_state=<span class="hljs-literal">True</span>, name=<span class="hljs-string">'encoder'</span>)(encoder_embedded)<font></font>
encoder_state = [state_h, state_c]<font></font>
<font></font>
decoder_input = layers.Input(shape=(<span class="hljs-literal">None</span>, ))<font></font>
decoder_embedded = layers.Embedding(input_dim=decoder_vocab, output_dim=<span class="hljs-number">64</span>)(decoder_input)<font></font>
<font></font>
<span class="hljs-comment">#  2     LSTM    </span><font></font>
decoder_output = layers.LSTM(<font></font>
    <span class="hljs-number">64</span>, name=<span class="hljs-string">'decoder'</span>)(decoder_embedded, initial_state=encoder_state)<font></font>
output = layers.Dense(<span class="hljs-number">10</span>)(decoder_output)<font></font>
<font></font>
model = tf.keras.Model([encoder_input, decoder_input], output)<font></font>
model.summary()<font></font>
</code></pre><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">طبقات RNN وخلايا RNN</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
توفر واجهة برمجة تطبيقات RNN ، بالإضافة إلى طبقات RNN المدمجة ، واجهات برمجة تطبيقات على مستوى الخلية. </font><font style="vertical-align: inherit;">على عكس طبقات RNN ، التي تعالج حزم كاملة من تسلسلات الإدخال ، تعالج خلية RNN خطوة زمنية واحدة فقط. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
الخلية داخل دورة </font></font><code>for</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">طبقة RNN. </font></font><code>tf.keras.layers.RNN</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">يمنحك </font><font style="vertical-align: inherit;">التفاف الخلية بطبقة </font><font style="vertical-align: inherit;">طبقة قادرة على معالجة حزم تسلسل ، على سبيل المثال </font></font><code>RNN(LSTMCell(10))</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
رياضيا ، </font></font><code>RNN(LSTMCell(10))</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">يعطي نفس النتيجة </font></font><code>LSTM(10)</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">في الواقع ، كان تنفيذ هذه الطبقة داخل TF v1.x فقط لإنشاء خلية RNN المقابلة ولفها في طبقة RNN. </font><font style="vertical-align: inherit;">ومع ذلك، فإن استخدام طبقات جزءا لا يتجزأ من </font></font><code>GRU</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">و </font></font><code>LSTM</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">يسمح استخدام CuDNN التي يمكن أن تعطيك أداء أفضل.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
هناك ثلاث خلايا RNN مدمجة ، كل منها يتوافق مع طبقة RNN الخاصة به.</font></font><br>
<br>
<ul>
<li><code>tf.keras.layers.SimpleRNNCell</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">يطابق الطبقة </font></font><code>SimpleRNN</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font></li>
<li><code>tf.keras.layers.GRUCell</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">يطابق الطبقة </font></font><code>GRU</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font></li>
<li><code>tf.keras.layers.LSTMCell</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">يطابق الطبقة </font></font><code>LSTM</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
إن استخلاص خلية مع فئة مشتركة </font></font><code>tf.keras.layers.RNN</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">يجعل من السهل جدًا تنفيذ بنى RNN المخصصة لبحثك.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">مجموعة حفظ عبر الدفعة</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
عند معالجة تسلسلات طويلة (ربما لا حصر لها) ، قد ترغب في استخدام نمط الحالة </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">عبر الدفعة</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
عادة ، يتم إعادة تعيين الحالة الداخلية لطبقة RNN مع كل حزمة بيانات جديدة (أي كل مثال يرى أن الطبقة يفترض أنها مستقلة عن الماضي). ستحافظ الطبقة على الحالة فقط طوال مدة معالجة هذا العنصر. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ومع ذلك ، إذا كان لديك تسلسلات طويلة جدًا ، فمن المفيد تقسيمها إلى تسلسل أقصر ونقلها إلى طبقة RNN بدورها دون إعادة تعيين حالة الطبقة. وبالتالي ، يمكن للطبقة تخزين معلومات حول التسلسل بأكمله ، على الرغم من أنها سترى تتابعًا واحدًا فقط في كل مرة. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
يمكنك القيام بذلك عن طريق تعيين `stateful = True` في المُنشئ.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
إذا كان لديك التسلسل `s = [t0، t1، ... t1546، t1547]` ، يمكنك تقسيمه على سبيل المثال إلى:</font></font><br>
<br>
<pre><code class="python hljs">s1 = [t0, t1, ... t100]<font></font>
s2 = [t101, ... t201]<font></font>
...<font></font>
s16 = [t1501, ... t1547]</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ثم يمكنك معالجتها باستخدام:</font></font><br>
<br>
<pre><code class="python hljs">lstm_layer = layers.LSTM(<span class="hljs-number">64</span>, stateful=<span class="hljs-literal">True</span>)
<span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> sub_sequences:<font></font>
  output = lstm_layer(s)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
عندما تريد تنظيف الحالة ، استخدم </font></font><code>layer.reset_states()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font><br>
<blockquote><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ملاحظة:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> في هذه الحالة ، يُفترض أن المثال </font></font><code>i</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">في هذه الحزمة هو استمرار لمثال </font></font><code>i</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">الحزمة السابقة. </font><font style="vertical-align: inherit;">هذا يعني أن جميع الحزم تحتوي على نفس عدد العناصر (حجم العبوة). </font><font style="vertical-align: inherit;">على سبيل المثال ، إذا كانت الحزمة تحتوي على </font></font><code>[sequence_A_from_t0_to_t100, sequence_B_from_t0_to_t100]</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">، يجب أن تحتوي الحزمة التالية </font></font><code>[sequence_A_from_t101_to_t200, sequence_B_from_t101_to_t200]</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font></blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">هنا مثال كامل:</font></font><br>
<br>
<pre><code class="python hljs">paragraph1 = np.random.random((<span class="hljs-number">20</span>, <span class="hljs-number">10</span>, <span class="hljs-number">50</span>)).astype(np.float32)<font></font>
paragraph2 = np.random.random((<span class="hljs-number">20</span>, <span class="hljs-number">10</span>, <span class="hljs-number">50</span>)).astype(np.float32)<font></font>
paragraph3 = np.random.random((<span class="hljs-number">20</span>, <span class="hljs-number">10</span>, <span class="hljs-number">50</span>)).astype(np.float32)<font></font>
<font></font>
lstm_layer = layers.LSTM(<span class="hljs-number">64</span>, stateful=<span class="hljs-literal">True</span>)<font></font>
output = lstm_layer(paragraph1)<font></font>
output = lstm_layer(paragraph2)<font></font>
output = lstm_layer(paragraph3)<font></font>
<font></font>
<span class="hljs-comment"># reset_states()      initial_state.</span>
<span class="hljs-comment">#  initial_state   ,      .</span>
lstm_layer.reset_states()</code></pre><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">RNN ثنائي الاتجاه</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
بالنسبة للتسلسلات بخلاف السلاسل الزمنية (مثل النصوص) ، غالبًا ما يحدث أن نموذج RNN يعمل بشكل أفضل إذا كان يعالج التسلسل ليس فقط من البداية إلى النهاية ، ولكن أيضًا بالعكس. على سبيل المثال ، للتنبؤ بالكلمة التالية في الجملة ، غالبًا ما يكون من المفيد معرفة السياق حول الكلمة ، وليس فقط الكلمات الموجودة أمامها. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
يوفر Keras واجهة برمجة تطبيقات بسيطة لإنشاء شبكات RNN ثنائية الاتجاه: غلاف </font></font><code>tf.keras.layers.Bidirectional</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font><br>
<br>
<pre><code class="python hljs">model = tf.keras.Sequential()<font></font>
<font></font>
model.add(layers.Bidirectional(layers.LSTM(<span class="hljs-number">64</span>, return_sequences=<span class="hljs-literal">True</span>), <font></font>
                               input_shape=(<span class="hljs-number">5</span>, <span class="hljs-number">10</span>)))<font></font>
model.add(layers.Bidirectional(layers.LSTM(<span class="hljs-number">32</span>)))<font></font>
model.add(layers.Dense(<span class="hljs-number">10</span>))<font></font>
<font></font>
model.summary()</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
تحت غطاء المحرك ، </font><font style="vertical-align: inherit;">سيتم نسخ </font></font><code>Bidirectional</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">طبقة RNN المنقولة </font></font><code>go_backwards</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">وسيتم </font><font style="vertical-align: inherit;">قلب حقل </font><font style="vertical-align: inherit;">الطبقة المنسوخة حديثًا ، وبالتالي سيتم معالجة بيانات الإدخال بترتيب عكسي. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
سيكون خرج ` </font></font><code>Bidirectional</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">RNN بشكل افتراضي هو مجموع ناتج الطبقة الأمامية وخرج الطبقة العكسية. </font><font style="vertical-align: inherit;">إذا كنت بحاجة إلى سلوك دمج آخر ، على سبيل المثال </font><font style="vertical-align: inherit;">تسلسل ، قم بتغيير المعلمة `merge_mode` في مُنشئ المجمّع` ثنائي الاتجاه`.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">تحسين الأداء و CuDNN Core في TensorFlow 2.0</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
في TensorFlow 2.0 ، يمكن استخدام طبقات LSTM و GRU المضمنة بشكل افتراضي نوى CuDNN إذا كان معالج الرسومات متاحًا. </font><font style="vertical-align: inherit;">مع هذا التغيير ، تكون الطبقات السابقة </font></font><code>keras.layers.CuDNNLSTM/CuDNNGRU</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">قديمة ، ويمكنك بناء نموذجك دون القلق بشأن المعدات التي ستعمل عليها. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
نظرًا لأن نواة CuDNN مبنية ببعض الافتراضات ، فإن هذا يعني أن الطبقة </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">لن تتمكن من استخدام طبقة نواة CuDNN إذا قمت بتغيير الإعدادات الافتراضية لطبقات LSTM أو GRU المدمجة</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">على سبيل المثال</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">تغيير وظيفة </font></font><code>activation</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">من </font></font><code>tanh</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">شيء آخر.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">تغيير وظيفة </font></font><code>recurrent_activation</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">من </font></font><code>sigmoid</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">شيء آخر.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">الاستخدام </font></font><code>recurrent_dropout</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">&gt; 0.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">تعيينها </font></font><code>unroll</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">إلى True، والذي يسبب LSTM / GRU لتتحلل الداخلية </font></font><code>tf.while_loop</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">في حلقة نشرها </font></font><code>for</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">تعيين </font></font><code>use_bias</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">على خطأ.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">استخدام الأقنعة عندما تكون بيانات الإدخال غير مبررة بشكل صحيح (إذا كان القناع يطابق البيانات الصحيحة المحاذاة بدقة ، فلا يزال من الممكن استخدام CuDNN. هذه هي الحالة الأكثر شيوعًا).</font></font></li>
</ul><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">عند الإمكان استخدم نوى CuDNN</font></font></h3><br>
<pre><code class="python hljs">batch_size = <span class="hljs-number">64</span>
<span class="hljs-comment">#    MNIST    (batch_size, 28, 28).</span>
<span class="hljs-comment">#     (28, 28) (   ).</span>
input_dim = <span class="hljs-number">28</span><font></font>
<font></font>
units = <span class="hljs-number">64</span>
output_size = <span class="hljs-number">10</span>  <span class="hljs-comment">#   0  9</span><font></font>
<font></font>
<span class="hljs-comment">#  RNN </span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">build_model</span>(<span class="hljs-params">allow_cudnn_kernel=True</span>):</span>
  <span class="hljs-comment"># CuDNN     ,     .</span>
  <span class="hljs-comment">#   `LSTM(units)`    CuDNN,</span>
  <span class="hljs-comment">#   RNN(LSTMCell(units))   non-CuDNN .</span>
  <span class="hljs-keyword">if</span> allow_cudnn_kernel:
    <span class="hljs-comment">#  LSTM      CuDNN.</span>
    lstm_layer = tf.keras.layers.LSTM(units, input_shape=(<span class="hljs-literal">None</span>, input_dim))
  <span class="hljs-keyword">else</span>:
    <span class="hljs-comment">#  LSTMCell  RNN    CuDNN.</span><font></font>
    lstm_layer = tf.keras.layers.RNN(<font></font>
        tf.keras.layers.LSTMCell(units),<font></font>
        input_shape=(<span class="hljs-literal">None</span>, input_dim))<font></font>
  model = tf.keras.models.Sequential([<font></font>
      lstm_layer,<font></font>
      tf.keras.layers.BatchNormalization(),<font></font>
      tf.keras.layers.Dense(output_size)]<font></font>
  )<font></font>
  <span class="hljs-keyword">return</span> model
</code></pre><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">تحميل مجموعة بيانات MNIST</font></font></h3><br>
<pre><code class="python hljs">mnist = tf.keras.datasets.mnist<font></font>
<font></font>
(x_train, y_train), (x_test, y_test) = mnist.load_data()<font></font>
x_train, x_test = x_train / <span class="hljs-number">255.0</span>, x_test / <span class="hljs-number">255.0</span>
sample, sample_label = x_train[<span class="hljs-number">0</span>], y_train[<span class="hljs-number">0</span>]</code></pre><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">إنشاء مثيل من النموذج وتجميعه</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
لقد اخترنا </font></font><code>sparse_categorical_crossentropy</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">كدالة للخسائر. </font><font style="vertical-align: inherit;">ناتج النموذج له بعد </font></font><code>[batch_size, 10]</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">إجابة النموذج متجه عدد صحيح ، كل من الأرقام في النطاق من 0 إلى 9.</font></font><br>
<br>
<pre><code class="python hljs">model = build_model(allow_cudnn_kernel=<span class="hljs-literal">True</span>)<font></font>
<font></font>
model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="hljs-literal">True</span>), <font></font>
              optimizer=<span class="hljs-string">'sgd'</span>,<font></font>
              metrics=[<span class="hljs-string">'accuracy'</span>])
</code></pre><br>
<pre><code class="python hljs">model.fit(x_train, y_train,<font></font>
          validation_data=(x_test, y_test),<font></font>
          batch_size=batch_size,<font></font>
          epochs=<span class="hljs-number">5</span>)</code></pre><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">قم ببناء نموذج جديد بدون نواة CuDNN</font></font></h3><br>
<pre><code class="python hljs">slow_model = build_model(allow_cudnn_kernel=<span class="hljs-literal">False</span>)<font></font>
slow_model.set_weights(model.get_weights())<font></font>
slow_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="hljs-literal">True</span>), <font></font>
                   optimizer=<span class="hljs-string">'sgd'</span>, <font></font>
                   metrics=[<span class="hljs-string">'accuracy'</span>])<font></font>
slow_model.fit(x_train, y_train, <font></font>
               validation_data=(x_test, y_test), <font></font>
               batch_size=batch_size,<font></font>
               epochs=<span class="hljs-number">1</span>)  <span class="hljs-comment">#         .</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
كما ترى ، فإن النموذج المصمم باستخدام CuDNN أسرع بكثير في التدريب من النموذج باستخدام نواة TensorFlow المعتادة. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
يمكن استخدام نفس النموذج مع دعم CuDNN للإخراج في بيئة المعالج الواحد. </font></font><code>tf.device</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">يشير </font><font style="vertical-align: inherit;">التعليق التوضيحي </font><font style="vertical-align: inherit;">ببساطة إلى الجهاز المستخدم. </font><font style="vertical-align: inherit;">سيتم تشغيل النموذج بشكل افتراضي على وحدة المعالجة المركزية إذا لم يكن GPU متاحًا. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
لا داعي للقلق بشأن الأجهزة التي تعمل عليها. </font><font style="vertical-align: inherit;">أليس هذا رائعا؟</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">with</span> tf.device(<span class="hljs-string">'CPU:0'</span>):<font></font>
  cpu_model = build_model(allow_cudnn_kernel=<span class="hljs-literal">True</span>)<font></font>
  cpu_model.set_weights(model.get_weights())<font></font>
  result = tf.argmax(cpu_model.predict_on_batch(tf.expand_dims(sample, <span class="hljs-number">0</span>)), axis=<span class="hljs-number">1</span>)<font></font>
  print(<span class="hljs-string">'Predicted result is: %s, target result is: %s'</span> % (result.numpy(), sample_label))<font></font>
  plt.imshow(sample, cmap=plt.get_cmap(<span class="hljs-string">'gray'</span>))
</code></pre><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">RNN مع إدخال قائمة / قاموس ، أو إدخال متداخل</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
تتيح لك الهياكل المتداخلة تضمين المزيد من المعلومات في خطوة واحدة. </font><font style="vertical-align: inherit;">على سبيل المثال ، قد يحتوي إطار الفيديو على إدخال صوت وفيديو في نفس الوقت. </font><font style="vertical-align: inherit;">قد يكون حجم البيانات في هذه الحالة:</font></font><br>
<br>
<pre><code class="plaintext hljs">[batch, timestep, {\"video\": [height, width, channel], \"audio\": [frequency]}]</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
في مثال آخر ، يمكن أن تحتوي البيانات المكتوبة بخط اليد على إحداثيات x و y لموضع القلم الحالي ، بالإضافة إلى معلومات الضغط. </font><font style="vertical-align: inherit;">لذلك يمكن تمثيل البيانات على النحو التالي:</font></font><br>
<br>
<pre><code class="plaintext hljs">[batch, timestep, {\"location\": [x, y], \"pressure\": [force]}]</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ينشئ الكود التالي مثالاً لخلية RNN مخصصة تعمل مع مثل هذه المدخلات المنظمة.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">حدد خلية مستخدم تدعم الإدخال / الإخراج المتداخل</font></font></h3><br>
<pre><code class="python hljs">NestedInput = collections.namedtuple(<span class="hljs-string">'NestedInput'</span>, [<span class="hljs-string">'feature1'</span>, <span class="hljs-string">'feature2'</span>])<font></font>
NestedState = collections.namedtuple(<span class="hljs-string">'NestedState'</span>, [<span class="hljs-string">'state1'</span>, <span class="hljs-string">'state2'</span>])<font></font>
<font></font>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">NestedCell</span>(<span class="hljs-params">tf.keras.layers.Layer</span>):</span><font></font>
<font></font>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, unit_1, unit_2, unit_3, **kwargs</span>):</span><font></font>
    self.unit_1 = unit_1<font></font>
    self.unit_2 = unit_2<font></font>
    self.unit_3 = unit_3<font></font>
    self.state_size = NestedState(state1=unit_1, <font></font>
                                  state2=tf.TensorShape([unit_2, unit_3]))<font></font>
    self.output_size = (unit_1, tf.TensorShape([unit_2, unit_3]))<font></font>
    super(NestedCell, self).__init__(**kwargs)<font></font>
<font></font>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">build</span>(<span class="hljs-params">self, input_shapes</span>):</span>
    <span class="hljs-comment"># #  input_shape  2 , [(batch, i1), (batch, i2, i3)]</span>
    input_1 = input_shapes.feature1[<span class="hljs-number">1</span>]<font></font>
    input_2, input_3 = input_shapes.feature2[<span class="hljs-number">1</span>:]<font></font>
<font></font>
    self.kernel_1 = self.add_weight(<font></font>
        shape=(input_1, self.unit_1), initializer=<span class="hljs-string">'uniform'</span>, name=<span class="hljs-string">'kernel_1'</span>)<font></font>
    self.kernel_2_3 = self.add_weight(<font></font>
        shape=(input_2, input_3, self.unit_2, self.unit_3),<font></font>
        initializer=<span class="hljs-string">'uniform'</span>,<font></font>
        name=<span class="hljs-string">'kernel_2_3'</span>)<font></font>
<font></font>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">call</span>(<span class="hljs-params">self, inputs, states</span>):</span>
    <span class="hljs-comment">#     [(batch, input_1), (batch, input_2, input_3)]</span>
    <span class="hljs-comment">#     [(batch, unit_1), (batch, unit_2, unit_3)]</span><font></font>
    input_1, input_2 = tf.nest.flatten(inputs)<font></font>
    s1, s2 = states<font></font>
<font></font>
    output_1 = tf.matmul(input_1, self.kernel_1)<font></font>
    output_2_3 = tf.einsum(<span class="hljs-string">'bij,ijkl-&gt;bkl'</span>, input_2, self.kernel_2_3)<font></font>
    state_1 = s1 + output_1<font></font>
    state_2_3 = s2 + output_2_3<font></font>
<font></font>
    output = [output_1, output_2_3]<font></font>
    new_states = NestedState(state1=state_1, state2=state_2_3)<font></font>
<font></font>
    <span class="hljs-keyword">return</span> output, new_states
</code></pre><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">قم ببناء نموذج RNN بإدخال / إخراج متداخل</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
دعونا نبني نموذج Keras يستخدم طبقة </font></font><code>tf.keras.layers.RNN</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">وخلية مخصصة حددناها للتو.</font></font><br>
<br>
<pre><code class="python hljs">unit_1 = <span class="hljs-number">10</span>
unit_2 = <span class="hljs-number">20</span>
unit_3 = <span class="hljs-number">30</span><font></font>
<font></font>
input_1 = <span class="hljs-number">32</span>
input_2 = <span class="hljs-number">64</span>
input_3 = <span class="hljs-number">32</span>
batch_size = <span class="hljs-number">64</span>
num_batch = <span class="hljs-number">100</span>
timestep = <span class="hljs-number">50</span><font></font>
<font></font>
cell = NestedCell(unit_1, unit_2, unit_3)<font></font>
rnn = tf.keras.layers.RNN(cell)<font></font>
<font></font>
inp_1 = tf.keras.Input((<span class="hljs-literal">None</span>, input_1))<font></font>
inp_2 = tf.keras.Input((<span class="hljs-literal">None</span>, input_2, input_3))<font></font>
<font></font>
outputs = rnn(NestedInput(feature1=inp_1, feature2=inp_2))<font></font>
<font></font>
model = tf.keras.models.Model([inp_1, inp_2], outputs)<font></font>
<font></font>
model.compile(optimizer=<span class="hljs-string">'adam'</span>, loss=<span class="hljs-string">'mse'</span>, metrics=[<span class="hljs-string">'accuracy'</span>])unit_1 = <span class="hljs-number">10</span>
unit_2 = <span class="hljs-number">20</span>
unit_3 = <span class="hljs-number">30</span><font></font>
<font></font>
input_1 = <span class="hljs-number">32</span>
input_2 = <span class="hljs-number">64</span>
input_3 = <span class="hljs-number">32</span>
batch_size = <span class="hljs-number">64</span>
num_batch = <span class="hljs-number">100</span>
timestep = <span class="hljs-number">50</span><font></font>
<font></font>
cell = NestedCell(unit_1, unit_2, unit_3)<font></font>
rnn = tf.keras.layers.RNN(cell)<font></font>
<font></font>
inp_1 = tf.keras.Input((<span class="hljs-literal">None</span>, input_1))<font></font>
inp_2 = tf.keras.Input((<span class="hljs-literal">None</span>, input_2, input_3))<font></font>
<font></font>
outputs = rnn(NestedInput(feature1=inp_1, feature2=inp_2))<font></font>
<font></font>
model = tf.keras.models.Model([inp_1, inp_2], outputs)<font></font>
<font></font>
model.compile(optimizer=<span class="hljs-string">'adam'</span>, loss=<span class="hljs-string">'mse'</span>, metrics=[<span class="hljs-string">'accuracy'</span>])
</code></pre><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">تدريب النموذج على البيانات بشكل عشوائي</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
نظرًا لعدم وجود مجموعة بيانات جيدة لهذا النموذج ، فإننا نستخدم بيانات عشوائية تم إنشاؤها بواسطة مكتبة Numpy للتوضيح.</font></font><br>
<br>
<pre><code class="python hljs">input_1_data = np.random.random((batch_size * num_batch, timestep, input_1))<font></font>
input_2_data = np.random.random((batch_size * num_batch, timestep, input_2, input_3))<font></font>
target_1_data = np.random.random((batch_size * num_batch, unit_1))<font></font>
target_2_data = np.random.random((batch_size * num_batch, unit_2, unit_3))<font></font>
input_data = [input_1_data, input_2_data]<font></font>
target_data = [target_1_data, target_2_data]<font></font>
<font></font>
model.fit(input_data, target_data, batch_size=batch_size)<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
باستخدام الطبقة ، ما </font></font><code>tf.keras.layers.RNN</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">عليك سوى تحديد المنطق الرياضي لخطوة واحدة داخل التسلسل ، وستتعامل الطبقة </font></font><code>tf.keras.layers.RNN</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">مع تكرار التسلسل نيابة عنك. </font><font style="vertical-align: inherit;">هذه طريقة قوية بشكل لا يصدق لنموذج سريع لأنواع جديدة من RNNs (مثل متغير LSTM). </font></font><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">بعد التحقق ، ستظهر الترجمة أيضًا على Tensorflow.org. </font><font style="vertical-align: inherit;">إذا كنت ترغب في المشاركة في ترجمة وثائق موقع Tensorflow.org إلى اللغة الروسية ، فيرجى الاتصال بشخص أو تعليقات. </font><font style="vertical-align: inherit;">هي موضع تقدير أي تصحيحات وتعليقات.</font></font></i></div>
      
    </div><p class="reference-to-source js-reference-to-source">Source: https://habr.com/ru/post/undefined/</p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../ar487798/index.html">كيف قمنا بالترحيل من Oracle JDK و Java Web Start إلى اعتماد و OpenJDK و OpenWebStart</a></li>
<li><a href="../ar487800/index.html">لماذا من المهم إخبار مقدم الطلب بالخطأ الذي حدث أثناء المقابلة (وكيفية القيام بذلك بشكل صحيح)</a></li>
<li><a href="../ar487802/index.html">APC Smart UPS غير المنقطعة ، وكيفية طهيها</a></li>
<li><a href="../ar487804/index.html">لقاء فرق النمو في Raiffeisenbank</a></li>
<li><a href="../ar487806/index.html">إنشاء API دينو صغيرة</a></li>
<li><a href="../ar487812/index.html">اختبار الطيف LED البولندي E27</a></li>
<li><a href="../ar487814/index.html">السرعة والموثوقية أعلى والسعر أقل. محركات الأقراص الصلبة Kingston KC2000 الجديدة</a></li>
<li><a href="../ar487822/index.html">AvitoTech On Tour: لقاء Android في نيجني نوفغورود</a></li>
<li><a href="../ar487824/index.html">نظرة عامة على مصابيح LED الطيف LED GU10 من أوروبا</a></li>
<li><a href="../ar487826/index.html">نظرة عامة على مصابيح LED من بولندا Spectrum Led E14</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>