<!doctype html>
<html class="no-js" lang="fr">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèø‚Äç‚öñÔ∏è üë®‚Äçüîß üíé R√©seau de neurones - formation sans professeur. M√©thode du gradient de politique üé† üòÇ üë¶üèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bonjour, Habr
 Cet article ouvre une s√©rie d'articles sur la fa√ßon de former des r√©seaux de neurones sans professeur. 
 (Apprentissage par renforcemen...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>R√©seau de neurones - formation sans professeur. M√©thode du gradient de politique</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/506384/"><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bonjour, Habr</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Cet article ouvre une s√©rie d'articles sur la fa√ßon de former des r√©seaux de neurones sans professeur. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 (Apprentissage par renforcement pour les r√©seaux de neurones) </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dans le cycle, je pr√©vois de faire trois articles sur la th√©orie et la mise en ≈ìuvre dans le code de trois algorithmes de formation pour les r√©seaux de neurones sans professeur. </font><font style="vertical-align: inherit;">Le premier article sera sur Gradient politique, le second sur Q-learning, le troisi√®me article sera final selon la m√©thode Actor-Critic. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bonne lecture.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Article 1 - Apprentissage par Gradient Politique sans Enseignant </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 (Gradient Politique pour Apprentissage par Renforcement)</font></font></h4><br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">introduction</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Parmi les algorithmes d'apprentissage automatique, une place particuli√®re est occup√©e par les algorithmes d'apprentissage automatique o√π l'algorithme apprend √† r√©soudre le probl√®me seul sans intervention humaine, en interagissant directement avec l'environnement dans lequel il apprend. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ces algorithmes ont re√ßu un nom commun - algorithmes d'apprentissage sans professeur, pour de tels algorithmes, vous n'avez pas besoin de collecter des bases de donn√©es, vous n'avez pas besoin de les classer ou de les baliser. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Il suffit qu'un √©l√®ve sans professeur donne seulement une r√©ponse en arri√®re √† ses actions ou d√©cisions - qu'elles soient bonnes ou non.</font></font><br>
<a name="habracut"></a><br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Chapitre 1. Formation des enseignants</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Alors qu'est-ce que c'est - Apprendre avec ou sans professeur. Nous examinerons cela plus en d√©tail avec des exemples tir√©s de l'apprentissage automatique moderne et des t√¢ches qu'il r√©sout. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La plupart des algorithmes d'apprentissage automatique modernes pour les probl√®mes de classification, de r√©gression, de segmentation sont essentiellement des algorithmes d'apprentissage avec un enseignant dans lesquels la personne est l'enseignant. Parce que c'est la personne qui marque les donn√©es qui dit √† l'algorithme quelle devrait √™tre la bonne r√©ponse et donc l'algorithme essaie de trouver une solution pour que la r√©ponse que l'algorithme donne lors de la r√©solution du probl√®me corresponde √† la r√©ponse que la personne a indiqu√©e pour la t√¢che donn√©e comme la bonne r√©ponse. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En utilisant l'exemple du probl√®me de classification pour l'ensemble de donn√©es Mnist, la bonne r√©ponse que la personne donne √† l'algorithme est l'√©tiquette de la classe du chiffre dans l'ensemble d'apprentissage.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/za/kq/b6/zakqb6ob1_xce07caywnurtsl7k.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dans l'ensemble de donn√©es Mnist, pour chaque image que l'algorithme de la machine doit apprendre √† classer, les utilisateurs ont pr√©d√©fini les √©tiquettes correctes √† quelle classe cette image appartient. Dans le processus d'apprentissage, l'algorithme pr√©disant la classe d'images compare sa classe obtenue pour une image particuli√®re avec la vraie classe pour la m√™me image et ajuste progressivement ses param√®tres dans le processus d'apprentissage de sorte que la classe pr√©dite par l'algorithme tend √† correspondre √† la classe sp√©cifi√©e par la personne. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ainsi, l'id√©e suivante peut √™tre r√©sum√©e - l'algorithme d'apprentissage avec l'enseignant est n'importe quel algorithme d'apprentissage machine, o√π nous donnons √† l'algorithme comment il doit √™tre fait correctement de notre point de vue.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Et peu importe comment proc√©der - indiquez √† quelle classe cette image doit √™tre affect√©e s'il s'agit d'une t√¢che de classification, ou dessinez les contours d'un objet s'il s'agit d'une t√¢che de segmentation ou dans quelle direction tourner le volant de la voiture si l'algorithme est pilote automatique, il est important que pour chaque situation sp√©cifique, nous Nous indiquons explicitement √† l'algorithme o√π se trouve la bonne r√©ponse, comment le faire correctement. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
C'est la cl√© pour comprendre en quoi l'algorithme d'apprentissage avec un enseignant diff√®re fondamentalement de l'algorithme d'apprentissage sans enseignant.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Chapitre 2. Apprendre sans professeur</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Apr√®s avoir compris ce que c'est - enseigner avec un enseignant, nous allons maintenant comprendre ce que c'est - enseigner sans enseignant. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Comme nous l'avons constat√© dans le dernier chapitre, lorsque nous enseignons avec un enseignant, pour chaque situation d'enseignement, nous donnons √† l'algorithme une compr√©hension de la bonne r√©ponse de notre point de vue, puis en partant du contraire - en apprenant sans enseignant, pour chaque situation sp√©cifique, nous ne donnons pas une telle r√©ponse √† l'algorithme sera. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Mais alors la question se pose, si nous ne donnons pas √† l'algorithme des instructions explicites sur la fa√ßon de proc√©der correctement, qu'apprendra l'algorithme? Comment l'algorithme sera form√© sans savoir o√π ajuster ses param√®tres internes afin de faire la bonne chose et finalement r√©soudre le probl√®me comme nous le souhaiterions.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
R√©fl√©chissons √† ce sujet. Mais il est important pour nous que l'algorithme r√©sout le probl√®me dans son ensemble, et comment exactement il agira dans le processus de r√©solution de ce probl√®me et de quelle mani√®re il ira pour le r√©soudre ne nous concerne pas, nous le c√©derons √† l'algorithme lui-m√™me, nous n'en attendons que le r√©sultat final. . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Par cons√©quent, nous laisserons l'algorithme comprendre le r√©sultat final, qu'il ait r√©solu bien notre probl√®me ou non. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ainsi, r√©sumant tout ce qui pr√©c√®de, nous arrivons √† la conclusion que nous appelons de tels algorithmes d'apprentissage sans enseignant o√π il n'y a aucune indication explicite pour l'algorithme comment le faire, mais il n'y a qu'une √©valuation g√©n√©rale de toutes ses actions dans le processus de r√©solution du probl√®me.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/qi/ci/nn/qicinnqzow8o5b7lacir_mrs4aa.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dans l'exemple d'un jeu o√π une raquette essaie d'attraper les cubes tombant d'en haut, nous ne disons pas √† l'algorithme de contr√¥ler la raquette √† quel moment pr√©cis o√π d√©placer la raquette. </font><font style="vertical-align: inherit;">Nous ne dirons √† l'algorithme que le r√©sultat de ses actions - at-il attrap√© un cube avec une raquette ou non. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/-f/dp/df/-fdpdf5nbjpawt1mutegmxzw9iu.gif"><br>
<br>
<img src="https://habrastorage.org/webt/ve/jk/wz/vejkwz-fezjvbjcjydzmx7q4hk8.gif"><br>
<br>
<img src="https://habrastorage.org/webt/ws/gv/nb/wsgvnb34jwzwk--6iehpg3y3lqw.gif"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
C'est l'essence m√™me de l'apprentissage sans professeur. </font><font style="vertical-align: inherit;">L'algorithme lui-m√™me doit apprendre √† d√©cider quoi faire dans chaque cas particulier sur la base de l'√©valuation finale de la totalit√© de toutes ses actions.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Chapitre 3. Agent, environnement et r√©compense</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Apr√®s avoir compris ce qu'est une formation sans enseignant, nous allons explorer les algorithmes qui peuvent apprendre √† r√©soudre un probl√®me sans nos conseils pour le faire correctement. </font></font><br>
 <br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Il est temps de nous pr√©senter la terminologie que nous utiliserons √† l'avenir. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nous appellerons un agent notre algorithme qui peut analyser l'√©tat de l'environnement et y effectuer certaines actions. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
L'environnement est le monde virtuel dans lequel notre agent existe et, par ses actions, peut changer son √©tat ... </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
R√©compense - r√©troaction de l'environnement √† l'agent en r√©ponse √† ses actions.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/23/dp/gw/23dpgwboqw80sv_1uj5_ubifms8.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
L'environnement dans lequel vit notre agent peut √™tre arbitrairement complexe, l'agent peut m√™me ne pas savoir comment il est structur√© pour prendre ses d√©cisions et effectuer des actions. </font><font style="vertical-align: inherit;">Pour l'agent, seuls les commentaires sous forme de r√©compense qu'il re√ßoit de l'environnement sont importants. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Si nous consid√©rons plus en d√©tail le processus d'interaction de l'agent avec l'environnement, alors il peut √™tre exprim√© par le sch√©ma suivant </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ml/f5/eq/mlf5eqvivcvcwodu66mfyddfpdo.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
St - √©tat de l'environnement √† l'√©tape t </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
at - action de l'agent √† l'√©tape t </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
rt - r√©compense √† l'√©tape t</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A chaque instant t, notre agent observe l'√©tat du milieu - St, ex√©cute l'action - √† laquelle il re√ßoit une r√©compense - rt du milieu, dont le champ passe √† l'√©tat St + 1, que notre agent observe, r√©alise l'action - √† + 1, pour qui re√ßoit une r√©compense du milieu - rt + 1 et de tels √©tats t, nous pouvons avoir un ensemble infini - n.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Chapitre 4. Param√©trage de la t√¢che d'apprentissage sans professeur</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Afin de former un agent, nous devons en quelque sorte param√©trer la t√¢che d'apprendre sans enseignant, en d'autres termes, pour comprendre quelles fonctions nous allons optimiser. </font><font style="vertical-align: inherit;">Dans l'apprentissage par renforcement - dans ce qui suit, nous appellerons l'apprentissage sans enseignant, il existe trois fonctions de base: </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
1) p (a | s) - fonction politique </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La fonction de probabilit√© de l'optimalit√© d'une action est a, selon l'√©tat de l'environnement -s. </font><font style="vertical-align: inherit;">Il nous montre comment l'action d'un a est optimale sous l'√©tat du milieu s. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
2) v (s) - fonction de valeur </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La </font><font style="vertical-align: inherit;">fonction de valeur d' </font><font style="vertical-align: inherit;">√©tat est s. </font><font style="vertical-align: inherit;">Il nous montre combien l'√©tat s est g√©n√©ralement pr√©cieux pour nous en termes de r√©compenses.3 </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
) Q (s, a) - Q-function</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Q est une fonction de strat√©gie optimale. Il nous permet, selon cette strat√©gie optimale, dans un √©tat - s de choisir l'action optimale pour cet √©tat - a </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nous consid√©rons d'abord la fonction - fonction politique, comme la fonction d'apprentissage par renforcement la plus simple et intuitive. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Puisque nous allons r√©soudre les probl√®mes d'apprentissage par renforcement √† travers les r√©seaux de neurones. Ensuite, sch√©matiquement, nous pouvons param√©trer une fonction de politique via un r√©seau de neurones comme suit.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/oa/fv/1x/oafv1xdki7zeejvxvy-tsgd5oeu.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nous fournirons des √©tats √† l'entr√©e du r√©seau neuronal et construirons la sortie du r√©seau neuronal de sorte que la couche de sortie du r√©seau neuronal soit la couche SoftMax, avec le nombre de sorties √©gal au nombre d'actions possibles pour l'agent dans notre environnement. </font><font style="vertical-align: inherit;">Ainsi, en passant l'√©tat s en sortie √† travers les couches du r√©seau neuronal, on obtient la distribution de probabilit√© des actions de l'agent dans l'√©tat s. </font><font style="vertical-align: inherit;">En fait, c'est ce dont nous avons besoin pour commencer √† former notre r√©seau neuronal et am√©liorer de mani√®re it√©rative la fonction politique, qui est maintenant essentiellement notre r√©seau neuronal, gr√¢ce √† l'algorithme d'erreur de r√©tropropagation.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Chapitre 5. Am√©lioration de la fonction politique gr√¢ce √† la formation des r√©seaux de neurones</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pour former le r√©seau neuronal, nous utilisons la m√©thode de descente en gradient. Puisque la derni√®re couche de notre r√©seau neuronal est la couche SoftMax, sa fonction de perte est: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/29/y9/u2/29y9u2wizl5w2yf5yeyw08nk1ic.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
o√π: </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
1 - v√©ritables √©tiquettes * log (pr√©dites √©tiquettes) </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
2 - la somme de tous les exemples </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Cependant, comment pouvons-nous former un r√©seau neuronal si nous n'avons pas encore les √©tiquettes correctes pour les actions des agents dans les √©tats S0-Sj? Et nous n'en avons pas besoin, au lieu des √©tiquettes correctes, nous utiliserons la r√©compense que l'agent a re√ßue du m√©dium en effectuant l'action que le r√©seau neuronal lui a pr√©dit.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/46/jp/or/46jporp-h9pj-bujcmytwhnfmze.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nous avons le droit de le faire car, pour la perte d'entropie crois√©e, yj sont de v√©ritables √©tiquettes pour la classe correcte et elles sont √©gales √† un, et pour la perte de fonction de politique, rj est la r√©compense que l'environnement a accumul√©e √† l'agent pour l'action qu'il a effectu√©e. </font><font style="vertical-align: inherit;">Autrement dit, rj sert de poids pour les gradients dans la propagation arri√®re de l'erreur lorsque nous formons le r√©seau neuronal. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Re√ßu une r√©compense positive - cela signifie que vous devez augmenter le poids du r√©seau neuronal o√π le gradient est dirig√©. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/2e/ge/bq/2egebqgdxjutx0tzxvwawunwffq.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Si la r√©compense est n√©gative, alors les poids correspondants dans le r√©seau neuronal, selon la direction du gradient o√π l'erreur est dirig√©e, nous r√©duisons.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Chapitre 6. Cr√©ation d'un ensemble de donn√©es pour la formation</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Afin de former notre agent - le r√©seau de neurones utilisant le Machine Learning classique - √† travers la m√©thode de r√©tromagation des erreurs, nous devons assembler un ensemble de donn√©es. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
De l'√©nonc√© du probl√®me, il est clair qu'√† l'entr√©e du r√©seau neuronal nous voulons soumettre l'√©tat du milieu S - une image d'un puits avec des cubes qui tombent et une raquette qui les attrape. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/4h/pv/hr/4hpvhry6ju1kcckfjj4vxu_7eps.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
et Y - que nous collecterons conform√©ment √† l'√©tat S, ce sera l'action pr√©dite du r√©seau neuronal - et la r√©compense que l'environnement a accumul√©e √† l'agent pour cette action - r</font></font><br>
<br>
<img src="https://habrastorage.org/webt/an/gh/wo/anghwor5xj9mqdbgnnjrjgvzbta.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Mais apr√®s tout, mercredi, au cours du jeu, un agent ne peut pas attribuer de r√©compense pour chaque action, par exemple, dans notre cas, un agent ne recevra une r√©compense positive que lorsque la raquette attrape un d√© tombant. Si la raquette n'attrape pas le cube et tombe au fond, mercredi, l'agent facturera une r√©compense n√©gative √† l'agent. Le reste du temps, quelle que soit la fa√ßon dont l'agent d√©place la raquette, jusqu'√† ce que le cube atteigne la raquette ou tombe au fond, mercredi, l'agent facturera une r√©compense √©gale √† z√©ro.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/7f/rh/5u/7frh5uda0syl7dbcutr8ez8fcq8.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Comme on peut le voir dans la description du processus de notre jeu, une r√©compense positive ou n√©gative chez un agent est extr√™mement rare, mais fondamentalement, quelle que soit son action, la r√©compense est nulle. Comment former l'agent de code la plupart du temps, il ne re√ßoit pas de r√©ponse de l'environnement √† ses actions. Du point de vue que notre agent est un r√©seau neuronal et que la r√©compense de l'environnement est nulle, alors les gradients pour la propagation inverse de l'erreur √† travers le r√©seau neuronal dans la plupart des cas dans l'ensemble de donn√©es seront nuls, les poids du r√©seau neuronal n'auront nulle part √† changer, ce qui signifie que notre agent n'apprendra rien. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Comment r√©soudre le probl√®me avec une r√©compense nulle dans la plupart des ensembles de donn√©es qui vont former l'agent? </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Il existe deux fa√ßons de sortir de cette situation:</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
la premi√®re consiste √† attribuer toutes les actions de l'agent qu'il a prises pendant le temps o√π le cube est tomb√© une seule et m√™me r√©compense finale de l'√©pisode +1 ou -1, selon que l'agent a attrap√© le cube ou non. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ainsi, nous consid√©rerons toutes les actions de l'agent s'il a attrap√© le cube correctement et consoliderons un tel comportement de l'agent pendant la formation, en leur attribuant une r√©compense positive. Si l'agent n'a pas attrap√© le cube, nous attribuerons une r√©compense n√©gative √† toutes les actions de l'agent dans l'√©pisode et le formerons √† √©viter une telle s√©quence d'actions √† l'avenir. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
le second - le prix final avec une certaine √©tape de remise dans l'ordre d√©croissant pour s'appliquer √† toutes les actions des agents dans cet √©pisode. En d'autres termes, plus l'action de l'agent est proche de la finale, plus la r√©compense +1 ou -1 pour cette action est proche.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En introduisant une r√©compense aussi r√©duite pour une action dans l'ordre d√©croissant √† mesure que l'action s'√©loigne de la fin de l'√©pisode, nous faisons comprendre √† l'agent que les derni√®res actions qu'il a prises sont plus importantes pour le r√©sultat de l'√©pisode du jeu que les actions qu'il a prises au d√©but. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Habituellement, la r√©compense actualis√©e est calcul√©e par la formule - La r√©compense finale de l'√©pisode est multipli√©e par le coefficient de r√©duction de la puissance du num√©ro d'√©tape moins un pour toutes les actions de l'agent dans l'√©pisode (pendant le temps o√π le cube est tomb√©). </font></font><br>
<br>
<img src="https://habrastorage.org/webt/4r/cf/w6/4rcfw67kmlniyfhczs_p2v2kb-c.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Gamma - coefficient de remise (diminution de la r√©compense). Il est toujours compris entre 0 et 1. G√©n√©ralement, la gamme est prise dans la r√©gion de 0,95. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Apr√®s avoir d√©cid√© quelles donn√©es nous collectons dans DataSet, ex√©cutez le simulateur d'environnement et jouez √† un jeu avec plusieurs √©pisodes plusieurs fois de suite, collectez des donn√©es sur:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l'√©tat de l'environnement, </font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> mesures prises par l'agent </font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">la r√©compense que l'agent a re√ßue.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pour faciliter la compr√©hension, appelons la chute d'un cube √† travers le puits - un √©pisode du jeu, nous supposons √©galement que le jeu lui-m√™me comprendra plusieurs √©pisodes. </font><font style="vertical-align: inherit;">Cela signifie que dans un jeu, nous l√¢cherons plusieurs d√©s √† leur tour dans le puits, et la raquette tentera de les attraper. </font><font style="vertical-align: inherit;">Pour chaque d√© capt√©, l'agent se verra attribuer +1 point, pour chaque d√© tomb√© au fond et la raquette ne l'a pas rattrap√©, l'agent se verra attribuer -1 point.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Chapitre 7. La structure interne de l'agent et l'environnement</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Environnement - puisque nous avons un environnement dans lequel l'agent doit exister, il consiste essentiellement en une matrice d'un puits √† l'int√©rieur de laquelle les cubes tombent √† la vitesse d'une ligne par horloge, et l'agent va dans une direction √©galement une cellule. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nous allons √©crire un simulateur de l'environnement qui peut d√©poser un cube de la ligne sup√©rieure sur une colonne arbitraire √† un moment al√©atoire, sait comment recevoir une commande d'un agent pour d√©placer la raquette d'une cellule dans l'une des directions, apr√®s quoi il v√©rifie si le cube qui tombe a √©t√© attrap√© par la raquette ou s'il est tomb√© Fond du puits. En fonction de cela, le simulateur rend √† l'agent la r√©compense qu'il a re√ßue pour son action.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Un agent est l'√©l√©ment principal de notre r√©seau de neurones, capable de renvoyer la probabilit√© de toutes les actions pour un √©tat donn√© de l'environnement par l'√©tat de l'environnement qui lui est consacr√© en entr√©e. </font><font style="vertical-align: inherit;">√Ä partir de la probabilit√© d'actions re√ßues du r√©seau neuronal, l'agent s√©lectionne le meilleur, l'envoie √† l'environnement, re√ßoit un retour de l'environnement sous la forme d'une r√©compense de l'environnement. </font><font style="vertical-align: inherit;">De plus, l'agent doit avoir un algorithme interne sur la base duquel il pourra apprendre √† maximiser la r√©compense re√ßue de l'environnement.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Chapitre 8. Formation des agents</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Afin de former l'agent, nous devons accumuler des statistiques en fonction des donn√©es de notre simulateur et des actions entreprises par l'agent. </font><font style="vertical-align: inherit;">Nous collecterons des donn√©es statistiques pour la formation en triplets de valeurs - l'√©tat de l'environnement, l'action de l'agent et la r√©compense de cette action.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/yo/qq/od/yoqqodgrfzatgri5dljps6uqnvw.png"><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Un peu de code sur la fa√ßon de collecter des statistiques</font></font></h4><br>
<img src="https://habrastorage.org/webt/ph/hr/6x/phhr6xb35udm3hyc3mr9rwv5yno.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nous mettons chacune de ces trois valeurs dans une m√©moire tampon sp√©ciale, o√π nous les stockons tout le temps pendant que nous simulons le jeu et accumulons des statistiques sur celui-ci.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Code pour organiser un tampon m√©moire:</font></font></h4><br>
<img src="https://habrastorage.org/webt/u2/55/ty/u255ty2pzaoeagitk_osyxzxirg.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Apr√®s avoir men√© une s√©rie de matchs de notre agent avec mercredi et ayant accumul√© des statistiques, nous pouvons proc√©der √† la formation de l'agent. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pour ce faire, nous r√©cup√©rons un lot de nos donn√©es sous forme de triplets de valeurs dans le tampon m√©moire avec des statistiques accumul√©es, le d√©compressons et le convertissons en tenseurs Pytorch. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/jo/se/6t/jose6tkxmmtzsttgrnc-q0xkrf0.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 Nous alimentons l'√©tat du milieu sous la forme du tenseur de Pytorch vers le r√©seau neuronal de l'agent, obtenons la distribution de probabilit√© pour chaque mouvement d'agent pour chaque √©tat du milieu dans le match, enregistrons ces probabilit√©s, multiplions la r√©compense re√ßue par l'agent par mouvement par le journal de ces probabilit√©s, puis prenons la moyenne des produits et rendre cette moyenne n√©gative: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/hx/om/3i/hxom3ikgq9n13pb6brfzsfsbmkw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
√âtape 1 </font></font><br>
<br>
<img src="https://habrastorage.org/webt/4j/gh/8l/4jgh8layvkcwg0dhrylxuwul2ri.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
√âtape 2</font></font><br>
<br>
<img src="https://habrastorage.org/webt/pn/ac/pj/pnacpjuzovqe4-zql7lxlarerly.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Apr√®s avoir obtenu la valeur de la fonction de perte, nous effectuons un passage inverse √† travers le r√©seau neuronal pour obtenir des gradients et faire de l'√©tape un optimiseur pour ajuster les poids. </font><font style="vertical-align: inherit;">Sur ce point, le cycle de formation de notre agent est en cours de t√©l√©chargement. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
√âtant donn√© qu'apr√®s que l'optimiseur a d√©plac√© les poids du r√©seau neuronal, nos donn√©es dans les statistiques collect√©es ne sont plus pertinentes, car un r√©seau neuronal avec des poids d√©cal√©s donnera des probabilit√©s compl√®tement diff√©rentes pour des actions sur les m√™mes conditions environnementales et la formation des agents ira √† nouveau mal. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Par cons√©quent, nous effa√ßons notre m√©moire tampon, perdons √† nouveau un certain nombre de jeux pour collecter des statistiques et red√©marrons le processus de formation des agents. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Il s'agit de la boucle d'apprentissage lorsque vous apprenez sans enseignant √† l'aide de la m√©thode Policy Gradient.</font></font><br>
<br>
<ol>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Accumulation de statistiques</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Formation d'agent</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">R√©initialiser les statistiques</font></font></li>
</ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 Nous r√©p√©tons le processus d'apprentissage tant de fois jusqu'√† ce que notre agent apprenne √† recevoir du syst√®me la r√©compense qui nous convient.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Chapitre 9. Exp√©riences avec l'agent et l'environnement</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Commen√ßons une s√©rie d'exp√©riences pour former notre agent. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pour les exp√©riences, nous s√©lectionnons les param√®tres environnementaux suivants: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/xi/ym/2_/xiym2_kitmamvf52hhuff6tkpqs.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pour l'agent, nous choisissons - nous construisons un r√©seau neuronal - il sera convolutionnel (comme nous travaillons avec l'image), il aura 9 sorties (1 droite, 2 gauche, 3 haut, 4 bas, sur la sortie) 5-droite-haut, 6-gauche-haut, 7-droite-bas, 8-gauche-bas, 9 ne rien faire) et SoftMax pour obtenir la probabilit√© de chaque action.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Architecture de r√©seau neuronal</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Taille d'image du premier neurone de la couche Conv2d 32 1 * 32 * 16 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Taille d'image de la couche MaxPool2d 32 * 16 * 8 Taille de l'image de la </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
deuxi√®me couche Conv2d 32 neurone 32 * 16 * 8 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Taille d'image de la couche MaxPool2d 32 * 8 * 4 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Aplatir - redresser l'image √† une taille de 1024 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Lin√©aire couche pour 1024 neurones Couche d' </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
abandon (0,25) Couche </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
lin√©aire pour 512 neurones </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Couche lin√©aire pour 256 neurones </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Couche lin√©aire pour 9 neurones et SoftMax</font></font><br>
<br>
<img src="https://habrastorage.org/webt/2d/tz/ne/2dtzneqr2lsxdkz-yvu9xonh0uw.png"><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Code de cr√©ation du r√©seau neuronal Pytorch</font></font></h4><br>
<img src="https://habrastorage.org/webt/wq/mx/yw/wqmxyw67mbjtmnpnbj86zry8y4a.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Un √† un, nous lancerons trois cycles de formation d'agent dans l'environnement dont les param√®tres ont √©t√© discut√©s ci-dessus:</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Exp√©rience n ¬∞ 1 - L'agent a appris √† r√©soudre le probl√®me en 13 600 cycles de jeu</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
√âtat initial de l'agent. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ws/gv/nb/wsgvnb34jwzwk--6iehpg3y3lqw.gif"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Calendrier </font><font style="vertical-align: inherit;">de </font><font style="vertical-align: inherit;">formation </font><font style="vertical-align: inherit;">de l'agent. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/jt/kd/3q/jtkd3qmdl7xixi-bwpkdp-tk0uy.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
√âtat de </font><font style="vertical-align: inherit;">formation </font><font style="vertical-align: inherit;">de l'agent.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/4a/v5/9l/4av59lvev4oget2gotgg-xsfy2w.gif"><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Exp√©rience n ¬∞ 2 - L'agent a appris √† r√©soudre le probl√®me en 8250 cycles de jeu</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
√âtat initial de l'agent. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ve/jk/wz/vejkwz-fezjvbjcjydzmx7q4hk8.gif"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Calendrier </font><font style="vertical-align: inherit;">de </font><font style="vertical-align: inherit;">formation </font><font style="vertical-align: inherit;">de l'agent. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/9e/zc/1x/9ezc1x4gfv7bilgb8sx-r3iemgy.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
√âtat de </font><font style="vertical-align: inherit;">formation </font><font style="vertical-align: inherit;">de l'agent.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/l9/r7/bn/l9r7bnstfbaqszje4npotjrze4o.gif"><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Exp√©rience n ¬∞ 3 - L'agent a appris √† r√©soudre le probl√®me en 19800 cycles de jeu</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
√âtat initial de l'agent. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/hz/wm/ss/hzwmssjogsxwzv0wrik-1nb8loc.gif"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Calendrier </font><font style="vertical-align: inherit;">de </font><font style="vertical-align: inherit;">formation </font><font style="vertical-align: inherit;">de l'agent. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/sh/xv/zd/shxvzdpumy7fnoctkb6bqnhie2o.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
√âtat de </font><font style="vertical-align: inherit;">formation </font><font style="vertical-align: inherit;">de l'agent.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/2h/ga/yh/2hgayhvtgypaq1fvu-i54d4l4ok.gif"><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Chapitre 10. Conclusions</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En regardant les graphiques, on peut dire que la formation de l'agent est bien s√ªr lente. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
L'agent recherchait depuis longtemps au moins une politique raisonnable pour ses actions depuis longtemps pour commencer √† recevoir une r√©compense positive. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
√Ä ce moment, √† la premi√®re √©tape du calendrier, la r√©compense pour le jeu augmente lentement, puis soudain, l'agent trouve une bonne option pour ses mouvements, et la r√©compense re√ßue par lui pour le jeu augmente fortement et augmente, puis s'approchant de la r√©compense maximale, l'agent repart une lente augmentation de son efficacit√© lorsqu'il am√©liore sa politique de coups qu'il a d√©j√† apprise, mais s'effor√ßant, comme tout algorithme gourmand, de prendre sa r√©compense compl√®tement.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Je voudrais √©galement noter le grand besoin de calculs pour la formation de l'agent √† l'aide de la m√©thode Policy Gradient, car La principale heure de fonctionnement de l'algorithme est la collecte de statistiques sur les mouvements de l'agent et non sur sa formation. Apr√®s avoir collect√© des statistiques sur les mouvements de l'ensemble du tableau, nous utilisons un seul lot de donn√©es pour la formation de l'agent, et nous jetons toutes les autres donn√©es comme d√©j√† inappropri√©es pour la formation. Et encore une fois, nous collectons de nouvelles donn√©es. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Vous pouvez toujours exp√©rimenter beaucoup avec cet algorithme et cet environnement - en modifiant la profondeur et la largeur du puits, en augmentant ou en diminuant le nombre de d√©s tombant pendant le jeu, ce qui rend ces d√©s de couleurs diff√©rentes. Observer ce que cet effet aura sur l'efficacit√© et la rapidit√© de la formation de l'agent.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En outre, un vaste champ d'exp√©riences est les param√®tres du r√©seau neuronal, dont nous formons essentiellement notre agent, vous pouvez changer les couches, les noyaux de convolution, activer et affiner la r√©gularisation. </font><font style="vertical-align: inherit;">Oui, et bien plus encore, vous pouvez essayer d'augmenter l'efficacit√© de la formation de l'agent. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ainsi, apr√®s avoir lanc√© des exp√©riences pratiques avec l'enseignement sans enseignant en utilisant la m√©thode du Gradient Politique, nous √©tions convaincus que l'enseignement sans enseignant a sa place et qu'il fonctionne vraiment. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
L'agent s'est entra√Æn√© ind√©pendamment pour maximiser sa r√©compense dans le jeu. </font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Lien vers GitHub avec un code adapt√© pour travailler dans un ordinateur portable Google Colab</font></font></a></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr506358/index.html">3 options de comportement judiciaire lorsqu'ils re√ßoivent des cas de verrous de poursuite</a></li>
<li><a href="../fr506362/index.html">Comment nous avons fait un bal de danse en ligne</a></li>
<li><a href="../fr506370/index.html">Comparer les performances de la contrainte de v√©rification et de la cl√© √©trang√®re dans SQL Server</a></li>
<li><a href="../fr506372/index.html">L'instinct de reproduction se retrouve dans l'intelligence artificielle</a></li>
<li><a href="../fr506380/index.html">Logiciel gratuit ou domestique. Cours standard ou gratuit</a></li>
<li><a href="../fr506386/index.html">Comment abandonner l'√©cole et transf√©rer un enfant dans un endroit √©loign√©</a></li>
<li><a href="../fr506392/index.html">Notification de Roskomnadzor sur le traitement des donn√©es personnelles en 2020</a></li>
<li><a href="../fr506394/index.html">Imprimante Anycubic Mega X 3D: une excellente imprimante √† un prix modeste</a></li>
<li><a href="../fr506396/index.html">Elon Musk: ¬´Le lidar est une perte de temps. Tous ceux qui comptent sur le lidar sont condamn√©s ¬ª</a></li>
<li><a href="../fr506398/index.html">Joel Spolsky: ¬´Pas l'utilisabilit√© seule¬ª</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>