<!doctype html>
<html class="no-js" lang="ja">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🐮 👩‍👩‍👧 💽 YOLOv4-Microsoft COCOデータセットで最も正確なリアルタイムニューラルネットワーク 👂🏼 👩🏽‍🏫 🚋</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Darknet YOLOv4は、GoogleのリアルタイムTensorFlow EfficientDetおよびFaceBook Pytorch / Detectron RetinaNet / MaskRCNNニューラルネットワークよりも高速で正確です。
 
 メディアに関する同じ記事：メディア
 コ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>YOLOv4-Microsoft COCOデータセットで最も正確なリアルタイムニューラルネットワーク</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/503200/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Darknet YOLOv4は、GoogleのリアルタイムTensorFlow EfficientDetおよびFaceBook Pytorch / Detectron RetinaNet / MaskRCNNニューラルネットワークよりも高速で正確です。</font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">メディアに関する同じ記事</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">：</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">メディア</font></font></a><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">コード</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">：</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/AlexeyAB/darknet </font></font></a><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">記事</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">：</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">arxiv.org/abs/2004.10934</font></font></a><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/3h/nc/sr/3hncsroz9wt8u3ycqskubgu1xk8.gif"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ニューラルネットワークを比較してオブジェクトを検出するためのニュアンスをいくつか示します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
私たちの目標は、科学を前進させるだけでなく、実際の製品で使用するためのオブジェクト検出アルゴリズムを開発することでした。</font><font style="vertical-align: inherit;">YOLOv4ニューラルネットワーク（608x608）の精度は、43.5％AP / 65.7％AP50 Microsoft-COCO-testdevです。</font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">62 FPS</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> -Tesla V100でYOLOv4（608x608バッチ= 1）-Darknet-frameworkを使用して</font></font><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">400 FPS</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> -RTX 2080 TiでYOLOv4（416x416バッチ= 4）-TensorRT + tkDNNを使用して</font></font><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">32 FPS</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> -YOLOv4（416x416バッチ= 1）でJetson AGX Xavier-TensorRT + tkDNNを使用</font></font><br>
<br>
<img src="https://habrastorage.org/webt/p_/ep/cl/p_epcl_aaw_trgeltekatagtqkg.jpeg"> <br>
<a name="habracut"></a><br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/1_SiUOYUoOI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">まず、いくつかの便利なリンク。</font></font></h3><br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">この記事の中で、YOLOv4で使用されている機能の詳細な説明を読むことができます：</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">medium.com/@jonathan_hui/yolov4-c9901eaa8e61</font></font></a></li>
<li>  YOLOv4: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://lutzroeder.github.io/netron/%3Furl%3D" rel="nofollow">lutzroeder.github.io/netron/?url=https%3A%2F%2Fraw.githubusercontent.com%2FAlexeyAB%2Fdarknet%2Fmaster%2Fcfg%2Fyolov4.cfg</a></li>
<li>     YOLOv4  GPU   Google-cloud  Jupyter Notebook –      ,   - «Open in Playground»,         [ ] –    ,  ,  ,    5    : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow">colab.research.google.com/drive/12QusaaRj_lUwCGDvQNfICpa7kA7_a2dE</a>      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow">www.youtube.com/watch?v=mKAEGSxwOAY</a></li>
<li>  Darknet   : <br>
 — <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow">  Darknet YOLOv4</a><br>
 — <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow">  Darknet YOLOv4</a><br>
 </li>
</ul><br>
<h3>   </h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
私たちのYOLOv4ニューラルネットワークと独自のDarknet DLフレームワーク（C / C ++ / CUDA）は、FPS速度とAP50で優れています：DLフレームワークとニューラルネットワークよりもMicrosoft COCOデータセットでの95およびAP50の精度：Google TensorFlow EfficientDet、FaceBook Detectron RetinaNet / MaskRCNN、PyTorch Yolov3-ASFF、その他多数... YOLOv4は、Microsoft COCOテストで62 FPS TitanVまたは34 FPS RTX 2070の速度で43.5％AP / 65.7％AP50の精度を達成します。他の最新の検出器とは異なり、YOLOv4は8〜16 GBのVRAMを搭載したnVidiaゲーミンググラフィックカードを持っている人。現在、大企業だけでなく数百のGPU / TPUでニューラルネットワークをトレーニングして、大きなミニバッチサイズを使用してより高い精度を実現できるため、YOLOv4では大きなミニロットが必要ないため、人工知能の制御を通常のユーザーに返しています。サイズは2〜8に制限できます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
YOLOV4は、リアルタイムの使用に最適です。ネットワークは</font><font style="vertical-align: inherit;">、AP（精度）/ FPS（速度）グラフ</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">のパレート最適性曲線上</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">にあります。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/2k/77/as/2k77aszzprngk0qmtistcehkz8c.png"><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GPUで測定されたオブジェクトを検出するための多くのニューラルネットワークの精度（AP）と速度（FPS）のグラフTitanV / TeslaV100、TitanXP / TeslaP100、TitanX / TeslaM40精度の2つの主な指標AP50：95とAP50</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
公正な比較のために、記事と同じアーキテクチャのGPUでのみ比較します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ほとんどの実用的なタスクには、検出器に最低限必要な要件があります。これらは、許容可能な最小の精度と速度です。通常、リアルタイムシステムの最小許容速度は30 FPS（フレーム/秒）以上です。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
グラフからわかるように、FPS 30以上のリアルタイムシステムでは、次のようになります。</font></font><br>
<br>
<ul>
<li> YOLOv4-608   RTX 2070  <b>450$</b> (34 FPS)   <b>43.5% AP / 65.7% AP50</b></li>
<li> EfficientDet-D2   TitanV  <b>2250$</b> (42 FPS)   <b>43.0% AP / 62.3% AP50</b></li>
<li> EfficientDet-D0   RTX 2070  <b>450$</b> (34 FPS)   <b>33.8% AP / 52.2% AP50</b></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
それら。 YOLOv4には</font><font style="vertical-align: inherit;">、EfficientDet-D2（Google-TensorFlow）よりも</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">5倍安価で</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">正確</font><b><font style="vertical-align: inherit;">な機器</font></b><font style="vertical-align: inherit;">が必要</font><font style="vertical-align: inherit;">です。 EfficientDet-D0（Google-TensorFlow）を使用すると、機器のコストは同じになりますが、精度はAPの10％低くなります。</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
さらに、一部の産業用システムでは、熱放散またはパッシブ冷却システムの使用が制限されています。この場合、お金があってもTitanVを使用できません。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
TensorRT + tkDNNを使用してRTX 2080 Ti GPUでYOLOv4（416x416）を使用すると、2x倍の速度を達成し、batch = 4を使用すると3x-4x倍高速になります。正直な比較のため、arxivの記事ではこれらの結果を示していません。組織：</font></font><br>
<img src="https://habrastorage.org/webt/ci/j7/uq/cij7uqas0ypsjcpsfkhvdxuyxzs.png"><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
YOLOv4ニューラルネットワーク（416x416）FP16（Tensor Cores）バッチ= </font><font style="vertical-align: inherit;">1で32 FPS計算機に到達nVidia Jetson AGX Xavierを使用してライブラリ+ tkDNN TensorRT：</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/ceccocats/tkDNN</font></font></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
速度が少し遅いため、CUDAでコンパイルされたOpenCV-dnnライブラリ：</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ドキュメント.opencv.org / master / da / d9d / tutorial_dnn_yolo.html</font></font></a><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
記事の一部のニューラルネットワークの速度（FPS）は、バッチサイズが大きい場合や、ネットワークを最適化してFPS値の増加を示す専用ソフトウェア（TensorRT）でテストする場合に示されることがあります。 TRT上のいくつかのネットワークとTRTのない他のネットワークの比較は公平ではありません。高いバッチサイズを使用すると、FPSが増加しますが、バッチ= 1と比較して（減少するのではなく）レイテンシも増加します。バッチ= 1のネットワークが40 FPSを示し、バッチ= 32のネットワークが60 FPSを示す場合、遅延はバッチ= 1の場合は25ミリ秒、バッチ= 32の場合は約500ミリ秒になります。 1秒あたり2パケット（各32画像）のみが処理されるため、多くの産業用システムでは、batch = 32の使用は受け入れられません。したがって、TensorRTを使用せずに、バッチ= 1でのみグラフの結果を比較しました。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
すべてのプロセスは、人またはコンピュータのいずれかによって制御できます。コンピュータシステムが低速のために大きな遅延で動作し、ミスが多すぎる場合、アクションの完全な制御を任せることはできません。この場合、人はプロセスを制御し、コンピュータシステムはヒントのみを提供します-これは推奨システムです-人は動作し、システムのみどこで間違いがあったかを伝えます。システムが迅速かつ高精度で動作する場合、そのようなシステムはプロセスを独立して制御でき、人はそれを管理します。したがって、精度とシステム速度は常に重要です。 YOLOv4 416x416の120 FPSがタスクにとって大きすぎると思われる場合、アルゴリズムをよりゆっくり正確に実行することをお勧めします。実際のタスクでは、Tesla V100 250ワットよりも弱いものを使用する可能性があります。たとえば、RTX 2060 / Jetson-Xavier 30-80ワット。この場合、YOLOv4 416x416で30 FPSを取得し、その他のニューラルネットワークは1-15 FPSで取得するか、まったく起動しません。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">さまざまなニューラルネットワークのトレーニングの機能</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
いくつかのTesla V100 32GB GPUでミニバッチ= 128サイズのEfficientDetをトレーニングする必要がありますが、YOLOv4はミニバッチ= 8 =バッチ/サブディビジョンの1つのTesla V100 32GB GPUでトレーニングされ、通常のゲームでトレーニングできますグラフィックカード8-16GB GPU-VRAM。</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
次のニュアンスは、ニューラルネットワークをトレーニングして自身のオブジェクトを検出するのが難しいことです。</font><font style="vertical-align: inherit;">同じ1080 Ti GPUで他のネットワークをトレーニングする時間に関係なく、上記のグラフに示されている精度は得られません。</font><font style="vertical-align: inherit;">ほとんどのネットワーク（EfficientDet、ASFFなど）は4〜128 GPU（syncBNを使用した大きなミニバッチサイズ）でトレーニングする必要があり、両方の条件を満たすことなく、ネットワーク解像度ごとに毎回新たにトレーニングする必要があります。両方の条件を満たすことで、それらによって宣言されたAPまたはAP50の精度を達成することは不可能です。</font></font><br>
 <br>
<img src="https://habrastorage.org/webt/p4/sx/p3/p4sxp3ewxd9owskis23n6dyrv58.jpeg"><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">オブジェクトの検出精度の、他の検出器のミニバッチのサイズへの依存性を確認できます。</font><font style="vertical-align: inherit;">8つのビデオカードの代わりに128のビデオカードを使用し、学習速度が10倍高く、最終的な精度が1.5 AP高い-MegDet：大型ミニバッチオブジェクト検出器</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">arxiv.org/abs/1711.07240</font></font></a></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Yolo ASFF：</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">arxiv.org/abs/1911.09516</font></font></a> <br>
<blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[43]に続いて、ミックスアップアルゴリズム[12]、コサイン[26]学習レートスケジュール、同期バッチ正規化技法[30]など、一連のトリックをトレーニングプロセスに導入します。</font></font></blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
EfficientDet：</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">arxiv.org/abs/1911.09070</font></font></a> <br>
<blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">同期されたバッチ正規化は、バッチノルム減衰0.99およびイプシロン1e-3を使用してすべての畳み込みの後に追加されます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
各モデルは、32 TPUv3コアでバッチ合計サイズ128の300エポックにトレーニングされます。</font></font></blockquote><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow">cloud.google.com/tpu/docs/types-zones#europe</a><br>
<blockquote>v3-32 TPU type (v3) – 32 TPU v3 cores – 512 GiB Total TPU memory</blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
バッチ= 128の同期バッチ正規化を使用してEfficientDetモデルをトレーニングするには、512 GB TPU / GPU-RAMを使用する必要がありますが、YOLOv4のトレーニングにはミニバッチ= 8と32 GB GPU-RAMのみを使用しました。それにもかかわらず、YOLOv4は、GPUあたり512x512の解像度（Tesla V100 32GB / 16GB）で1回のみトレーニングされますが、パブリックネットワークよりも高速で正確です。同時に、より小さなミニバッチサイズとGPU-VRAMを使用しても、他のニューラルネットワークのような劇的な精度の損失は発生しません。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/xi/ol/rs/xiolrsvx4vzpjvahb6kvambdvgq.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">出典：</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">arxiv.org/abs/2004.10934</font></font></a></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
ダウンロードする代わりに、PCでローカルに人工知能をトレーニングできますクラウドへのデータセット-これにより、個人データの保護が保証され、誰でも人工知能トレーニングを利用できるようになります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ネットワーク解像度512x512でネットワークを1回トレーニングするだけで十分です。その後、[416x416-512x512-608x608]の範囲のさまざまなネットワーク解像度で使用できます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
他のほとんどのモデルは、ネットワークの解像度ごとに個別にトレーニングする必要があります。そのため、トレーニングには何倍も時間がかかります。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">オブジェクト検出アルゴリズムの測定精度の特徴</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
あるアルゴリズムがうまく機能せず、別のアルゴリズムがうまく機能する画像をいつでも見つけることができ、逆もまた同様です。したがって、検出アルゴリズムをテストするために、MSCOCO test-devデータセット-20,000個の画像と80種類のオブジェクトの大規模なセットが使用されます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
アルゴリズムが各画像のハッシュとその座標を覚えているだけではないように（オーバーフィッティング）、オブジェクト検出の精度は、アルゴリズムがトレーニング中に見なかった画像とラベルで常にチェックされます-これにより、アルゴリズムが画像/ビデオ上のオブジェクトを検出できることが保証されます見たことがない。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
誰もが正確さの計算を間違えることがないように、パブリックドメインでは、検出したtest-devテストイメージのみが存在し、その結果をCodaLab評価サーバーに送信します。CodaLab評価サーバーでは、プログラム自体が、ユーザーがアクセスできないテストアノテーションと結果を比較します。 。</font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">MSCOCOデータセットは3つの部分で構成されています</font></font></a><br>
<br>
<ol>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">チュートリアル：120,000枚の画像と各オブジェクトの座標を含むjsonファイル</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">検証セット：5,000画像と各オブジェクトの座標を含むjsonファイル</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">テストスイート：オブジェクトの座標なしの41,000 jpg-images（これらの画像の一部は、タスクの精度を決定するために使用されます：オブジェクト検出、インスタンスセグメンテーション、キーポイントなど）</font></font></li>
</ol><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">YOLOv4開発の特徴</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
YOLOv4を開発するとき、私はYOLOv4ニューラルネットワークとDarknetフレームワークの両方をC / C ++ / CUDAで自分で開発する必要がありました。</font><font style="vertical-align: inherit;">なぜなら </font><font style="vertical-align: inherit;">Darknetでは、チェーンルールの自動微分と自動実行は行われません。すべてのグラデーションは、CUDA C ++で手動で実装する必要があります。</font><font style="vertical-align: inherit;">一方、私たちはチェーンルールの厳格な遵守から逸脱し、逆伝播を変更し、学習の安定性と精度を高めるために非常に重要なことを試すことができます。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ニューラルネットワークを作成する際の追加の調査結果</font></font></h3><br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">オブジェクトの分類に最適なネットワークが、オブジェクトの検出に使用されるネットワークのバックボーンとして最適であるとは限りません</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">分類の精度が向上した機能でトレーニングされた重みを使用すると、（一部のネットワークでは）検出器の精度に悪影響を与える可能性があります</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">さまざまな調査で述べられているすべての機能がネットワークの精度を向上させるわけではありません。</font></font></li>
<li>                .</li>
<li>      BFLOPS  ,   BFLOPS    </li>
<li>                  ,     receptive field     ,       stride=2 / conv3x3,    weights (filters)      . </li>
</ul><br>
<h3>   YOLOv4</h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
トレーニング済みのYOLOv4モデルを使用したオブジェクト検出はOpenCV-dnnライブラリ</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/opencv/opencv/issues/17148に組み込まれている</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ため、Darknetフレームワークを使用せずにOpenCVから直接YOLOv4を使用できます。</font><font style="vertical-align: inherit;">OpenCVライブラリは、CPU、GPU（nVidia GPU）、VPU（Intel Myriad X）でのニューラルネットワークの実装をサポートしています。</font><font style="vertical-align: inherit;">詳細：</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">docs.opencv.org/master/da/d9d/tutorial_dnn_yolo.html </font></font></a><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">OpenCV</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">（dnn）フレームワーク：</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">C ++の例：</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/opencv/opencv/blob/master/samples/dnn/object_detection.cpp</font></font></a></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pythonの例：</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/opencv/opencv/blob/master/samples/dnn/object_detection.py</font></font></a></li>
</ul><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Darknet</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">フレームワーク：</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">YOLOv4を使用してオブジェクトを検出する手順：</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/AlexeyAB/darknet#how-to-use-on-the-command-line</font></font></a></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ニューラルネットワークをトレーニングして独自のオブジェクトを検出する手順：</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/AlexeyAB/darknet#how-to-train-to-detect-your-custom-objects</font></font></a></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ILSVRC2012データセット（ImageNet）でCSPDarknet53分類器をトレーニングする手順：</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/AlexeyAB/darknet/wiki/Train-Classifier-on-ImageNet-</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">（ILSVRC2012）</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">MS COCOデータセットでYOLOv4をトレーニングするための手順：</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/AlexeyAB/darknet/wiki/Train-Detector-on-MS-COCO-</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">（trainvalno5k-2014）-dataset</font></font></li>
</ul><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">tkDNN + TensorRT</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - </font><b><font style="vertical-align: inherit;">YOLOv4</font></b><font style="vertical-align: inherit;">を使用した最大オブジェクト検出速度：TensorRT + tkDNN </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/ceccocats/tkDNN</font></font></a><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">400 FPS-RTX 2080 TiのYOLOv4（416x416バッチ= 4）</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">32 FPS-Jetson AGX XavierのYOLOv4（416x416バッチ= 1）</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
YOLOv4の使用を拡張して、3D回転Bboxまたはキーポイント/顔のランドマークを検出できます。例：</font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/ouyanghuiyu/darknet_face_with_landmark</font></font></a><br>
<br>
<img src="https://habrastorage.org/webt/z7/vs/dv/z7vsdvhcpfbrgmdv1byhbpzd1cu.jpeg"></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../ja503186/index.html">マイクロソフトの初心者向けの10の無料コース</a></li>
<li><a href="../ja503192/index.html">2時間で仮想化。パート4.基本的な操作</a></li>
<li><a href="../ja503194/index.html">ISAは間違いを許しません</a></li>
<li><a href="../ja503196/index.html">アイビーリーグの450の無料コース</a></li>
<li><a href="../ja503198/index.html">Minecraft RTXベータフレームレンダリング分析</a></li>
<li><a href="../ja503202/index.html">CCTVシステム：ロシアの8つの主なトレンドとその機能</a></li>
<li><a href="../ja503204/index.html">Xiaomi Redmi 4 Pro（プライム、プレミアム）をAndroid 10でフラッシュする方法</a></li>
<li><a href="../ja503208/index.html">投資するのに最適な時期はいつですか？</a></li>
<li><a href="../ja503210/index.html">フィッシングサイトを根絶することはできますか？</a></li>
<li><a href="../ja503212/index.html">オンラインコースを完了するための30のライフハック</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>