<!doctype html>
<html class="no-js" lang="ja">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ‘©â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ ğŸ¦ ğŸ¨ Kubesprayã‚’ä½¿ç”¨ã—ã¦OpenStackã«Kubernetesã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹ ğŸ¹ ğŸ™ğŸ¼ ğŸ‚ğŸ½</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Kubernetes . . , . Kubernetes . , . , Kubespray, . Kubernetes Kubespray OpenStack (Open Telekom Cloud).
 

 Kubernetes Kubespray , Ansible. Kubespray ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Kubesprayã‚’ä½¿ç”¨ã—ã¦OpenStackã«Kubernetesã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/southbridge/blog/456792/"><p><img src="https://habrastorage.org/webt/oz/5b/xn/oz5bxnm9_ej5v-po5gjbk93otk0.jpeg"></p><br>
<p>Kubernetes            .          .    ,         .  Kubernetes   .       ,      .     ,  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=">Kubespray,</a>   .       Kubernetes   Kubespray   OpenStack (Open Telekom Cloud).</p><a name="habracut"></a><br>
<p>   Kubernetes Kubespray   ,     Ansible.   Kubespray         .     Â«  Â» Terraform.   Kubespray <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="> Terraform</a>   AWS, OpenStack  Packet.      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="> OpenStack,</a>      .</p><br>
<h2 id="trebovaniya"></h2><br>
<p>      .     :   Kubespray     .</p><br>
<p> Kubespray   :</p><br>
<ul>
<li>Python 2.7 ( )</li>
<li>Ansible 2.7 ( )</li>
<li>Jinja 2.9 ( )</li>
</ul><br>
<p>   OpenStack:</p><br>
<ul>
<li>Terraform 0.11 ( )</li>
</ul><br>
<p>  Terraform      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="> Hashicorp</a>   .          PATH.    terraform  ,   .    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=">.</a></p><br>
<p>     Ansible    . . <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=">  Ansible.</a>    Ubuntu   Ansible  .</p><br>
<pre><code class="plaintext hljs">sudo apt update<font></font>
sudo apt install ansible</code></pre><br>
<p>    Kubespray.    .     .</p><br>
<pre><code class="plaintext hljs">git clone https://github.com/kubernetes-sigs/kubespray<font></font>
sudo pip install -r requirements.txt</code></pre><br>
<p>  Open Telekom Cloud,       .ostackrc       .</p><br>
<h2 id="planirovanie-klastera"> </h2><br>
<p>Kubernetes  ,       .        .       Kubernetes       .         etcd    .      IP,      .</p><br>
<p>    CNI (Container Network Interface).   (cilium, calico, flannel, weave net  . .),    flannel,    . Calico  ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=">  OpenStack Neutron</a>     podâ€™.</p><br>
<p>      Kubernetes  ,     .</p><br>
<h2 id="nastroyka-konfiguracii-klastera">  </h2><br>
<p>     ,      $CLUSTER.</p><br>
<pre><code class="plaintext hljs">cp -LRp contrib/terraform/openstack/sample-inventory \<font></font>
inventory/$CLUSTER<font></font>
cd inventory/$CLUSTER<font></font>
ln -s ../../contrib/terraform/openstack/hosts<font></font>
ln -s ../../contrib</code></pre><br>
<p>     inventory/$CLUSTER/cluster.tf.</p><br>
<pre><code class="plaintext hljs"># your Kubernetes cluster name here<font></font>
<font></font>
cluster_name = "k8s-test-cluster"<font></font>
az_list=["eu-de-01", "eu-de-02"]<font></font>
dns_nameservers=["100.125.4.25", "8.8.8.8"]<font></font>
<font></font>
# SSH key to use for access to nodes<font></font>
public_key_path = "~/.ssh/id_rsa.pub"<font></font>
<font></font>
# image to use for bastion, masters, standalone etcd instances, and nodes<font></font>
image = "Standard_CentOS_7_latest"<font></font>
<font></font>
# user on the node (ex. core on Container Linux, ubuntu on Ubuntu, etc.)<font></font>
ssh_user = "linux"<font></font>
<font></font>
# 0|1 bastion nodes<font></font>
number_of_bastions = 0<font></font>
flavor_bastion = "s2.xlarge.4"<font></font>
<font></font>
# standalone etcds<font></font>
number_of_etcd = 0<font></font>
flavor_etcd = "s2.xlarge.4"<font></font>
<font></font>
# masters<font></font>
number_of_k8s_masters = 0<font></font>
number_of_k8s_masters_no_etcd = 0<font></font>
number_of_k8s_masters_no_floating_ip = 1<font></font>
number_of_k8s_masters_no_floating_ip_no_etcd = 0<font></font>
flavor_k8s_master = "s2.xlarge.4"<font></font>
<font></font>
# nodes<font></font>
number_of_k8s_nodes = 0<font></font>
number_of_k8s_nodes_no_floating_ip = 2<font></font>
flavor_k8s_node = "s2.xlarge.4"<font></font>
<font></font>
# GlusterFS<font></font>
# either 0 or more than one<font></font>
#number_of_gfs_nodes_no_floating_ip = 1<font></font>
#gfs_volume_size_in_gb = 150<font></font>
# Container Linux does not support GlusterFS<font></font>
image_gfs = "Standard_CentOS_7_latest"<font></font>
# May be different from other nodes<font></font>
#ssh_user_gfs = "linux"<font></font>
#flavor_gfs_node = "s2.xlarge.4"<font></font>
<font></font>
# networking<font></font>
network_name = "k8s-test-network"<font></font>
external_net = "Externel_Network_ID"<font></font>
subnet_cidr = "192.168.100.0/24"<font></font>
floatingip_pool = "admin_external_net"<font></font>
bastion_allowed_remote_ips = ["0.0.0.0/0"]</code></pre><br>
<p>   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=">.</a>              Kubernetes     CentOS 7  s2.xlarge.4. etcd    .</p><br>
<h2 id="razvertyvanie-infrastruktury"> </h2><br>
<p>        Terraform.   ,     .  .</p><br>
<p><img src="https://habrastorage.org/webt/vk/2o/aw/vk2oawuj3cgahv3v3i7ockh_fbe.png"></p><br>
<p>   Terraform,    inventory/$CLUSTER/    .    .      init    .</p><br>
<pre><code class="plaintext hljs">terraform init ../../contrib/terraform/openstack</code></pre><br>
<p>    .  Terraform        apply.</p><br>
<pre><code class="plaintext hljs">terraform apply -var-file=cluster.tf ../../contrib/terraform/openstack</code></pre><br>
<p>   Terraform    ,      .</p><br>
<pre><code class="plaintext hljs">Apply complete! Resources: 3 added, 0 changed, 0 destroyed.</code></pre><br>
<p>   ,    Ansible,        .</p><br>
<pre><code class="plaintext hljs">$ ansible -i inventory/$CLUSTER/hosts -m ping all<font></font>
example-k8s_node-1 | SUCCESS =&gt; {<font></font>
    "changed": false,<font></font>
    "ping": "pong"<font></font>
}<font></font>
example-etcd-1 | SUCCESS =&gt; {<font></font>
    "changed": false,<font></font>
    "ping": "pong"<font></font>
}<font></font>
example-k8s-master-1 | SUCCESS =&gt; {<font></font>
    "changed": false,<font></font>
    "ping": "pong"<font></font>
}</code></pre><br>
<h2 id="razvertyvanie-klastera-kubernetes">  Kubernetes</h2><br>
<p> ,     Kubernetes.       inventory/$CLUSTER/group_vars/all/all.yml.     cloud_provider  openstack,   bin_dir â€” ,    .       .</p><br>
<pre><code class="plaintext hljs">## Directory where etcd data stored<font></font>
etcd_data_dir: /var/lib/etcd<font></font>
<font></font>
## Directory where the binaries will be installed<font></font>
bin_dir: /usr/local/bin<font></font>
<font></font>
## The access_ip variable is used to define how other nodes should access<font></font>
## the node.  This is used in flannel to allow other flannel nodes to see<font></font>
## this node for example.  The access_ip is really useful AWS and Google<font></font>
## environments where the nodes are accessed remotely by the "public" ip,<font></font>
## but don't know about that address themselves.<font></font>
#access_ip: 1.1.1.1<font></font>
<font></font>
## External LB example config<font></font>
## apiserver_loadbalancer_domain_name: "elb.some.domain"<font></font>
#loadbalancer_apiserver:<font></font>
#  address: 1.2.3.4<font></font>
#  port: 1234<font></font>
<font></font>
## Internal loadbalancers for apiservers<font></font>
#loadbalancer_apiserver_localhost: true<font></font>
<font></font>
## Local loadbalancer should use this port instead, if defined.<font></font>
## Defaults to kube_apiserver_port (6443)<font></font>
#nginx_kube_apiserver_port: 8443<font></font>
<font></font>
### OTHER OPTIONAL VARIABLES<font></font>
## For some things, kubelet needs to load kernel modules.  For example, dynamic kernel services are needed<font></font>
## for mounting persistent volumes into containers.  These may not be loaded by preinstall kubernetes<font></font>
## processes.  For example, ceph and rbd backed volumes.  Set to true to allow kubelet to load kernel<font></font>
## modules.<font></font>
#kubelet_load_modules: false<font></font>
<font></font>
## Upstream dns servers used by dnsmasq<font></font>
#upstream_dns_servers:<font></font>
#  - 8.8.8.8<font></font>
#  - 8.8.4.4<font></font>
<font></font>
## There are some changes specific to the cloud providers<font></font>
## for instance we need to encapsulate packets with some network plugins<font></font>
## If set the possible values are either 'gce', 'aws', 'azure', 'openstack', 'vsphere', 'oci', or 'external'<font></font>
## When openstack is used make sure to source in the openstack credentials<font></font>
## like you would do when using nova-client before starting the playbook.<font></font>
## Note: The 'external' cloud provider is not supported.<font></font>
## TODO(riverzhang): https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/#running-cloud-controller-manager<font></font>
cloud_provider: openstack<font></font>
<font></font>
## Set these proxy values in order to update package manager and docker daemon to use proxies<font></font>
#http_proxy: ""<font></font>
#https_proxy: ""<font></font>
<font></font>
## Refer to roles/kubespray-defaults/defaults/main.yml before modifying no_proxy<font></font>
#no_proxy: ""<font></font>
<font></font>
## Some problems may occur when downloading files over https proxy due to ansible bug<font></font>
## https://github.com/ansible/ansible/issues/32750. Set this variable to False to disable<font></font>
## SSL validation of get_url module. Note that kubespray will still be performing checksum validation.<font></font>
#download_validate_certs: False<font></font>
<font></font>
## If you need exclude all cluster nodes from proxy and other resources, add other resources here.<font></font>
#additional_no_proxy: ""<font></font>
<font></font>
## Certificate Management<font></font>
## This setting determines whether certs are generated via scripts.<font></font>
## Chose 'none' if you provide your own certificates.<font></font>
## Option is  "script", "none"<font></font>
## note: vault is removed<font></font>
#cert_management: script<font></font>
<font></font>
## Set to true to allow pre-checks to fail and continue deployment<font></font>
#ignore_assert_errors: false<font></font>
<font></font>
## The read-only port for the Kubelet to serve on with no authentication/authorization. Uncomment to enable.<font></font>
#kube_read_only_port: 10255<font></font>
<font></font>
## Set true to download and cache container<font></font>
download_container: false<font></font>
<font></font>
## Deploy container engine<font></font>
# Set false if you want to deploy container engine manually.<font></font>
#deploy_container_engine: true<font></font>
<font></font>
## Set Pypi repo and cert accordingly<font></font>
#pyrepo_index: https://pypi.example.com/simple<font></font>
#pyrepo_cert: /etc/ssl/certs/ca-certificates.crt</code></pre><br>
<p>   inventory/$CLUSTER/group_vars/k8s-cluster/k8s-cluster.yml.   kube_network_plugin  flannel  calico ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=">  OpenStack Neutron</a>).     flannel,    .   resolvconf_mode  docker_dns.    Kubespray  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="> Docker-.</a>        .</p><br>
<pre><code class="plaintext hljs"># Kubernetes configuration dirs and system namespace.<font></font>
# Those are where all the additional config stuff goes<font></font>
# the kubernetes normally puts in /srv/kubernetes.<font></font>
# This puts them in a sane location and namespace.<font></font>
# Editing those values will almost surely break something.<font></font>
kube_config_dir: /etc/kubernetes<font></font>
kube_script_dir: "{{ bin_dir }}/kubernetes-scripts"<font></font>
kube_manifest_dir: "{{ kube_config_dir }}/manifests"<font></font>
<font></font>
# This is where all the cert scripts and certs will be located<font></font>
kube_cert_dir: "{{ kube_config_dir }}/ssl"<font></font>
<font></font>
# This is where all of the bearer tokens will be stored<font></font>
kube_token_dir: "{{ kube_config_dir }}/tokens"<font></font>
<font></font>
# This is where to save basic auth file<font></font>
kube_users_dir: "{{ kube_config_dir }}/users"<font></font>
<font></font>
kube_api_anonymous_auth: true<font></font>
<font></font>
## Change this to use another Kubernetes version, e.g. a current beta release<font></font>
kube_version: v1.13.3<font></font>
<font></font>
# kubernetes image repo define<font></font>
kube_image_repo: "gcr.io/google-containers"<font></font>
<font></font>
# Where the binaries will be downloaded.<font></font>
# Note: ensure that you've enough disk space (about 1G)<font></font>
local_release_dir: "/tmp/releases"<font></font>
# Random shifts for retrying failed ops like pushing/downloading<font></font>
retry_stagger: 5<font></font>
<font></font>
# This is the group that the cert creation scripts chgrp the<font></font>
# cert files to. Not really changeable...<font></font>
kube_cert_group: kube-cert<font></font>
<font></font>
# Cluster Loglevel configuration<font></font>
kube_log_level: 2<font></font>
<font></font>
# Directory where credentials will be stored<font></font>
credentials_dir: "{{ inventory_dir }}/credentials"<font></font>
<font></font>
# Users to create for basic auth in Kubernetes API via HTTP<font></font>
# Optionally add groups for user<font></font>
kube_api_pwd: "{{ lookup('password', credentials_dir + '/kube_user.creds length=15 chars=ascii_letters,digits') }}"<font></font>
kube_users:<font></font>
  kube:<font></font>
    pass: "{{kube_api_pwd}}"<font></font>
    role: admin<font></font>
    groups:<font></font>
      - system:masters<font></font>
<font></font>
## It is possible to activate / deactivate selected authentication methods (basic auth, static token auth)<font></font>
#kube_oidc_auth: false<font></font>
#kube_basic_auth: false<font></font>
#kube_token_auth: false<font></font>
<font></font>
## Variables for OpenID Connect Configuration https://kubernetes.io/docs/admin/authentication/<font></font>
## To use OpenID you have to deploy additional an OpenID Provider (e.g Dex, Keycloak, ...)<font></font>
<font></font>
# kube_oidc_url: https:// ...<font></font>
# kube_oidc_client_id: kubernetes<font></font>
## Optional settings for OIDC<font></font>
# kube_oidc_ca_file: "{{ kube_cert_dir }}/ca.pem"<font></font>
# kube_oidc_username_claim: sub<font></font>
# kube_oidc_username_prefix: oidc:<font></font>
# kube_oidc_groups_claim: groups<font></font>
# kube_oidc_groups_prefix: oidc:<font></font>
<font></font>
# Choose network plugin (cilium, calico, contiv, weave or flannel)<font></font>
# Can also be set to 'cloud', which lets the cloud provider setup appropriate routing<font></font>
kube_network_plugin: flannel<font></font>
<font></font>
# Setting multi_networking to true will install Multus: https://github.com/intel/multus-cni<font></font>
kube_network_plugin_multus: false<font></font>
<font></font>
# Kubernetes internal network for services, unused block of space.<font></font>
kube_service_addresses: 10.233.0.0/18<font></font>
<font></font>
# internal network. When used, it will assign IP<font></font>
# addresses from this range to individual pods.<font></font>
# This network must be unused in your network infrastructure!<font></font>
kube_pods_subnet: 10.233.64.0/18<font></font>
<font></font>
# internal network node size allocation (optional). This is the size allocated<font></font>
# to each node on your network.  With these defaults you should have<font></font>
# room for 4096 nodes with 254 pods per node.<font></font>
kube_network_node_prefix: 24<font></font>
<font></font>
# The port the API Server will be listening on.<font></font>
kube_apiserver_ip: "{{ kube_service_addresses|ipaddr('net')|ipaddr(1)|ipaddr('address') }}"<font></font>
kube_apiserver_port: 6443 # (https)<font></font>
#kube_apiserver_insecure_port: 8080 # (http)<font></font>
# Set to 0 to disable insecure port - Requires RBAC in authorization_modes and kube_api_anonymous_auth: true<font></font>
kube_apiserver_insecure_port: 0 # (disabled)<font></font>
<font></font>
# Kube-proxy proxyMode configuration.<font></font>
# Can be ipvs, iptables<font></font>
kube_proxy_mode: ipvs<font></font>
<font></font>
# A string slice of values which specify the addresses to use for NodePorts.<font></font>
# Values may be valid IP blocks (e.g. 1.2.3.0/24, 1.2.3.4/32).<font></font>
# The default empty string slice ([]) means to use all local addresses.<font></font>
# kube_proxy_nodeport_addresses_cidr is retained for legacy config<font></font>
kube_proxy_nodeport_addresses: &gt;-<font></font>
  {%- if kube_proxy_nodeport_addresses_cidr is defined -%}<font></font>
  [{{ kube_proxy_nodeport_addresses_cidr }}]<font></font>
  {%- else -%}<font></font>
  []<font></font>
  {%- endif -%}<font></font>
# If non-empty, will use this string as identification instead of the actual hostname<font></font>
#kube_override_hostname: &gt;-<font></font>
#  {%- if cloud_provider is defined and cloud_provider in [ 'aws' ] -%}<font></font>
#  {%- else -%}<font></font>
#  {{ inventory_hostname }}<font></font>
#  {%- endif -%}<font></font>
<font></font>
## Encrypting Secret Data at Rest (experimental)<font></font>
kube_encrypt_secret_data: false<font></font>
<font></font>
# DNS configuration.<font></font>
# Kubernetes cluster name, also will be used as DNS domain<font></font>
cluster_name: cluster.local<font></font>
# Subdomains of DNS domain to be resolved via /etc/resolv.conf for hostnet pods<font></font>
ndots: 2<font></font>
# Can be dnsmasq_kubedns, kubedns, coredns, coredns_dual, manual or none<font></font>
dns_mode: coredns<font></font>
# Set manual server if using a custom cluster DNS server<font></font>
#manual_dns_server: 10.x.x.x<font></font>
# Enable nodelocal dns cache<font></font>
enable_nodelocaldns: False<font></font>
nodelocaldns_ip: 169.254.25.10<font></font>
<font></font>
# Can be docker_dns, host_resolvconf or none<font></font>
resolvconf_mode: docker_dns<font></font>
# Deploy netchecker app to verify DNS resolve as an HTTP service<font></font>
deploy_netchecker: false<font></font>
# Ip address of the kubernetes skydns service<font></font>
skydns_server: "{{ kube_service_addresses|ipaddr('net')|ipaddr(3)|ipaddr('address') }}"<font></font>
skydns_server_secondary: "{{ kube_service_addresses|ipaddr('net')|ipaddr(4)|ipaddr('address') }}"<font></font>
dnsmasq_dns_server: "{{ kube_service_addresses|ipaddr('net')|ipaddr(2)|ipaddr('address') }}"<font></font>
dns_domain: "{{ cluster_name }}"<font></font>
<font></font>
## Container runtime<font></font>
## docker for docker and crio for cri-o.<font></font>
container_manager: docker<font></font>
<font></font>
## Settings for containerized control plane (etcd/kubelet/secrets)<font></font>
etcd_deployment_type: docker<font></font>
kubelet_deployment_type: host<font></font>
helm_deployment_type: host<font></font>
<font></font>
# K8s image pull policy (imagePullPolicy)<font></font>
k8s_image_pull_policy: IfNotPresent<font></font>
<font></font>
# audit log for kubernetes<font></font>
kubernetes_audit: false<font></font>
<font></font>
# dynamic kubelet configuration<font></font>
dynamic_kubelet_configuration: false<font></font>
<font></font>
# define kubelet config dir for dynamic kubelet<font></font>
#kubelet_config_dir:<font></font>
default_kubelet_config_dir: "{{ kube_config_dir }}/dynamic_kubelet_dir"<font></font>
dynamic_kubelet_configuration_dir: "{{ kubelet_config_dir | default(default_kubelet_config_dir) }}"<font></font>
<font></font>
# pod security policy (RBAC must be enabled either by having 'RBAC' in authorization_modes or kubeadm enabled)<font></font>
podsecuritypolicy_enabled: false<font></font>
<font></font>
# Make a copy of kubeconfig on the host that runs Ansible in {{ inventory_dir }}/artifacts<font></font>
# kubeconfig_localhost: false<font></font>
# Download kubectl onto the host that runs Ansible in {{ bin_dir }}<font></font>
# kubectl_localhost: false<font></font>
<font></font>
# dnsmasq<font></font>
# dnsmasq_upstream_dns_servers:<font></font>
#  - /resolvethiszone.with/10.0.4.250<font></font>
#  - 8.8.8.8<font></font>
<font></font>
#  Enable creation of QoS cgroup hierarchy, if true top level QoS and pod cgroups are created. (default true)<font></font>
# kubelet_cgroups_per_qos: true<font></font>
<font></font>
# A comma separated list of levels of node allocatable enforcement to be enforced by kubelet.<font></font>
# Acceptable options are 'pods', 'system-reserved', 'kube-reserved' and ''. Default is "".<font></font>
# kubelet_enforce_node_allocatable: pods<font></font>
<font></font>
## Supplementary addresses that can be added in kubernetes ssl keys.<font></font>
## That can be useful for example to setup a keepalived virtual IP<font></font>
# supplementary_addresses_in_ssl_keys: [10.0.0.1, 10.0.0.2, 10.0.0.3]<font></font>
<font></font>
## Running on top of openstack vms with cinder enabled may lead to unschedulable pods due to NoVolumeZoneConflict restriction in kube-scheduler.<font></font>
## See https://github.com/kubernetes-sigs/kubespray/issues/2141<font></font>
## Set this variable to true to get rid of this issue<font></font>
volume_cross_zone_attachment: false<font></font>
# Add Persistent Volumes Storage Class for corresponding cloud provider ( OpenStack is only supported now )<font></font>
persistent_volumes_enabled: false<font></font>
<font></font>
## Container Engine Acceleration<font></font>
## Enable container acceleration feature, for example use gpu acceleration in containers<font></font>
# nvidia_accelerator_enabled: true<font></font>
## Nvidia GPU driver install. Install will by done by a (init) pod running as a daemonset.<font></font>
## Important: if you use Ubuntu then you should set in all.yml 'docker_storage_options: -s overlay2'<font></font>
## Array with nvida_gpu_nodes, leave empty or comment if you dont't want to install drivers.<font></font>
## Labels and taints won't be set to nodes if they are not in the array.<font></font>
# nvidia_gpu_nodes:<font></font>
#   - kube-gpu-001<font></font>
# nvidia_driver_version: "384.111"<font></font>
## flavor can be tesla or gtx<font></font>
# nvidia_gpu_flavor: gtx</code></pre><br>
<p>,   inventory/$CLUSTER/group_vars/k8s-cluster/addons.yml    dashboard_enabled  true,    .  :</p><br>
<pre><code class="plaintext hljs"># Kubernetes dashboard<font></font>
# RBAC required. see docs/getting-started.md for access details.<font></font>
dashboard_enabled: true<font></font>
<font></font>
# Helm deployment<font></font>
helm_enabled: false<font></font>
<font></font>
# Registry deployment<font></font>
registry_enabled: false<font></font>
# registry_namespace: kube-system<font></font>
# registry_storage_class: ""<font></font>
# registry_disk_size: "10Gi"<font></font>
<font></font>
# Metrics Server deployment<font></font>
metrics_server_enabled: false<font></font>
# metrics_server_kubelet_insecure_tls: true<font></font>
# metrics_server_metric_resolution: 60s<font></font>
# metrics_server_kubelet_preferred_address_types: "InternalIP"<font></font>
<font></font>
# Local volume provisioner deployment<font></font>
local_volume_provisioner_enabled: false<font></font>
# local_volume_provisioner_namespace: kube-system<font></font>
# local_volume_provisioner_storage_classes:<font></font>
#   local-storage:<font></font>
#     host_dir: /mnt/disks<font></font>
#     mount_dir: /mnt/disks<font></font>
#   fast-disks:<font></font>
#     host_dir: /mnt/fast-disks<font></font>
#     mount_dir: /mnt/fast-disks<font></font>
#     block_cleaner_command:<font></font>
#       - "/scripts/shred.sh"<font></font>
#       - "2"<font></font>
#     volume_mode: Filesystem<font></font>
#     fs_type: ext4<font></font>
<font></font>
# CephFS provisioner deployment<font></font>
cephfs_provisioner_enabled: false<font></font>
# cephfs_provisioner_namespace: "cephfs-provisioner"<font></font>
# cephfs_provisioner_cluster: ceph<font></font>
# cephfs_provisioner_monitors: "172.24.0.1:6789,172.24.0.2:6789,172.24.0.3:6789"<font></font>
# cephfs_provisioner_admin_id: admin<font></font>
# cephfs_provisioner_secret: secret<font></font>
# cephfs_provisioner_storage_class: cephfs<font></font>
# cephfs_provisioner_reclaim_policy: Delete<font></font>
# cephfs_provisioner_claim_root: /volumes<font></font>
# cephfs_provisioner_deterministic_names: true<font></font>
<font></font>
# Nginx ingress controller deployment<font></font>
ingress_nginx_enabled: false<font></font>
# ingress_nginx_host_network: false<font></font>
# ingress_nginx_nodeselector:<font></font>
#   node.kubernetes.io/node: ""<font></font>
# ingress_nginx_tolerations:<font></font>
#   - key: "node.kubernetes.io/master"<font></font>
#     operator: "Equal"<font></font>
#     value: ""<font></font>
#     effect: "NoSchedule"<font></font>
# ingress_nginx_namespace: "ingress-nginx"<font></font>
# ingress_nginx_insecure_port: 80<font></font>
# ingress_nginx_secure_port: 443<font></font>
# ingress_nginx_configmap:<font></font>
#   map-hash-bucket-size: "128"<font></font>
#   ssl-protocols: "SSLv2"<font></font>
# ingress_nginx_configmap_tcp_services:<font></font>
#   9000: "default/example-go:8080"<font></font>
# ingress_nginx_configmap_udp_services:<font></font>
#   53: "kube-system/kube-dns:53"<font></font>
<font></font>
# Cert manager deployment<font></font>
cert_manager_enabled: false<font></font>
# cert_manager_namespace: "cert-manager"</code></pre><br>
<p> ,  ansible-playbook   ,   .</p><br>
<pre><code class="plaintext hljs">ansible-playbook --become -i inventory/$CLUSTER/hosts cluster.yml</code></pre><br>
<p>Ansible   ,      ,   ,    .</p><br>
<p><img src="https://habrastorage.org/webt/2d/sg/tz/2dsgtzytttaugaufa1aolqy06wy.png"></p><br>
<h2 id="testirovanie"></h2><br>
<p>     ,   root-   kubectl   kubectl cluster-info,     .           .     ,     Kubernetes    .</p><br>
<pre><code class="plaintext hljs"># Create service account<font></font>
kubectl create serviceaccount cluster-admin-dashboard-sa<font></font>
<font></font>
# Bind ClusterAdmin role to the service account<font></font>
kubectl create clusterrolebinding cluster-admin-dashboard-sa \<font></font>
  --clusterrole=cluster-admin \<font></font>
  --serviceaccount=default:cluster-admin-dashboard-sa<font></font>
<font></font>
# Parse the token<font></font>
kubectl describe secret $(kubectl -n kube-system get secret | awk '/^cluster-admin-dashboard-sa-token-/{print $1}') | awk '$1=="token:"{print $2}'</code></pre><br>
<p>        .       Kubernetes,         localhost   8001.         URL localhost:8001.   Token,    .</p><br>
<p><img src="https://habrastorage.org/webt/vr/gh/k2/vrghk2vujok6oexyyg-reeygrwk.png"></p><br>
<p>      Kubernetes.     ,       Kubernetes   OpenStack.</p></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../ja456768/index.html">ä¾‹ã¨ã—ã¦ãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨ã—ãŸãƒ¢ãƒã‚¤ãƒ«ã‚²ãƒ¼ãƒ ã®ãƒ­ãƒ¼ã‚«ãƒ©ã‚¤ã‚ºã«ãŠã‘ã‚‹4ã¤ã®å•é¡Œ-ç¿»è¨³</a></li>
<li><a href="../ja456770/index.html">è»æ‹¡ç«¶äº‰</a></li>
<li><a href="../ja456774/index.html">Kotlin / Scalaã‚’ä½¿ç”¨ã—ãŸå¾Œã«Javaã§è¦‹é€ƒã—ãŸã‚‚ã®</a></li>
<li><a href="../ja456782/index.html">ãƒ¢ãƒ‡ãƒ«æŒ‡å‘ã®è¨­è¨ˆ-ãƒã‚§ãƒ«ãƒãƒ–ã‚¤ãƒªã‚’ç¹°ã‚Šè¿”ã•ãªã„æ–¹æ³•</a></li>
<li><a href="../ja456790/index.html">PostgreSQLãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆã€‚ç¬¬16å·</a></li>
<li><a href="../ja456794/index.html">Web UIã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼šæœ¨è£½ã®éå»ã€å¥‡å¦™ãªç¾åœ¨ã€ãã—ã¦æ˜ã‚‹ã„æœªæ¥</a></li>
<li><a href="../ja456796/index.html">ã‚¹ãƒãƒ¼ãƒ«ãƒãƒ«è«¸å³¶-è²©å£²å‰ã®ã€Œç§ã¯æ•ã‚‰ã‚ã‚ŒãŸã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®æ–°ã—ã„åå‰</a></li>
<li><a href="../ja456798/index.html">SDL 2ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ï¼šãƒ¬ãƒƒã‚¹ãƒ³5-ãƒ†ã‚¯ã‚¹ãƒãƒ£</a></li>
<li><a href="../ja456804/index.html">ãŠé‡‘ã«å¾“ã£ã¦ãã ã•ã„ï¼šRTMã‚°ãƒ«ãƒ¼ãƒ—ãŒæš—å·ã‚¦ã‚©ãƒ¬ãƒƒãƒˆã§CãŠã‚ˆã³Cã‚µãƒ¼ãƒãƒ¼ã®ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’éš ã—å§‹ã‚ãŸæ–¹æ³•</a></li>
<li><a href="../ja456806/index.html">ã™ã¹ã¦ã®å¿ƒé…äº‹ã‹ã‚‰1ã¤ã®ãƒœãƒƒãƒˆ</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>