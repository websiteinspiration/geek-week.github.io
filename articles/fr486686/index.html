<!doctype html>
<html class="no-js" lang="fr">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🔟 🕺🏼 ⛹🏼 À propos de l'implémentation d'une bibliothèque d'apprentissage en profondeur dans Python 🤶🏾 📃 👨🏻‍🌾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Les technologies d'apprentissage en profondeur ont parcouru un long chemin en peu de temps - des réseaux neuronaux simples aux architectures assez com...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>À propos de l'implémentation d'une bibliothèque d'apprentissage en profondeur dans Python</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ruvds/blog/486686/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Les technologies d'apprentissage en profondeur ont parcouru un long chemin en peu de temps - des réseaux neuronaux simples aux architectures assez complexes. Pour soutenir la diffusion rapide de ces technologies, diverses bibliothèques et plates-formes d'apprentissage en profondeur ont été développées. L'un des principaux objectifs de ces bibliothèques est de fournir aux développeurs des interfaces simples pour créer et former des modèles de réseaux neuronaux. Ces bibliothèques permettent à leurs utilisateurs d'accorder plus d'attention aux tâches à résoudre et non aux subtilités de la mise en œuvre du modèle. Pour ce faire, vous devrez peut-être masquer l'implémentation des mécanismes de base derrière plusieurs niveaux d'abstraction. Et cela, à son tour, complique la compréhension des principes de base sur lesquels reposent les bibliothèques d'apprentissage en profondeur.</font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><img src="https://habrastorage.org/webt/bp/yi/sl/bpyislfb1o7e-qh7exvklh1oxuw.jpeg"></a><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
L'article, dont nous publions la traduction, vise à analyser les caractéristiques du dispositif des blocs de construction de bas niveau des bibliothèques d'apprentissage en profondeur. </font><font style="vertical-align: inherit;">Tout d'abord, nous parlons brièvement de l'essence de l'apprentissage en profondeur. </font><font style="vertical-align: inherit;">Cela nous permettra de comprendre les exigences fonctionnelles du logiciel respectif. </font><font style="vertical-align: inherit;">Ensuite, nous envisageons de développer une bibliothèque d'apprentissage en profondeur simple mais fonctionnelle en Python à l'aide de NumPy. </font><font style="vertical-align: inherit;">Cette bibliothèque est capable de fournir une formation de bout en bout pour les modèles de réseau neuronal simples. </font><font style="vertical-align: inherit;">En cours de route, nous parlerons des différentes composantes des cadres d'apprentissage en profondeur. </font><font style="vertical-align: inherit;">La bibliothèque que nous allons considérer est assez petite, moins de 100 lignes de code. </font><font style="vertical-align: inherit;">Et cela signifie qu'il sera assez simple de le comprendre. </font><font style="vertical-align: inherit;">Le code de projet complet, dont nous traiterons, se trouve </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ici</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<a name="habracut"></a><br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">informations générales</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En règle générale, les bibliothèques d'apprentissage en profondeur (telles que TensorFlow et PyTorch) sont constituées des composants illustrés dans la figure suivante.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c49/b9f/396/c49b9f39652c0260a9e30ee4e5dea146.jpg"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Composants du cadre d'apprentissage en profondeur</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Analysons ces composants.</font></font><br>
<br>
<h3><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">▍ Opérateurs</font></font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Les concepts d '«opérateur» et de «couche» (couche) sont généralement utilisés de manière interchangeable. </font><font style="vertical-align: inherit;">Ce sont les éléments de base de tout réseau neuronal. </font><font style="vertical-align: inherit;">Les opérateurs sont des fonctions vectorielles qui transforment les données. </font><font style="vertical-align: inherit;">Parmi les opérateurs fréquemment utilisés, on peut distinguer comme les couches d'activation linéaires et convolutives, les couches de sous-échantillonnage (pooling), semi-linéaires (ReLU) et sigmoïdes (sigmoïdes).</font></font><br>
<br>
<h3><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">▍Optimiseurs (optimiseurs)</font></font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Les optimiseurs sont le fondement des bibliothèques d'apprentissage en profondeur. </font><font style="vertical-align: inherit;">Ils décrivent des méthodes d'ajustement des paramètres du modèle en utilisant certains critères et en tenant compte de l'objectif d'optimisation. </font><font style="vertical-align: inherit;">Parmi les optimiseurs bien connus, on peut citer SGD, RMSProp et Adam.</font></font><br>
<br>
<h3><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">▍ Fonctions de perte</font></font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Les fonctions de perte sont des expressions mathématiques analytiques et différenciables qui sont utilisées comme substitut à l'objectif d'optimisation lors de la résolution d'un problème. </font><font style="vertical-align: inherit;">Par exemple, la fonction d'entropie croisée et la fonction linéaire par morceaux sont généralement utilisées dans les problèmes de classification.</font></font><br>
<br>
<h3><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">▍ Initialiseurs</font></font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Les initialiseurs fournissent des valeurs initiales pour les paramètres du modèle. </font><font style="vertical-align: inherit;">Ce sont ces valeurs que les paramètres ont au début de l'entraînement. </font><font style="vertical-align: inherit;">Les initialiseurs jouent un rôle important dans la formation des réseaux de neurones, car des paramètres initiaux infructueux peuvent signifier que le réseau apprendra lentement ou peut-être pas du tout. </font><font style="vertical-align: inherit;">Il existe de nombreuses façons d'initialiser les poids d'un réseau neuronal. </font><font style="vertical-align: inherit;">Par exemple, vous pouvez leur attribuer de petites valeurs aléatoires à partir de la distribution normale. </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Voici une</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> page où vous pourrez découvrir les différents types d'initialiseurs.</font></font><br>
<br>
<h3><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">▍ Régularisateurs</font></font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Les régularisateurs sont des outils qui évitent le recyclage du réseau et aident le réseau à se généraliser. Vous pouvez gérer le recyclage du réseau de manière explicite ou implicite. Les méthodes explicites impliquent des limitations structurelles sur les poids. Par exemple, minimiser leurs normes L1 et L2, ce qui, en conséquence, rend les valeurs de poids mieux dispersées et réparties plus uniformément. Les méthodes implicites sont représentées par des opérateurs spécialisés qui effectuent la transformation des représentations intermédiaires. Cela se fait soit par une normalisation explicite, par exemple, en utilisant la technique de normalisation des paquets (BatchNorm), soit en modifiant la connectivité réseau à l'aide des algorithmes DropOut et DropConnect.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Les composants ci-dessus appartiennent généralement à la partie interface de la bibliothèque. </font><font style="vertical-align: inherit;">Ici, par «partie interface», j'entends les entités avec lesquelles l'utilisateur peut interagir. </font><font style="vertical-align: inherit;">Ils lui donnent des outils pratiques pour concevoir efficacement une architecture de réseau neuronal. </font><font style="vertical-align: inherit;">Si nous parlons des mécanismes internes des bibliothèques, ils peuvent fournir un support pour le calcul automatique des gradients de la fonction de perte, en tenant compte de divers paramètres du modèle. </font><font style="vertical-align: inherit;">Cette technique est communément appelée différenciation automatique (AD).</font></font><br>
<br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Différenciation automatique</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Chaque bibliothèque d'apprentissage en profondeur offre à l'utilisateur des capacités de différenciation automatique. Cela lui donne l'opportunité de se concentrer sur la description de la structure du modèle (graphique des calculs) et de transférer la tâche de calcul des gradients au module AD. Prenons un exemple qui nous permettra de savoir comment tout cela fonctionne. Supposons que nous voulons calculer les dérivées partielles de la fonction suivante par rapport à ses variables d'entrée X₁ et X₂: </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Y = sin (x₁) + X₁ * X₂ </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La figure suivante, que j'ai empruntée </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ici</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , montre le graphique des calculs et le calcul des dérivés à l'aide d'une règle de chaîne.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/605/635/189/605635189c56a2f87927ec1a0c5b6318.png"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Graphique de calcul et calcul des dérivées par une règle de chaîne</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Ce que vous voyez ici est quelque chose comme un «mode inverse» de différenciation automatique. </font><font style="vertical-align: inherit;">L'algorithme bien connu de rétro-propagation d'erreur est un cas particulier de l'algorithme décrit ci-dessus pour le cas où la fonction située en haut est une fonction de perte. </font><font style="vertical-align: inherit;">AD exploite le fait que toute fonction complexe se compose d'opérations arithmétiques élémentaires et de fonctions élémentaires. </font><font style="vertical-align: inherit;">Par conséquent, les dérivés peuvent être calculés en appliquant une règle de chaîne à ces opérations.</font></font><br>
<br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">la mise en oeuvre</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dans la section précédente, nous avons examiné les composants nécessaires à la création d'une bibliothèque d'apprentissage en profondeur conçue pour créer et former de bout en bout des réseaux de neurones. Afin de ne pas compliquer l'exemple, </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j'imite</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ici le modèle de conception de la bibliothèque </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;">Caffe</font></a><font style="vertical-align: inherit;"> . Ici, nous déclarons deux classes abstraites - </font></font><code>Function</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">et </font></font><code>Optimizer</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. De plus, il existe une classe </font></font><code>Tensor</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, qui est une structure simple contenant deux tableaux NumPy multidimensionnels. L'un d'eux est conçu pour stocker les valeurs des paramètres, l'autre - pour stocker leurs gradients. Tous les paramètres des différentes couches (opérateurs) seront de type </font></font><code>Tensor</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Avant d'aller plus loin, jetez un œil au plan général de la bibliothèque.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d05/98c/068/d0598c068139ecda1f2aacbd9ea5f068.jpg"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Diagramme UML de la bibliothèque</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Au moment de la rédaction de ce document, cette bibliothèque contient une implémentation de la couche linéaire, de la fonction d'activation ReLU, de la couche SoftMaxLoss et de l'optimiseur SGD. En conséquence, il s'avère que la bibliothèque peut être utilisée pour former des modèles de classification composés de couches entièrement connectées et utilisant une fonction d'activation non linéaire. Voyons maintenant quelques détails sur les classes abstraites que nous avons.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Une classe abstraite</font></font><code>Function</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">fournit une interface pour les opérateurs. Voici son code:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword">class</span>&nbsp; <span class="hljs-title">Function</span>(<span class="hljs-params">object</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self</span>):</span>&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">raise</span> NotImplementedError<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backward</span>(<span class="hljs-params">self</span>):</span>&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">raise</span> NotImplementedError<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getParams</span>(<span class="hljs-params">self</span>):</span>&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span> []</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Tous les opérateurs sont implémentés via l'héritage d'une classe abstraite </font></font><code>Function</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Chaque opérateur doit fournir une mise en œuvre des méthodes </font></font><code>forward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">et </font></font><code>backward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Les opérateurs peuvent contenir une implémentation d'une méthode facultative </font></font><code>getParams()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">qui renvoie leurs paramètres (le cas échéant). La méthode </font></font><code>forward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">reçoit les données d'entrée et renvoie le résultat de leur transformation par l'opérateur. De plus, il résout les problèmes internes nécessaires au calcul des gradients. Le procédé </font></font><code>backward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">accepte les dérivées partielles de la fonction de perte par rapport aux sorties de l'opérateur et met en œuvre le calcul des dérivées partielles de la fonction de perte par rapport aux données d'entrée de l'opérateur et aux paramètres (le cas échéant). Notez que la méthode</font></font><code>backward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, en substance, offre à notre bibliothèque la possibilité d'effectuer une différenciation automatique. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Afin de traiter tout cela avec un exemple spécifique, regardons l'implémentation de la fonction </font></font><code>Linear</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Linear</span>(<span class="hljs-params">Function</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,in_nodes,out_nodes</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.weights = Tensor((in_nodes,out_nodes))<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.bias&nbsp; &nbsp; = Tensor((<span class="hljs-number">1</span>,out_nodes))<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.type = <span class="hljs-string">'linear'</span><font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self,x</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;output = np.dot(x,self.weights.data)+self.bias.data<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.input = x&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span> output<font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backward</span>(<span class="hljs-params">self,d_y</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.weights.grad += np.dot(self.input.T,d_y)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.bias.grad&nbsp; &nbsp; += np.sum(d_y,axis=<span class="hljs-number">0</span>,keepdims=<span class="hljs-literal">True</span>)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;grad_input &nbsp; &nbsp; &nbsp; &nbsp; = np.dot(d_y,self.weights.data.T)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span> grad_input<font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getParams</span>(<span class="hljs-params">self</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span> [self.weights,self.bias]</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La méthode </font></font><code>forward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">implémente la transformation de la vue </font></font><code>Y = X*W+b</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">et renvoie le résultat. De plus, il enregistre la valeur d'entrée </font></font><code>X</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, car il est nécessaire de calculer la dérivée partielle de </font></font><code>dY</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">la fonction de perte par rapport à la valeur de sortie </font></font><code>Y</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">dans la méthode </font></font><code>backward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. La méthode </font></font><code>backward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">reçoit les dérivées partielles, calculées par rapport à la valeur d'entrée </font></font><code>X</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">et aux paramètres </font></font><code>W</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">et </font></font><code>b</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. De plus, il renvoie les dérivées partielles calculées par rapport à la valeur d'entrée </font></font><code>X</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, qui seront transférées à la couche précédente. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Une classe abstraite </font></font><code>Optimizer</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">fournit une interface pour les optimiseurs:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Optimizer</span>(<span class="hljs-params">object</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,parameters</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.parameters = parameters<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">step</span>(<span class="hljs-params">self</span>):</span>&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">raise</span> NotImplementedError<font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">zeroGrad</span>(<span class="hljs-params">self</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> self.parameters:<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;p.grad = <span class="hljs-number">0.</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Tous les optimiseurs sont implémentés en héritant de la classe de base </font></font><code>Optimizer</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Une classe décrivant une optimisation particulière devrait fournir une implémentation de la méthode </font></font><code>step()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Cette méthode met à jour les paramètres du modèle en utilisant leurs dérivées partielles calculées par rapport à la valeur optimisée de la fonction de perte. </font><font style="vertical-align: inherit;">Un lien vers divers paramètres du modèle est fourni dans la fonction </font></font><code>__init__()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Veuillez noter que la fonctionnalité universelle de réinitialisation des valeurs de gradient est implémentée dans la classe de base elle-même. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Maintenant, pour mieux comprendre tout cela, considérons un exemple spécifique - la mise en œuvre de l'algorithme de descente de gradient stochastique (SGD) avec prise en charge de l'ajustement de la quantité de mouvement et de la réduction des poids:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">SGD</span>(<span class="hljs-params">Optimizer</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,parameters,lr=<span class="hljs-number">.001</span>,weight_decay=<span class="hljs-number">0.0</span>,momentum = <span class="hljs-number">.9</span></span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;super().__init__(parameters)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.lr &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = lr<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.weight_decay = weight_decay<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.momentum &nbsp; &nbsp; = momentum<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.velocity &nbsp; &nbsp; = []<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> parameters:<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.velocity.append(np.zeros_like(p.grad))<font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">step</span>(<span class="hljs-params">self</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> p,v <span class="hljs-keyword">in</span> zip(self.parameters,self.velocity):<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v = self.momentum*v+p.grad+self.weight_decay*p.data<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;p.data=p.data-self.lr*v</code></pre><br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La solution au vrai problème</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nous avons maintenant tout le nécessaire pour former le modèle de réseau de neurones (profond) à l'aide de notre bibliothèque. </font><font style="vertical-align: inherit;">Pour cela, nous avons besoin des entités suivantes:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Modèle: graphique de calcul.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Données et valeur cible: données pour la formation du réseau.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fonction de perte: substitut à l'objectif d'optimisation.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Optimizer: un mécanisme de mise à jour des paramètres du modèle.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Le pseudo-code suivant décrit un cycle de test typique:</font></font><br>
<br>
<pre><code class="python hljs">model <span class="hljs-comment"># </span>
data,target <span class="hljs-comment"># </span>
loss_fn <span class="hljs-comment"># </span>
optim <span class="hljs-comment">#,         </span>
Repeat:<span class="hljs-comment">#   ,    ,     </span>
&nbsp;&nbsp;&nbsp;optim.zeroGrad() <span class="hljs-comment">#    </span>
&nbsp;&nbsp;&nbsp;output = model.forward(data) <span class="hljs-comment">#   </span>
&nbsp;&nbsp;&nbsp;loss &nbsp; = loss_fn(output,target) <span class="hljs-comment"># </span>
&nbsp;&nbsp;&nbsp;grad &nbsp; = loss.backward() <span class="hljs-comment">#      </span>
&nbsp;&nbsp;&nbsp;model.backward(grad) <span class="hljs-comment">#    </span>
&nbsp;&nbsp;&nbsp;optim.step() <span class="hljs-comment">#  </span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bien que cela ne soit pas nécessaire dans la bibliothèque d'apprentissage en profondeur, il peut être utile d'inclure les fonctionnalités ci-dessus dans une classe distincte. </font><font style="vertical-align: inherit;">Cela nous permettra de ne pas répéter les mêmes actions lors de l'apprentissage de nouveaux modèles (cette idée correspond à la philosophie des abstractions de haut niveau de frameworks comme </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Keras</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ). </font><font style="vertical-align: inherit;">Pour ce faire, déclarez une classe </font></font><code>Model</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Model</span>():</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.computation_graph = []<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.parameters&nbsp; &nbsp; &nbsp; &nbsp; = []<font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">add</span>(<span class="hljs-params">self,layer</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.computation_graph.append(layer)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.parameters+=layer.getParams()<font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__innitializeNetwork</span>(<span class="hljs-params">self</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> self.computation_graph:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">if</span> f.type==<span class="hljs-string">'linear'</span>:<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;weights,bias = f.getParams()<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;weights.data = <span class="hljs-number">.01</span>*np.random.randn(weights.data.shape[<span class="hljs-number">0</span>],weights.data.shape[<span class="hljs-number">1</span>])<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bias.data&nbsp; &nbsp; = <span class="hljs-number">0.</span><font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fit</span>(<span class="hljs-params">self,data,target,batch_size,num_epochs,optimizer,loss_fn</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loss_history = []<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.__innitializeNetwork()<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;data_gen = DataGenerator(data,target,batch_size)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;itr = <span class="hljs-number">0</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(num_epochs):
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> X,Y <span class="hljs-keyword">in</span> data_gen:<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;optimizer.zeroGrad()<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> self.computation_graph: X=f.forward(X)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loss = loss_fn.forward(X,Y)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;grad = loss_fn.backward()<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> self.computation_graph[::<span class="hljs-number">-1</span>]: grad = f.backward(grad)&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loss_history+=[loss]<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(<span class="hljs-string">"Loss at epoch = {} and iteration = {}: {}"</span>.format(epoch,itr,loss_history[<span class="hljs-number">-1</span>]))<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;itr+=<span class="hljs-number">1</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;optimizer.step()<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span> loss_history<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span>(<span class="hljs-params">self,data</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;X = data<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> self.computation_graph: X = f.forward(X)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span> X</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Cette classe comprend les fonctionnalités suivantes:</font></font><br>
<br>
<ul>
<li>  :  <code>add()</code>   ,    .        <code>computation_graph</code>.</li>
<li> : ,   ,       ,    .</li>
<li> :    <code>fit()</code>       .       ,    .</li>
<li>  :  <code>predict()</code>   ,       ,   .</li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Étant donné que cette classe n'est pas la composante de base des systèmes d'apprentissage en profondeur, je l'ai implémentée dans un module distinct </font></font><code>utilities.py</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Notez que la méthode </font></font><code>fit()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">utilise une classe </font></font><code>DataGenerator</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">dont l'implémentation est dans le même module. </font><font style="vertical-align: inherit;">Cette classe est juste un wrapper pour les données de formation et génère des mini-packages pour chaque itération de la formation.</font></font><br>
<br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Formation modèle</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Considérons maintenant le dernier morceau de code dans lequel le modèle de réseau neuronal est formé à l'aide de la bibliothèque décrite ci-dessus. </font><font style="vertical-align: inherit;">Je vais former un réseau multicouche sur des données disposées en spirale. </font><font style="vertical-align: inherit;">J'ai été incité par </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">cette</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> publication. </font><font style="vertical-align: inherit;">Le code pour générer ces données et pour les visualiser se trouve dans le fichier </font></font><code>utilities.py</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/895/eb0/856/895eb085662368f9c1d171153a301fb0.png"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Données avec trois classes disposées en spirale&nbsp;</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
La figure précédente montre la visualisation des données sur lesquelles nous allons former le modèle. </font><font style="vertical-align: inherit;">Ces données sont séparables de manière non linéaire. </font><font style="vertical-align: inherit;">Nous pouvons espérer qu'un réseau avec une couche cachée puisse trouver correctement les frontières de décision non linéaires. </font><font style="vertical-align: inherit;">Si vous assemblez tout ce dont nous avons parlé, vous obtenez le fragment de code suivant qui vous permet de former le modèle:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">import</span> dl_numpy <span class="hljs-keyword">as</span> DL
<span class="hljs-keyword">import</span> utilities<font></font>
<font></font>
batch_size&nbsp; &nbsp; &nbsp; &nbsp; = <span class="hljs-number">20</span>
num_epochs&nbsp; &nbsp; &nbsp; &nbsp; = <span class="hljs-number">200</span>
samples_per_class = <span class="hljs-number">100</span>
num_classes &nbsp; &nbsp; &nbsp; = <span class="hljs-number">3</span>
hidden_units&nbsp; &nbsp; &nbsp; = <span class="hljs-number">100</span><font></font>
data,target &nbsp; &nbsp; &nbsp; = utilities.genSpiralData(samples_per_class,num_classes)<font></font>
model &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = utilities.Model()<font></font>
model.add(DL.Linear(<span class="hljs-number">2</span>,hidden_units))<font></font>
model.add(DL.ReLU())<font></font>
model.add(DL.Linear(hidden_units,num_classes))<font></font>
optim &nbsp; = DL.SGD(model.parameters,lr=<span class="hljs-number">1.0</span>,weight_decay=<span class="hljs-number">0.001</span>,momentum=<span class="hljs-number">.9</span>)<font></font>
loss_fn = DL.SoftmaxWithLoss()<font></font>
model.fit(data,target,batch_size,num_epochs,optim,loss_fn)<font></font>
predicted_labels = np.argmax(model.predict(data),axis=<span class="hljs-number">1</span>)<font></font>
accuracy &nbsp; &nbsp; &nbsp; &nbsp; = np.sum(predicted_labels==target)/len(target)<font></font>
print(<span class="hljs-string">"Model Accuracy = {}"</span>.format(accuracy))<font></font>
utilities.plot2DDataWithDecisionBoundary(data,target,model)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
L'image ci-dessous montre les mêmes données et les limites décisives du modèle entraîné.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/502/b0b/84f/502b0b84fee2c960c918494e8d63e33a.png"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Données et limites de décision du modèle formé</font></font></font></i><br>
 <br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sommaire</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Étant donné la complexité croissante des modèles d'apprentissage en profondeur, il y a une tendance à augmenter les capacités des bibliothèques respectives et à augmenter la quantité de code nécessaire pour implémenter ces capacités. Mais les fonctionnalités les plus élémentaires de ces bibliothèques peuvent toujours être implémentées sous une forme relativement compacte. Bien que la bibliothèque que nous avons créée puisse être utilisée pour la formation de bout en bout de réseaux simples, elle est encore, à bien des égards, limitée. Nous parlons de limitations dans le domaine des capacités qui permettent aux cadres d'apprentissage profond d'être utilisés dans des domaines tels que la vision industrielle, la reconnaissance vocale et textuelle. Ceci, bien sûr, les possibilités de tels cadres ne sont pas limitées. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Je crois que tout le monde peut bifurquer le </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">projet</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, dont nous avons examiné ici le code et, en tant qu'exercice, y introduisons ce qu'ils aimeraient y voir. </font><font style="vertical-align: inherit;">Voici quelques mécanismes que vous pouvez essayer de mettre en œuvre vous-même:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Opérateurs: convolution, sous-échantillonnage.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Optimiseurs: Adam, RMSProp.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Régulateurs: BatchNorm, DropOut.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
J'espère que ce matériel vous a permis au moins de voir du coin de l'œil ce qui se passe dans les entrailles des bibliothèques pour le deep learning. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Chers lecteurs! </font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Quelles bibliothèques d'apprentissage en profondeur utilisez-vous?</font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><img src="https://habrastorage.org/files/1ba/550/d25/1ba550d25e8846ce8805de564da6aa63.png"></a></div>
      
    </div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr486686/">https://habr.com/ru/post/fr486686/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr486676/index.html">Inévitabilité de la pénétration du FPGA dans les centres de données</a></li>
<li><a href="../fr486678/index.html">Quartz dans ASP.NET Core</a></li>
<li><a href="../fr486680/index.html">ML, VR & Robots (et un peu de cloud)</a></li>
<li><a href="../fr486682/index.html">Docker Compose: simplifiez l'utilisation de Makefile</a></li>
<li><a href="../fr486684/index.html">Ma réponse à ceux qui croient que la valeur du TDD est exagérée</a></li>
<li><a href="../fr486688/index.html">Node.js, Tor, Puppeteer et Cheerio: grattage web anonyme</a></li>
<li><a href="../fr486690/index.html">5 conseils pour écrire des fonctions fléchées de qualité</a></li>
<li><a href="../fr486692/index.html">Fonctionnalités de la console Chrome que vous n'avez peut-être jamais utilisées</a></li>
<li><a href="../fr486694/index.html">Nouvelles du monde d'OpenStreetMap n ° 496 (14/01/2020/20/01/2020)</a></li>
<li><a href="../fr486702/index.html">Événements numériques à Moscou du 3 au 9 février</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>