<!doctype html>
<html class="no-js" lang="fr">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üîü üï∫üèº ‚õπüèº √Ä propos de l'impl√©mentation d'une biblioth√®que d'apprentissage en profondeur dans Python ü§∂üèæ üìÉ üë®üèª‚Äçüåæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Les technologies d'apprentissage en profondeur ont parcouru un long chemin en peu de temps - des r√©seaux neuronaux simples aux architectures assez com...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>√Ä propos de l'impl√©mentation d'une biblioth√®que d'apprentissage en profondeur dans Python</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ruvds/blog/486686/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Les technologies d'apprentissage en profondeur ont parcouru un long chemin en peu de temps - des r√©seaux neuronaux simples aux architectures assez complexes. Pour soutenir la diffusion rapide de ces technologies, diverses biblioth√®ques et plates-formes d'apprentissage en profondeur ont √©t√© d√©velopp√©es. L'un des principaux objectifs de ces biblioth√®ques est de fournir aux d√©veloppeurs des interfaces simples pour cr√©er et former des mod√®les de r√©seaux neuronaux. Ces biblioth√®ques permettent √† leurs utilisateurs d'accorder plus d'attention aux t√¢ches √† r√©soudre et non aux subtilit√©s de la mise en ≈ìuvre du mod√®le. Pour ce faire, vous devrez peut-√™tre masquer l'impl√©mentation des m√©canismes de base derri√®re plusieurs niveaux d'abstraction. Et cela, √† son tour, complique la compr√©hension des principes de base sur lesquels reposent les biblioth√®ques d'apprentissage en profondeur.</font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><img src="https://habrastorage.org/webt/bp/yi/sl/bpyislfb1o7e-qh7exvklh1oxuw.jpeg"></a><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
L'article, dont nous publions la traduction, vise √† analyser les caract√©ristiques du dispositif des blocs de construction de bas niveau des biblioth√®ques d'apprentissage en profondeur. </font><font style="vertical-align: inherit;">Tout d'abord, nous parlons bri√®vement de l'essence de l'apprentissage en profondeur. </font><font style="vertical-align: inherit;">Cela nous permettra de comprendre les exigences fonctionnelles du logiciel respectif. </font><font style="vertical-align: inherit;">Ensuite, nous envisageons de d√©velopper une biblioth√®que d'apprentissage en profondeur simple mais fonctionnelle en Python √† l'aide de NumPy. </font><font style="vertical-align: inherit;">Cette biblioth√®que est capable de fournir une formation de bout en bout pour les mod√®les de r√©seau neuronal simples. </font><font style="vertical-align: inherit;">En cours de route, nous parlerons des diff√©rentes composantes des cadres d'apprentissage en profondeur. </font><font style="vertical-align: inherit;">La biblioth√®que que nous allons consid√©rer est assez petite, moins de 100 lignes de code. </font><font style="vertical-align: inherit;">Et cela signifie qu'il sera assez simple de le comprendre. </font><font style="vertical-align: inherit;">Le code de projet complet, dont nous traiterons, se trouve </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ici</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<a name="habracut"></a><br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">informations g√©n√©rales</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En r√®gle g√©n√©rale, les biblioth√®ques d'apprentissage en profondeur (telles que TensorFlow et PyTorch) sont constitu√©es des composants illustr√©s dans la figure suivante.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c49/b9f/396/c49b9f39652c0260a9e30ee4e5dea146.jpg"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Composants du cadre d'apprentissage en profondeur</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Analysons ces composants.</font></font><br>
<br>
<h3><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ñç Op√©rateurs</font></font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Les concepts d '¬´op√©rateur¬ª et de ¬´couche¬ª (couche) sont g√©n√©ralement utilis√©s de mani√®re interchangeable. </font><font style="vertical-align: inherit;">Ce sont les √©l√©ments de base de tout r√©seau neuronal. </font><font style="vertical-align: inherit;">Les op√©rateurs sont des fonctions vectorielles qui transforment les donn√©es. </font><font style="vertical-align: inherit;">Parmi les op√©rateurs fr√©quemment utilis√©s, on peut distinguer comme les couches d'activation lin√©aires et convolutives, les couches de sous-√©chantillonnage (pooling), semi-lin√©aires (ReLU) et sigmo√Ødes (sigmo√Ødes).</font></font><br>
<br>
<h3><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ñçOptimiseurs (optimiseurs)</font></font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Les optimiseurs sont le fondement des biblioth√®ques d'apprentissage en profondeur. </font><font style="vertical-align: inherit;">Ils d√©crivent des m√©thodes d'ajustement des param√®tres du mod√®le en utilisant certains crit√®res et en tenant compte de l'objectif d'optimisation. </font><font style="vertical-align: inherit;">Parmi les optimiseurs bien connus, on peut citer SGD, RMSProp et Adam.</font></font><br>
<br>
<h3><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ñç Fonctions de perte</font></font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Les fonctions de perte sont des expressions math√©matiques analytiques et diff√©renciables qui sont utilis√©es comme substitut √† l'objectif d'optimisation lors de la r√©solution d'un probl√®me. </font><font style="vertical-align: inherit;">Par exemple, la fonction d'entropie crois√©e et la fonction lin√©aire par morceaux sont g√©n√©ralement utilis√©es dans les probl√®mes de classification.</font></font><br>
<br>
<h3><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ñç Initialiseurs</font></font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Les initialiseurs fournissent des valeurs initiales pour les param√®tres du mod√®le. </font><font style="vertical-align: inherit;">Ce sont ces valeurs que les param√®tres ont au d√©but de l'entra√Ænement. </font><font style="vertical-align: inherit;">Les initialiseurs jouent un r√¥le important dans la formation des r√©seaux de neurones, car des param√®tres initiaux infructueux peuvent signifier que le r√©seau apprendra lentement ou peut-√™tre pas du tout. </font><font style="vertical-align: inherit;">Il existe de nombreuses fa√ßons d'initialiser les poids d'un r√©seau neuronal. </font><font style="vertical-align: inherit;">Par exemple, vous pouvez leur attribuer de petites valeurs al√©atoires √† partir de la distribution normale. </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Voici une</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> page o√π vous pourrez d√©couvrir les diff√©rents types d'initialiseurs.</font></font><br>
<br>
<h3><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ñç R√©gularisateurs</font></font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Les r√©gularisateurs sont des outils qui √©vitent le recyclage du r√©seau et aident le r√©seau √† se g√©n√©raliser. Vous pouvez g√©rer le recyclage du r√©seau de mani√®re explicite ou implicite. Les m√©thodes explicites impliquent des limitations structurelles sur les poids. Par exemple, minimiser leurs normes L1 et L2, ce qui, en cons√©quence, rend les valeurs de poids mieux dispers√©es et r√©parties plus uniform√©ment. Les m√©thodes implicites sont repr√©sent√©es par des op√©rateurs sp√©cialis√©s qui effectuent la transformation des repr√©sentations interm√©diaires. Cela se fait soit par une normalisation explicite, par exemple, en utilisant la technique de normalisation des paquets (BatchNorm), soit en modifiant la connectivit√© r√©seau √† l'aide des algorithmes DropOut et DropConnect.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Les composants ci-dessus appartiennent g√©n√©ralement √† la partie interface de la biblioth√®que. </font><font style="vertical-align: inherit;">Ici, par ¬´partie interface¬ª, j'entends les entit√©s avec lesquelles l'utilisateur peut interagir. </font><font style="vertical-align: inherit;">Ils lui donnent des outils pratiques pour concevoir efficacement une architecture de r√©seau neuronal. </font><font style="vertical-align: inherit;">Si nous parlons des m√©canismes internes des biblioth√®ques, ils peuvent fournir un support pour le calcul automatique des gradients de la fonction de perte, en tenant compte de divers param√®tres du mod√®le. </font><font style="vertical-align: inherit;">Cette technique est commun√©ment appel√©e diff√©renciation automatique (AD).</font></font><br>
<br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Diff√©renciation automatique</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Chaque biblioth√®que d'apprentissage en profondeur offre √† l'utilisateur des capacit√©s de diff√©renciation automatique. Cela lui donne l'opportunit√© de se concentrer sur la description de la structure du mod√®le (graphique des calculs) et de transf√©rer la t√¢che de calcul des gradients au module AD. Prenons un exemple qui nous permettra de savoir comment tout cela fonctionne. Supposons que nous voulons calculer les d√©riv√©es partielles de la fonction suivante par rapport √† ses variables d'entr√©e X‚ÇÅ et X‚ÇÇ: </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Y = sin (x‚ÇÅ) + X‚ÇÅ * X‚ÇÇ </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La figure suivante, que j'ai emprunt√©e </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ici</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , montre le graphique des calculs et le calcul des d√©riv√©s √† l'aide d'une r√®gle de cha√Æne.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/605/635/189/605635189c56a2f87927ec1a0c5b6318.png"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Graphique de calcul et calcul des d√©riv√©es par une r√®gle de cha√Æne</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Ce que vous voyez ici est quelque chose comme un ¬´mode inverse¬ª de diff√©renciation automatique. </font><font style="vertical-align: inherit;">L'algorithme bien connu de r√©tro-propagation d'erreur est un cas particulier de l'algorithme d√©crit ci-dessus pour le cas o√π la fonction situ√©e en haut est une fonction de perte. </font><font style="vertical-align: inherit;">AD exploite le fait que toute fonction complexe se compose d'op√©rations arithm√©tiques √©l√©mentaires et de fonctions √©l√©mentaires. </font><font style="vertical-align: inherit;">Par cons√©quent, les d√©riv√©s peuvent √™tre calcul√©s en appliquant une r√®gle de cha√Æne √† ces op√©rations.</font></font><br>
<br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">la mise en oeuvre</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dans la section pr√©c√©dente, nous avons examin√© les composants n√©cessaires √† la cr√©ation d'une biblioth√®que d'apprentissage en profondeur con√ßue pour cr√©er et former de bout en bout des r√©seaux de neurones. Afin de ne pas compliquer l'exemple, </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j'imite</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ici le mod√®le de conception de la biblioth√®que </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;">Caffe</font></a><font style="vertical-align: inherit;"> . Ici, nous d√©clarons deux classes abstraites - </font></font><code>Function</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">et </font></font><code>Optimizer</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. De plus, il existe une classe </font></font><code>Tensor</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, qui est une structure simple contenant deux tableaux NumPy multidimensionnels. L'un d'eux est con√ßu pour stocker les valeurs des param√®tres, l'autre - pour stocker leurs gradients. Tous les param√®tres des diff√©rentes couches (op√©rateurs) seront de type </font></font><code>Tensor</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Avant d'aller plus loin, jetez un ≈ìil au plan g√©n√©ral de la biblioth√®que.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d05/98c/068/d0598c068139ecda1f2aacbd9ea5f068.jpg"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Diagramme UML de la biblioth√®que</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Au moment de la r√©daction de ce document, cette biblioth√®que contient une impl√©mentation de la couche lin√©aire, de la fonction d'activation ReLU, de la couche SoftMaxLoss et de l'optimiseur SGD. En cons√©quence, il s'av√®re que la biblioth√®que peut √™tre utilis√©e pour former des mod√®les de classification compos√©s de couches enti√®rement connect√©es et utilisant une fonction d'activation non lin√©aire. Voyons maintenant quelques d√©tails sur les classes abstraites que nous avons.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Une classe abstraite</font></font><code>Function</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">fournit une interface pour les op√©rateurs. Voici son code:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword">class</span>&nbsp; <span class="hljs-title">Function</span>(<span class="hljs-params">object</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self</span>):</span>&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">raise</span> NotImplementedError<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backward</span>(<span class="hljs-params">self</span>):</span>&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">raise</span> NotImplementedError<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getParams</span>(<span class="hljs-params">self</span>):</span>&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span> []</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Tous les op√©rateurs sont impl√©ment√©s via l'h√©ritage d'une classe abstraite </font></font><code>Function</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Chaque op√©rateur doit fournir une mise en ≈ìuvre des m√©thodes </font></font><code>forward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">et </font></font><code>backward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Les op√©rateurs peuvent contenir une impl√©mentation d'une m√©thode facultative </font></font><code>getParams()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">qui renvoie leurs param√®tres (le cas √©ch√©ant). La m√©thode </font></font><code>forward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">re√ßoit les donn√©es d'entr√©e et renvoie le r√©sultat de leur transformation par l'op√©rateur. De plus, il r√©sout les probl√®mes internes n√©cessaires au calcul des gradients. Le proc√©d√© </font></font><code>backward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">accepte les d√©riv√©es partielles de la fonction de perte par rapport aux sorties de l'op√©rateur et met en ≈ìuvre le calcul des d√©riv√©es partielles de la fonction de perte par rapport aux donn√©es d'entr√©e de l'op√©rateur et aux param√®tres (le cas √©ch√©ant). Notez que la m√©thode</font></font><code>backward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, en substance, offre √† notre biblioth√®que la possibilit√© d'effectuer une diff√©renciation automatique. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Afin de traiter tout cela avec un exemple sp√©cifique, regardons l'impl√©mentation de la fonction </font></font><code>Linear</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Linear</span>(<span class="hljs-params">Function</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,in_nodes,out_nodes</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.weights = Tensor((in_nodes,out_nodes))<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.bias&nbsp; &nbsp; = Tensor((<span class="hljs-number">1</span>,out_nodes))<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.type = <span class="hljs-string">'linear'</span><font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self,x</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;output = np.dot(x,self.weights.data)+self.bias.data<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.input = x&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span> output<font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backward</span>(<span class="hljs-params">self,d_y</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.weights.grad += np.dot(self.input.T,d_y)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.bias.grad&nbsp; &nbsp; += np.sum(d_y,axis=<span class="hljs-number">0</span>,keepdims=<span class="hljs-literal">True</span>)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;grad_input &nbsp; &nbsp; &nbsp; &nbsp; = np.dot(d_y,self.weights.data.T)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span> grad_input<font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getParams</span>(<span class="hljs-params">self</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span> [self.weights,self.bias]</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La m√©thode </font></font><code>forward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">impl√©mente la transformation de la vue </font></font><code>Y = X*W+b</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">et renvoie le r√©sultat. De plus, il enregistre la valeur d'entr√©e </font></font><code>X</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, car il est n√©cessaire de calculer la d√©riv√©e partielle de </font></font><code>dY</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">la fonction de perte par rapport √† la valeur de sortie </font></font><code>Y</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">dans la m√©thode </font></font><code>backward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. La m√©thode </font></font><code>backward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">re√ßoit les d√©riv√©es partielles, calcul√©es par rapport √† la valeur d'entr√©e </font></font><code>X</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">et aux param√®tres </font></font><code>W</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">et </font></font><code>b</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. De plus, il renvoie les d√©riv√©es partielles calcul√©es par rapport √† la valeur d'entr√©e </font></font><code>X</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, qui seront transf√©r√©es √† la couche pr√©c√©dente. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Une classe abstraite </font></font><code>Optimizer</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">fournit une interface pour les optimiseurs:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Optimizer</span>(<span class="hljs-params">object</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,parameters</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.parameters = parameters<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">step</span>(<span class="hljs-params">self</span>):</span>&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">raise</span> NotImplementedError<font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">zeroGrad</span>(<span class="hljs-params">self</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> self.parameters:<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;p.grad = <span class="hljs-number">0.</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Tous les optimiseurs sont impl√©ment√©s en h√©ritant de la classe de base </font></font><code>Optimizer</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Une classe d√©crivant une optimisation particuli√®re devrait fournir une impl√©mentation de la m√©thode </font></font><code>step()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Cette m√©thode met √† jour les param√®tres du mod√®le en utilisant leurs d√©riv√©es partielles calcul√©es par rapport √† la valeur optimis√©e de la fonction de perte. </font><font style="vertical-align: inherit;">Un lien vers divers param√®tres du mod√®le est fourni dans la fonction </font></font><code>__init__()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Veuillez noter que la fonctionnalit√© universelle de r√©initialisation des valeurs de gradient est impl√©ment√©e dans la classe de base elle-m√™me. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Maintenant, pour mieux comprendre tout cela, consid√©rons un exemple sp√©cifique - la mise en ≈ìuvre de l'algorithme de descente de gradient stochastique (SGD) avec prise en charge de l'ajustement de la quantit√© de mouvement et de la r√©duction des poids:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">SGD</span>(<span class="hljs-params">Optimizer</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,parameters,lr=<span class="hljs-number">.001</span>,weight_decay=<span class="hljs-number">0.0</span>,momentum = <span class="hljs-number">.9</span></span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;super().__init__(parameters)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.lr &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = lr<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.weight_decay = weight_decay<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.momentum &nbsp; &nbsp; = momentum<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.velocity &nbsp; &nbsp; = []<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> parameters:<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.velocity.append(np.zeros_like(p.grad))<font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">step</span>(<span class="hljs-params">self</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> p,v <span class="hljs-keyword">in</span> zip(self.parameters,self.velocity):<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v = self.momentum*v+p.grad+self.weight_decay*p.data<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;p.data=p.data-self.lr*v</code></pre><br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La solution au vrai probl√®me</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nous avons maintenant tout le n√©cessaire pour former le mod√®le de r√©seau de neurones (profond) √† l'aide de notre biblioth√®que. </font><font style="vertical-align: inherit;">Pour cela, nous avons besoin des entit√©s suivantes:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mod√®le: graphique de calcul.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Donn√©es et valeur cible: donn√©es pour la formation du r√©seau.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fonction de perte: substitut √† l'objectif d'optimisation.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Optimizer: un m√©canisme de mise √† jour des param√®tres du mod√®le.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Le pseudo-code suivant d√©crit un cycle de test typique:</font></font><br>
<br>
<pre><code class="python hljs">model <span class="hljs-comment"># </span>
data,target <span class="hljs-comment"># </span>
loss_fn <span class="hljs-comment"># </span>
optim <span class="hljs-comment">#,         </span>
Repeat:<span class="hljs-comment">#   ,    ,     </span>
&nbsp;&nbsp;&nbsp;optim.zeroGrad() <span class="hljs-comment">#    </span>
&nbsp;&nbsp;&nbsp;output = model.forward(data) <span class="hljs-comment">#   </span>
&nbsp;&nbsp;&nbsp;loss &nbsp; = loss_fn(output,target) <span class="hljs-comment"># </span>
&nbsp;&nbsp;&nbsp;grad &nbsp; = loss.backward() <span class="hljs-comment">#      </span>
&nbsp;&nbsp;&nbsp;model.backward(grad) <span class="hljs-comment">#    </span>
&nbsp;&nbsp;&nbsp;optim.step() <span class="hljs-comment">#  </span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bien que cela ne soit pas n√©cessaire dans la biblioth√®que d'apprentissage en profondeur, il peut √™tre utile d'inclure les fonctionnalit√©s ci-dessus dans une classe distincte. </font><font style="vertical-align: inherit;">Cela nous permettra de ne pas r√©p√©ter les m√™mes actions lors de l'apprentissage de nouveaux mod√®les (cette id√©e correspond √† la philosophie des abstractions de haut niveau de frameworks comme </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Keras</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ). </font><font style="vertical-align: inherit;">Pour ce faire, d√©clarez une classe </font></font><code>Model</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Model</span>():</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.computation_graph = []<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.parameters&nbsp; &nbsp; &nbsp; &nbsp; = []<font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">add</span>(<span class="hljs-params">self,layer</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.computation_graph.append(layer)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.parameters+=layer.getParams()<font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__innitializeNetwork</span>(<span class="hljs-params">self</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> self.computation_graph:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">if</span> f.type==<span class="hljs-string">'linear'</span>:<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;weights,bias = f.getParams()<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;weights.data = <span class="hljs-number">.01</span>*np.random.randn(weights.data.shape[<span class="hljs-number">0</span>],weights.data.shape[<span class="hljs-number">1</span>])<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bias.data&nbsp; &nbsp; = <span class="hljs-number">0.</span><font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fit</span>(<span class="hljs-params">self,data,target,batch_size,num_epochs,optimizer,loss_fn</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loss_history = []<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.__innitializeNetwork()<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;data_gen = DataGenerator(data,target,batch_size)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;itr = <span class="hljs-number">0</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(num_epochs):
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> X,Y <span class="hljs-keyword">in</span> data_gen:<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;optimizer.zeroGrad()<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> self.computation_graph: X=f.forward(X)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loss = loss_fn.forward(X,Y)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;grad = loss_fn.backward()<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> self.computation_graph[::<span class="hljs-number">-1</span>]: grad = f.backward(grad)&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loss_history+=[loss]<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(<span class="hljs-string">"Loss at epoch = {} and iteration = {}: {}"</span>.format(epoch,itr,loss_history[<span class="hljs-number">-1</span>]))<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;itr+=<span class="hljs-number">1</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;optimizer.step()<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span> loss_history<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span>(<span class="hljs-params">self,data</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;X = data<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> self.computation_graph: X = f.forward(X)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span> X</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Cette classe comprend les fonctionnalit√©s suivantes:</font></font><br>
<br>
<ul>
<li>  :  <code>add()</code>   ,    .        <code>computation_graph</code>.</li>
<li> : ,   ,       ,    .</li>
<li> :    <code>fit()</code>       .       ,    .</li>
<li>  :  <code>predict()</code>   ,       ,   .</li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
√âtant donn√© que cette classe n'est pas la composante de base des syst√®mes d'apprentissage en profondeur, je l'ai impl√©ment√©e dans un module distinct </font></font><code>utilities.py</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Notez que la m√©thode </font></font><code>fit()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">utilise une classe </font></font><code>DataGenerator</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">dont l'impl√©mentation est dans le m√™me module. </font><font style="vertical-align: inherit;">Cette classe est juste un wrapper pour les donn√©es de formation et g√©n√®re des mini-packages pour chaque it√©ration de la formation.</font></font><br>
<br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Formation mod√®le</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Consid√©rons maintenant le dernier morceau de code dans lequel le mod√®le de r√©seau neuronal est form√© √† l'aide de la biblioth√®que d√©crite ci-dessus. </font><font style="vertical-align: inherit;">Je vais former un r√©seau multicouche sur des donn√©es dispos√©es en spirale. </font><font style="vertical-align: inherit;">J'ai √©t√© incit√© par </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">cette</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> publication. </font><font style="vertical-align: inherit;">Le code pour g√©n√©rer ces donn√©es et pour les visualiser se trouve dans le fichier </font></font><code>utilities.py</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/895/eb0/856/895eb085662368f9c1d171153a301fb0.png"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Donn√©es avec trois classes dispos√©es en spirale&nbsp;</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
La figure pr√©c√©dente montre la visualisation des donn√©es sur lesquelles nous allons former le mod√®le. </font><font style="vertical-align: inherit;">Ces donn√©es sont s√©parables de mani√®re non lin√©aire. </font><font style="vertical-align: inherit;">Nous pouvons esp√©rer qu'un r√©seau avec une couche cach√©e puisse trouver correctement les fronti√®res de d√©cision non lin√©aires. </font><font style="vertical-align: inherit;">Si vous assemblez tout ce dont nous avons parl√©, vous obtenez le fragment de code suivant qui vous permet de former le mod√®le:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">import</span> dl_numpy <span class="hljs-keyword">as</span> DL
<span class="hljs-keyword">import</span> utilities<font></font>
<font></font>
batch_size&nbsp; &nbsp; &nbsp; &nbsp; = <span class="hljs-number">20</span>
num_epochs&nbsp; &nbsp; &nbsp; &nbsp; = <span class="hljs-number">200</span>
samples_per_class = <span class="hljs-number">100</span>
num_classes &nbsp; &nbsp; &nbsp; = <span class="hljs-number">3</span>
hidden_units&nbsp; &nbsp; &nbsp; = <span class="hljs-number">100</span><font></font>
data,target &nbsp; &nbsp; &nbsp; = utilities.genSpiralData(samples_per_class,num_classes)<font></font>
model &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = utilities.Model()<font></font>
model.add(DL.Linear(<span class="hljs-number">2</span>,hidden_units))<font></font>
model.add(DL.ReLU())<font></font>
model.add(DL.Linear(hidden_units,num_classes))<font></font>
optim &nbsp; = DL.SGD(model.parameters,lr=<span class="hljs-number">1.0</span>,weight_decay=<span class="hljs-number">0.001</span>,momentum=<span class="hljs-number">.9</span>)<font></font>
loss_fn = DL.SoftmaxWithLoss()<font></font>
model.fit(data,target,batch_size,num_epochs,optim,loss_fn)<font></font>
predicted_labels = np.argmax(model.predict(data),axis=<span class="hljs-number">1</span>)<font></font>
accuracy &nbsp; &nbsp; &nbsp; &nbsp; = np.sum(predicted_labels==target)/len(target)<font></font>
print(<span class="hljs-string">"Model Accuracy = {}"</span>.format(accuracy))<font></font>
utilities.plot2DDataWithDecisionBoundary(data,target,model)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
L'image ci-dessous montre les m√™mes donn√©es et les limites d√©cisives du mod√®le entra√Æn√©.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/502/b0b/84f/502b0b84fee2c960c918494e8d63e33a.png"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Donn√©es et limites de d√©cision du mod√®le form√©</font></font></font></i><br>
 <br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sommaire</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
√âtant donn√© la complexit√© croissante des mod√®les d'apprentissage en profondeur, il y a une tendance √† augmenter les capacit√©s des biblioth√®ques respectives et √† augmenter la quantit√© de code n√©cessaire pour impl√©menter ces capacit√©s. Mais les fonctionnalit√©s les plus √©l√©mentaires de ces biblioth√®ques peuvent toujours √™tre impl√©ment√©es sous une forme relativement compacte. Bien que la biblioth√®que que nous avons cr√©√©e puisse √™tre utilis√©e pour la formation de bout en bout de r√©seaux simples, elle est encore, √† bien des √©gards, limit√©e. Nous parlons de limitations dans le domaine des capacit√©s qui permettent aux cadres d'apprentissage profond d'√™tre utilis√©s dans des domaines tels que la vision industrielle, la reconnaissance vocale et textuelle. Ceci, bien s√ªr, les possibilit√©s de tels cadres ne sont pas limit√©es. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Je crois que tout le monde peut bifurquer le </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">projet</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, dont nous avons examin√© ici le code et, en tant qu'exercice, y introduisons ce qu'ils aimeraient y voir. </font><font style="vertical-align: inherit;">Voici quelques m√©canismes que vous pouvez essayer de mettre en ≈ìuvre vous-m√™me:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Op√©rateurs: convolution, sous-√©chantillonnage.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Optimiseurs: Adam, RMSProp.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">R√©gulateurs: BatchNorm, DropOut.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
J'esp√®re que ce mat√©riel vous a permis au moins de voir du coin de l'≈ìil ce qui se passe dans les entrailles des biblioth√®ques pour le deep learning. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Chers lecteurs! </font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Quelles biblioth√®ques d'apprentissage en profondeur utilisez-vous?</font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><img src="https://habrastorage.org/files/1ba/550/d25/1ba550d25e8846ce8805de564da6aa63.png"></a></div>
      
    </div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr486686/">https://habr.com/ru/post/fr486686/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr486676/index.html">In√©vitabilit√© de la p√©n√©tration du FPGA dans les centres de donn√©es</a></li>
<li><a href="../fr486678/index.html">Quartz dans ASP.NET Core</a></li>
<li><a href="../fr486680/index.html">ML, VR & Robots (et un peu de cloud)</a></li>
<li><a href="../fr486682/index.html">Docker Compose: simplifiez l'utilisation de Makefile</a></li>
<li><a href="../fr486684/index.html">Ma r√©ponse √† ceux qui croient que la valeur du TDD est exag√©r√©e</a></li>
<li><a href="../fr486688/index.html">Node.js, Tor, Puppeteer et Cheerio: grattage web anonyme</a></li>
<li><a href="../fr486690/index.html">5 conseils pour √©crire des fonctions fl√©ch√©es de qualit√©</a></li>
<li><a href="../fr486692/index.html">Fonctionnalit√©s de la console Chrome que vous n'avez peut-√™tre jamais utilis√©es</a></li>
<li><a href="../fr486694/index.html">Nouvelles du monde d'OpenStreetMap n ¬∞ 496 (14/01/2020/20/01/2020)</a></li>
<li><a href="../fr486702/index.html">√âv√©nements num√©riques √† Moscou du 3 au 9 f√©vrier</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>