<!doctype html>
<html class="no-js" lang="pt">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🗽 🙅🏾 ☝🏿 Como reconhecemos equipamentos de proteção individual 👨🏻‍🔧 👨🏿‍🚀 ⬇️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Provavelmente, você sempre se perguntou como treinar uma rede neural para reconhecer pessoas de capacete e colete laranja! Não? Mas nós lhe contaremos...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Como reconhecemos equipamentos de proteção individual</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/center2m/blog/499248/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Provavelmente, você sempre se perguntou como treinar uma rede neural para reconhecer pessoas de capacete e colete laranja! Não? Mas nós lhe contaremos assim mesmo. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nosso nome é Tatyana Voronova e Elvira Dyaminova. Estamos envolvidos na análise de dados na empresa Center 2M, trabalhamos muito com as fábricas e empresas mais reais. Devido a violações de segurança, elas sofrem perdas multimilionárias, os funcionários ficam feridos, por isso seria bom poder detectar essas violações sistematicamente e o mais cedo possível. O melhor de tudo - automaticamente. Portanto, temos problemas associados ao reconhecimento de equipamentos de proteção individual (EPI) em vídeo e à identificação de pessoas ou equipamentos na zona de perigo.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/gv/2u/-q/gv2u-qjuorn0awf-hlwjbuu3dc4.jpeg" alt="imagem"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Na maioria das vezes, chegam-nos pedidos para determinar capacetes (mais precisamente, sua ausência) e roupas de trabalho. </font><font style="vertical-align: inherit;">Já adquirimos experiência na execução de tais tarefas e agora podemos descrever os problemas que encontramos e como resolvê-los.</font></font><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Como, nos termos da cooperação, não temos o direito de publicar imagens dos objetos do cliente, ilustraremos o artigo com imagens da Internet, nas quais as pessoas de capacete geralmente sorriem e ficam ótimas. </font><font style="vertical-align: inherit;">Infelizmente, no domínio público, não para todos os recursos das tarefas que enfrentamos na realidade, você pode encontrar bons exemplos. </font><font style="vertical-align: inherit;">Em particular, na vida as pessoas de capacete têm menos probabilidade de sorrir, e o problema dos trabalhadores carecas (falaremos sobre isso um pouco mais tarde) na Internet ainda não foi revelado! </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Imagem da Internet (tamanho 1920x1280):</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/f9/9q/nb/f99qnblemmbik0caykgir0zfiag.jpeg" alt="imagem"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O reconhecimento de EPI pode ser reduzido a um dos dois problemas clássicos da visão computacional: classificação de imagens e detecção de objetos. Na prática, descobriu-se que era melhor não usar uma dessas abordagens, mas escolher a mais adequada para cada caso específico, além de combiná-las de maneira flexível. Por exemplo, podemos primeiro determinar onde as pessoas estão na imagem, depois classificar as imagens cortadas por silhueta nas classes "em roupas de trabalho" e "sem" e detectar a presença de um capacete pela segunda passagem. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nas figuras pré-cortadas de pessoas, a classificação da presença de capacetes e macacões se parece com esta (vista da figura original): </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O resultado do trabalho dos modelos para a classificação de macacões e capacetes</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/su/ad/lg/suadlgvrpviurbpxflbg6y3bm8s.jpeg" alt="imagem"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nas mesmas figuras humanas previamente selecionadas, a aplicação da abordagem desta vez com detecção de capacetes. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O resultado do modelo para a classificação de vestuário de trabalho e um modelo para detectar capacetes:</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/qj/jb/8l/qjjb8lebwt3t-6xttueihk2gaqw.jpeg" alt="imagem"></div><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Etapa um: detecção humana</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A qualidade da definição de objetos pequenos (capacetes / óculos / luvas) em armações grandes é mais ou menos. É muito mais fácil para um computador, como uma pessoa, primeiro entender onde as pessoas estão e só então descobrir o que está vestindo. Então, tudo começa com a identificação das pessoas no quadro. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Como resultado das experiências, descobrimos que a rede neural Faster R-CNN com o Inception v2 como uma extração de recursos é adequada para detectar pessoas. O TensorFlow já possui </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">redes neurais pré-</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> treinadas </font><font style="vertical-align: inherit;">para detectar objetos. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para nós, o Faster R-CNN Inception v2 (treinado no conjunto de dados COCO) é o método básico que tentamos primeiro ao resolver esses problemas. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Inicialmente, detectamos pessoas no quadro (e depois nas pessoas encontradas encontramos EPI):</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/uk/xp/gy/ukxpgyllgs-olfg9xgbpmzkuwly.jpeg" alt="imagem"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Observe que aumentamos a caixa delimitadora "com uma pessoa" ao longo </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">do eixo y</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> :</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/ad/8x/dp/ad8xdpfnx8f1fke9afg9u5q-9xs.jpeg" alt="imagem"></div><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nesta fotografia, o trabalhador foi baleado com boa luz e em um fundo contrastante (com imagens encontradas na Internet, isso acontece o tempo todo). </font><font style="vertical-align: inherit;">Portanto, a caixa delimitadora com a pessoa foi bem construída. </font><font style="vertical-align: inherit;">No entanto, em nossa prática, existem casos frequentes (especialmente em condições de visibilidade insuficiente) quando o modelo de detecção corta um capacete em uma pessoa, após o que é inútil procurá-lo em uma imagem cortada. </font><font style="vertical-align: inherit;">Nesse sentido, ao longo do eixo y, aumentamos a caixa delimitadora prevista em 15% antes de passar para o segundo estágio.</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ao detectar pessoas, encontramos pequenos problemas desagradáveis. Em primeiro lugar, quando duas pessoas andam ou ficam atrás uma da outra, geralmente começam a ser detectadas como uma pessoa. Em segundo lugar, acontece que um objeto estático entra no campo de visão da câmera, no qual o modelo pode reconhecer uma pessoa, como um hidrante. Esses problemas podem ser resolvidos de várias maneiras. Por exemplo, como fizemos: reconcilie-os e aceite-os, pois, em geral, o modelo é adequado para nós em termos de produtividade e qualidade. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Um problema mais fundamental é que as instalações industriais nas quais existe uma “zona de perigo” costumam ser enormes e, consequentemente, as pessoas nos quadros são muito pequenas. Nosso método básico baseado no Faster R-CNN Inception v2 mostrou resultados ruins nesses casos e, no final, tentamos</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nas R-CNN mais rápidas</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Os resultados foram impressionantes, as pessoas foram bem reconhecidas mesmo à distância, mas a velocidade foi muito menor que o modelo básico. </font><font style="vertical-align: inherit;">Com recursos suficientes e a necessidade de alta precisão, você pode usar o </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Faster R-CNN Nas</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Segunda etapa: determinação de violadores maliciosos</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dependendo da tarefa, o seguinte é frequentemente usado:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Modelo de Classificação de Imagem - Iniciação v3</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Modelo de Detecção de Objetos - Iniciação R-CNN mais rápida v2</font></font></li>
</ul><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Classificação de vestuário de trabalho e capacetes</font></font></h3> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Testamos diferentes arquiteturas de rede neural para classificar imagens e, finalmente, decidimos pelo Inception v3, decidindo aproveitar o fato de que ele foi projetado para funcionar com tamanhos de imagem variáveis. Já tínhamos muitas fotos recortadas com as pessoas e não era difícil calcular os valores medianos para altura e largura. Então chegamos à conclusão de que, para o treinamento dos classificadores, começamos a trazer imagens para um tamanho de 150x400.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para treinar a rede para reconhecer o EPI, primeiro é necessário coletar um conjunto de dados a partir de exemplos rotulados. </font><font style="vertical-align: inherit;">Nesse processo, existem sutilezas, cuja realização vem com a experiência. </font><font style="vertical-align: inherit;">Por exemplo, é melhor remover as pessoas cortadas acima dos quadris do conjunto de dados. </font><font style="vertical-align: inherit;">Isso aproximará o conjunto de dados das condições reais, pois na maioria das vezes as pessoas são vistas em altura total em vídeo das câmeras de vigilância. </font><font style="vertical-align: inherit;">Casos de sobreposição, é claro, também acontecem, mas silhuetas completas para a amostra-alvo são muito mais características. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Exemplos do nosso conjunto de dados de vestuário de trabalho:</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/k8/22/sh/k822shy7m4ddtteqqjp1mfs09b8.jpeg" alt="imagem"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Não inventamos nada específico como métrica; usamos recall e precisão. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Modelo para classificar a presença / ausência de roupas de trabalho: </font></font><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">resultados em uma amostra de validação</font></font></i><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/lw/ql/hu/lwqlhu_7-efmfxzrxug-d64pafq.jpeg" alt="imagem"></div><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Detecção de EPI</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O modelo de classificação funciona mais rápido que o modelo para detectar objetos, mas devido ao fato de os óculos e luvas de segurança serem pequenos na imagem, é difícil criar um bom classificador para esses EPIs. </font><font style="vertical-align: inherit;">Portanto, treinamos a rede neural Faster R-CNN em um conjunto de dados com seis classes:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">glasses / not_glasses</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">luvas / não_ luvas</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">capacete / not_helmet</font></font></li>
</ul><br>
<img src="https://habrastorage.org/webt/pw/iy/i_/pwiyi_nep5v7k8wlrz6yxvz8ug4.jpeg" alt="imagem"><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Coleta e marcação de dados</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Os principais problemas estavam relacionados ao conjunto de dados dos capacetes. Era uma maneira fascinante: passamos por pessoas carecas, com capacetes nas mãos e até mesmo por pessoas carecas com capacetes nas mãos. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Como no começo da jornada não tínhamos muitos quadros de condições reais, coletamos o conjunto de dados da melhor maneira possível: filmamos, tiramos fotos da Internet ou de locais de construção. Um pouco mais tarde, começamos a receber muitos vídeos de várias empresas e começamos a enriquecer o conjunto de dados apenas com quadros de condições reais. Em algum momento, o número de imagens marcadas excedeu 5k e a qualidade da adição de novos exemplos deixou de melhorar. Nesse sentido, revisamos a abordagem da marcação.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Vamos descrever os estágios de melhoria do conjunto de dados do capacete usando o exemplo de imagens da Internet, para que o ângulo e a qualidade não correspondam exatamente ao que tínhamos. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Além da imagem acima, cortada acima dos quadris, removemos imagens nas quais os capacetes são cortados mais da metade para evitar confusão com as tampas.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/dl/zh/eu/dlzheusrhwz0grtyrxdxalkkupq.jpeg" alt="imagem"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Também enfrentamos o fato de que, se uma pessoa tem um capacete nas mãos, muitas vezes o modelo não vê violações: existe um capacete? Há sim. Portanto, removemos do conjunto de dados de treinamento todos os quadros em que uma pessoa segura um capacete com a mão, mesmo que o capacete esteja em sua cabeça naquele momento. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Em geral, tentamos remover imagens com um fundo iluminado ou em salas escuras e minimizamos o número de fotos tiradas por nós, deixando principalmente imagens da produção. Como resultado, reduzimos o conjunto de dados pela metade. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Além disso, enriquecemos o conjunto de dados com pessoas carecas, caso contrário, elas sempre estarão em capacetes, mesmo que não seja assim, e com loiras com quadrados, para as quais, com um determinado ângulo, o detector também determina o capacete.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Após remover imagens inadequadas, prosseguimos diretamente para a marcação (para detecção de objetos). Acabou não sendo tão simples. Acontece que a qualidade do detector final depende em grande parte do que exatamente a área da imagem é marcada como "capacete" ou "luvas". Inicialmente, alocamos capacetes e óculos de proteção sem agarrar rostos e luvas com as mãos seguras. No entanto, com a experiência, aprimoramos gradualmente nossa abordagem, observando erros do primeiro e do segundo tipo, onde as pessoas seguram capacetes nas mãos e algo em torno de algo longo acaba sendo uma "luva". Agora, ao marcar capacetes e óculos, tentamos agarrar o rosto até a ponta do nariz e, ao contrário, ao marcar luvas, nos limitamos a um pincel.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/ka/us/re/kausredcadkqks_cn012broh2qq.jpeg" alt="imagem"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Como resultado de nossas manipulações no conjunto de dados, obtivemos os seguintes resultados. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Modelo para detectar a presença / ausência de EPI usando capacetes como exemplo: </font></font><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Resultados em uma amostra de validação antes do início do "trabalho global" no conjunto de dados</font></font></i><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/oo/rx/k-/oorxk-v05lijjyljtrp5lvtbsq4.jpeg" alt="imagem"></div><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Resultados finais na amostra de validação</font></font></i><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/y1/nn/0u/y1nn0uy66v5qmwji-zk2rwotnle.jpeg" alt="imagem"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A completude do reconhecimento de capacetes diminuiu um pouco, mas ao mesmo tempo, as métricas para detectar violações melhoraram, e é isso que queríamos alcançar. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Modelo para classificar a presença / ausência de capacetes: </font></font><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Resultados em uma amostra de validação antes do início do "trabalho global" no conjunto de dados</font></font></i><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/wi/eu/fp/wieufpwnxltfvgf1kvkvisr3fk4.jpeg" alt="imagem"></div><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Resultados finais na amostra de validação</font></font></i><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/et/ct/d1/etctd1ymh_9jmjvc3lgaz_mw47o.jpeg" alt="imagem"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Deve-se notar que não temos uma divisão em óculos e óculos para visão, eles ficam sob a mesma etiqueta “óculos” e luvas de tons claros podem ser percebidas como uma escova nua. </font><font style="vertical-align: inherit;">Tentamos maximizar a gama de cores de capacetes e roupas de trabalho em nossos conjuntos de dados, mas, para garantir a confiabilidade, adicionamos a técnica mais simples e mais confiável: se necessário, para detectar luvas, informamos aos clientes que cores brilhantes ajudam a aumentar a precisão. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
No momento, temos modelos universais que usamos para o show inicial para o cliente. </font><font style="vertical-align: inherit;">No entanto, deve-se entender que é impossível criar um modelo universal para todos, é necessário adaptar-se a cada cliente, identificar e levar em conta novas nuances, enriquecer conjuntos de dados ou criá-los novamente para atender a requisitos específicos.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/kl/-l/9t/kl-l9tfsjdr528i4xgl5flmtqjk.jpeg" alt="imagem"><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bônus</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Normalmente, os clientes desejam processar o maior número possível de câmeras, usando o mínimo de recursos possível. </font><font style="vertical-align: inherit;">Butch, é claro, é uma coisa boa, mas truques adicionais para otimizar o processo não são proibidos. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Por exemplo, meus colegas e eu do centro de clientes IBM em Moscou tínhamos a hipótese de que reunir várias pessoas cortadas para detectar mais capacetes aumentaria o número de câmeras por servidor com uma perda sem precisão de precisão. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Como base, decidimos usar o tamanho de 1000x600 para a tela na qual as pessoas serão "aplicadas". </font><font style="vertical-align: inherit;">Duas opções de layout foram inicialmente consideradas:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Largura e altura fixas (200x600), com essa abordagem, há 5 pessoas no quadro.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Largura e altura fixas (125x600), 8 pessoas.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Essa decisão se deve ao fato de que, com dados fixos, sabemos exatamente o número de pessoas na foto, o que nos fornece uma previsão da carga. </font><font style="vertical-align: inherit;">No entanto, durante o desenvolvimento, consideramos essa opção:</font></font><br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Altura fixa e largura proporcional (*** x600), número diferente de pessoas.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Supunha-se que, com tamanhos crescentes e manutenção de proporções, os resultados serão melhores em comparação com outras opções de layout. </font><font style="vertical-align: inherit;">O número de pessoas variou de 3 a 5 (+/–). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Como resultado, obtivemos que a opção com largura e altura fixas (200x600) é a melhor dentre as consideradas. </font><font style="vertical-align: inherit;">Obviamente, este método não é adequado para detectar óculos e luvas, porque os objetos são pequenos, mas para detectar capacetes / falta de capacetes, esse método mostrou bons resultados. </font></font><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Por exemplo, em uma amostra de validação:</font></font></i><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/ar/wg/zy/arwgzypgxzuk6ykdrj57p851-co.jpeg" alt="imagem"></div><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/ii/lw/y5/iilwy5ihzrc57ezgdnjp3gs646s.jpeg" alt="imagem"></div><br>
<i> :   (<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" class="user_link">tvoronova</a>),  (<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" class="user_link">elviraa</a>)</i></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt499238/index.html">Solda em casa: gravação em fluxo</a></li>
<li><a href="../pt499240/index.html">Através de espinhos para as estrelas, ou análise de dados nos assuntos do céu</a></li>
<li><a href="../pt499242/index.html">Pesquisadores transmitiram dados de um PC de mesa através de vibrações em uma mesa</a></li>
<li><a href="../pt499244/index.html">Organizações sinérgicas. parte II</a></li>
<li><a href="../pt499246/index.html">Investigação da função logística como lei do desenvolvimento da indústria</a></li>
<li><a href="../pt499252/index.html">Criando um jogo de corrida pseudo-tridimensional</a></li>
<li><a href="../pt499254/index.html">Membro do Comitê de Programa PyConRu 2020 responde a perguntas sobre Python: uma aparência atualizada e um pouco de parseltang</a></li>
<li><a href="../pt499262/index.html">Hackathon final online para SMZhack autônomo: projetos que atingirão as pessoas</a></li>
<li><a href="../pt499268/index.html">Consciência espacial: o que os óculos Hololens podem fazer?</a></li>
<li><a href="../pt499272/index.html">Пайка компонентов 0201. Слабонервных просьба удалиться от экранов</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>