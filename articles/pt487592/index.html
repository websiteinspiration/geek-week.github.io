<!doctype html>
<html class="no-js" lang="pt">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üìû üöû ‚öïÔ∏è VMware vSAN 6.7 - E o trov√£o atingiu ü§¥üèæ üíÖ üé≠</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="O ano de 2018 estava terminando ... 
 
 Uma vez, em um claro dia de dezembro, nossa empresa decidiu comprar um novo hardware. N√£o, √© claro, isso n√£o a...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>VMware vSAN 6.7 - E o trov√£o atingiu</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/487592/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O ano de 2018 estava terminando ... </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Uma vez, em um claro dia de dezembro, nossa empresa decidiu comprar um novo hardware. N√£o, √© claro, isso n√£o aconteceu da noite para o dia. A decis√£o foi tomada anteriormente. Muito mais cedo. Mas, como sempre, nem sempre nossos desejos coincidem com as capacidades dos acionistas. E n√£o havia dinheiro, e continuamos. Mas, finalmente, esse momento de alegria chegou quando a aquisi√ß√£o foi aprovada em todos os n√≠veis. Tudo estava bem, os trabalhadores de colarinho branco aplaudiam alegremente, estavam cansados ‚Äã‚Äãde processar documentos durante 25 horas por m√™s em servidores de 7 anos e pediam persistentemente ao Departamento de TI que criasse algo para lhes dar mais tempo para outras coisas igualmente importantes .</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Prometemos reduzir o tempo de processamento de documentos em 3 vezes, em at√© 8 horas. </font><font style="vertical-align: inherit;">Para isso, um pardal foi disparado de um canh√£o. </font><font style="vertical-align: inherit;">Essa op√ß√£o parecia a √∫nica, j√° que nossa equipe n√£o tinha e nunca teve um administrador de banco de dados para aplicar todos os tipos de otimiza√ß√£o de consulta (DBA).</font></font><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A configura√ß√£o do equipamento selecionado era, obviamente, muito alta. </font><font style="vertical-align: inherit;">Estes eram tr√™s servidores da empresa </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">HPE</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - DL560 Gen10. </font><font style="vertical-align: inherit;">Cada um deles possu√≠a 4 processadores Intel Xeon Platinum 8164 2.0Ghz com 26 n√∫cleos, 256 DDR4 RAM, al√©m de 8 SSD de 800Gb SAS (800Gb WD Ultrastar DC SS530 WUSTR6480ASS204 SSD) + 8 1.92Tb SSD (Western Digital Ultrastar DC SS530 )</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Esses "peda√ßos de ferro" foram projetados para o cluster VMware (HA + DRS + vSAN). Que trabalha conosco h√° quase 3 anos em servidores similares da 7¬™ e 8¬™ gera√ß√µes, tamb√©m da HPE. A prop√≥sito, n√£o houve problemas at√© a HPE se recusar a dar suporte a eles e atualizar o ESXi da vers√£o 6.0, mesmo para a 6.5, sem um pandeiro. Bem, ok, como resultado, foi poss√≠vel atualizar. Alterando a imagem de instala√ß√£o, removendo m√≥dulos de problemas incompat√≠veis da imagem de instala√ß√£o, etc. Isso tamb√©m adicionou combust√≠vel ao fogo de nosso desejo de combinar tudo de novo. Obviamente, se n√£o fossem os novos "truques" do vSAN, no caix√£o vimos uma atualiza√ß√£o de todo o sistema da vers√£o 6.0 para o mais recente, e n√£o seria necess√°rio escrever um artigo, mas n√£o estamos procurando maneiras f√°ceis ...</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ent√£o, compramos esse equipamento e decidimos substituir o antigo obsoleto. Aplicamos o √∫ltimo </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SPP</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> a cada novo servidor, instalado em cada uma delas duas placas de rede Ethernet 10G (uma para redes de usu√°rios e a segunda para SAN, 656596-B21 HP Ethernet 10Gb 2 portas 530T). Sim, cada novo servidor veio com uma placa de rede SFP + sem m√≥dulos, mas nossa infraestrutura de rede implicava em Ethernet (duas pilhas de comutadores DELL 4032N para redes LAN e SAN), e o distribuidor da HP em Moscou n√£o possu√≠a m√≥dulos HPE 813874-B21 e n√≥s eles n√£o esperaram.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Quando chegou a hora de instalar o ESXi e incorporar novos n√≥s em um data center comum da VMware, ocorreu um "milagre". Como se viu, o HPE ESXi Custom ISO vers√£o 6.5 e inferior n√£o foi projetado para ser instalado nos novos servidores Gen10. Apenas hardcore, apenas 6.7. E tivemos que seguir inconscientemente os preceitos da "empresa virtual". </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Um novo cluster HA + DRS foi criado, um cluster vSAN, tudo em estrita conformidade com o VMware HCL e </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">este documento</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Tudo foi configurado de acordo com o Feng Shui e apenas "alarmes" peri√≥dicos eram suspeitos no monitoramento do vSAN sobre valores de par√¢metros diferentes de zero nesta se√ß√£o:</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/800/65a/ff5/80065aff58945762f1660b556d61216e.png" alt="imagem"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
N√≥s, com toda a tranquilidade, transferimos todas as m√°quinas virtuais (cerca de 50 pe√ßas) para novos servidores e para o novo armazenamento vSAN, que j√° foi constru√≠do em discos SSD, verificamos o desempenho do processamento de documentos no novo ambiente (a prop√≥sito, ele economizou muito mais tempo do que o planejado) . At√© a base mais pesada ser transferida para o novo cluster, a opera√ß√£o, mencionada no in√≠cio do artigo, levava cerca de 4 horas em vez de 25! Essa foi uma contribui√ß√£o significativa para o clima de Ano Novo de todos os participantes do processo. Alguns provavelmente come√ßaram a sonhar com um pr√™mio. Ent√£o todos sa√≠ram felizes para o feriado de Ano Novo.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Quando os dias da semana do novo ano de 2019 come√ßaram, nada pressagiava uma cat√°strofe. Todos os servi√ßos, transferidos para novas capacidades, sem exageros, decolaram! Somente eventos na se√ß√£o de ressincroniza√ß√£o de objetos se tornaram muito mais. E depois de algumas semanas, aconteceu um problema. No in√≠cio da manh√£, quase todos os principais servi√ßos da empresa (1s, MSSQL, SMB, Exchange etc.) pararam de responder ou come√ßaram a responder com um longo atraso. Toda a infraestrutura mergulhou em um caos completo, e ningu√©m sabia o que havia acontecido e o que fazer. Todas as m√°quinas virtuais no vCenter pareciam "verdes", n√£o havia erros no monitoramento. Reiniciar n√£o ajudou. Al√©m disso, ap√≥s uma reinicializa√ß√£o, algumas m√°quinas n√£o podiam nem inicializar, exibindo v√°rios erros de processo no console. O inferno parecia chegar at√© n√≥s e o diabo estava esfregando as m√£os em antecipa√ß√£o.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Sob press√£o de um estresse s√©rio, foi poss√≠vel determinar a fonte do desastre. Esse problema acabou sendo o armazenamento distribu√≠do vSAN. Ocorreu uma corrup√ß√£o de dados n√£o controlada em discos de m√°quinas virtuais, √† primeira vista - sem motivo. Naquela √©poca, a √∫nica solu√ß√£o que parecia racional era entrar em contato com o suporte t√©cnico da VMware com gritos: SOS, salve-ajuda! </font></font><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
E essa decis√£o, posteriormente, salvou a Empresa da perda de dados relevantes, incluindo caixas de correio de funcion√°rios, bancos de dados e arquivos compartilhados. Juntos, estamos falando de mais de 30 terabytes de informa√ß√£o.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ele √© obrigado a prestar homenagem √† equipe de suporte da VMware que n√£o "jogou futebol" com o titular da assinatura b√°sica de suporte t√©cnico, mas incluiu esse caso no segmento Enterpise e o processo girava 24 horas por dia. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O que aconteceu depois:</font></font><br>
<br>
<ol>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O suporte t√©cnico da VMware colocou duas quest√µes principais: como recuperar dados e como resolver o problema de corrup√ß√£o de dados "fantasma" em discos de m√°quina virtual no cluster de combate "vSAN". </font><font style="vertical-align: inherit;">A prop√≥sito, os dados n√£o estavam em lugar algum para se recuperar, uma vez que o armazenamento adicional era ocupado por c√≥pias de backup e simplesmente n√£o havia lugar para implantar servi√ßos de ‚Äúcombate‚Äù.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Enquanto eu, juntamente com a VMware, tentava reunir os objetos "danificados" no cluster vSAN, meus colegas exploraram urgentemente um novo armazenamento que poderia acomodar todos os mais de 30 terabytes de dados da empresa.</font></font></li>
<li>  ,   ,     VMware      ,           ,     ¬´¬ª      - -   .         , ? </li>
<li>        .</li>
<li> ,  ¬´ ¬ª   .</li>
<li>          ,   ,   ¬´¬ª        .</li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Eu tive que sacrificar temporariamente (por alguns dias) a efici√™ncia do correio, por uma quest√£o de 6 terabytes adicionais de espa√ßo livre na loja, para lan√ßar os principais servi√ßos dos quais dependia a receita da empresa.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Milhares de linhas de bate-papo com colegas de l√≠ngua inglesa da VMware foram salvas "para mem√≥ria", eis um pequeno trecho de nossas conversas:</font></font></li>
</ol><br>
<pre><code class="plaintext">I understood that you are now migrating all the VMs out of vSAN datastore.<font></font>
May I know, how the migration task is going on.? How many VMs left and how much time is expected to migrate the remaining VMs. ?<font></font>
There are 6 vms still need to be migrated. 1 of them is fail so far.<font></font>
How much time is expected to complete the migration for the working VMs..?<font></font>
I think atleast 2-3 hours<font></font>
ok<font></font>
Can you please SSH to vCenter server ?<font></font>
you on it<font></font>
/localhost/Datacenter ###CLUB/computers/###Cluster&gt; vsan.check_state .<font></font>
2019-02-02 05:22:34 +0300: Step 1: Check for inaccessible vSAN objects<font></font>
Detected 3 objects to be inaccessible<font></font>
Detected 7aa2265c-6e46-2f49-df40-20677c0084e0 on esxi-dl560-gen10-2.####.lan to be inaccessible<font></font>
Detected 99c3515c-bee0-9faa-1f13-20677c038dd8 on esxi-dl560-gen10-3.####.lan to be inaccessible<font></font>
Detected f1ca455c-d47e-11f7-7e90-20677c038de0 on esxi-dl560-gen10-1.####.lan to be inaccessible<font></font>
2019-02-02 05:22:34 +0300: Step 2: Check for invalid/inaccessible VMs<font></font>
Detected VM 'i.#####.ru' as being 'inaccessible'<font></font>
2019-02-02 05:22:34 +0300: Step 3: Check for VMs for which VC/hostd/vmx are out of sync<font></font>
Did not find VMs for which VC/hostd/vmx are out of sync<font></font>
/localhost/Datacenter ###CLUB/computers/###Cluster&gt;<font></font>
Thank you<font></font>
second vm with issues: sd.####.ru</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Como esse problema se manifestou (al√©m dos servi√ßos da organiza√ß√£o firmemente ca√≠dos). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Crescimento exponencial de erros de soma de verifica√ß√£o (CRC) "fora do azul" durante a troca de dados com discos no modo HBA. </font><font style="vertical-align: inherit;">Como verificar isso - digite o seguinte comando no console de cada n√≥ do ESXi:</font></font><br>
<br>
<pre><code class="cs">while true; do clear; for disk in $(localcli vsan storage list | grep -B10 'ity Tier: tr' |grep "VSAN UUID"|awk '{print $3}'|sort -u);do echo ==DISK==$disk====;vsish -e get /vmkModules/lsom/disks/$disk/checksumErrors | grep -v ':0';done; sleep 3; done</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Como resultado da execu√ß√£o, voc√™ pode ver erros CRC para cada disco no cluster vSAN deste n√≥ (os valores zero n√£o ser√£o exibidos). </font><font style="vertical-align: inherit;">Se voc√™ possui valores positivos e, al√©m disso, eles est√£o em constante crescimento, h√° uma raz√£o para tarefas que surgem constantemente na se√ß√£o Monitor -&gt; vSAN -&gt; Resincing objects do cluster. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Como recuperar discos de m√°quinas virtuais que n√£o clonam ou migram por meios padr√£o? </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Quem pensaria usando o poderoso comando cat:</font></font><br>
<br>
<pre><code class="bash">1. cd      vSAN<font></font>
[root@esxi-dl560-gen10-1:~] cd /vmfs/volumes/vsanDatastore/estaff<font></font>
<font></font>
2. grep vmdk     uuid<font></font>
<font></font>
[root@esxi-dl560-gen10-1:/vmfs/volumes/vsan:52f53dfd12dddc84-f712dbefac32cd1a/2636a75c-e8f1-d9ca-9a00-20677c0084e0] grep vsan *vmdk<font></font>
estaff.vmdk:RW 10485760 VMFS "vsan://3836a75c-d2dc-5f5d-879c-20677c0084e0"<font></font>
estaff_1.vmdk:RW 41943040 VMFS "vsan://3736a75c-e412-a6c8-6ce4-20677c0084e0"<font></font>
[root@esxi-dl560-gen10-1:/vmfs/volumes/vsan:52f53dfd12dddc84-f712dbefac32cd1a/2636a75c-e8f1-d9ca-9a00-20677c0084e0]<font></font>
<font></font>
3.    VM  ,  :<font></font>
<font></font>
mkdir /vmfs/volumes/POWERVAULT/estaff<font></font>
<font></font>
4.   vmx  <font></font>
<font></font>
cp *vmx *vmdk /vmfs/volumes/POWERVAULT/estaff<font></font>
<font></font>
5.      ,      ^_^<font></font>
<font></font>
/usr/lib/vmware/osfs/bin/objtool open -u 3836a75c-d2dc-5f5d-879c-20677c0084e0; sleep 1; cat /vmfs/devices/vsan/3836a75c-d2dc-5f5d-879c-20677c0084e0 &gt;&gt; /vmfs/volumes/POWERVAULT/estaff/estaff-flat.vmdk<font></font>
<font></font>
6. cd   :<font></font>
<font></font>
 cd /vmfs/volumes/POWERVAULT/estaff<font></font>
<font></font>
7.    - estaff.vmdk     <font></font>
<font></font>
[root@esxi-dl560-gen10-1:/tmp] cat estaff.vmdk<font></font>
# Disk DescriptorFile<font></font>
version=4<font></font>
encoding="UTF-8"<font></font>
CID=a7bb7cdc<font></font>
parentCID=ffffffff<font></font>
createType="vmfs"<font></font>
<font></font>
# Extent description<font></font>
RW 10485760 VMFS "vsan://3836a75c-d2dc-5f5d-879c-20677c0084e0"      &lt;&lt;&lt;&lt;&lt;     "estaff-flat.vmdk"<font></font>
<font></font>
# The Disk Data Base<font></font>
#DDB<font></font>
<font></font>
ddb.adapterType = "ide"<font></font>
ddb.deletable = "true"<font></font>
ddb.geometry.cylinders = "10402"<font></font>
ddb.geometry.heads = "16"<font></font>
ddb.geometry.sectors = "63"<font></font>
ddb.longContentID = "6379fa7fdf6009c344bd9a64a7bb7cdc"<font></font>
ddb.thinProvisioned = "1"<font></font>
ddb.toolsInstallType = "1"<font></font>
ddb.toolsVersion = "10252"<font></font>
ddb.uuid = "60 00 C2 92 c7 97 ca ae-8d da 1c e2 3c df cf a5"<font></font>
ddb.virtualHWVersion = "8"<font></font>
[root@esxi-dl560-gen10-1:/tmp]</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Como reconhecer discos naa.xxxx ... em grupos de discos:</font></font><br>
<br>
<pre><code class="bash">[root@esxi-dl560-gen10-1:/vmfs/volumes] vdq -Hi<font></font>
Mappings:<font></font>
   DiskMapping[0]:<font></font>
           SSD:  naa.5000c5003024eb43<font></font>
            MD:  naa.5000cca0aa0025f4<font></font>
            MD:  naa.5000cca0aa00253c<font></font>
            MD:  naa.5000cca0aa0022a8<font></font>
            MD:  naa.5000cca0aa002500<font></font>
<font></font>
   DiskMapping[2]:<font></font>
           SSD:  naa.5000c5003024eb47<font></font>
            MD:  naa.5000cca0aa002698<font></font>
            MD:  naa.5000cca0aa0029c4<font></font>
            MD:  naa.5000cca0aa002950<font></font>
            MD:  naa.5000cca0aa0028cc<font></font>
<font></font>
   DiskMapping[4]:<font></font>
           SSD:  naa.5000c5003024eb4f<font></font>
            MD:  naa.5000c50030287137<font></font>
            MD:  naa.5000c50030287093<font></font>
            MD:  naa.5000c50030287027<font></font>
            MD:  naa.5000c5003024eb5b<font></font>
            MD:  naa.5000c50030287187<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Como descobrir UUIDs vUAN para cada naa ....:</font></font><br>
<br>
<pre><code class="bash">[root@esxi-dl560-gen10-1:/vmfs/volumes] localcli vsan storage list | grep -B15 'ity Tier: tr' | grep -E '^naa|VSAN UUID'<font></font>
<font></font>
naa.5000cca0aa002698:<font></font>
   VSAN UUID: 52247b7d-fed5-a2f2-a2e8-5371fa7ef8ed<font></font>
naa.5000cca0aa0029c4:<font></font>
   VSAN UUID: 52309c55-3ecc-3fe8-f6ec-208701d83813<font></font>
naa.5000c50030287027:<font></font>
   VSAN UUID: 523d7ea5-a926-3acd-2d58-0c1d5889a401<font></font>
naa.5000cca0aa0022a8:<font></font>
   VSAN UUID: 524431a2-4291-cb49-7070-8fa1d5fe608d<font></font>
naa.5000c50030287187:<font></font>
   VSAN UUID: 5255739f-286c-8808-1ab9-812454968734<font></font>
naa.5000cca0aa0025f4: &lt;&lt;&lt;&lt;&lt;&lt;&lt;<font></font>
   VSAN UUID: 52b1d17e-02cc-164b-17fa-9892df0c1726<font></font>
naa.5000cca0aa00253c:<font></font>
   VSAN UUID: 52bd28f3-d84e-e1d5-b4dc-54b75456b53f<font></font>
naa.5000cca0aa002950:<font></font>
   VSAN UUID: 52d6e04f-e1af-cfb2-3230-dd941fd8a032<font></font>
naa.5000c50030287137:<font></font>
   VSAN UUID: 52df506a-36ea-f113-137d-41866c923901<font></font>
naa.5000cca0aa002500:<font></font>
   VSAN UUID: 52e2ce99-1836-c825-6600-653e8142e10f<font></font>
naa.5000cca0aa0028cc:<font></font>
   VSAN UUID: 52e89346-fd30-e96f-3bd6-8dbc9e9b4436<font></font>
naa.5000c50030287093:<font></font>
   VSAN UUID: 52ecacbe-ef3b-aa6e-eba3-6e713a0eb3b2<font></font>
naa.5000c5003024eb5b:<font></font>
   VSAN UUID: 52f1eecb-befa-12d6-8457-a031eacc1cab</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
E a coisa mais importante. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O problema acabou sendo a opera√ß√£o incorreta do firmware do controlador RAID e do driver HPE com vSAN. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Anteriormente, no VMware 6.7 U1, o firmware compat√≠vel para o controlador HPE Smart Array P816i-a SR Gen10 no vSAN HCL era a vers√£o 1.98 (que acabou sendo fatal para a nossa organiza√ß√£o) e agora </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">diz 1,65</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Al√©m disso, a vers√£o 1.99, que resolveu o problema na √©poca (31 de janeiro de 2019), j√° estava nas lixeiras da HPE, mas n√£o a passou para a VMware ou para n√≥s, referindo-se √† falta de certifica√ß√£o, apesar de nossas isen√ß√µes de responsabilidade e tudo o mais. , dizem eles, o principal para n√≥s √© resolver o problema do armazenamento e √© isso. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Como resultado, o problema foi finalmente resolvido somente ap√≥s tr√™s meses, quando a vers√£o de firmware 1.99 para o controlador de disco foi lan√ßada!</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Que conclus√µes tirei?</font></font><br>
<br>
<ol>
<li>     (   ),         .</li>
<li>     !   .</li>
<li>       ¬´¬ª ,   ¬´¬ª    ¬´¬ª   ,    30%      ¬´¬ª.</li>
<li>HPE,    ,           .</li>
<li> ,       :<br>
<br>
<ul>
<li>HPE -           .   ,     Enterprise       . ,    ,       ).</li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">N√£o previ uma situa√ß√£o em que espa√ßo em disco adicional possa ser necess√°rio para colocar c√≥pias de todos os servidores da empresa em caso de emerg√™ncia.</font></font></li>
</ul></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Al√©m disso, √† luz do que aconteceu, para a VMware, n√£o comprarei mais hardware para grandes empresas, outros fornecedores que n√£o a DELL. </font><font style="vertical-align: inherit;">Por que, porque a DELL, at√© onde eu sei, adquiriu a VMware e agora a integra√ß√£o de hardware e software nessa dire√ß√£o deve estar o mais pr√≥xima poss√≠vel.</font></font></li>
</ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Como se costuma dizer, queimado no leite, sopre na √°gua. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Isso √© tudo pessoal. </font><font style="vertical-align: inherit;">Desejo que voc√™ nunca entre em situa√ß√µes t√£o terr√≠veis. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pelo que me lembro, j√° vou me assustar!</font></font></div>
      
    </div><p class="reference-to-source js-reference-to-source">Source: https://habr.com/ru/post/undefined/</p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt487570/index.html">Melhores pr√°ticas do Redis, parte 2</a></li>
<li><a href="../pt487578/index.html">Otimiza√ß√£o do CMake para bibliotecas est√°ticas</a></li>
<li><a href="../pt487582/index.html">Deuses n√£o queimam panelas</a></li>
<li><a href="../pt487584/index.html">Githabifica√ß√£o da seguran√ßa da informa√ß√£o</a></li>
<li><a href="../pt487588/index.html">Quarkus: Veterin√°rio subat√¥mico supers√¥nico</a></li>
<li><a href="../pt487594/index.html">Mnem√¥nicos divertidos: coletamos a mem√≥ria auditiva do visual</a></li>
<li><a href="../pt487596/index.html">Eugene Varavva, desenvolvedor do Google. Como descrever o Google em 5 palavras</a></li>
<li><a href="../pt487604/index.html">Incorpore breves notas do programador: duplica√ß√£o de se√ß√£o na mem√≥ria do microcontrolador</a></li>
<li><a href="../pt487606/index.html">Visualiza√ß√£o de linhas de tens√£o e movimentos de cargas eletrost√°ticas, simula√ß√£o de movimento planet√°rio do sistema solar</a></li>
<li><a href="../pt487610/index.html">RealWorld: aiohttp, Tortoise ORM</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>