<!doctype html>
<html class="no-js" lang="de">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üèæ üõ∞Ô∏è üî≥ Wie wir pers√∂nliche Schutzausr√ºstung erkennen ü§´ üôã üï∞Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Wahrscheinlich haben Sie sich Ihr ganzes Leben lang gefragt, wie Sie ein neuronales Netzwerk trainieren k√∂nnen, um Menschen in Helmen und orangefarben...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Wie wir pers√∂nliche Schutzausr√ºstung erkennen</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/center2m/blog/499248/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wahrscheinlich haben Sie sich Ihr ganzes Leben lang gefragt, wie Sie ein neuronales Netzwerk trainieren k√∂nnen, um Menschen in Helmen und orangefarbenen Westen zu erkennen! Nein? Aber wir werden es dir trotzdem sagen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Unser Name ist Tatyana Voronova und Elvira Dyaminova. Wir besch√§ftigen uns mit Datenanalysen im Unternehmen Center 2M und arbeiten viel mit den realsten Fabriken und Unternehmen zusammen. Aufgrund von Sicherheitsverletzungen erleiden sie Verluste in H√∂he von mehreren Millionen Dollar, Mitarbeiter werden verletzt, daher w√§re es sch√∂n, solche Verst√∂√üe systematisch und so fr√ºh wie m√∂glich erkennen zu k√∂nnen. Das Beste von allem - automatisch. Wir haben also Probleme mit der Erkennung pers√∂nlicher Schutzausr√ºstung (PSA) auf Video und der Identifizierung von Personen oder Ausr√ºstungen in der Gefahrenzone.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/gv/2u/-q/gv2u-qjuorn0awf-hlwjbuu3dc4.jpeg" alt="Bild"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Zum gr√∂√üten Teil erhalten wir Auftr√§ge zur Bestimmung von Helmen (genauer gesagt deren Abwesenheit) und Arbeitskleidung. </font><font style="vertical-align: inherit;">Wir haben bereits Erfahrungen mit der Ausf√ºhrung solcher Aufgaben gesammelt und k√∂nnen nun die aufgetretenen Probleme und deren L√∂sung beschreiben.</font></font><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Da wir im Rahmen der Zusammenarbeit nicht das Recht haben, Filmmaterial von den Objekten des Kunden zu ver√∂ffentlichen, werden wir den Artikel mit Bildern aus dem Internet illustrieren, auf denen Menschen in Helmen oft l√§cheln und gut aussehen. </font><font style="vertical-align: inherit;">Leider finden Sie im √∂ffentlichen Bereich nicht f√ºr alle Merkmale der Aufgaben, denen wir in der Realit√§t gegen√ºberstehen, gute Beispiele. </font><font style="vertical-align: inherit;">Insbesondere im Leben l√§cheln Menschen in Helmen seltener, und das Problem der Glatzk√∂pfigen (wir werden etwas sp√§ter dar√ºber sprechen) im Internet wurde nicht wirklich aufgedeckt! </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bild aus dem Internet (Gr√∂√üe 1920x1280):</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/f9/9q/nb/f99qnblemmbik0caykgir0zfiag.jpeg" alt="Bild"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die Erkennung von PSA kann auf eines von zwei klassischen Problemen des Computer-Sehens reduziert werden: Klassifizierung von Bildern und Erkennung von Objekten. In der Praxis stellte sich heraus, dass es besser war, keinen dieser Ans√§tze zu verwenden, sondern den f√ºr den jeweiligen Fall am besten geeigneten auszuw√§hlen und flexibel zu kombinieren. Zum Beispiel k√∂nnen wir zuerst bestimmen, wo sich Personen auf dem Bild befinden, dann die durch die Silhouette geschnittenen Bilder in Klassen ‚Äûin Arbeitskleidung‚Äú und ‚Äûohne‚Äú klassifizieren und das Vorhandensein eines Helms beim zweiten Durchgang erkennen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bei vorgeschnittenen Personenfiguren sieht die Klassifizierung des Vorhandenseins von Helmen und Overalls folgenderma√üen aus (Ansicht des Originalbildes): </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das Ergebnis der Arbeit der Modelle zur Klassifizierung von Overalls und Helmen</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/su/ad/lg/suadlgvrpviurbpxflbg6y3bm8s.jpeg" alt="Bild"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Auf den gleichen zuvor ausgew√§hlten menschlichen Figuren erfolgt diesmal die Anwendung des Ansatzes mit Erkennung f√ºr Helme. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das Ergebnis des Modells zur Klassifizierung von Arbeitskleidung und eines Modells zur Erkennung von Helmen:</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/qj/jb/8l/qjjb8lebwt3t-6xttueihk2gaqw.jpeg" alt="Bild"></div><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Stufe eins: menschliche Entdeckung</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die Qualit√§t der Definition kleiner Objekte (Helme / Brillen / Handschuhe) auf gro√üen Rahmen ist mittelm√§√üig. F√ºr einen Computer wie eine Person ist es viel einfacher, zuerst zu verstehen, wo sich Menschen befinden, und erst dann herauszufinden, was sie tragen. Alles beginnt also mit der Identifizierung der Personen im Rahmen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Als Ergebnis der Experimente haben wir herausgefunden, dass das schnellere neuronale R-CNN-Netzwerk mit Inception v2 als Merkmalsextraktion gut zur Erkennung von Personen geeignet ist. TensorFlow verf√ºgt bereits √ºber </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">vorab</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> trainierte </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;">neuronale Netze</font></a><font style="vertical-align: inherit;"> zur Erkennung von Objekten. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
F√ºr uns ist Faster R-CNN Inception v2 (trainiert im COCO-Datensatz) die grundlegende Methode, die wir zuerst versuchen, um solche Probleme zu l√∂sen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Zun√§chst erkennen wir Personen auf dem Rahmen (und dann auf den gefundenen Personen, die wir PSA finden):</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/uk/xp/gy/ukxpgyllgs-olfg9xgbpmzkuwly.jpeg" alt="Bild"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Beachten Sie, dass wir den Begrenzungsrahmen ‚Äûmit einer Person‚Äú entlang </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">der y-Achse</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> vergr√∂√üert haben </font><font style="vertical-align: inherit;">:</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/ad/8x/dp/ad8xdpfnx8f1fke9afg9u5q-9xs.jpeg" alt="Bild"></div><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Auf diesem Foto wurde der Arbeiter in gutem Licht und vor einem kontrastierenden Hintergrund aufgenommen (bei Bildern im Internet passiert dies st√§ndig). </font><font style="vertical-align: inherit;">Daher war der Begrenzungsrahmen mit der Person gut gebaut. </font><font style="vertical-align: inherit;">In unserer Praxis gibt es jedoch h√§ufig F√§lle (insbesondere bei unzureichender Sicht), in denen das Erkennungsmodell einen Helm bei einer Person abschneidet. Danach ist es sinnlos, auf einem beschnittenen Bild danach zu suchen. </font><font style="vertical-align: inherit;">In dieser Hinsicht erh√∂hen wir entlang der y-Achse den vorhergesagten Begrenzungsrahmen um 15%, bevor wir zur zweiten Stufe √ºbergehen.</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Beim Erkennen von Personen sto√üen wir auf kleine unangenehme Probleme. Erstens, wenn zwei Personen hintereinander gehen oder stehen, werden sie h√§ufig als eine Person erkannt. Zweitens tritt ein statisches Objekt in das Sichtfeld der Kamera ein, in dem das Modell eine Person wie einen Hydranten erkennen kann. Diese Probleme k√∂nnen auf verschiedene Arten gel√∂st werden. Zum Beispiel, wie wir es gemacht haben: Vers√∂hnen und akzeptieren Sie sie, da das Modell im Allgemeinen in Bezug auf Produktivit√§t und Qualit√§t f√ºr uns geeignet ist. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ein grundlegenderes Problem ist, dass Industrier√§ume, in denen es eine ‚ÄûGefahrenzone‚Äú gibt, oft riesig sind und dementsprechend die Personen in den Rahmen sehr klein sind. Unsere auf Faster R-CNN Inception v2 basierende Basismethode zeigte in solchen F√§llen schlechte Ergebnisse, und am Ende haben wir es versucht</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Schneller R-CNN Nas</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Die Ergebnisse waren beeindruckend, die Leute waren auch in der Ferne gut bekannt, aber die Geschwindigkeit war viel niedriger als beim Basismodell. </font><font style="vertical-align: inherit;">Mit ausreichenden Ressourcen und der Notwendigkeit einer hohen Genauigkeit k√∂nnen Sie </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Faster R-CNN Nas verwenden</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zweite Stufe: Ermittlung b√∂swilliger Verst√∂√üe</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Je nach Aufgabe werden h√§ufig folgende verwendet:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bildklassifizierungsmodell - Inception v3</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Objekterkennungsmodell - Schnellerer R-CNN-Beginn v2</font></font></li>
</ul><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Klassifizierung von Arbeitskleidung und Helmen</font></font></h3> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wir haben verschiedene neuronale Netzwerkarchitekturen getestet, um Bilder zu klassifizieren, und uns schlie√ülich f√ºr Inception v3 entschieden, um die Tatsache zu nutzen, dass es f√ºr die Arbeit mit variablen Bildgr√∂√üen ausgelegt ist. Wir hatten bereits viele ausgeschnittene Bilder mit Menschen, und es war nicht schwierig, die Medianwerte f√ºr H√∂he und Breite zu berechnen. So kamen wir zu dem Schluss, dass f√ºr das Training von Klassifikatoren begonnen wurde, Bilder auf eine Gr√∂√üe von 150x400 zu bringen.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Um das Netzwerk f√ºr die Erkennung von PSA zu schulen, muss zun√§chst ein Datensatz aus beschrifteten Beispielen gesammelt werden. </font><font style="vertical-align: inherit;">In diesem Prozess gibt es Feinheiten, deren Verwirklichung mit Erfahrung einhergeht. </font><font style="vertical-align: inherit;">Zum Beispiel ist es besser, Personen, die √ºber den H√ºften geschnitten sind, aus dem Datensatz zu entfernen. </font><font style="vertical-align: inherit;">Dadurch wird der Datensatz den tats√§chlichen Bedingungen n√§her gebracht, da die meisten Personen auf Video von √úberwachungskameras in voller H√∂he gesehen werden. </font><font style="vertical-align: inherit;">Nat√ºrlich kommt es auch zu √úberlappungsf√§llen, aber vollst√§ndige Silhouetten f√ºr die Zielprobe sind viel charakteristischer. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Beispiele aus unserem Workwear-Datensatz:</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/k8/22/sh/k822shy7m4ddtteqqjp1mfs09b8.jpeg" alt="Bild"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wir haben nichts Spezifisches als Metrik erfunden, wir verwenden R√ºckruf und Pr√§zision. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Modell zur Klassifizierung des Vorhandenseins / Nichtvorhandenseins von Arbeitskleidung: </font></font><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ergebnisse einer Validierungsstichprobe</font></font></i><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/lw/ql/hu/lwqlhu_7-efmfxzrxug-d64pafq.jpeg" alt="Bild"></div><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PSA-Erkennung</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das Klassifizierungsmodell arbeitet schneller als das Modell zum Erkennen von Objekten. Aufgrund der Tatsache, dass Schutzbrillen und Handschuhe im Bild klein sind, ist es schwierig, einen guten Klassifizierer f√ºr eine solche PSA zu erstellen. </font><font style="vertical-align: inherit;">Daher haben wir das schnellere neuronale R-CNN-Netzwerk auf einem Datensatz mit sechs Klassen trainiert:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Gl√§ser / not_glasses</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Handschuhe / not_gloves</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Helm / nicht_Helm</font></font></li>
</ul><br>
<img src="https://habrastorage.org/webt/pw/iy/i_/pwiyi_nep5v7k8wlrz6yxvz8ug4.jpeg" alt="Bild"><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Datenerfassung und Markup</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die Hauptprobleme betrafen den Helmdatensatz. Es war ein faszinierender Weg: Wir gingen durch Glatzk√∂pfige, Menschen mit Helmen in den H√§nden und sogar durch Glatzk√∂pfige mit Helmen in den H√§nden. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Da wir zu Beginn der Reise nicht viele Bilder unter realen Bedingungen hatten, haben wir den Datensatz so gut wie m√∂glich gesammelt: uns selbst gefilmt, Bilder aus dem Internet oder von Baustellen aufgenommen. Wenig sp√§ter erhielten wir viele Videos von verschiedenen Unternehmen, sodass wir den Datensatz nur mit Frames realer Bedingungen anreicherten. Irgendwann √ºberschritt die Anzahl der mit Tags versehenen Bilder 5 KB, und die Qualit√§t durch Hinzuf√ºgen neuer Beispiele verbesserte sich nicht mehr. In dieser Hinsicht haben wir den Ansatz f√ºr das Markup √ºberarbeitet.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wir werden die Phasen der Verbesserung des Helmdatensatzes am Beispiel von Bildern aus dem Internet beschreiben, sodass Winkel und Qualit√§t nicht ganz mit denen √ºbereinstimmen, die wir hatten. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Zus√§tzlich zu dem obigen Bild, das √ºber den H√ºften zugeschnitten ist, haben wir Bilder entfernt, in denen die Helme mehr als zur H√§lfte zugeschnitten sind, um Verwechslungen mit Kappen zu vermeiden.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/dl/zh/eu/dlzheusrhwz0grtyrxdxalkkupq.jpeg" alt="Bild"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wir haben uns auch der Tatsache gestellt, dass wenn eine Person einen Helm in der Hand hat, das Modell oft keine Verst√∂√üe sah: Gibt es einen Helm? Es gibt. Daher haben wir alle Rahmen aus dem Trainingsdatensatz entfernt, in denen eine Person einen Helm mit der Hand h√§lt, auch wenn sich der Helm gerade auf dem Kopf befindet. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Im Allgemeinen haben wir versucht, Bilder mit beleuchtetem Hintergrund oder in dunklen R√§umen zu entfernen, und dann haben wir die Anzahl der von uns aufgenommenen Fotos minimiert, wobei haupts√§chlich Filmmaterial aus der Produktion √ºbrig blieb. Infolgedessen haben wir den Datensatz um die H√§lfte reduziert. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Zus√§tzlich haben wir den Datensatz mit Glatzk√∂pfigen angereichert, ansonsten sind sie immer in Helmen, auch wenn dies nicht der Fall ist, und mit Blondinen mit Quadraten, f√ºr die der Detektor mit einem bestimmten Winkel auch den Helm bestimmt.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nachdem wir ungeeignete Bilder entfernt hatten, gingen wir direkt zum Markup (zum Erkennen von Objekten). Es stellte sich heraus, dass es nicht so einfach war. Es stellt sich heraus, dass die Qualit√§t des Enddetektors weitgehend davon abh√§ngt, was genau der Bereich im Bild als "Helm" oder "Handschuhe" markiert ist. Zun√§chst verteilten wir Helme und Schutzbrillen, ohne die Gesichter zu greifen, und Handschuhe mit den H√§nden. Mit der Erfahrung haben wir unseren Ansatz jedoch schrittweise verbessert, indem wir uns Fehler der ersten und zweiten Art angesehen haben, bei denen Menschen Helme in den H√§nden halten und etwas Rundes an etwas Langem sich als ‚ÄûHandschuh‚Äú herausstellt. Beim Markieren von Helmen und Brillen versuchen wir nun, das Gesicht bis zur Nasenspitze zu greifen, und beim Markieren von Handschuhen beschr√§nken wir uns im Gegenteil auf einen Pinsel.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/ka/us/re/kausredcadkqks_cn012broh2qq.jpeg" alt="Bild"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Als Ergebnis unserer Manipulationen am Datensatz haben wir die folgenden Ergebnisse erhalten. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Modell zum Erkennen des Vorhandenseins / Nichtvorhandenseins von PSA am Beispiel von Helmen: </font></font><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ergebnisse einer Validierungsprobe vor Beginn der ‚Äûglobalen Arbeit‚Äú am Datensatz</font></font></i><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/oo/rx/k-/oorxk-v05lijjyljtrp5lvtbsq4.jpeg" alt="Bild"></div><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Endergebnisse der Validierungsprobe</font></font></i><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/y1/nn/0u/y1nn0uy66v5qmwji-zk2rwotnle.jpeg" alt="Bild"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die Vollst√§ndigkeit der Erkennung von Helmen lie√ü leicht nach, gleichzeitig verbesserten sich die Metriken zur Erkennung von Verst√∂√üen, und dies wollten wir erreichen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Modell zur Klassifizierung des Vorhandenseins / Nichtvorhandenseins von Helmen: </font></font><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ergebnisse einer Validierungsstichprobe vor Beginn der ‚Äûglobalen Arbeit‚Äú am Datensatz</font></font></i><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/wi/eu/fp/wieufpwnxltfvgf1kvkvisr3fk4.jpeg" alt="Bild"></div><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Endergebnisse der Validierungsprobe</font></font></i><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/et/ct/d1/etctd1ymh_9jmjvc3lgaz_mw47o.jpeg" alt="Bild"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Es sollte beachtet werden, dass wir keine Unterteilung in Schutzbrillen und Brillen f√ºr das Sehen haben, sie fallen unter das gleiche Etikett ‚ÄûBrille‚Äú und Handschuhe mit hellen Farbt√∂nen k√∂nnen als blo√üer Pinsel wahrgenommen werden. </font><font style="vertical-align: inherit;">Wir haben versucht, den Farbumfang von Helmen und Arbeitskleidung in unseren Datens√§tzen zu maximieren. Aus Gr√ºnden der Zuverl√§ssigkeit haben wir dies jedoch mit der einfachsten und zuverl√§ssigsten Technik erg√§nzt: Um Handschuhe zu erkennen, teilen wir unseren Kunden bei Bedarf mit, dass helle Farben zur Erh√∂hung der Genauigkeit beitragen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Im Moment haben wir universelle Modelle, die wir f√ºr die erste Show f√ºr den Kunden verwenden. </font><font style="vertical-align: inherit;">Es versteht sich jedoch, dass es unm√∂glich ist, ein universelles Modell f√ºr alle zu erstellen. Es ist notwendig, sich an jeden Kunden anzupassen, neue Nuancen zu identifizieren und zu ber√ºcksichtigen, Datens√§tze anzureichern oder neu zu erstellen, um bestimmte Anforderungen zu erf√ºllen.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/kl/-l/9t/kl-l9tfsjdr528i4xgl5flmtqjk.jpeg" alt="Bild"><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bonus</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In der Regel m√∂chten Kunden so viele Kameras wie m√∂glich mit m√∂glichst wenigen Ressourcen verarbeiten. </font><font style="vertical-align: inherit;">Butch ist nat√ºrlich eine gute Sache, aber zus√§tzliche Tricks zur Optimierung des Prozesses sind nicht verboten. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Zum Beispiel hatten meine Kollegen und ich vom Moskauer IBM-Kundencenter die Hypothese, dass das Zusammenf√ºgen mehrerer Personen zur weiteren Erkennung von Helmen die Anzahl der Kameras pro Server mit einem prinzipienlosen Genauigkeitsverlust erh√∂hen w√ºrde. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Als Grundlage haben wir beschlossen, die Gr√∂√üe von 1000 x 600 f√ºr die Leinwand zu verwenden, auf die Personen "angewendet" werden. </font><font style="vertical-align: inherit;">Zun√§chst wurden zwei Layoutoptionen in Betracht gezogen:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Feste Breite und H√∂he (200x600), bei diesem Ansatz befinden sich 5 Personen auf dem Rahmen.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Feste Breite und H√∂he (125x600), 8 Personen.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Diese Entscheidung beruhte auf der Tatsache, dass wir mit festen Daten genau die Anzahl der Personen auf dem Foto kennen, was uns eine Prognose der Belastung gibt. </font><font style="vertical-align: inherit;">W√§hrend der Entwicklung haben wir jedoch eine solche Option in Betracht gezogen:</font></font><br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Feste H√∂he und proportionale Breite (*** x600), unterschiedliche Anzahl von Personen.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Es wurde angenommen, dass mit zunehmender Gr√∂√üe und Beibehaltung der Proportionen die Ergebnisse im Vergleich zu anderen Layoutoptionen besser sind. </font><font style="vertical-align: inherit;">Die Anzahl der Personen lag zwischen 3 und 5 (+/‚Äì). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Als Ergebnis haben wir festgestellt, dass die Option mit einer festen Breite und H√∂he (200 x 600) die beste unter den in Betracht gezogenen ist. </font><font style="vertical-align: inherit;">Nat√ºrlich ist diese Methode nicht zum Erkennen von Brillen und Handschuhen geeignet, da die Objekte klein sind, aber zum Erkennen von Helmen / fehlenden Helmen zeigte diese Methode gute Ergebnisse. </font></font><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zum Beispiel in einem Validierungsbeispiel:</font></font></i><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/ar/wg/zy/arwgzypgxzuk6ykdrj57p851-co.jpeg" alt="Bild"></div><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/ii/lw/y5/iilwy5ihzrc57ezgdnjp3gs646s.jpeg" alt="Bild"></div><br>
<i> :   (<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" class="user_link">tvoronova</a>),  (<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" class="user_link">elviraa</a>)</i></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de499238/index.html">L√∂ten zu Hause: Stream-Aufnahme</a></li>
<li><a href="../de499240/index.html">Durch Dornen zu den Sternen oder Datenanalyse in den Angelegenheiten des Himmels</a></li>
<li><a href="../de499242/index.html">Die Forscher √ºbertrugen Daten von einem Desktop-PC durch Vibrationen √ºber einen Tisch</a></li>
<li><a href="../de499244/index.html">Synergetische Organisationen. Teil II</a></li>
<li><a href="../de499246/index.html">Untersuchung der logistischen Funktion als Gesetz der Industrieentwicklung</a></li>
<li><a href="../de499252/index.html">Erstellen eines pseudo-dreidimensionalen Rennspiels</a></li>
<li><a href="../de499254/index.html">Das Mitglied des PyConRu 2020-Programmkomitees beantwortet Fragen zu Python: ein aktuelles Aussehen und ein bisschen Parseltang</a></li>
<li><a href="../de499262/index.html">Letzter Online-Hackathon f√ºr selbstst√§ndige SMZhack: Projekte, die die Menschen treffen werden</a></li>
<li><a href="../de499268/index.html">Raumbewusstsein: Was kann eine Hololens-Brille?</a></li>
<li><a href="../de499274/index.html">Wir haben die neue Kapsel zerlegt. Wir wissen, wie viele Mikrofone und wie es funktioniert</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>