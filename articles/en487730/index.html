<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚úåüèª üôåüèø üë®üèæ‚Äçüåæ Natural Language Processing. Results 2019 and trends for 2020 üëèüèª üõÉ ü§¶üèø</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello everyone. With some delay, I decided to publish this article. Every year I try to summarize what happened in the field of natural language proce...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Natural Language Processing. Results 2019 and trends for 2020</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/huawei/blog/487730/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hello everyone. </font><font style="vertical-align: inherit;">With some delay, I decided to publish this article. </font><font style="vertical-align: inherit;">Every year I try to summarize what happened in the field of natural language processing. </font><font style="vertical-align: inherit;">This year was no exception.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">BERTs, BERTs are everywhere</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Let's start in order. </font><font style="vertical-align: inherit;">If you have not left for the remote Siberian taiga or a vacation in Goa for the past year and a half, then you must have heard the word BERT. </font><font style="vertical-align: inherit;">Appearing at the very end of 2018, over the past time, this model has gained such popularity that just such a picture will be just right:</font></font><br>
<br>
<img src="https://habrastorage.org/webt/cu/vm/_i/cuvm_irxzrscw8rctmtyoqywxss.png"><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
BERTs really captivated everything that could be filled in NLP. </font><font style="vertical-align: inherit;">They began to be used for classification, recognition of named entities, and even for machine translation. </font><font style="vertical-align: inherit;">Simply put, you cannot bypass them and you still have to tell what it is. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/5j/mo/sq/5jmosqk9vhjts6ai88v8hcrdhci.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The picture shows a comparison of the hero of the occasion (left) with two models that also sounded. </font><font style="vertical-align: inherit;">On the right is the immediate predecessor of BERT - the </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ELMo</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> model </font><font style="vertical-align: inherit;">.</font></font><br>
<br>
<div class="spoiler"><b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Lyrical digression.</font></font></b><div class="spoiler_text"><img src="https://habrastorage.org/getpro/habr/post_images/8a1/bb1/e07/8a1bb1e076e3b3b1b2637343e28359d4.jpg" alt="image"><br>
         ¬´ ¬ª:           ,        ,   Elmo,  Bert ‚Äî   ;    ,   ,   , ‚Äî    .         .  ,    ,   .<br>
</div></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Allen AI</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ELMo model </font><font style="vertical-align: inherit;">is a kind of successor to the entire development of the region in previous years - namely, a bi-directional recurrent neural network, plus several new tricks to boot. </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">OpenAI</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> colleagues have </font><font style="vertical-align: inherit;">decided what can be done better. And for this you just need to apply the </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Transformer</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> architecture presented the year before Google </font><font style="vertical-align: inherit;">to this task. I believe that over the past 2.5 years, everyone has already managed to get acquainted with this architecture, so I will not dwell on it in detail. For those who wish to receive communion, I refer to my </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">review from the 2017th year</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
They (OpenAI employees) called their </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GPT-2</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> model </font><font style="vertical-align: inherit;">. And then, on this model, they </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚Äôve done a pretty good job</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. But let‚Äôs leave it on their conscience, and return to our sheep, that is, the models. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
One of the most important ELMo tricks was pre-training on a large, unallocated case. It turned out very well, and colleagues from Google decided that we can do even better. In addition to applying the Transformer architecture (which was already in GPT-2), BERT, which stands for Bidirectional Encoder Representations from Transformers, that is, vector representations from a bidirectional encoder based on the Transformer architecture, contained several more important things. Specifically, the most important was the way to train on a large case.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/lb/hw/yw/lbhwywgm70j3shvnrtzrnx6clyy.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The picture shows a method for marking up unallocated data. Two layout methods are specifically shown at once. First, a sequence of tokens (words) is taken, for example, a sentence, and in this sequence one arbitrary token ([MASK]) is masked. And the model in the learning process should guess what kind of token was disguised. The second way - two sentences are taken sequentially or from arbitrary places in the text. And the model must guess whether these sentences were sequential ([CLS] and [SEP]). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The idea of ‚Äã‚Äãsuch training was extremely effective. The answer from sworn friends from Facebook was the </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">RoBERTa</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> model </font><font style="vertical-align: inherit;">, an article about this model is called ‚ÄúSustainably Optimized BERT Training‚Äù. Further more.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
I will not list all the ways to improve the training of a large language model based on the Transfomer architecture due to the fact that it is simply boring. </font><font style="vertical-align: inherit;">I mention, perhaps, only the work of my colleagues from Hong Kong - </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ERNIE</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">In their work, colleagues enrich training through the use of knowledge graphs. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Before moving on, here are some useful links: an article about </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">BERT</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">As well as a </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">set of</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> trained BERT and ELMo models for the Russian language.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Small models</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
But enough about BERTs. There are several more important trends. First of all, this is a trend to reduce the size of the model. The same BERT is very demanding on resources, and many began to think about how to maintain (or not really lose) quality, reduce the required resources for the models to work. Google colleagues came up with a little BERT, I'm not joking - </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ALBERT: A little BERT</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . You can see that the small BERT even surpasses its older brother in most tasks, while having an order of magnitude less parameters. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/y5/su/h3/y5suh3uzlmgy16l8stcoahmio4w.png"> <br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Another approach to the same bar was made again by my colleagues from Hong Kong. They came up with a tiny BERT - </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">TinyBERT</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . (If at this point you thought that the names began to be repeated, I am inclined to agree with you.)</font></font><br>
<br>
<img src="https://habrastorage.org/webt/7_/bb/el/7_bbelco1m2dewd3gj6cjqq2upw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The fundamental difference between the two above models is that if ALBERT uses tricky tricks to reduce the original BERT model, for example, parameter sharing and reducing the dimension of internal vector representations through matrix decomposition, then TinyBERT uses a fundamentally different approach, namely the distillation of knowledge, that is, there is a small model that learns to repeat after her older sister in the learning process.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Small cases</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In recent years (since about 1990, when the Internet appeared), there has been an increase in available buildings. Then came the algorithms that became capable of processing such large cases (this is what we call the ‚Äúdeep learning revolution‚Äù, this is already the year since 2013). And, as a result, it began to be perceived normally that in order to get good quality in some task, huge arrays of marked-up data are needed - corpus of texts in our case. For example, typical cases for learning machine translation tasks today are measured in millions of pairs of sentences. It has long been obvious that for many tasks it is impossible to assemble such cases in a reasonable amount of time and for a reasonable amount of money. For a long time it was not very clear what to do about it. But last year (who would you think?) BERT came onto the scene.This model was able to pre-train on large volumes of unallocated texts, and the finished model was easy to adapt to the task with a small case.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/7_/bb/el/7_bbelco1m2dewd3gj6cjqq2upw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
All of the tasks listed in this table have training corps in the size of several </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">thousand</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> units. </font><font style="vertical-align: inherit;">That is, two to three orders of magnitude less. </font><font style="vertical-align: inherit;">And this is another reason why BERT (and its descendants and relatives) have become so popular.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">New trends</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Well, in the end, a couple of new trends, as I saw them. First of all, this is a fundamental change in attitude to the text. If all the previous time in most tasks the text was perceived only as input material, and the output was something useful, for example, a class label. Now, the community has the opportunity to remember that the text is primarily a means of communication, that is, you can also "talk" to the model - ask questions and receive answers in the form of a human-readable text. This is what the new article from Google </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">T5</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> says </font><font style="vertical-align: inherit;">(the name can be translated as ‚Äúfive times transformer‚Äù).</font></font><br>
<br>
<img src="https://habrastorage.org/webt/ba/vz/mj/bavzmjwryypmza-ywo18njxfbjy.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Another important trend is that the region is re-learning to work with long texts. Since the 70s, the community has ways to work with text of arbitrary lengths - take the same TF-IDF. But these models have their own quality limit. But the new deep learning models were not able to work with long texts (the same BERT has a limit of 512 tokens of the length of the input text). But lately, at least two works have appeared that from different sides approach the problem of long text. The first work from the group of Ruslan Salakhutdinov called Transformer-XL. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ci/op/gj/ciopgjs1htbc2gmucz7dwkiwqtk.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In this work, the idea is revived that made recurrent networks so popular - you can save the previous state and use it to build the next one, even if you don‚Äôt roll the gradient backward in time (BPTT). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Second one</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">the work</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> works with Legendre polynomials and with their help allows you to process sequences of tens of thousands of tokens with recurrent neural networks. </font></font><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
On this, I would like to finish the review of the changes that have taken place and emerging trends. </font><font style="vertical-align: inherit;">Let's see what will happen this year, I‚Äôm sure that a lot of interesting things. </font><font style="vertical-align: inherit;">Video of my speech on the same topic on the Data Tree:</font></font><br>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/cdlAUcaOCDY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
PS We will soon have some more interesting announcements, do not switch!</font></font></div>
      
    </div><p class="reference-to-source js-reference-to-source">Source: https://habr.com/ru/post/undefined/</p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../en487706/index.html">Service Discovery in distributed systems using the Consul example. Alexander Sigachev</a></li>
<li><a href="../en487716/index.html">Perfect SAST. Parser</a></li>
<li><a href="../en487720/index.html">On competitive corutinism (using reactive programming as an example)</a></li>
<li><a href="../en487724/index.html">BlazingPizza: Blazor app from start to finish. Part 2. Add a component</a></li>
<li><a href="../en487728/index.html">@Pythonetc compilation, January 2020</a></li>
<li><a href="../en487734/index.html">Speeding up Entity Framework Core</a></li>
<li><a href="../en487738/index.html">Schema animation in SCADA</a></li>
<li><a href="../en487740/index.html">Assembling a portable magnetometer</a></li>
<li><a href="../en487742/index.html">Arbitrage Trading (Bellman-Ford Algorithm)</a></li>
<li><a href="../en487744/index.html">FARO Introduces FOCUS S 70 Laser 3D Scanner</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>