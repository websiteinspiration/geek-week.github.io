<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üòù ‚ôãÔ∏è üë®üèª‚Äçüíº ML, VR & Robots (and a bit of cloud) üå∏ üìû üöµ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello everyone! 
 
 I want to talk about a very boring project where robotics, Machine Learning (and together this is Robot Learning), virtual reality...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>ML, VR & Robots (and a bit of cloud)</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/recognitor/blog/486680/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hello everyone! </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
I want to talk about a very boring project where robotics, Machine Learning (and together this is Robot Learning), virtual reality and a bit of cloud technology intersected. </font><font style="vertical-align: inherit;">And all this actually makes sense. </font><font style="vertical-align: inherit;">After all, it‚Äôs really convenient to move into a robot, show what to do, and then train weights on the ML server using the stored data. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Under the cut, we will tell how it works now, and some details about each of the aspects that had to be developed.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/t0/kx/wi/t0kxwiicqakswvomnrwz-iycc2o.jpeg"><br>
<a name="habracut"></a><br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">What for</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
For starters, it‚Äôs worth revealing a bit. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
It seems that robots armed with Deep Learning are about to oust people from their jobs everywhere. </font><font style="vertical-align: inherit;">In fact, everything is not so smooth. </font><font style="vertical-align: inherit;">Where actions are strictly repeated, processes are already really well automated. </font><font style="vertical-align: inherit;">If we are talking about ‚Äúsmart robots‚Äù, that is, applications where computer vision and algorithms are already enough. </font><font style="vertical-align: inherit;">But there are also many extremely complicated stories. </font><font style="vertical-align: inherit;">Robots can hardly cope with the variety of objects that have to deal with, and the diversity of the environment.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Key points</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
There are 3 key things in terms of implementation that are not yet found everywhere: </font></font><br>
<br>
<ul>
<li>       (data-driven learning). ..   ,    ,     ,    . ,     .</li>
<li>   ()    </li>
<li>  -  (Human-machine collaboration) </li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The second is also important because right now we will observe a change in approaches to learning, algorithms, behind them, and computing tools. </font><font style="vertical-align: inherit;">Perception and control algorithms will become more flexible. </font><font style="vertical-align: inherit;">A robot upgrade costs money. </font><font style="vertical-align: inherit;">And the calculator can be used more efficiently if it will serve several robots at once. </font><font style="vertical-align: inherit;">This concept is called ‚Äúcloud robotics‚Äù. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
With the latter, everything is simple - AI is not sufficiently developed right now to provide 100% reliability and accuracy in all situations that are required by business. </font><font style="vertical-align: inherit;">Therefore, the supervisor operator, who can sometimes help wards robots, will not hurt.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Scheme</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
To begin with, about a software / network platform that provides all the described functionality: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/fd/6y/fi/fd6yfi2svby-3l7hkn9lxxt3neo.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Components:</font></font><br>
<br>
<ol>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The robot sends a 3D video stream to the server and receives control in response. </font></font></li>
<li>    :  - ,      (, , , )</li>
<li>  ML  ( ),        ,   ,  .      ‚Äî  3D   ,     . </li>
<li> -  ,  3D       ,   UI   .   ‚Äî . </li>
</ol><br>
<h4> </h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
There are 2 modes of functioning of the robot: automatic and manual. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In manual mode, the robot works if the ML service is not yet trained. </font><font style="vertical-align: inherit;">Then the robot goes from automatic to manual either at the request of the operator (I saw strange behaviors while watching the robot), or when ML services themselves detect an anomaly. </font><font style="vertical-align: inherit;">About the detection of anomalies will be later - this is a very important part, without which it is impossible to apply the proposed approach. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The evolution of control is as follows:</font></font><br>
<br>
<ol>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The task for the robot is formed in human-readable terms and performance indicators are described.</font></font><br>
</li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The operator connects to the robot in VR and performs the task within the existing workflow for some time</font></font><br>
</li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ML part is trained on the received data</font></font><br>
</li>
<li>      ,     ML             <br>
</li>
<li>              <br>
</li>
</ol><br>
<h4>       3D</h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Very often, robots use the ROS (robot operating system) environment, which in fact is a framework for managing ‚Äúnodes‚Äù (nodes), each of which provides part of the robot‚Äôs functionality. In general, this is a relatively convenient way of programming robots, which in some ways resembles the microservice architecture of web applications in their essence. The main advantage of ROS is the industry standard and there are already a huge number of modules needed to create a robot. Even industrial robotic arms can have a ROS interface module. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The simplest thing is to create a bridge model between our server part and ROS. For example, </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">such</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Now our project uses a more developed version of the ROS ‚Äúnode‚Äù, which logs in and polls the microservice of the register to which relay server a particular robot can connect. The source code is given only as an example of instructions for installing the ROS module. At first, when you master this framework (ROS), everything looks pretty unfriendly, but the documentation is pretty good, and after a couple of weeks, developers begin to use its functionality quite confidently. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
From interesting - the problem of compression of the 3D data stream, which must be produced directly on the robot.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
It‚Äôs not so easy to compress the depth map. </font><font style="vertical-align: inherit;">Even with a small degree of compression of the RGB stream, a very serious local distortion of brightness from true in pixels at the borders or when moving objects is allowed. </font><font style="vertical-align: inherit;">The eye almost does not notice this, but as soon as the same distortions are allowed in the depth map, when rendering 3D everything becomes very bad: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/kv/1l/6-/kv1l6-qgxaqxhsf1a-zl67hymtk.png"><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
(from the </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">article</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ) </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
These defects at the edges greatly spoil the 3D scene, because </font><font style="vertical-align: inherit;">there is just a lot of garbage in the air.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We began to use frame-by-frame compression - JPEG for RGB and PNG for a depth map with small hacks. </font><font style="vertical-align: inherit;">This method compresses the 30FPS stream for a 3D scanner resolution of 640x480 at 25 Mbps. </font><font style="vertical-align: inherit;">Better compression can also be provided if traffic is critical to the application. </font><font style="vertical-align: inherit;">There are commercial 3D stream codecs that can also be used to compress this stream.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Virtual reality control</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
After we calibrated the reference frame of the camera and the robot (and we already wrote </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">an article about calibration</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ), the robot arm can be controlled in virtual reality. </font><font style="vertical-align: inherit;">The controller sets both the position in 3D XYZ and the orientation. </font><font style="vertical-align: inherit;">For some roboruk, only 3 coordinates will be enough, but with a large number of degrees of freedom, the orientation of the tool specified by the controller must also be transmitted. </font><font style="vertical-align: inherit;">In addition, there are enough controls on the controllers to execute robot commands such as turning on / off the pump, controlling the gripper, and others. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Initially, it was decided to use the JavaScript framework for virtual reality A-frame, based on the WebVR engine. </font><font style="vertical-align: inherit;">And the first results (video demonstration at the end of the article for the 4-coordinate arm) were obtained on the A-frame.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In fact, it turned out that WebVR (or A-frame) was an unsuccessful solution for several reasons:</font></font><br>
<br>
<ul>
<li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">compatibility mainly with FireFox</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , while it was in FireFox that the A-frame framework did not release texture resources (the rest of the browsers coped) until the memory consumption reached 16GB</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">limited interaction with VR controllers and helmet. </font><font style="vertical-align: inherit;">So, for example, it was not possible to add additional marks with which you can set the position, for example, of the operator's elbows.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The application required multithreading or several processes. </font><font style="vertical-align: inherit;">In one thread / process, it was necessary to unpack the video frames, in another - draw. </font><font style="vertical-align: inherit;">As a result, everything was organized through workers, but the unpacking time reached 30 ms, and rendering in VR should be done at a frequency of 90FPS.</font></font><br>
</li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
All these shortcomings resulted in the fact that the rendering of the frame did not have time in the allotted 10ms and there were very unpleasant twitches in VR. Probably, everything could be overcome, but the identity of each browser was a little annoying. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Now we decided to leave for C #, OpenTK and C # port of the OpenVR library. There is still an alternative - Unity. They write that Unity is for beginners ... but difficult. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The most important thing that needed to be found and known for gaining freedom:</font></font><br>
<br>
<pre><code class="cs hljs">VRTextureBounds_t bounds = <span class="hljs-keyword">new</span> VRTextureBounds_t() { uMin = <span class="hljs-number">0</span>, vMin = <span class="hljs-number">0</span>, uMax = <span class="hljs-number">1f</span>, vMax = <span class="hljs-number">1f</span> }; <font></font>
OpenVR.Compositor.Submit(EVREye.Eye_Left, <span class="hljs-keyword">ref</span> leftTexture, <span class="hljs-keyword">ref</span> bounds, EVRSubmitFlags.Submit_Default);<font></font>
OpenVR.Compositor.Submit(EVREye.Eye_Right, <span class="hljs-keyword">ref</span> rightTexture, <span class="hljs-keyword">ref</span> bounds, EVRSubmitFlags.Submit_Default);
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
(this is the code for sending two textures to the left and right eyes of the helmet) </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
i.e. </font><font style="vertical-align: inherit;">draw in OpenGL in the texture what different eyes see, and send it to glasses. </font><font style="vertical-align: inherit;">Joy knew no bounds when it turned out to fill the left eye with red, and the right with blue. </font><font style="vertical-align: inherit;">Just a couple of days and now the depth and RGB map coming via webSocket was transferred to the polygonal model in 10ms instead of 30 on JS. </font><font style="vertical-align: inherit;">And then just interrogate the coordinates and buttons of the controllers, enter the event system for the buttons, process user clicks, enter the State Machine for the UI and now you can grab a glass from the espresso:</font></font><br>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/EuBNaGZctf8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Now the quality of the Realsense D435 is somewhat depressing, but it will pass as soon as we install at least </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">such an interesting 3D scanner from Microsoft</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , the point cloud of which is much more accurate.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Server side</font></font></h4><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Relay server</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
The main functional element is the server relay (server in the middle), which receives a video stream from the robot with 3D images and sensor readings and the state of the robot and distributes it among consumers. Input data - packed frames and sensor readings coming over TCP / IP. Distribution to consumers is carried out by web-sockets (a very convenient mechanism for streaming to several consumers, including a browser). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In addition, the staging server stores the data stream in the S3 cloud storage so that it can later be used for training. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Each relay server supports the http API, which allows you to find out its current state, which is convenient for monitoring current connections.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The relay task is quite difficult, both from the point of view of computing and from the point of view of traffic. Therefore, here we followed the logic that relay servers are deployed on a variety of cloud servers. And that means that you need to keep track of who is connecting where (especially if robots and operators are in different regions). </font></font><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Register</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
The most reliable now will be hard to set for each robot which servers it can connect to (redundancy will not hurt). ML management service is associated with the robot, it polls the relay server to determine which one the robot is connected to and is connected to the corresponding one, provided, of course, it has sufficient rights for this. The operator‚Äôs application works in a similar way.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The most pleasant! Due to the fact that the training of robots is a service, the service is visible only to us inside. So, its front-end can be as convenient as possible for us! Those. it‚Äôs a console in the browser (there is a </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">terminalJS</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> library </font><font style="vertical-align: inherit;">that </font><font style="vertical-align: inherit;">is beautiful in its simplicity </font><font style="vertical-align: inherit;">, which is very easy to modify if you want additional functions, such as TAB auto-completion or playing call history) and looks like this: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/wl/vl/yo/wlvlyojuytiepjvhzexlxr-ixcq.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
This, of course, is a separate topic for discussion, why the command line so comfortable. By the way, it is especially convenient to do unit testing of such a frontend.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In addition to the http API, this service implements a mechanism for registering users with temporary tokens, login / logout operators, administrators and robots, session support, session encryption keys for traffic encryption between the relay server and the robot. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
All this is done in Python with Flask - a very close stack for ML developers (i.e. us). </font><font style="vertical-align: inherit;">Yes, in addition, the existing CI / CD infrastructure for microservices is on friendly terms with Flask.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Delay problem</font></font></h4> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
If we want to control the manipulators in real time, then the minimum delay is extremely useful. </font><font style="vertical-align: inherit;">If the delay becomes too large (more than 300 ms), then it is very difficult to control the manipulators based on the image in the virtual helmet. </font><font style="vertical-align: inherit;">In our solution, due to frame-by-frame compression (i.e., there is no buffering) and not using standard tools like GStreamer, the delay, even taking into account the intermediate server, is about 150-200 ms. </font><font style="vertical-align: inherit;">The transmission time over the network of them is about 80ms. </font><font style="vertical-align: inherit;">The rest of the delay is caused by the Realsense D435 camera and the limited capture frequency.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Of course, this is a full-height problem that arises in the ‚Äútracking‚Äù mode, when the manipulator in its reality constantly follows the operator‚Äôs controller in virtual reality. </font><font style="vertical-align: inherit;">In the mode of moving to a given point XYZ, the delay does not cause any problems for the operator.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ML part</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
There are 2 types of services: management and training. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The training service collects the data stored in the S3 storage and starts the re-training of the model weights. </font><font style="vertical-align: inherit;">At the end of the training, weights are sent to the management service. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The management service is no different in terms of input and output data from the operator‚Äôs application. </font><font style="vertical-align: inherit;">Likewise, the input RGBD (RGB + Depth) stream, sensor readings and robot status, the output - control commands. </font><font style="vertical-align: inherit;">Due to this identity, it appears possible to train in the framework of the concept of ‚Äúdata-driven training‚Äù.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The state of the robot (and sensor readings) is a key story for ML. It defines the context. For example, a robot will have a state machine that is characteristic of its operation, which largely determines what kind of control is necessary. These 2 values ‚Äã‚Äãare transmitted along with each frame: the operating mode and the state vector of the robot. </font></font><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">And a little about training:</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
In the demonstration at the end of the article was the task of finding an object (a children's cube) on a 3D scene. This is a basic task for pick &amp; place applications. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The training was based on a pair of ‚Äúbefore and after‚Äù frames and target designation obtained with manual control: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/n6/v2/s3/n6v2s3v59u7eumxhdh3gxucngxq.jpeg"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Due to the presence of two depth maps, it was easy to calculate the mask of the object moved in the frame: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/6a/vm/k-/6avmk-ue2jfndgwzy08zdb0qtqe.jpeg"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In addition, xyz are projected onto the camera plane and you can select the neighborhood of the captured object:</font></font><br>
<br>
<img src="https://habrastorage.org/webt/ha/_s/o-/ha_so-yh4xmxwsp4dkoo42tct9i.jpeg"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Actually with this neighborhood and will work. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
First we get XY by training Unet a convolutional network for cube segmentation. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Then, we need to determine the depth and understand if the image is abnormal in front of us. </font><font style="vertical-align: inherit;">This is done using an auto encoder in RGB and a conditional auto encoder in depth. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Model architecture for training auto encoder: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/xf/q4/ex/xfq4exzon5b63cucrz6aqi8kflm.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As a result, the logic of work:</font></font><br>
<br>
<ol>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">search for a maximum on the ‚Äúheat map‚Äù (determine the angular u = x / zv = y / z coordinates of the object) that exceeds the threshold</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">then the auto encoder reconstructs the neighborhood of the found point for all hypotheses in depth (with a given step from min_depth to max_depth) and selects the depth at which the discrepancy between reconstruction and input is minimal</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Having the angular coordinates u, v and depth, you can get the coordinates x, y, z</font></font></li>
</ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
An example of auto-encoder reconstruction of a map of cube depths with a correctly defined depth: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/av/1t/bu/av1tbu5dyvflda-_-lpbdw0t0qs.jpeg"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In part, the idea of ‚Äã‚Äãa depth search method is based on an </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">article about sets of auto-encoders</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
This approach works well for objects of various shapes. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
But, in general, there are many different approaches for finding an XYZ object from an RGBD image. Of course, it is necessary in practice and on a large amount of data to choose the most accurate method. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
There was also the task of detecting anomalies, for this we need a segmentation convolutional network to learn from the available masks. Then, according to this mask, you can evaluate the accuracy of the auto-encoder reconstruction in the depth map and RGB. Due to this discrepancy, one can decide on the presence of an anomaly.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Due to this method, it is possible to detect the appearance of previously unseen objects in the frame, which are nevertheless detected by the primary search algorithm.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Demonstration</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Checking and debugging of the entire created software platform was carried out at the stand:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3D camera Realsense D435</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">4 coordinate Dobot Magician</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">VR helmet HTC Vive</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Servers on Yandex Cloud (reduces latency compared to AWS cloud)</font></font></li>
</ul><br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/G5rBhaxHL_E" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In the video, we teach how to find a cube in a 3D scene by performing a task in VR pick &amp; place. </font><font style="vertical-align: inherit;">About 50 examples were enough for training on a cube. </font><font style="vertical-align: inherit;">Then the object changes and about 30 more examples are shown. </font><font style="vertical-align: inherit;">After retraining, the robot can find a new object. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The entire process took about 15 minutes, of which about half - training model weights. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
And in this video, YuMi controls in VR. </font><font style="vertical-align: inherit;">To learn how to manipulate objects, you need to evaluate the orientation and location of the tool. </font><font style="vertical-align: inherit;">Mathematics is built on a similar principle, but is now at the testing and development stage.</font></font><br>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/Q0yTey1srBM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Conclusion</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Big data and Deep learning is not all. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We are changing the approach to learning, moving towards how people learn new things - through repeating what they see. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The mathematical apparatus ‚Äúunder the hood‚Äù, which we will develop on real applications, is aimed at the problem of context-sensitive interpretation and control. </font><font style="vertical-align: inherit;">The context here is natural information available from robot sensors or external information about the current process. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
And, the more technological processes we master, the more the structure of the ‚Äúbrain in the clouds‚Äù will be developed, and its individual parts will be trained. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Strengths of this approach:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">the possibility of learning how to manipulate variable objects </font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">learning in a changing environment (e.g. mobile robots)</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">poorly structured tasks</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">short time to market; </font><font style="vertical-align: inherit;">You can perform the target even in manual mode using the operators</font></font><br>
</li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Limitation:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">need for reliable and good internet</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">additional methods are needed to achieve high accuracy, for example, cameras in the manipulator itself</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We are currently working on applying our approach to the standard pick &amp; place task of various objects. </font><font style="vertical-align: inherit;">But it seems to us (naturally!) That he is capable of more. </font><font style="vertical-align: inherit;">Any ideas where else to try your hand? </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Thank you for attention!</font></font></div>
      
    </div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/en486680/">https://habr.com/ru/post/en486680/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../en486670/index.html">Electronic passport of the Russian Federation, 2020th part of Marleson ballet</a></li>
<li><a href="../en486672/index.html">OpenVINO Hackathon: Recognizing Voice and Emotion on the Raspberry Pi</a></li>
<li><a href="../en486674/index.html">The digest of fresh materials from the world of the front-end for the last week No. 400 (January 27 - February 2, 2020)</a></li>
<li><a href="../en486676/index.html">Inevitability of FPGA penetration into data centers</a></li>
<li><a href="../en486678/index.html">Quartz in ASP.NET Core</a></li>
<li><a href="../en486682/index.html">Docker Compose: Simplify Using Makefile</a></li>
<li><a href="../en486684/index.html">My answer to those who believe that the value of TDD is exaggerated</a></li>
<li><a href="../en486686/index.html">About implementing a deep learning library in Python</a></li>
<li><a href="../en486688/index.html">Node.js, Tor, Puppeteer and Cheerio: anonymous web scraping</a></li>
<li><a href="../en486690/index.html">5 tips for writing quality arrow functions</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>