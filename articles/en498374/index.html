<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚ñ™Ô∏è ü§∑ üèÆ GPU Computing - Why, When, and How. Plus some tests üíÆ üö£üèª üí∏</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Everyone has long known that on video cards you can not only play toys, but also perform things that are not related to games, for example, train a ne...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>GPU Computing - Why, When, and How. Plus some tests</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/dbtc/blog/498374/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Everyone has long known that on video cards you can not only play toys, but also perform things that are not related to games, for example, train a neural network, remember cryptocurrency, or perform scientific calculations. How it happened, you can read it </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">here</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , but I wanted to touch on the topic of </font></font><i><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">why the GPU may be of interest to the</font></font></strong></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ordinary programmer (not related to GameDev) </font></font><i><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">how to approach</font></font></strong></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> the development of the GPU without spending a lot of time on it, </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">decide</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> whether look in this direction, and " </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">figure out on your fingers" what profit you can get.</font></font></strong>&nbsp;<br>
<br>
<div style="text-align:center;"><img width="800" src="https://habrastorage.org/getpro/habr/post_images/3ee/2ac/893/3ee2ac8936a685e6993966cfa40f53fd.jpg"></div><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The article was written based on my </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">presentation</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> in HighLoad ++. </font><font style="vertical-align: inherit;">It mainly discusses the technologies offered by NVIDIA. </font><font style="vertical-align: inherit;">I have no purpose to advertise any products, I just give them as an example, and for sure something similar can be found in competing manufacturers.</font></font><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Why count on the GPU?</font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Two processors can be compared according to different criteria, probably the most popular are the frequency and number of cores, the size of caches, etc., but in the end, we are interested in how many operations a processor can perform per unit of time, what kind of operation this is, but a separate question A common metric is the number of floating point operations per second - flops. And when we want to compare warm with soft, and in our case GPU with CPU, this metric comes in handy. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The graph below shows the growth of these same flops over time for processors and video cards.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5e2/048/3f5/5e20483f59e87b0a395b0fae0e6495c5.png"></div><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(Data is collected from open sources, there is no data for 2019-20 years, because not everything is so beautiful there, but the GPUs still win)</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Well, it‚Äôs tempting, isn't it? </font><font style="vertical-align: inherit;">We shift all the calculations from the CPU to the GPU and get eight times the best performance! </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
But, of course, not everything is so simple. </font><font style="vertical-align: inherit;">You can‚Äôt just take and transfer everything to the GPU, why, we‚Äôll talk further.</font></font><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GPU architecture and its comparison with the CPU</font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
I bring to many a familiar picture with the architecture of the CPU and the basic elements:</font></font><br>
<br>
<div style="text-align:center;"><img width="400" src="https://habrastorage.org/getpro/habr/post_images/df0/8c2/4c3/df08c24c3fe92cd97356670729c318cd.png"></div><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">CPU Core</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
What's so special? </font><font style="vertical-align: inherit;">One core and a bunch of auxiliary blocks. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Now let's look at the GPU architecture:</font></font><br>
<br>
<div style="text-align:center;"><img width="400" src="https://habrastorage.org/getpro/habr/post_images/0fe/138/0cc/0fe1380ccbb321b289d16e39a499009a.png"></div><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GPU Core</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
A video card has a lot of processing cores, usually several thousand, but they are combined into blocks; for NVIDIA video cards, usually 32 each, and have common elements, including and registers. The architecture of the GPU core and logical elements is much simpler than on the CPU, namely, there are no prefetchers, brunch predictors, and much more. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Well, these are the key points of differences in the architecture of the CPU and GPU, and, in fact, they impose restrictions or, conversely, open up the possibilities for what we can effectively read on the GPU.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
I didn‚Äôt mention one more important point, usually the video card and the processor do not ‚Äúrummage‚Äù between each other and write data to the video card and read the result back - these are separate operations and may turn out to be a ‚Äúbottleneck‚Äù in your system, a graph of the pumping time versus size data is given later in the article.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GPU limitations and features</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
What limitations does this architecture impose on executable algorithms:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">If we are calculating on a GPU, then we cannot select only one core, a whole block of cores will be allocated (32 for NVIDIA).</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">All cores execute the same instructions, but with different data (we will talk about this later), such calculations are called Single-Instruction-Multiple-Data or SIMD (although NVIDIA introduces its refinement).&nbsp;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Due to the relatively simple set of logic blocks and general registers, the GPU really does not like branching, and indeed the complex logic in the algorithms.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
What opportunities does it open:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Actually, the acceleration of those same SIMD calculations. </font><font style="vertical-align: inherit;">The simplest example is the elementwise addition of matrices, and let's analyze it.</font></font></li>
</ul><br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Reduction of classical algorithms to SIMD representation</font></font></h1><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Transformation</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We have two arrays, A and B, and we want to add an element from array B to each element of array A. Below is an example in C, although I hope it will be clear to those who do not speak this language:</font></font><br>
<br>
<pre><code class="cpp hljs"><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">func</span><span class="hljs-params">(<span class="hljs-keyword">float</span> *A, <span class="hljs-keyword">float</span> *B, size)</span>
</span>{ 
   <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; size; i++) <font></font>
   { <font></font>
       A[i] += B[i]<font></font>
   } <font></font>
}<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Classical loopback of elements in a loop and linear runtime. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Now let's see how such code will look for the GPU:</font></font><br>
<br>
<pre><code class="cpp hljs"><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">func</span><span class="hljs-params">(<span class="hljs-keyword">float</span> *A, <span class="hljs-keyword">float</span> *B, size)</span> 
</span>{ 
   <span class="hljs-keyword">int</span> i = threadIdx.x; 
   <span class="hljs-keyword">if</span> (i &lt; size) <font></font>
      A[i] += B[i] <font></font>
}<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
And here it‚Äôs already interesting, the threadIdx variable appeared, which we did not seem to declare anywhere. Yes, its system provides us. Imagine that in the previous example the array consists of three elements, and you want to run it in three parallel threads. To do this, you would need to add another parameter - the index or stream number. This is what the video card does for us, though it passes the index as a static variable and can work with several dimensions at once - x, y, z. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Another nuance, if you are going to start a large number of parallel streams at once, then the streams will have to be divided into blocks (an architectural feature of video cards). The maximum block size depends on the video card, and the index of the element for which we perform calculations will need to be obtained as follows:</font></font><br>
<br>
<pre><code class="cpp hljs"><span class="hljs-keyword">int</span> i = blockIdx.x * blockDim.x + threadIdx.x; <span class="hljs-comment">// blockIdx ‚Äì  , blockDim ‚Äì  , threadIdx ‚Äì    </span>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As a result, what we have: a lot of parallel-running threads that execute the same code, but with different indices, and, accordingly, data, i.e. </font><font style="vertical-align: inherit;">the same SIMD. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
This is the simplest example, but if you want to work with the GPU, you need to bring your task to the same form. </font><font style="vertical-align: inherit;">Unfortunately, this is not always possible and in some cases may become the subject of a doctoral dissertation, but nevertheless, classical algorithms can still be brought to this form.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aggregation</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Let's now see how the aggregation cast to the SIMD representation will look:</font></font><br>
&nbsp;<br>
<div style="text-align:center;"><img width="400" src="https://habrastorage.org/getpro/habr/post_images/ecd/78a/bba/ecd78abbaff0c1be8799c1337f7652f8.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We have an array of n elements. </font><font style="vertical-align: inherit;">At the first stage, we start n / 2 threads and each thread adds two elements, i.e. </font><font style="vertical-align: inherit;">in one iteration, we add together half of the elements in the array. </font><font style="vertical-align: inherit;">And then in the loop we repeat the same thing for the newly created array, until we aggregate the last two elements. </font><font style="vertical-align: inherit;">As you can see, the smaller the size of the array, the less parallel threads we can start, i.e. </font><font style="vertical-align: inherit;">on a GPU, it makes sense to aggregate arrays of a sufficiently large size. </font><font style="vertical-align: inherit;">Such an algorithm can be used to calculate the sum of elements (by the way, do not forget about the possible overflow of the type of data you are working with), search for maximum, minimum, or just search.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sorting</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
But sorting already looks a lot more complicated. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The two most popular sorting algorithms on the GPU are:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bitonic-sort</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Radix-sort</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
But radix-sort is still used more often, and production-ready implementation can be found in some libraries. </font><font style="vertical-align: inherit;">I will not analyze in detail how these algorithms work; those who are interested can find a description of radix-sort at </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">https://www.codeproject.com/Articles/543451/Parallel-Radix-Sort-on-the-GPU-using-Cplusplus- AMP</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> and </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">https://stackoverflow.com/a/26229897</font></font></a><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
But the idea is that even such a non-linear algorithm as sorting can be reduced to a SIMD view. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
And now, before looking at the real numbers that can be obtained from the GPU, let's figure out how to program for this miracle of technology?</font></font><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Where to start</font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The most common two technologies that can be used for programming under the GPU:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Opencl</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cuda</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
OpenCL is a standard that is supported by most video card manufacturers, including </font><font style="vertical-align: inherit;">and on mobile devices, also code written in OpenCL can be run on the CPU. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
You can use OpenCL from C / C ++, there are binders to other languages. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
For OpenCL, I liked the book </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">OpenCL in Action the</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> most </font><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">It also describes different algorithms on the GPU, including </font><font style="vertical-align: inherit;">Bitonic-sort and Radix-sort. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
CUDA is NVIDIA's proprietary technology and SDK. </font><font style="vertical-align: inherit;">You can write in C / C ++ or use bindings to other languages.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Comparing OpenCL and CUDA is somewhat not correct, because </font><font style="vertical-align: inherit;">one is the standard, the other is the whole SDK. </font><font style="vertical-align: inherit;">Nevertheless, many people choose CUDA for development for video cards, despite the fact that the technology is proprietary, although free and only works on NVIDIA cards. </font><font style="vertical-align: inherit;">There are several reasons for this:</font></font><br>
<br>
<ul>
<li>  API</li>
<li>    </li>
<li>,   GPU,      (host) </li>
<li> ,  ..  </li>
<li>   </li>
<li>  </li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The peculiarities include the fact that CUDA comes with its own compiler, which can also compile standard C / C ++ code. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The most comprehensive CUDA book I came across was </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Professional CUDA C Programming</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , although it‚Äôs already a little outdated, nevertheless it discusses a lot of technical nuances of programming for NVIDIA cards. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
But what if I do not want to spend a couple of months reading these books, writing my own program for a video card, testing and debugging, and then finding out that this is not for me?&nbsp; </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As I said, there are a large number of libraries that hide the complexity of development under the GPU: XGBoost, cuBLAS, TensorFlow, PyTorch and others, we will consider the </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;">thrust</font></a><font style="vertical-align: inherit;"> library</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, since it is less specialized than the other libraries above, but at the same time it implements basic algorithms, for example, sorting, searching, aggregation, and with a high probability it can be applicable in your tasks. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Thrust is a C ++ library that aims to "replace" standard STL algorithms with GPU-based algorithms. </font><font style="vertical-align: inherit;">For example, sorting an array of numbers using this library on a video card would look like this:</font></font><br>
<br>
<pre><code class="cpp hljs"><span class="hljs-function">thrust::host_vector&lt;DataType&gt; <span class="hljs-title">h_vec</span><span class="hljs-params">(size)</span></span>; <span class="hljs-comment">//    </span>
<span class="hljs-built_in">std</span>::generate(h_vec.begin(), h_vec.end(), rand); <span class="hljs-comment">//   </span>
thrust::device_vector&lt;DataType&gt; d_vec = h_vec; <span class="hljs-comment">//        &nbsp;</span>
thrust::sort(d_vec.begin(), d_vec.end()); <span class="hljs-comment">//    </span>
thrust::copy(d_vec.begin(), d_vec.end(), h_vec.begin()); <span class="hljs-comment">//   ,     </span>
</code></pre><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(do not forget that the example must be compiled by a compiler from NVIDIA)</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As you can see, thrust :: sort is very similar to a similar algorithm from STL. This library hides many difficulties, in particular the development of a subprogram (more precisely, the kernel), which will be executed on the video card, but at the same time deprives of flexibility. For example, if we want to sort several gigabytes of data, it would be logical to send a piece of data to the card to start sorting, and while sorting is in progress, send more data to the card. This approach is called latency hiding and allows more efficient use of server map resources, but, unfortunately, when we use high-level libraries, such opportunities remain hidden. But for prototyping and measuring performance, they are just the same, especially with thrust you can measure which overhead the data transfer provides. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
I wrote a small </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">benchmark</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> using this library, which runs several popular algorithms with different amounts of data on the GPU, let's see what the results are.</font></font><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GPU Algorithm Results</font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
To test the GPU, I took an instance in AWS with a Tesla k80 video card, this is not the most powerful server card to date (the most powerful Tesla v100), but the most affordable and has on board:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">4992 CUDA Kernels</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">24 GB of memory</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">480 Gb / s - memory bandwidth&nbsp;</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
And for tests on the CPU, I took an instance with an Intel Xeon processor CPU E5-2686 v4 @ 2.30GHz</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Transformation</font></font></h2><br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/930/6e1/687/9306e1687be5ee95c29c8aac7b2ae337.png"></div><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Transformation execution time on the GPU and CPU in ms</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
As you can see, the usual transformation of the array elements is approximately the same in time, both on the GPU and on the CPU. </font><font style="vertical-align: inherit;">And why? </font><font style="vertical-align: inherit;">Because the overhead for sending data to the card and back eats up the entire performance boost (we'll talk about overhead separately), and there are relatively few calculations on the card. </font><font style="vertical-align: inherit;">Also, do not forget that processors also support SIMD instructions, and compilers in simple cases can effectively use them.&nbsp;</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Let's now see how efficiently aggregation is done on the GPU.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aggregation</font></font></h2><br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c92/d0e/cb9/c92d0ecb96c32866000e6948f5da61f9.png"></div><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aggregation execution time on GPU and CPU in ms</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
In the aggregation example, we already see a significant performance increase with an increase in data volume. </font><font style="vertical-align: inherit;">It is also worth paying attention to the fact that we pump a large amount of data into the memory of the card, and only one aggregated value is taken back, i.e. </font><font style="vertical-align: inherit;">Overhead for transferring data from the card to RAM is minimal. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Let's move on to the most interesting example - sorting.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sorting</font></font></h2><br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fde/504/8da/fde5048da5084d1f0902c9362b21d939.png"></div><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sorting time to the GPU and CPU in ms</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Despite the fact that we send the entire data array to the video card and vice versa, sorting to the GPU 800 MB of data is approximately 25 times faster than on the processor.</font></font><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Data transfer overhead</font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As can be seen from the transformation example, it is not always obvious whether the GPU will be effective even in those tasks that parallel well. </font><font style="vertical-align: inherit;">The reason for this is an overhead to transfer data from the computer‚Äôs RAM to the video card‚Äôs memory (in game consoles, by the way, the memory is shared between the CPU and GPU, and there is no need to transfer data). </font><font style="vertical-align: inherit;">One of the characteristics of a video card is the memory bandwidth or memory bandwidth, which determines the theoretical bandwidth of the card. </font><font style="vertical-align: inherit;">For Tesla k80 it is 480 GB / s, for Tesla v100 it is already 900 GB / s. </font><font style="vertical-align: inherit;">Also, the PCI Express version and the implementation of how you will transfer data to the card will affect the throughput, for example, this can be done in several parallel streams.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Let's look at the practical results that were obtained for the Tesla k80 graphics card in the Amazon cloud:</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/798/fb5/613/798fb56139f6158566232bc6283b24e7.png"></div><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Time to transfer data to the GPU, sort and transfer data back to RAM in ms </font></font></i><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">HtoD - transfer data to the </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
GPU </font><font style="vertical-align: inherit;">video card </font><font style="vertical-align: inherit;">Execution - sort on the video card </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
DtoH - copy data from the video card to RAM</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
The first thing to note is that reading data from the video card is faster than write them down there. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The second - when working with a video card, you can get latency from 350 microseconds, and this may already be enough for some low latency applications. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The chart below shows an overhead for more data:</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d18/653/b96/d18653b96af325f35fade713bdaa8dae.png"></div><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Time to transfer data to the GPU, sort and transfer data back to RAM in ms</font></font></i><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Server use</font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The most common question is how does a game video card differ from a server one? </font><font style="vertical-align: inherit;">According to the characteristics, they are very similar, but prices differ significantly.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/203/01b/741/20301b7418ee616d9611f42d2b4a8f5d.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The main differences between the server (NVIDIA) and the game card:</font></font><br>
<br>
<ul>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Manufacturer's warranty</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (the game card is not designed for server use)</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Possible </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">virtualization</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> issues </font><font style="vertical-align: inherit;">for a consumer graphics card</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Availability of error correction mechanism on the server card</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The number of parallel threads (not CUDA cores) or support for Hyper-Q, which allows you to work with the card from several threads on the CPU, for example, upload data to one card from one thread and start calculations from another</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
These are, perhaps, the main important differences that I found.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Multithreading</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
After we figured out how to run the simplest algorithm on the video card and what results can be expected, the next logical question is how the video card will behave when processing several parallel requests. As an answer, I have two graphs of computing on the GPU and a processor with 4 and 32 cores:</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0a1/1f6/022/0a11f6022198a582929f384be357fe43.png"></div><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The time taken to perform mathematical calculations on the GPU and CPU with matrices of 1000 x 60 in ms</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
. This graph performs calculations with matrices of 1000 x 60 elements. </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Calculations are</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> started </font><font style="vertical-align: inherit;">from several program streams, a separate stream is created for the GPU for each CPU stream (the very Hyper-Q is used).&nbsp; </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As you can see, the processor copes with this load very well, while the latency for one request per GPU increases significantly with an increase in the number of parallel requests.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e09/22c/7fb/e0922c7fba0ef001cca97c7a99817c83.png"></div><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The time for performing mathematical calculations on the GPU and CPU with matrices 10,000 x 60 in ms.</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
On the second graph, the same calculations, but with matrices 10 times longer, and the GPU behaves much better under such a load. These graphs are very indicative, and we can conclude: the behavior under load depends on the nature of the load itself. A processor can also handle matrix calculations quite efficiently, but to a certain extent. For a video card, it is characteristic that for a small computing load, performance drops approximately linearly. With an increase in load and the number of parallel threads, the video card copes better.&nbsp;</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
It is difficult to hypothesize how the GPU will behave in various situations, but as you can see, under certain conditions, a server card can process requests from several parallel streams quite efficiently. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We will discuss a few more questions that you may have if you still decide to use the GPU in your projects.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Resource limit</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As we have already said, the two main resources of a video card are computing cores and memory. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
For example, we have several processes or containers using a video card, and we would like to be able to share the video card between them. </font><font style="vertical-align: inherit;">Unfortunately, there is no simple API for this. </font><font style="vertical-align: inherit;">NVIDIA offers </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">vGPU</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> technology </font><font style="vertical-align: inherit;">, but I did not find the Tesla k80 card in the list of supported ones, and as far as I can understand from the description, the technology is more focused on virtual displays than on calculations. </font><font style="vertical-align: inherit;">Perhaps AMD offers something more suitable. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Therefore, if you plan to use the GPU in your projects, you should rely on the fact that the application will use the video card exclusively, or you will programmatically control the amount of allocated memory and the number of cores used for calculations.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Containers and GPU</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
If you figured out the resource limit, then the following logical question: what if there are several video cards in the server? </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Again, you can decide at the application level which GPU it will use. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Another more convenient way is Docker containers. </font><font style="vertical-align: inherit;">You can use regular containers, but NVIDIA offers its </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">NGC</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> containers </font><font style="vertical-align: inherit;">, with optimized versions of various software, libraries and drivers. </font><font style="vertical-align: inherit;">For one container, you can limit the number of GPUs used and their visibility to the container. </font><font style="vertical-align: inherit;">Overhead on container usage is about 3%.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Work in a cluster</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Another question, what if you want to perform one task on multiple GPUs within the same server or cluster? </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
If you chose a library similar to thrust or a lower-level solution, then the task will have to be solved manually. </font><font style="vertical-align: inherit;">High-level frameworks, for example, for machine learning or neural networks, usually support the ability to use multiple cards out of the box. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In addition, I would like to note that, for example, NVIDIA offers an interface for direct data exchange between cards - </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">NVLINK</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , which is significantly faster than PCI Express. </font><font style="vertical-align: inherit;">And there is technology for direct access to the memory of the card from other PCI Express devices - </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GPUDirect RDMA</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , incl. </font><font style="vertical-align: inherit;">and </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">network</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Recommendations</font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
If you are considering using the GPU in your projects, then the GPU is most likely suitable for you if:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Your task can be reduced to a SIMD view</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">It is possible to load most of the data on the map before calculations (cache)</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The challenge involves intensive computing</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
You should also ask questions in advance:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">How many parallel queries will be&nbsp;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">What latency do you expect</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Do you need one card for your load? Do you need a server with several cards or a cluster of GPU servers&nbsp;</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
That's all, I hope that the material will be useful to you and help you make the right decision!</font></font><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">References</font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Benchmark and results on github - </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">https://github.com/tishden/gpu_benchmark/tree/master/cuda</font></font></a><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
In addition to the topic, a recording of the report </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ÄúGPU Databases - Architecture, Performance and Prospects for Use‚Äù</font></font></a><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
NVIDIA NGC Containers Webinar - </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">http : //bit.ly/2UmVIVt</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> or </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">http://bit.ly/2x4vJKF</font></font></a></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../en498362/index.html">Kingston maintains leadership in SSD shipments: how do we do it?</a></li>
<li><a href="../en498366/index.html">What algorithms do Yandex developers implement every day</a></li>
<li><a href="../en498368/index.html">The story of one switch</a></li>
<li><a href="../en498370/index.html">SAP UI5 and Confirmation Windows: Again About the Context</a></li>
<li><a href="../en498372/index.html">Network simulator tutorial ns-3. Chapter 5</a></li>
<li><a href="../en498378/index.html">Announcement of Slurm's Evening School by Agile</a></li>
<li><a href="../en498380/index.html">Overton's window in action: how a pandemic is used to limit our freedom</a></li>
<li><a href="../en498390/index.html">IAR + Clion = friendship</a></li>
<li><a href="../en498392/index.html">18 GitLab features go open source</a></li>
<li><a href="../en498394/index.html">7 free analogues of Screaming Frog and Netpeak Spider</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>