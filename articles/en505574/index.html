<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèΩ‚Äçü§ù‚Äçüë©üèº üõçÔ∏è üï∫ Reinforced learning through competitive neural networks üë©‚Äçüåæ üå∏ üò≤</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In the classic game "tic-tac-toe" there is the opportunity to present all the likely moves - and never lose. I used this opportunity as a metric of my...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Reinforced learning through competitive neural networks</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/505574/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In the classic game "tic-tac-toe" there is the opportunity to present all the likely moves - and never lose. I used this opportunity as a metric of my training in the neural network of the game. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Reinforced training will be useful for tasks with an ambiguous decision, complicated by the many options for choosing an action with different outcomes for each. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Of course, tic-tac-toe does not look like a difficult game to train them with reinforcements. However, it is well suited for mastering the training methodology through competitive networks, which will improve quality and reduce the time spent on training the network. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Next, I will describe the general learning algorithm with reinforcement through competitive networks in the context of a tic-tac-toe game with a demonstration of a trained network to make ‚Äúmeaningful‚Äù moves, that is, to play.</font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Recording a game of a trained network. </font></font></a><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Train the network from scratch. </font></font></a><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sources.</font></font></a><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
You can also enter a pre- </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;">trained</font></a><font style="vertical-align: inherit;"> model from GitHub by clicking on the corresponding button to immediately start testing a neural network.</font></font><br>
<a name="habracut"></a><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The first steps in training neural networks</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In addition to the fact that a neuron has an activation function, which amends the resulting solution of a neural network, we can also say that neurons are network memory. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Each layer increases the training time due to the propagation of the inverse error through the layers ‚Äúupward‚Äù, and the signal gradually fades before it reaches the ‚Äúupper‚Äù layers, which begin the path of decision making. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
After several options for network settings, I came to the conclusion that for a simple game with a 3x3 field it would be enough to use a single-layer network with 128 neurons. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The network should not have too much memory, this can lead to retraining - the complete memorization of all options for the outcome of the game. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The strength of neural networks in the expressiveness of approximating a solution based on input data in conditions of limited memory.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">General rules for promoting agents</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
For relative forecasting by a neural network, each cell has a dynamic reward depending on its significance for the agent at the moment. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
At the output, the neural network predicts the index of the cell where the agent will go. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Rewards of cells have the following distribution: </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The fewer moves made, the higher the reward from 0.1 to 1.0 An </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
occupied cell has a reward of -1.0 A </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
losing cell will receive a reward -0.4 A </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
free cell has a reward 0.1 A </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
move to a free cell increases its reward to 0.2 A </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
winning cell of an opponent has a reward of 0.5 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Winning slot will bring a reward 1.0</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Competitive neural network training</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
At the competition, agents will be trained in a competitive environment, which will lead to new outcomes of the game and improve the quality of training for new situations. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Each agent has its own field to train to go to a free cell and build winning combinations of moves. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Agents play 9 games at home, then move on to the competitive field for 1 game, where the game is played until the winner with a limit of 9 moves, then everything is repeated again. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
At the end of each game, both networks are trained on a new experience of rivalry on a common playing field.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Opponent Prevention</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The network needs to be trained to compete for victory on the field, i.e. </font><font style="vertical-align: inherit;">reward for successful prevention of winning an opponent by increasing cell reward. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Another metric for training neural networks are the indicators of victory in competitions. </font><font style="vertical-align: inherit;">If the margin of victory for one player is too large, then the network is probably learning incorrectly, and the reason for this is incorrect rewards for the actions of agents, or any other actions and their rewards have not been taken into account. </font><font style="vertical-align: inherit;">The best result of training can be considered a situation when the networks will go almost equal, winning and losing about the same number of times.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Competitive Learning with a Man</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The implementation of training a neural network to play with humans is not much different from competition between agents. </font><font style="vertical-align: inherit;">The only serious difference is that the person initially plays reasonably. </font><font style="vertical-align: inherit;">A party with such an opponent creates additional situations for the agent, which will favorably affect his playing experience and, accordingly, his training.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Completion</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The neural network learned to play tic-tac-toe only after the introduction of the competitive algorithm, which allowed it to learn how to make moves in response to the moves of the opponent, although not perfectly, as originally planned. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In general, I think the project has been successfully completed - the goal has been achieved.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Thank you for the attention!</font></font></h2><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ps Train competitive networks, this allows you to look at simple games from a different angle.</font></font></i></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../en505554/index.html">At the Competition ‚ÄúYoung Technicians and Inventors‚Äù real young inventors are not needed</a></li>
<li><a href="../en505556/index.html">Apple tracks looted iPhones and gives police looters</a></li>
<li><a href="../en505558/index.html">Amazon DeepLens Deep Learning Camera. Unpacking, connecting and deploying a project</a></li>
<li><a href="../en505560/index.html">The second set for a product management program at the CS center: what students say</a></li>
<li><a href="../en505568/index.html">Transferring files using pipes and other little things on Delphi</a></li>
<li><a href="../es486176/index.html">Memo de correspondencia de correo electr√≥nico corporativo</a></li>
<li><a href="../es486178/index.html">FOSS News No. 1 - revisi√≥n de noticias gratuitas y de c√≥digo abierto del 27 de enero al 2 de febrero de 2020</a></li>
<li><a href="../es486180/index.html">Consejos y fuentes para crear aplicaciones sin servidor</a></li>
<li><a href="../es486184/index.html">C√≥mo usar la b√∫squeda de manera efectiva</a></li>
<li><a href="../es486186/index.html">Nube catastr√≥fica: c√≥mo funciona</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>