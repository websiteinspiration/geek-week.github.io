<!doctype html>
<html class="no-js" lang="de">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🆒 📦 🕴🏼 Random Forest, die Methode der Hauptkomponenten und die Optimierung von Hyperparametern: Ein Beispiel für die Lösung des Klassifizierungsproblems in Python 👨‍👨‍👧‍👧 🧖 🖊️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Datenverarbeitungs- und Analysespezialisten verfügen über zahlreiche Tools zum Erstellen von Klassifizierungsmodellen. Eine der beliebtesten und zuver...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Random Forest, die Methode der Hauptkomponenten und die Optimierung von Hyperparametern: Ein Beispiel für die Lösung des Klassifizierungsproblems in Python</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ruvds/blog/488342/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Datenverarbeitungs- und Analysespezialisten verfügen über zahlreiche Tools zum Erstellen von Klassifizierungsmodellen. </font><font style="vertical-align: inherit;">Eine der beliebtesten und zuverlässigsten Methoden zur Entwicklung solcher Modelle ist die Verwendung des Random Forest (RF) -Algorithmus. </font><font style="vertical-align: inherit;">Um zu versuchen, die Leistung eines mit dem </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">RF-</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Algorithmus erstellten Modells zu verbessern </font><font style="vertical-align: inherit;">, können Sie die Optimierung des Hyperparameters des Modells ( </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hyperparameter Tuning</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , HT) verwenden. </font><font style="vertical-align: inherit;">
Darüber hinaus gibt es einen weit verbreiteten Ansatz, nach dem die Daten vor der Übertragung in das Modell mithilfe der </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;">Hauptkomponentenanalyse</font></a><font style="vertical-align: inherit;"> verarbeitet werden</font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/webt/tt/m5/h7/ttm5h7jbbx2u2wuc1var1azxwew.jpeg"></a><br>
<br><font style="vertical-align: inherit;"></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, PCA). </font><font style="vertical-align: inherit;">Aber lohnt es sich zu benutzen? </font><font style="vertical-align: inherit;">Ist es nicht der Hauptzweck des RF-Algorithmus, dem Analytiker zu helfen, die Bedeutung der Merkmale zu interpretieren?</font></font><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ja, die Verwendung des PCA-Algorithmus kann zu einer geringfügigen Komplikation der Interpretation jedes „Merkmals“ bei der Analyse der „Bedeutung von Merkmalen“ des RF-Modells führen. Der PCA-Algorithmus reduziert jedoch die Dimension des Merkmalsraums, was zu einer Verringerung der Anzahl von Merkmalen führen kann, die vom RF-Modell verarbeitet werden müssen. Bitte beachten Sie, dass das Rechenvolumen einer der Hauptnachteile des Random Forest-Algorithmus ist (dh die Fertigstellung des Modells kann lange dauern). Die Anwendung des PCA-Algorithmus kann ein sehr wichtiger Bestandteil der Modellierung sein, insbesondere in Fällen, in denen sie mit Hunderten oder sogar Tausenden von Funktionen arbeiten. Wenn das Wichtigste darin besteht, einfach das effektivste Modell zu erstellen und gleichzeitig die Genauigkeit der Bestimmung der Wichtigkeit der Attribute zu opfern, ist die PCA möglicherweise einen Versuch wert.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nun zum Punkt. </font><font style="vertical-align: inherit;">Wir werden mit einem </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Brustkrebs-Datensatz arbeiten</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;">Scikit-Learn "Brustkrebs"</font></a><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Wir werden drei Modelle erstellen und deren Wirksamkeit vergleichen. </font><font style="vertical-align: inherit;">Wir sprechen nämlich über folgende Modelle:</font></font><br>
<br>
<ol>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Das Basismodell basiert auf dem RF-Algorithmus (wir werden dieses RF-Modell abkürzen).</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Das gleiche Modell wie Nr. 1, jedoch eines, bei dem eine Verringerung der Dimension des Merkmalsraums unter Verwendung der Hauptkomponentenmethode (RF + PCA) angewendet wird.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Das gleiche Modell wie Nr. 2, jedoch mit Hyperparameteroptimierung (RF + PCA + HT) erstellt.</font></font></li>
</ol><br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1. Daten importieren</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Laden Sie zunächst die Daten und erstellen Sie einen Pandas-Datenrahmen. </font><font style="vertical-align: inherit;">Da wir einen vorab gelöschten „Spielzeug“ -Datensatz von Scikit-learn verwenden, können wir danach bereits mit dem Modellierungsprozess beginnen. </font><font style="vertical-align: inherit;">Aber selbst wenn Sie solche Daten verwenden, wird empfohlen, dass Sie immer mit der Arbeit beginnen, indem Sie eine vorläufige Analyse der Daten mit den folgenden Befehlen durchführen, die auf den Datenrahmen ( </font></font><code>df</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">) </font><font style="vertical-align: inherit;">angewendet werden </font><font style="vertical-align: inherit;">:</font></font><br>
<br>
<ul>
<li><code>df.head()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - um einen Blick auf den neuen Datenrahmen zu werfen und zu sehen, ob er wie erwartet aussieht.</font></font></li>
<li><code>df.info()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- um die Merkmale von Datentypen und Spalteninhalten herauszufinden. </font><font style="vertical-align: inherit;">Möglicherweise muss die Datentypkonvertierung durchgeführt werden, bevor Sie fortfahren können.</font></font></li>
<li><code>df.isna()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- um sicherzustellen, dass die Daten keine Werte enthalten </font></font><code>NaN</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Die entsprechenden Werte müssen gegebenenfalls irgendwie verarbeitet werden, oder es kann erforderlich sein, ganze Zeilen aus dem Datenrahmen zu entfernen.</font></font></li>
<li><code>df.describe()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - Ermittlung der minimalen, maximalen und durchschnittlichen Werte der Indikatoren in den Spalten, Ermittlung der Indikatoren für das mittlere Quadrat und die wahrscheinliche Abweichung in den Spalten.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In unserem Datensatz ist eine Spalte </font></font><code>cancer</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(Krebs) die Zielvariable, deren Wert wir mithilfe des Modells vorhersagen möchten. </font></font><code>0</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">bedeutet "keine Krankheit". </font></font><code>1</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- "das Vorhandensein der Krankheit."</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_breast_cancer<font></font>
columns = [<span class="hljs-string">'mean radius'</span>, <span class="hljs-string">'mean texture'</span>, <span class="hljs-string">'mean perimeter'</span>, <span class="hljs-string">'mean area'</span>, <span class="hljs-string">'mean smoothness'</span>, <span class="hljs-string">'mean compactness'</span>, <span class="hljs-string">'mean concavity'</span>, <span class="hljs-string">'mean concave points'</span>, <span class="hljs-string">'mean symmetry'</span>, <span class="hljs-string">'mean fractal dimension'</span>, <span class="hljs-string">'radius error'</span>, <span class="hljs-string">'texture error'</span>, <span class="hljs-string">'perimeter error'</span>, <span class="hljs-string">'area error'</span>, <span class="hljs-string">'smoothness error'</span>, <span class="hljs-string">'compactness error'</span>, <span class="hljs-string">'concavity error'</span>, <span class="hljs-string">'concave points error'</span>, <span class="hljs-string">'symmetry error'</span>, <span class="hljs-string">'fractal dimension error'</span>, <span class="hljs-string">'worst radius'</span>, <span class="hljs-string">'worst texture'</span>, <span class="hljs-string">'worst perimeter'</span>, <span class="hljs-string">'worst area'</span>, <span class="hljs-string">'worst smoothness'</span>, <span class="hljs-string">'worst compactness'</span>, <span class="hljs-string">'worst concavity'</span>, <span class="hljs-string">'worst concave points'</span>, <span class="hljs-string">'worst symmetry'</span>, <span class="hljs-string">'worst fractal dimension'</span>]<font></font>
dataset = load_breast_cancer()<font></font>
data = pd.DataFrame(dataset[<span class="hljs-string">'data'</span>], columns=columns)<font></font>
data[<span class="hljs-string">'cancer'</span>] = dataset[<span class="hljs-string">'target'</span>]<font></font>
display(data.head())<font></font>
display(data.info())<font></font>
display(data.isna().sum())<font></font>
display(data.describe())</code></pre><br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/bce/fb8/b60/bcefb8b60462b7658b40e1e56f7744ab.png"></div><br>
<i><font color="#999999">      .       .  , cancer,   ,    . 0  « ». 1 — « »</font></i><br>
 <br>
<h2><font color="#3AC1EF">2.        </font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Teilen Sie nun die Daten mit der Scikit-Lernfunktion auf </font></font><code>train_test_split</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Wir möchten dem Modell so viele Trainingsdaten wie möglich geben. Wir müssen jedoch über genügend Daten verfügen, um das Modell zu testen. Im Allgemeinen können wir sagen, dass mit zunehmender Anzahl von Zeilen im Datensatz auch die Datenmenge zunimmt, die als lehrreich angesehen werden kann. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wenn beispielsweise Millionen von Zeilen vorhanden sind, können Sie den Satz aufteilen, indem Sie 90% der Zeilen für Trainingsdaten und 10% für Testdaten markieren. Der Testdatensatz enthält jedoch nur 569 Zeilen. Und das ist nicht so sehr für das Training und Testen des Modells. Um in Bezug auf die Trainings- und Verifizierungsdaten fair zu sein, werden wir den Satz in zwei gleiche Teile aufteilen - 50% - Trainingsdaten und 50% - Verifizierungsdaten. Wir installieren</font></font><code>stratify=y</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> um sicherzustellen, dass sowohl der Trainings- als auch der Testdatensatz das gleiche Verhältnis von 0 und 1 wie der Originaldatensatz haben.</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<font></font>
X = data.drop(<span class="hljs-string">'cancer'</span>, axis=<span class="hljs-number">1</span>)&nbsp;&nbsp;<font></font>
y = data[<span class="hljs-string">'cancer'</span>]&nbsp;<font></font>
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="hljs-number">0.50</span>, random_state = <span class="hljs-number">2020</span>, stratify=y)</code></pre><br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3. Datenskalierung</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bevor Sie mit der Modellierung fortfahren, müssen Sie die Daten durch </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Skalieren</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> „zentrieren“ und „standardisieren“ </font><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Die Skalierung erfolgt aufgrund der Tatsache, dass unterschiedliche Größen in unterschiedlichen Einheiten ausgedrückt werden. </font><font style="vertical-align: inherit;">Mit diesem Verfahren können Sie einen „fairen Kampf“ zwischen den Zeichen organisieren, um deren Bedeutung zu bestimmen. </font><font style="vertical-align: inherit;">Außerdem konvertieren wir </font></font><code>y_train</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">vom Pandas-Datentyp </font></font><code>Series</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">in das NumPy-Array, damit das Modell später mit den entsprechenden Zielen arbeiten kann.</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler<font></font>
ss = StandardScaler()<font></font>
X_train_scaled = ss.fit_transform(X_train)<font></font>
X_test_scaled = ss.transform(X_test)<font></font>
y_train = np.array(y_train)</code></pre><br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">4. Training des Grundmodells (Modell Nr. 1, RF)</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Erstellen Sie nun Modellnummer 1. </font><font style="vertical-align: inherit;">Darin erinnern wir uns, dass nur der Random Forest-Algorithmus verwendet wird. </font><font style="vertical-align: inherit;">Es verwendet alle Funktionen und wird mit den Standardwerten konfiguriert (Details zu diesen Einstellungen finden Sie in der Dokumentation zu </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">sklearn.ensemble.RandomForestClassifier</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ). </font><font style="vertical-align: inherit;">Initialisieren Sie das Modell. </font><font style="vertical-align: inherit;">Danach werden wir sie in skalierten Daten schulen. </font><font style="vertical-align: inherit;">Die Genauigkeit des Modells kann an den Trainingsdaten gemessen werden:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier
<span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> recall_score<font></font>
rfc = RandomForestClassifier()<font></font>
rfc.fit(X_train_scaled, y_train)<font></font>
display(rfc.score(X_train_scaled, y_train))<font></font>
<span class="hljs-comment"># 1.0</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wenn wir wissen möchten, welche Merkmale für das RF-Modell für die Vorhersage von Brustkrebs am wichtigsten sind, können wir Indikatoren für den Schweregrad von Merkmalen anhand des Attributs visualisieren und quantifizieren </font></font><code>feature_importances_</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">:</font></font><br>
<br>
<pre><code class="python hljs">feats = {}
<span class="hljs-keyword">for</span> feature, importance <span class="hljs-keyword">in</span> zip(data.columns, rfc_1.feature_importances_):<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;feats[feature] = importance<font></font>
importances = pd.DataFrame.from_dict(feats, orient=<span class="hljs-string">'index'</span>).rename(columns={<span class="hljs-number">0</span>: <span class="hljs-string">'Gini-Importance'</span>})<font></font>
importances = importances.sort_values(by=<span class="hljs-string">'Gini-Importance'</span>, ascending=<span class="hljs-literal">False</span>)<font></font>
importances = importances.reset_index()<font></font>
importances = importances.rename(columns={<span class="hljs-string">'index'</span>: <span class="hljs-string">'Features'</span>})<font></font>
sns.set(font_scale = <span class="hljs-number">5</span>)<font></font>
sns.set(style=<span class="hljs-string">"whitegrid"</span>, color_codes=<span class="hljs-literal">True</span>, font_scale = <span class="hljs-number">1.7</span>)<font></font>
fig, ax = plt.subplots()<font></font>
fig.set_size_inches(<span class="hljs-number">30</span>,<span class="hljs-number">15</span>)<font></font>
sns.barplot(x=importances[<span class="hljs-string">'Gini-Importance'</span>], y=importances[<span class="hljs-string">'Features'</span>], data=importances, color=<span class="hljs-string">'skyblue'</span>)<font></font>
plt.xlabel(<span class="hljs-string">'Importance'</span>, fontsize=<span class="hljs-number">25</span>, weight = <span class="hljs-string">'bold'</span>)<font></font>
plt.ylabel(<span class="hljs-string">'Features'</span>, fontsize=<span class="hljs-number">25</span>, weight = <span class="hljs-string">'bold'</span>)<font></font>
plt.title(<span class="hljs-string">'Feature Importance'</span>, fontsize=<span class="hljs-number">25</span>, weight = <span class="hljs-string">'bold'</span>)<font></font>
display(plt.show())<font></font>
display(importances)</code></pre><br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a02/a8f/cd2/a02a8fcd28f87af338f364a70faeca3e.jpg"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Visualisierung der „Wichtigkeit“ von Zeichen</font></font></font></i><br>
 <br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/830/176/e1f/830176e1fc9ce63bfedf2d727619253b.png"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Signifikanzindikatoren</font></font></font></i><br>
<br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">5. Die Methode der Hauptkomponenten</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Fragen wir nun, wie wir das grundlegende HF-Modell verbessern können. Mithilfe der Technik zum Reduzieren der Dimension des Merkmalsraums ist es möglich, den anfänglichen Datensatz durch weniger Variablen darzustellen und gleichzeitig die Menge an Rechenressourcen zu reduzieren, die erforderlich sind, um den Betrieb des Modells sicherzustellen. Mithilfe der PCA können Sie die kumulative Stichprobenvarianz dieser Merkmale untersuchen, um zu verstehen, welche Merkmale den größten Teil der Varianz in den Daten erklären. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wir initialisieren das PCA ( </font></font><code>pca_test</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">) - </font><font style="vertical-align: inherit;">Objekt </font><font style="vertical-align: inherit;">und geben die Anzahl der Komponenten (Features) an, die berücksichtigt werden müssen. Wir setzen diesen Indikator auf 30, um die erklärte Varianz aller generierten Komponenten zu sehen, bevor wir entscheiden, wie viele Komponenten wir benötigen. Dann übertragen wir auf die </font></font><code>pca_test</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">skalierten Daten</font></font><code>X_train</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">mit der Methode </font></font><code>pca_test.fit()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Danach visualisieren wir die Daten.</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns
<span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> PCA<font></font>
pca_test = PCA(n_components=<span class="hljs-number">30</span>)<font></font>
pca_test.fit(X_train_scaled)<font></font>
sns.set(style=<span class="hljs-string">'whitegrid'</span>)<font></font>
plt.plot(np.cumsum(pca_test.explained_variance_ratio_))<font></font>
plt.xlabel(<span class="hljs-string">'number of components'</span>)<font></font>
plt.ylabel(<span class="hljs-string">'cumulative explained variance'</span>)<font></font>
plt.axvline(linewidth=<span class="hljs-number">4</span>, color=<span class="hljs-string">'r'</span>, linestyle = <span class="hljs-string">'--'</span>, x=<span class="hljs-number">10</span>, ymin=<span class="hljs-number">0</span>, ymax=<span class="hljs-number">1</span>)<font></font>
display(plt.show())<font></font>
evr = pca_test.explained_variance_ratio_<font></font>
cvr = np.cumsum(pca_test.explained_variance_ratio_)<font></font>
pca_df = pd.DataFrame()<font></font>
pca_df[<span class="hljs-string">'Cumulative Variance Ratio'</span>] = cvr<font></font>
pca_df[<span class="hljs-string">'Explained Variance Ratio'</span>] = evr<font></font>
display(pca_df.head(<span class="hljs-number">10</span>))</code></pre><br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6eb/65f/acc/6eb65facc6c8b05f1e910d3b2b676d5e.jpg"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nachdem die Anzahl der verwendeten Komponenten 10 überschreitet, erhöht die Erhöhung ihrer Anzahl die erklärte Varianz nicht wesentlich</font></font></font></i><br>
 <br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f12/e3c/915/f12e3c915d761e1d4623051dac74cd8d.png"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dieser Datenrahmen enthält Indikatoren wie Kumulative Variance Ratio (kumulative Größe der Varianz der Daten erklärten) und erklärte Varianz Ratio (Anteil der einzelnen Komponenten auf das Gesamtvolumen der erklärten Varianz)</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Wenn man sich den oben Datenrahmen aussehen, es stellt sich heraus</font><font style="vertical-align: inherit;">dass der PCA unter Verwendung von 30 Variablen 10 bewegen zu Komponenten ermöglicht es, 95% der Datenstreuung zu erklären. Die anderen 20 Komponenten machen weniger als 5% der Varianz aus, was bedeutet, dass wir sie ablehnen können. Nach dieser Logik verwenden wir die PCA, um die Anzahl der Komponenten für</font></font><code>X_train</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">und</font><font style="vertical-align: inherit;">von 30 auf 10 zu reduzieren</font></font><code>X_test</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Wir schreiben diese künstlich erstellten Datensätze mit reduzierter Dimension in</font></font><code>X_train_scaled_pca</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">und in</font></font><code>X_test_scaled_pca</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font><br>
<br>
<pre><code class="python hljs">pca = PCA(n_components=<span class="hljs-number">10</span>)<font></font>
pca.fit(X_train_scaled)<font></font>
X_train_scaled_pca = pca.transform(X_train_scaled)<font></font>
X_test_scaled_pca = pca.transform(X_test_scaled)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Jede Komponente ist eine lineare Kombination von Quellvariablen mit entsprechenden „Gewichten“. </font><font style="vertical-align: inherit;">Wir können diese „Gewichte“ für jede Komponente sehen, indem wir einen Datenrahmen erstellen.</font></font><br>
<br>
<pre><code class="python hljs">pca_dims = []
<span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>, len(pca_df)):<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;pca_dims.append(<span class="hljs-string">'PCA Component {}'</span>.format(x))<font></font>
pca_test_df = pd.DataFrame(pca_test.components_, columns=columns, index=pca_dims)<font></font>
pca_test_df.head(<span class="hljs-number">10</span>).T</code></pre><br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/086/a28/ae4/086a28ae45e9048811cf813d4868902e.png"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Komponenteninformationsdatenrahmen</font></font></font></i><br>
 <br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">6. Training des grundlegenden RF-Modells nach Anwendung der Hauptkomponentenmethode auf die Daten (Modell Nr. 2, RF + PCA)</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Jetzt können wir zu einem anderen Grund RF-Modelldaten übergeben </font></font><code>X_train_scaled_pca</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">und </font></font><code>y_train</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">und können herausfinden, ob es eine Verbesserung der Genauigkeit der Vorhersagen durch das Modell ausgestellt ist.</font></font><br>
<br>
<pre><code class="python hljs">rfc = RandomForestClassifier()<font></font>
rfc.fit(X_train_scaled_pca, y_train)<font></font>
display(rfc.score(X_train_scaled_pca, y_train))<font></font>
<span class="hljs-comment"># 1.0</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Modelle vergleichen unten.</font></font><br>
<br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">7. Optimierung von Hyperparametern. </font><font style="vertical-align: inherit;">Runde 1: RandomizedSearchCV</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nach der Verarbeitung der Daten mit der Hauptkomponentenmethode können Sie versuchen, die </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Optimierung von Modellhyperparametern</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> zu verwenden, um die Qualität der vom RF-Modell erzeugten Vorhersagen zu verbessern. Hyperparameter können als „Einstellungen“ des Modells betrachtet werden. Einstellungen, die für einen Datensatz perfekt sind, funktionieren für einen anderen nicht - deshalb müssen Sie sie optimieren. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Sie können mit dem RandomizedSearchCV-Algorithmus beginnen, mit dem Sie einen weiten Wertebereich ziemlich grob untersuchen können. Beschreibungen aller Hyperparameter für RF-Modelle finden Sie </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">hier</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Im Laufe der Arbeit generieren wir eine Entität </font></font><code>param_dist</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, die für jeden Hyperparameter einen Wertebereich enthält, der getestet werden muss. Als nächstes initialisieren wir das Objekt.</font></font><code>rs</code><font style="vertical-align: inherit;"></font><code>RandomizedSearchCV()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Übergeben </font><font style="vertical-align: inherit;">der Funktion </font><font style="vertical-align: inherit;">, Übergeben des RF-Modells </font></font><code>param_dist</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, der Anzahl der Iterationen und der Anzahl der </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Kreuzvalidierungen</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , die durchgeführt werden müssen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Mit dem Hyperparameter </font></font><code>verbose</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">können Sie die Informationsmenge steuern, die das Modell während seines Betriebs anzeigt (wie die Ausgabe von Informationen während des Trainings des Modells). </font><font style="vertical-align: inherit;">Mit dem Hyperparameter </font></font><code>n_jobs</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">können Sie angeben, wie viele Prozessorkerne Sie verwenden müssen, um den Betrieb des Modells sicherzustellen. </font><font style="vertical-align: inherit;">Das Festlegen </font></font><code>n_jobs</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">eines Werts </font></font><code>-1</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">führt zu einem schnelleren Modell, da hierfür alle Prozessorkerne verwendet werden. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wir werden uns mit der Auswahl der folgenden Hyperparameter befassen:</font></font><br>
<br>
<ul>
<li><code>n_estimators</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - die Anzahl der "Bäume" im "zufälligen Wald".</font></font></li>
<li><code>max_features</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - Die Anzahl der Features, für die die Aufteilung ausgewählt werden soll.</font></font></li>
<li><code>max_depth</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - maximale Tiefe der Bäume.</font></font></li>
<li><code>min_samples_split</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - Die Mindestanzahl von Objekten, die zum Teilen eines Baumknotens erforderlich sind.</font></font></li>
<li><code>min_samples_leaf</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - die Mindestanzahl von Objekten in den Blättern.</font></font></li>
<li><code>bootstrap</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - Verwendung zum Erstellen von Teilmusterbäumen mit Rückgabe.</font></font></li>
</ul><br>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> RandomizedSearchCV<font></font>
n_estimators = [int(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> np.linspace(start = <span class="hljs-number">100</span>, stop = <span class="hljs-number">1000</span>, num = <span class="hljs-number">10</span>)]<font></font>
max_features = [<span class="hljs-string">'log2'</span>, <span class="hljs-string">'sqrt'</span>]<font></font>
max_depth = [int(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> np.linspace(start = <span class="hljs-number">1</span>, stop = <span class="hljs-number">15</span>, num = <span class="hljs-number">15</span>)]<font></font>
min_samples_split = [int(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> np.linspace(start = <span class="hljs-number">2</span>, stop = <span class="hljs-number">50</span>, num = <span class="hljs-number">10</span>)]<font></font>
min_samples_leaf = [int(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> np.linspace(start = <span class="hljs-number">2</span>, stop = <span class="hljs-number">50</span>, num = <span class="hljs-number">10</span>)]<font></font>
bootstrap = [<span class="hljs-literal">True</span>, <span class="hljs-literal">False</span>]<font></font>
param_dist = {<span class="hljs-string">'n_estimators'</span>: n_estimators,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'max_features'</span>: max_features,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'max_depth'</span>: max_depth,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'min_samples_split'</span>: min_samples_split,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'min_samples_leaf'</span>: min_samples_leaf,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'bootstrap'</span>: bootstrap}<font></font>
rs = RandomizedSearchCV(rfc_2,&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;param_dist,&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;n_iter = <span class="hljs-number">100</span>,&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;cv = <span class="hljs-number">3</span>,&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;verbose = <span class="hljs-number">1</span>,&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;n_jobs=<span class="hljs-number">-1</span>,&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;random_state=<span class="hljs-number">0</span>)<font></font>
rs.fit(X_train_scaled_pca, y_train)<font></font>
rs.best_params_<font></font>
<span class="hljs-comment"># {'n_estimators': 700,</span>
<span class="hljs-comment"># 'min_samples_split': 2,</span>
<span class="hljs-comment"># 'min_samples_leaf': 2,</span>
<span class="hljs-comment"># 'max_features': 'log2',</span>
<span class="hljs-comment"># 'max_depth': 11,</span>
<span class="hljs-comment"># 'bootstrap': True}</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Mit den Werten der Parameter </font></font><code>n_iter = 100</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">und </font></font><code>cv = 3</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">haben wir 300 RF-Modelle erstellt und zufällig Kombinationen der </font><font style="vertical-align: inherit;">oben </font><font style="vertical-align: inherit;">dargestellten Hyperparameter ausgewählt. </font></font><code>best_params_ </code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Informationen zu einer Reihe von Parametern, mit denen Sie das beste Modell erstellen können, finden Sie </font><font style="vertical-align: inherit;">im Attribut </font><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Zum jetzigen Zeitpunkt liefert dies jedoch möglicherweise nicht die interessantesten Daten zu den Parameterbereichen, die es wert sind, in der nächsten Optimierungsrunde untersucht zu werden. </font><font style="vertical-align: inherit;">Um herauszufinden, in welchem ​​Wertebereich es sich lohnt, weiter zu suchen, können wir leicht einen Datenrahmen erhalten, der die Ergebnisse des RandomizedSearchCV-Algorithmus enthält.</font></font><br>
<br>
<pre><code class="python hljs">rs_df = pd.DataFrame(rs.cv_results_).sort_values(<span class="hljs-string">'rank_test_score'</span>).reset_index(drop=<span class="hljs-literal">True</span>)<font></font>
rs_df = rs_df.drop([<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'mean_fit_time'</span>,&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'std_fit_time'</span>,&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'mean_score_time'</span>,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'std_score_time'</span>,&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'params'</span>,&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'split0_test_score'</span>,&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'split1_test_score'</span>,&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'split2_test_score'</span>,&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'std_test_score'</span>],<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;axis=<span class="hljs-number">1</span>)<font></font>
rs_df.head(<span class="hljs-number">10</span>)</code></pre><br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/617/b8c/20b/617b8c20b787acc3c76c23d9235b4b5a.png"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ergebnisse des RandomizedSearchCV-Algorithmus</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Nun erstellen wir Balkendiagramme, auf denen auf der X-Achse die Hyperparameterwerte und auf der Y-Achse die von den Modellen angezeigten Durchschnittswerte angegeben sind. </font><font style="vertical-align: inherit;">Auf diese Weise können Sie nachvollziehen, welche Werte von Hyperparametern im Durchschnitt ihre beste Leistung zeigen.</font></font><br>
<br>
<pre><code class="python hljs">fig, axs = plt.subplots(ncols=<span class="hljs-number">3</span>, nrows=<span class="hljs-number">2</span>)<font></font>
sns.set(style=<span class="hljs-string">"whitegrid"</span>, color_codes=<span class="hljs-literal">True</span>, font_scale = <span class="hljs-number">2</span>)<font></font>
fig.set_size_inches(<span class="hljs-number">30</span>,<span class="hljs-number">25</span>)<font></font>
sns.barplot(x=<span class="hljs-string">'param_n_estimators'</span>, y=<span class="hljs-string">'mean_test_score'</span>, data=rs_df, ax=axs[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>], color=<span class="hljs-string">'lightgrey'</span>)<font></font>
axs[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>].set_ylim([<span class="hljs-number">.83</span>,<span class="hljs-number">.93</span>])axs[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>].set_title(label = <span class="hljs-string">'n_estimators'</span>, size=<span class="hljs-number">30</span>, weight=<span class="hljs-string">'bold'</span>)<font></font>
sns.barplot(x=<span class="hljs-string">'param_min_samples_split'</span>, y=<span class="hljs-string">'mean_test_score'</span>, data=rs_df, ax=axs[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>], color=<span class="hljs-string">'coral'</span>)<font></font>
axs[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>].set_ylim([<span class="hljs-number">.85</span>,<span class="hljs-number">.93</span>])axs[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>].set_title(label = <span class="hljs-string">'min_samples_split'</span>, size=<span class="hljs-number">30</span>, weight=<span class="hljs-string">'bold'</span>)<font></font>
sns.barplot(x=<span class="hljs-string">'param_min_samples_leaf'</span>, y=<span class="hljs-string">'mean_test_score'</span>, data=rs_df, ax=axs[<span class="hljs-number">0</span>,<span class="hljs-number">2</span>], color=<span class="hljs-string">'lightgreen'</span>)<font></font>
axs[<span class="hljs-number">0</span>,<span class="hljs-number">2</span>].set_ylim([<span class="hljs-number">.80</span>,<span class="hljs-number">.93</span>])axs[<span class="hljs-number">0</span>,<span class="hljs-number">2</span>].set_title(label = <span class="hljs-string">'min_samples_leaf'</span>, size=<span class="hljs-number">30</span>, weight=<span class="hljs-string">'bold'</span>)<font></font>
sns.barplot(x=<span class="hljs-string">'param_max_features'</span>, y=<span class="hljs-string">'mean_test_score'</span>, data=rs_df, ax=axs[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>], color=<span class="hljs-string">'wheat'</span>)<font></font>
axs[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>].set_ylim([<span class="hljs-number">.88</span>,<span class="hljs-number">.92</span>])axs[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>].set_title(label = <span class="hljs-string">'max_features'</span>, size=<span class="hljs-number">30</span>, weight=<span class="hljs-string">'bold'</span>)<font></font>
sns.barplot(x=<span class="hljs-string">'param_max_depth'</span>, y=<span class="hljs-string">'mean_test_score'</span>, data=rs_df, ax=axs[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>], color=<span class="hljs-string">'lightpink'</span>)<font></font>
axs[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>].set_ylim([<span class="hljs-number">.80</span>,<span class="hljs-number">.93</span>])axs[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>].set_title(label = <span class="hljs-string">'max_depth'</span>, size=<span class="hljs-number">30</span>, weight=<span class="hljs-string">'bold'</span>)<font></font>
sns.barplot(x=<span class="hljs-string">'param_bootstrap'</span>,y=<span class="hljs-string">'mean_test_score'</span>, data=rs_df, ax=axs[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>], color=<span class="hljs-string">'skyblue'</span>)<font></font>
axs[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>].set_ylim([<span class="hljs-number">.88</span>,<span class="hljs-number">.92</span>])<font></font>
axs[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>].set_title(label = <span class="hljs-string">'bootstrap'</span>, size=<span class="hljs-number">30</span>, weight=<span class="hljs-string">'bold'</span>)<font></font>
plt.show()</code></pre><br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/418/311/ba6/418311ba6c38bfcebbf152af810d6b58.jpg"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Analyse der Werte von Hyperparametern</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Wenn wir die obigen Diagramme analysieren, können wir einige interessante Dinge feststellen, die darüber sprechen, wie sich durchschnittlich jeder Wert eines Hyperparameters auf das Modell auswirkt.</font></font><br>
<br>
<ul>
<li><code>n_estimators</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">: Werte von 300, 500, 700 zeigen anscheinend die besten durchschnittlichen Ergebnisse.</font></font></li>
<li><code>min_samples_split</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">: Kleine Werte wie 2 und 7 scheinen die besten Ergebnisse zu zeigen. </font><font style="vertical-align: inherit;">Der Wert 23 sieht auch gut aus. Sie können mehrere Werte dieses Hyperparameters über 2 sowie mehrere Werte von etwa 23 untersuchen.</font></font></li>
<li><code>min_samples_leaf</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">: Es besteht das Gefühl, dass kleine Werte dieses Hyperparameters bessere Ergebnisse liefern. </font><font style="vertical-align: inherit;">Dies bedeutet, dass wir Werte zwischen 2 und 7 erfahren können.</font></font></li>
<li><code>max_features</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">: Option </font></font><code>sqrt</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">gibt das höchste durchschnittliche Ergebnis.</font></font></li>
<li><code>max_depth</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">: Es gibt keine klare Beziehung zwischen dem Wert des Hyperparameters und dem Ergebnis des Modells, aber es besteht das Gefühl, dass die Werte 2, 3, 7, 11, 15 gut aussehen.</font></font></li>
<li><code>bootstrap</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">: Der Wert </font></font><code>False</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">zeigt das beste Durchschnittsergebnis.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Mit diesen Erkenntnissen können wir nun zur zweiten Runde der Optimierung von Hyperparametern übergehen. </font><font style="vertical-align: inherit;">Dies wird den Wertebereich einschränken, an dem wir interessiert sind.</font></font><br>
<br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">8. Optimierung von Hyperparametern. </font><font style="vertical-align: inherit;">Runde 2: GridSearchCV (endgültige Vorbereitung der Parameter für Modell Nr. 3, RF + PCA + HT)</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nach der Anwendung des RandomizedSearchCV-Algorithmus verwenden wir den GridSearchCV-Algorithmus, um eine genauere Suche nach der besten Kombination von Hyperparametern durchzuführen. Die gleichen Hyperparameter werden hier untersucht, aber jetzt wenden wir eine "gründlichere" Suche nach ihrer besten Kombination an. Mit dem GridSearchCV-Algorithmus wird jede Kombination von Hyperparametern untersucht. Dies erfordert viel mehr Rechenressourcen als die Verwendung des RandomizedSearchCV-Algorithmus, wenn wir die Anzahl der Suchiterationen unabhängig voneinander festlegen. Zum Beispiel erfordert die Untersuchung von 10 Werten für jeden der 6 Hyperparameter mit Kreuzvalidierung in 3 Blöcken 10⁶ x 3 oder 3.000.000 Modellschulungen. Aus diesem Grund verwenden wir den GridSearchCV-Algorithmus, nachdem wir nach der Anwendung von RandomizedSearchCV die Wertebereiche der untersuchten Parameter eingegrenzt haben.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Mit dem, was wir mithilfe von RandomizedSearchCV herausgefunden haben, untersuchen wir die Werte der Hyperparameter, die sich am besten gezeigt haben:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> GridSearchCV<font></font>
n_estimators = [<span class="hljs-number">300</span>,<span class="hljs-number">500</span>,<span class="hljs-number">700</span>]<font></font>
max_features = [<span class="hljs-string">'sqrt'</span>]<font></font>
max_depth = [<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">7</span>,<span class="hljs-number">11</span>,<span class="hljs-number">15</span>]<font></font>
min_samples_split = [<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">22</span>,<span class="hljs-number">23</span>,<span class="hljs-number">24</span>]<font></font>
min_samples_leaf = [<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>,<span class="hljs-number">7</span>]<font></font>
bootstrap = [<span class="hljs-literal">False</span>]<font></font>
param_grid = {<span class="hljs-string">'n_estimators'</span>: n_estimators,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'max_features'</span>: max_features,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'max_depth'</span>: max_depth,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'min_samples_split'</span>: min_samples_split,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'min_samples_leaf'</span>: min_samples_leaf,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'bootstrap'</span>: bootstrap}<font></font>
gs = GridSearchCV(rfc_2, param_grid, cv = <span class="hljs-number">3</span>, verbose = <span class="hljs-number">1</span>, n_jobs=<span class="hljs-number">-1</span>)<font></font>
gs.fit(X_train_scaled_pca, y_train)<font></font>
rfc_3 = gs.best_estimator_<font></font>
gs.best_params_<font></font>
<span class="hljs-comment"># {'bootstrap': False,</span>
<span class="hljs-comment"># 'max_depth': 7,</span>
<span class="hljs-comment"># 'max_features': 'sqrt',</span>
<span class="hljs-comment"># 'min_samples_leaf': 3,</span>
<span class="hljs-comment"># 'min_samples_split': 2,</span>
<span class="hljs-comment"># 'n_estimators': 500}</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Hier wenden wir die Kreuzvalidierung in 3 Blöcken für 540 (3 x 1 x 5 x 6 x 6 x 1) Modellschulungen an, was 1620 Modellschulungen ergibt. </font><font style="vertical-align: inherit;">Nachdem wir RandomizedSearchCV und GridSearchCV verwendet haben, können wir uns nun dem Attribut zuwenden, </font></font><code>best_params_</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">um herauszufinden, welche Werte von Hyperparametern es dem Modell ermöglichen, am besten mit dem untersuchten Datensatz zu arbeiten (diese Werte sind am Ende des vorherigen Codeblocks zu sehen). . </font><font style="vertical-align: inherit;">Diese Parameter werden verwendet, um Modellnummer 3 zu erstellen.</font></font><br>
<br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">9. Bewertung der Qualität der Modelle anhand der Verifizierungsdaten</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Jetzt können Sie die erstellten Modelle anhand der Verifizierungsdaten auswerten. </font><font style="vertical-align: inherit;">Wir sprechen nämlich über diese drei Modelle, die ganz am Anfang des Materials beschrieben wurden. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Schauen Sie sich diese Modelle an:</font></font><br>
<br>
<pre><code class="python hljs">y_pred = rfc.predict(X_test_scaled)<font></font>
y_pred_pca = rfc.predict(X_test_scaled_pca)<font></font>
y_pred_gs = gs.best_estimator_.predict(X_test_scaled_pca)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Erstellen Sie Fehlermatrizen für die Modelle und finden Sie heraus, wie gut jeder von ihnen Brustkrebs vorhersagen kann:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> confusion_matrix<font></font>
conf_matrix_baseline = pd.DataFrame(confusion_matrix(y_test, y_pred), index = [<span class="hljs-string">'actual 0'</span>, <span class="hljs-string">'actual 1'</span>], columns = [<span class="hljs-string">'predicted 0'</span>, <span class="hljs-string">'predicted 1'</span>])<font></font>
conf_matrix_baseline_pca = pd.DataFrame(confusion_matrix(y_test, y_pred_pca), index = [<span class="hljs-string">'actual 0'</span>, <span class="hljs-string">'actual 1'</span>], columns = [<span class="hljs-string">'predicted 0'</span>, <span class="hljs-string">'predicted 1'</span>])<font></font>
conf_matrix_tuned_pca = pd.DataFrame(confusion_matrix(y_test, y_pred_gs), index = [<span class="hljs-string">'actual 0'</span>, <span class="hljs-string">'actual 1'</span>], columns = [<span class="hljs-string">'predicted 0'</span>, <span class="hljs-string">'predicted 1'</span>])<font></font>
display(conf_matrix_baseline)<font></font>
display(<span class="hljs-string">'Baseline Random Forest recall score'</span>, recall_score(y_test, y_pred))<font></font>
display(conf_matrix_baseline_pca)<font></font>
display(<span class="hljs-string">'Baseline Random Forest With PCA recall score'</span>, recall_score(y_test, y_pred_pca))<font></font>
display(conf_matrix_tuned_pca)<font></font>
display(<span class="hljs-string">'Hyperparameter Tuned Random Forest With PCA Reduced Dimensionality recall score'</span>, recall_score(y_test, y_pred_gs))</code></pre><br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f48/a9e/92f/f48a9e92fd5fdca613d6073e00bae2c6.png"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ergebnisse der Arbeit der drei Modelle.</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Hier wird die Metrik als "Rückruf" bezeichnet. </font><font style="vertical-align: inherit;">Tatsache ist, dass es sich um eine Krebsdiagnose handelt. </font><font style="vertical-align: inherit;">Daher sind wir äußerst daran interessiert, falsch negative Prognosen von Modellen zu minimieren. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In Anbetracht dessen können wir den Schluss ziehen, dass das grundlegende HF-Modell die besten Ergebnisse lieferte. </font><font style="vertical-align: inherit;">Die Vollständigkeitsrate betrug 94,97%. </font><font style="vertical-align: inherit;">Im Testdatensatz wurden 179 krebskranke Patienten erfasst. </font><font style="vertical-align: inherit;">Das Modell fand 170 von ihnen.</font></font><br>
<br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zusammenfassung</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Diese Studie liefert eine wichtige Beobachtung. </font><font style="vertical-align: inherit;">Manchmal funktioniert das RF-Modell, das die Hauptkomponentenmethode und die Optimierung von Hyperparametern in großem Maßstab verwendet, möglicherweise nicht so gut wie das üblichste Modell mit Standardeinstellungen. </font><font style="vertical-align: inherit;">Dies ist jedoch kein Grund, sich nur auf die einfachsten Modelle zu beschränken. </font><font style="vertical-align: inherit;">Ohne verschiedene Modelle auszuprobieren, ist es unmöglich zu sagen, welches das beste Ergebnis zeigt. </font><font style="vertical-align: inherit;">Und bei Modellen, mit denen das Vorhandensein von Krebs bei Patienten vorhergesagt wird, können wir sagen, dass je besser das Modell ist, desto mehr Leben können gerettet werden. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Liebe Leser! </font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Welche Aufgaben lösen Sie mit maschinellen Lernmethoden?</font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/files/1ba/550/d25/1ba550d25e8846ce8805de564da6aa63.png"></a></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de488330/index.html">Englisch mit George Karlin: Wir analysieren den genialen Stand-up über phrasenbezogene Einheiten</a></li>
<li><a href="../de488332/index.html">Null, eins, zwei, Freddy wird dich abholen</a></li>
<li><a href="../de488336/index.html">Tipps zur Verwendung des Wave Function Collapse-Algorithmus</a></li>
<li><a href="../de488338/index.html">Google-Praktika: Zürich, London und Silicon Valley</a></li>
<li><a href="../de488340/index.html">Beruf: Backend-Entwickler</a></li>
<li><a href="../de488346/index.html">Installieren von or-tools mit SCIP und GLPK in einer virtuellen Python 3.7-Umgebung unter Linux</a></li>
<li><a href="../de488348/index.html">Webinar „Zehn agilste Herausforderungen und Möglichkeiten, sie in einer Stunde zu überwinden“ 17. Februar um 20:00 Uhr Moskauer Zeit</a></li>
<li><a href="../de488352/index.html">VDI-Kostenvergleich: On-Premise versus Public Cloud</a></li>
<li><a href="../de488356/index.html">Schulung für Dassault Systèmes-Produkte an der Staatlichen Marine Technischen Universität St. Petersburg</a></li>
<li><a href="../de488360/index.html">Big Data Mythen und digitale Kultur</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>