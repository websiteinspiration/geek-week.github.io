<!doctype html>
<html class="no-js" lang="de">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üÜí üì¶ üï¥üèº Random Forest, die Methode der Hauptkomponenten und die Optimierung von Hyperparametern: Ein Beispiel f√ºr die L√∂sung des Klassifizierungsproblems in Python üë®‚Äçüë®‚Äçüëß‚Äçüëß üßñ üñäÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Datenverarbeitungs- und Analysespezialisten verf√ºgen √ºber zahlreiche Tools zum Erstellen von Klassifizierungsmodellen. Eine der beliebtesten und zuver...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Random Forest, die Methode der Hauptkomponenten und die Optimierung von Hyperparametern: Ein Beispiel f√ºr die L√∂sung des Klassifizierungsproblems in Python</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ruvds/blog/488342/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Datenverarbeitungs- und Analysespezialisten verf√ºgen √ºber zahlreiche Tools zum Erstellen von Klassifizierungsmodellen. </font><font style="vertical-align: inherit;">Eine der beliebtesten und zuverl√§ssigsten Methoden zur Entwicklung solcher Modelle ist die Verwendung des Random Forest (RF) -Algorithmus. </font><font style="vertical-align: inherit;">Um zu versuchen, die Leistung eines mit dem </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">RF-</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Algorithmus erstellten Modells zu verbessern </font><font style="vertical-align: inherit;">, k√∂nnen Sie die Optimierung des Hyperparameters des Modells ( </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hyperparameter Tuning</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , HT) verwenden. </font><font style="vertical-align: inherit;">
Dar√ºber hinaus gibt es einen weit verbreiteten Ansatz, nach dem die Daten vor der √úbertragung in das Modell mithilfe der </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;">Hauptkomponentenanalyse</font></a><font style="vertical-align: inherit;"> verarbeitet werden</font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/webt/tt/m5/h7/ttm5h7jbbx2u2wuc1var1azxwew.jpeg"></a><br>
<br><font style="vertical-align: inherit;"></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, PCA). </font><font style="vertical-align: inherit;">Aber lohnt es sich zu benutzen? </font><font style="vertical-align: inherit;">Ist es nicht der Hauptzweck des RF-Algorithmus, dem Analytiker zu helfen, die Bedeutung der Merkmale zu interpretieren?</font></font><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ja, die Verwendung des PCA-Algorithmus kann zu einer geringf√ºgigen Komplikation der Interpretation jedes ‚ÄûMerkmals‚Äú bei der Analyse der ‚ÄûBedeutung von Merkmalen‚Äú des RF-Modells f√ºhren. Der PCA-Algorithmus reduziert jedoch die Dimension des Merkmalsraums, was zu einer Verringerung der Anzahl von Merkmalen f√ºhren kann, die vom RF-Modell verarbeitet werden m√ºssen. Bitte beachten Sie, dass das Rechenvolumen einer der Hauptnachteile des Random Forest-Algorithmus ist (dh die Fertigstellung des Modells kann lange dauern). Die Anwendung des PCA-Algorithmus kann ein sehr wichtiger Bestandteil der Modellierung sein, insbesondere in F√§llen, in denen sie mit Hunderten oder sogar Tausenden von Funktionen arbeiten. Wenn das Wichtigste darin besteht, einfach das effektivste Modell zu erstellen und gleichzeitig die Genauigkeit der Bestimmung der Wichtigkeit der Attribute zu opfern, ist die PCA m√∂glicherweise einen Versuch wert.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nun zum Punkt. </font><font style="vertical-align: inherit;">Wir werden mit einem </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Brustkrebs-Datensatz arbeiten</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;">Scikit-Learn "Brustkrebs"</font></a><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Wir werden drei Modelle erstellen und deren Wirksamkeit vergleichen. </font><font style="vertical-align: inherit;">Wir sprechen n√§mlich √ºber folgende Modelle:</font></font><br>
<br>
<ol>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Das Basismodell basiert auf dem RF-Algorithmus (wir werden dieses RF-Modell abk√ºrzen).</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Das gleiche Modell wie Nr. 1, jedoch eines, bei dem eine Verringerung der Dimension des Merkmalsraums unter Verwendung der Hauptkomponentenmethode (RF + PCA) angewendet wird.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Das gleiche Modell wie Nr. 2, jedoch mit Hyperparameteroptimierung (RF + PCA + HT) erstellt.</font></font></li>
</ol><br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1. Daten importieren</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Laden Sie zun√§chst die Daten und erstellen Sie einen Pandas-Datenrahmen. </font><font style="vertical-align: inherit;">Da wir einen vorab gel√∂schten ‚ÄûSpielzeug‚Äú -Datensatz von Scikit-learn verwenden, k√∂nnen wir danach bereits mit dem Modellierungsprozess beginnen. </font><font style="vertical-align: inherit;">Aber selbst wenn Sie solche Daten verwenden, wird empfohlen, dass Sie immer mit der Arbeit beginnen, indem Sie eine vorl√§ufige Analyse der Daten mit den folgenden Befehlen durchf√ºhren, die auf den Datenrahmen ( </font></font><code>df</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">) </font><font style="vertical-align: inherit;">angewendet werden </font><font style="vertical-align: inherit;">:</font></font><br>
<br>
<ul>
<li><code>df.head()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - um einen Blick auf den neuen Datenrahmen zu werfen und zu sehen, ob er wie erwartet aussieht.</font></font></li>
<li><code>df.info()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- um die Merkmale von Datentypen und Spalteninhalten herauszufinden. </font><font style="vertical-align: inherit;">M√∂glicherweise muss die Datentypkonvertierung durchgef√ºhrt werden, bevor Sie fortfahren k√∂nnen.</font></font></li>
<li><code>df.isna()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- um sicherzustellen, dass die Daten keine Werte enthalten </font></font><code>NaN</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Die entsprechenden Werte m√ºssen gegebenenfalls irgendwie verarbeitet werden, oder es kann erforderlich sein, ganze Zeilen aus dem Datenrahmen zu entfernen.</font></font></li>
<li><code>df.describe()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - Ermittlung der minimalen, maximalen und durchschnittlichen Werte der Indikatoren in den Spalten, Ermittlung der Indikatoren f√ºr das mittlere Quadrat und die wahrscheinliche Abweichung in den Spalten.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In unserem Datensatz ist eine Spalte </font></font><code>cancer</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(Krebs) die Zielvariable, deren Wert wir mithilfe des Modells vorhersagen m√∂chten. </font></font><code>0</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">bedeutet "keine Krankheit". </font></font><code>1</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- "das Vorhandensein der Krankheit."</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_breast_cancer<font></font>
columns = [<span class="hljs-string">'mean radius'</span>, <span class="hljs-string">'mean texture'</span>, <span class="hljs-string">'mean perimeter'</span>, <span class="hljs-string">'mean area'</span>, <span class="hljs-string">'mean smoothness'</span>, <span class="hljs-string">'mean compactness'</span>, <span class="hljs-string">'mean concavity'</span>, <span class="hljs-string">'mean concave points'</span>, <span class="hljs-string">'mean symmetry'</span>, <span class="hljs-string">'mean fractal dimension'</span>, <span class="hljs-string">'radius error'</span>, <span class="hljs-string">'texture error'</span>, <span class="hljs-string">'perimeter error'</span>, <span class="hljs-string">'area error'</span>, <span class="hljs-string">'smoothness error'</span>, <span class="hljs-string">'compactness error'</span>, <span class="hljs-string">'concavity error'</span>, <span class="hljs-string">'concave points error'</span>, <span class="hljs-string">'symmetry error'</span>, <span class="hljs-string">'fractal dimension error'</span>, <span class="hljs-string">'worst radius'</span>, <span class="hljs-string">'worst texture'</span>, <span class="hljs-string">'worst perimeter'</span>, <span class="hljs-string">'worst area'</span>, <span class="hljs-string">'worst smoothness'</span>, <span class="hljs-string">'worst compactness'</span>, <span class="hljs-string">'worst concavity'</span>, <span class="hljs-string">'worst concave points'</span>, <span class="hljs-string">'worst symmetry'</span>, <span class="hljs-string">'worst fractal dimension'</span>]<font></font>
dataset = load_breast_cancer()<font></font>
data = pd.DataFrame(dataset[<span class="hljs-string">'data'</span>], columns=columns)<font></font>
data[<span class="hljs-string">'cancer'</span>] = dataset[<span class="hljs-string">'target'</span>]<font></font>
display(data.head())<font></font>
display(data.info())<font></font>
display(data.isna().sum())<font></font>
display(data.describe())</code></pre><br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/bce/fb8/b60/bcefb8b60462b7658b40e1e56f7744ab.png"></div><br>
<i><font color="#999999">      .       .  , cancer,   ,    . 0  ¬´ ¬ª. 1 ‚Äî ¬´ ¬ª</font></i><br>
 <br>
<h2><font color="#3AC1EF">2.        </font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Teilen Sie nun die Daten mit der Scikit-Lernfunktion auf </font></font><code>train_test_split</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Wir m√∂chten dem Modell so viele Trainingsdaten wie m√∂glich geben. Wir m√ºssen jedoch √ºber gen√ºgend Daten verf√ºgen, um das Modell zu testen. Im Allgemeinen k√∂nnen wir sagen, dass mit zunehmender Anzahl von Zeilen im Datensatz auch die Datenmenge zunimmt, die als lehrreich angesehen werden kann. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wenn beispielsweise Millionen von Zeilen vorhanden sind, k√∂nnen Sie den Satz aufteilen, indem Sie 90% der Zeilen f√ºr Trainingsdaten und 10% f√ºr Testdaten markieren. Der Testdatensatz enth√§lt jedoch nur 569 Zeilen. Und das ist nicht so sehr f√ºr das Training und Testen des Modells. Um in Bezug auf die Trainings- und Verifizierungsdaten fair zu sein, werden wir den Satz in zwei gleiche Teile aufteilen - 50% - Trainingsdaten und 50% - Verifizierungsdaten. Wir installieren</font></font><code>stratify=y</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> um sicherzustellen, dass sowohl der Trainings- als auch der Testdatensatz das gleiche Verh√§ltnis von 0 und 1 wie der Originaldatensatz haben.</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<font></font>
X = data.drop(<span class="hljs-string">'cancer'</span>, axis=<span class="hljs-number">1</span>)&nbsp;&nbsp;<font></font>
y = data[<span class="hljs-string">'cancer'</span>]&nbsp;<font></font>
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="hljs-number">0.50</span>, random_state = <span class="hljs-number">2020</span>, stratify=y)</code></pre><br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3. Datenskalierung</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bevor Sie mit der Modellierung fortfahren, m√ºssen Sie die Daten durch </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Skalieren</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ‚Äûzentrieren‚Äú und ‚Äûstandardisieren‚Äú </font><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Die Skalierung erfolgt aufgrund der Tatsache, dass unterschiedliche Gr√∂√üen in unterschiedlichen Einheiten ausgedr√ºckt werden. </font><font style="vertical-align: inherit;">Mit diesem Verfahren k√∂nnen Sie einen ‚Äûfairen Kampf‚Äú zwischen den Zeichen organisieren, um deren Bedeutung zu bestimmen. </font><font style="vertical-align: inherit;">Au√üerdem konvertieren wir </font></font><code>y_train</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">vom Pandas-Datentyp </font></font><code>Series</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">in das NumPy-Array, damit das Modell sp√§ter mit den entsprechenden Zielen arbeiten kann.</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler<font></font>
ss = StandardScaler()<font></font>
X_train_scaled = ss.fit_transform(X_train)<font></font>
X_test_scaled = ss.transform(X_test)<font></font>
y_train = np.array(y_train)</code></pre><br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">4. Training des Grundmodells (Modell Nr. 1, RF)</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Erstellen Sie nun Modellnummer 1. </font><font style="vertical-align: inherit;">Darin erinnern wir uns, dass nur der Random Forest-Algorithmus verwendet wird. </font><font style="vertical-align: inherit;">Es verwendet alle Funktionen und wird mit den Standardwerten konfiguriert (Details zu diesen Einstellungen finden Sie in der Dokumentation zu </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">sklearn.ensemble.RandomForestClassifier</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ). </font><font style="vertical-align: inherit;">Initialisieren Sie das Modell. </font><font style="vertical-align: inherit;">Danach werden wir sie in skalierten Daten schulen. </font><font style="vertical-align: inherit;">Die Genauigkeit des Modells kann an den Trainingsdaten gemessen werden:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier
<span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> recall_score<font></font>
rfc = RandomForestClassifier()<font></font>
rfc.fit(X_train_scaled, y_train)<font></font>
display(rfc.score(X_train_scaled, y_train))<font></font>
<span class="hljs-comment"># 1.0</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wenn wir wissen m√∂chten, welche Merkmale f√ºr das RF-Modell f√ºr die Vorhersage von Brustkrebs am wichtigsten sind, k√∂nnen wir Indikatoren f√ºr den Schweregrad von Merkmalen anhand des Attributs visualisieren und quantifizieren </font></font><code>feature_importances_</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">:</font></font><br>
<br>
<pre><code class="python hljs">feats = {}
<span class="hljs-keyword">for</span> feature, importance <span class="hljs-keyword">in</span> zip(data.columns, rfc_1.feature_importances_):<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;feats[feature] = importance<font></font>
importances = pd.DataFrame.from_dict(feats, orient=<span class="hljs-string">'index'</span>).rename(columns={<span class="hljs-number">0</span>: <span class="hljs-string">'Gini-Importance'</span>})<font></font>
importances = importances.sort_values(by=<span class="hljs-string">'Gini-Importance'</span>, ascending=<span class="hljs-literal">False</span>)<font></font>
importances = importances.reset_index()<font></font>
importances = importances.rename(columns={<span class="hljs-string">'index'</span>: <span class="hljs-string">'Features'</span>})<font></font>
sns.set(font_scale = <span class="hljs-number">5</span>)<font></font>
sns.set(style=<span class="hljs-string">"whitegrid"</span>, color_codes=<span class="hljs-literal">True</span>, font_scale = <span class="hljs-number">1.7</span>)<font></font>
fig, ax = plt.subplots()<font></font>
fig.set_size_inches(<span class="hljs-number">30</span>,<span class="hljs-number">15</span>)<font></font>
sns.barplot(x=importances[<span class="hljs-string">'Gini-Importance'</span>], y=importances[<span class="hljs-string">'Features'</span>], data=importances, color=<span class="hljs-string">'skyblue'</span>)<font></font>
plt.xlabel(<span class="hljs-string">'Importance'</span>, fontsize=<span class="hljs-number">25</span>, weight = <span class="hljs-string">'bold'</span>)<font></font>
plt.ylabel(<span class="hljs-string">'Features'</span>, fontsize=<span class="hljs-number">25</span>, weight = <span class="hljs-string">'bold'</span>)<font></font>
plt.title(<span class="hljs-string">'Feature Importance'</span>, fontsize=<span class="hljs-number">25</span>, weight = <span class="hljs-string">'bold'</span>)<font></font>
display(plt.show())<font></font>
display(importances)</code></pre><br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a02/a8f/cd2/a02a8fcd28f87af338f364a70faeca3e.jpg"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Visualisierung der ‚ÄûWichtigkeit‚Äú von Zeichen</font></font></font></i><br>
 <br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/830/176/e1f/830176e1fc9ce63bfedf2d727619253b.png"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Signifikanzindikatoren</font></font></font></i><br>
<br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">5. Die Methode der Hauptkomponenten</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Fragen wir nun, wie wir das grundlegende HF-Modell verbessern k√∂nnen. Mithilfe der Technik zum Reduzieren der Dimension des Merkmalsraums ist es m√∂glich, den anf√§nglichen Datensatz durch weniger Variablen darzustellen und gleichzeitig die Menge an Rechenressourcen zu reduzieren, die erforderlich sind, um den Betrieb des Modells sicherzustellen. Mithilfe der PCA k√∂nnen Sie die kumulative Stichprobenvarianz dieser Merkmale untersuchen, um zu verstehen, welche Merkmale den gr√∂√üten Teil der Varianz in den Daten erkl√§ren. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wir initialisieren das PCA ( </font></font><code>pca_test</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">) - </font><font style="vertical-align: inherit;">Objekt </font><font style="vertical-align: inherit;">und geben die Anzahl der Komponenten (Features) an, die ber√ºcksichtigt werden m√ºssen. Wir setzen diesen Indikator auf 30, um die erkl√§rte Varianz aller generierten Komponenten zu sehen, bevor wir entscheiden, wie viele Komponenten wir ben√∂tigen. Dann √ºbertragen wir auf die </font></font><code>pca_test</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">skalierten Daten</font></font><code>X_train</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">mit der Methode </font></font><code>pca_test.fit()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Danach visualisieren wir die Daten.</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns
<span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> PCA<font></font>
pca_test = PCA(n_components=<span class="hljs-number">30</span>)<font></font>
pca_test.fit(X_train_scaled)<font></font>
sns.set(style=<span class="hljs-string">'whitegrid'</span>)<font></font>
plt.plot(np.cumsum(pca_test.explained_variance_ratio_))<font></font>
plt.xlabel(<span class="hljs-string">'number of components'</span>)<font></font>
plt.ylabel(<span class="hljs-string">'cumulative explained variance'</span>)<font></font>
plt.axvline(linewidth=<span class="hljs-number">4</span>, color=<span class="hljs-string">'r'</span>, linestyle = <span class="hljs-string">'--'</span>, x=<span class="hljs-number">10</span>, ymin=<span class="hljs-number">0</span>, ymax=<span class="hljs-number">1</span>)<font></font>
display(plt.show())<font></font>
evr = pca_test.explained_variance_ratio_<font></font>
cvr = np.cumsum(pca_test.explained_variance_ratio_)<font></font>
pca_df = pd.DataFrame()<font></font>
pca_df[<span class="hljs-string">'Cumulative Variance Ratio'</span>] = cvr<font></font>
pca_df[<span class="hljs-string">'Explained Variance Ratio'</span>] = evr<font></font>
display(pca_df.head(<span class="hljs-number">10</span>))</code></pre><br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6eb/65f/acc/6eb65facc6c8b05f1e910d3b2b676d5e.jpg"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nachdem die Anzahl der verwendeten Komponenten 10 √ºberschreitet, erh√∂ht die Erh√∂hung ihrer Anzahl die erkl√§rte Varianz nicht wesentlich</font></font></font></i><br>
 <br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f12/e3c/915/f12e3c915d761e1d4623051dac74cd8d.png"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dieser Datenrahmen enth√§lt Indikatoren wie Kumulative Variance Ratio (kumulative Gr√∂√üe der Varianz der Daten erkl√§rten) und erkl√§rte Varianz Ratio (Anteil der einzelnen Komponenten auf das Gesamtvolumen der erkl√§rten Varianz)</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Wenn man sich den oben Datenrahmen aussehen, es stellt sich heraus</font><font style="vertical-align: inherit;">dass der PCA unter Verwendung von 30 Variablen 10 bewegen zu Komponenten erm√∂glicht es, 95% der Datenstreuung zu erkl√§ren. Die anderen 20 Komponenten machen weniger als 5% der Varianz aus, was bedeutet, dass wir sie ablehnen k√∂nnen. Nach dieser Logik verwenden wir die PCA, um die Anzahl der Komponenten f√ºr</font></font><code>X_train</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">und</font><font style="vertical-align: inherit;">von 30 auf 10 zu reduzieren</font></font><code>X_test</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Wir schreiben diese k√ºnstlich erstellten Datens√§tze mit reduzierter Dimension in</font></font><code>X_train_scaled_pca</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">und in</font></font><code>X_test_scaled_pca</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font><br>
<br>
<pre><code class="python hljs">pca = PCA(n_components=<span class="hljs-number">10</span>)<font></font>
pca.fit(X_train_scaled)<font></font>
X_train_scaled_pca = pca.transform(X_train_scaled)<font></font>
X_test_scaled_pca = pca.transform(X_test_scaled)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Jede Komponente ist eine lineare Kombination von Quellvariablen mit entsprechenden ‚ÄûGewichten‚Äú. </font><font style="vertical-align: inherit;">Wir k√∂nnen diese ‚ÄûGewichte‚Äú f√ºr jede Komponente sehen, indem wir einen Datenrahmen erstellen.</font></font><br>
<br>
<pre><code class="python hljs">pca_dims = []
<span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>, len(pca_df)):<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;pca_dims.append(<span class="hljs-string">'PCA Component {}'</span>.format(x))<font></font>
pca_test_df = pd.DataFrame(pca_test.components_, columns=columns, index=pca_dims)<font></font>
pca_test_df.head(<span class="hljs-number">10</span>).T</code></pre><br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/086/a28/ae4/086a28ae45e9048811cf813d4868902e.png"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Komponenteninformationsdatenrahmen</font></font></font></i><br>
 <br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">6. Training des grundlegenden RF-Modells nach Anwendung der Hauptkomponentenmethode auf die Daten (Modell Nr. 2, RF + PCA)</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Jetzt k√∂nnen wir zu einem anderen Grund RF-Modelldaten √ºbergeben </font></font><code>X_train_scaled_pca</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">und </font></font><code>y_train</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">und k√∂nnen herausfinden, ob es eine Verbesserung der Genauigkeit der Vorhersagen durch das Modell ausgestellt ist.</font></font><br>
<br>
<pre><code class="python hljs">rfc = RandomForestClassifier()<font></font>
rfc.fit(X_train_scaled_pca, y_train)<font></font>
display(rfc.score(X_train_scaled_pca, y_train))<font></font>
<span class="hljs-comment"># 1.0</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Modelle vergleichen unten.</font></font><br>
<br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">7. Optimierung von Hyperparametern. </font><font style="vertical-align: inherit;">Runde 1: RandomizedSearchCV</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nach der Verarbeitung der Daten mit der Hauptkomponentenmethode k√∂nnen Sie versuchen, die </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Optimierung von Modellhyperparametern</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> zu verwenden, um die Qualit√§t der vom RF-Modell erzeugten Vorhersagen zu verbessern. Hyperparameter k√∂nnen als ‚ÄûEinstellungen‚Äú des Modells betrachtet werden. Einstellungen, die f√ºr einen Datensatz perfekt sind, funktionieren f√ºr einen anderen nicht - deshalb m√ºssen Sie sie optimieren. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Sie k√∂nnen mit dem RandomizedSearchCV-Algorithmus beginnen, mit dem Sie einen weiten Wertebereich ziemlich grob untersuchen k√∂nnen. Beschreibungen aller Hyperparameter f√ºr RF-Modelle finden Sie </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">hier</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Im Laufe der Arbeit generieren wir eine Entit√§t </font></font><code>param_dist</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, die f√ºr jeden Hyperparameter einen Wertebereich enth√§lt, der getestet werden muss. Als n√§chstes initialisieren wir das Objekt.</font></font><code>rs</code><font style="vertical-align: inherit;"></font><code>RandomizedSearchCV()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">√úbergeben </font><font style="vertical-align: inherit;">der Funktion </font><font style="vertical-align: inherit;">, √úbergeben des RF-Modells </font></font><code>param_dist</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, der Anzahl der Iterationen und der Anzahl der </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Kreuzvalidierungen</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , die durchgef√ºhrt werden m√ºssen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Mit dem Hyperparameter </font></font><code>verbose</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">k√∂nnen Sie die Informationsmenge steuern, die das Modell w√§hrend seines Betriebs anzeigt (wie die Ausgabe von Informationen w√§hrend des Trainings des Modells). </font><font style="vertical-align: inherit;">Mit dem Hyperparameter </font></font><code>n_jobs</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">k√∂nnen Sie angeben, wie viele Prozessorkerne Sie verwenden m√ºssen, um den Betrieb des Modells sicherzustellen. </font><font style="vertical-align: inherit;">Das Festlegen </font></font><code>n_jobs</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">eines Werts </font></font><code>-1</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">f√ºhrt zu einem schnelleren Modell, da hierf√ºr alle Prozessorkerne verwendet werden. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wir werden uns mit der Auswahl der folgenden Hyperparameter befassen:</font></font><br>
<br>
<ul>
<li><code>n_estimators</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - die Anzahl der "B√§ume" im "zuf√§lligen Wald".</font></font></li>
<li><code>max_features</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - Die Anzahl der Features, f√ºr die die Aufteilung ausgew√§hlt werden soll.</font></font></li>
<li><code>max_depth</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - maximale Tiefe der B√§ume.</font></font></li>
<li><code>min_samples_split</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - Die Mindestanzahl von Objekten, die zum Teilen eines Baumknotens erforderlich sind.</font></font></li>
<li><code>min_samples_leaf</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - die Mindestanzahl von Objekten in den Bl√§ttern.</font></font></li>
<li><code>bootstrap</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - Verwendung zum Erstellen von Teilmusterb√§umen mit R√ºckgabe.</font></font></li>
</ul><br>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> RandomizedSearchCV<font></font>
n_estimators = [int(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> np.linspace(start = <span class="hljs-number">100</span>, stop = <span class="hljs-number">1000</span>, num = <span class="hljs-number">10</span>)]<font></font>
max_features = [<span class="hljs-string">'log2'</span>, <span class="hljs-string">'sqrt'</span>]<font></font>
max_depth = [int(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> np.linspace(start = <span class="hljs-number">1</span>, stop = <span class="hljs-number">15</span>, num = <span class="hljs-number">15</span>)]<font></font>
min_samples_split = [int(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> np.linspace(start = <span class="hljs-number">2</span>, stop = <span class="hljs-number">50</span>, num = <span class="hljs-number">10</span>)]<font></font>
min_samples_leaf = [int(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> np.linspace(start = <span class="hljs-number">2</span>, stop = <span class="hljs-number">50</span>, num = <span class="hljs-number">10</span>)]<font></font>
bootstrap = [<span class="hljs-literal">True</span>, <span class="hljs-literal">False</span>]<font></font>
param_dist = {<span class="hljs-string">'n_estimators'</span>: n_estimators,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'max_features'</span>: max_features,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'max_depth'</span>: max_depth,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'min_samples_split'</span>: min_samples_split,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'min_samples_leaf'</span>: min_samples_leaf,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'bootstrap'</span>: bootstrap}<font></font>
rs = RandomizedSearchCV(rfc_2,&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;param_dist,&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;n_iter = <span class="hljs-number">100</span>,&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;cv = <span class="hljs-number">3</span>,&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;verbose = <span class="hljs-number">1</span>,&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;n_jobs=<span class="hljs-number">-1</span>,&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;random_state=<span class="hljs-number">0</span>)<font></font>
rs.fit(X_train_scaled_pca, y_train)<font></font>
rs.best_params_<font></font>
<span class="hljs-comment"># {'n_estimators': 700,</span>
<span class="hljs-comment"># 'min_samples_split': 2,</span>
<span class="hljs-comment"># 'min_samples_leaf': 2,</span>
<span class="hljs-comment"># 'max_features': 'log2',</span>
<span class="hljs-comment"># 'max_depth': 11,</span>
<span class="hljs-comment"># 'bootstrap': True}</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Mit den Werten der Parameter </font></font><code>n_iter = 100</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">und </font></font><code>cv = 3</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">haben wir 300 RF-Modelle erstellt und zuf√§llig Kombinationen der </font><font style="vertical-align: inherit;">oben </font><font style="vertical-align: inherit;">dargestellten Hyperparameter ausgew√§hlt. </font></font><code>best_params_ </code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Informationen zu einer Reihe von Parametern, mit denen Sie das beste Modell erstellen k√∂nnen, finden Sie </font><font style="vertical-align: inherit;">im Attribut </font><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Zum jetzigen Zeitpunkt liefert dies jedoch m√∂glicherweise nicht die interessantesten Daten zu den Parameterbereichen, die es wert sind, in der n√§chsten Optimierungsrunde untersucht zu werden. </font><font style="vertical-align: inherit;">Um herauszufinden, in welchem ‚Äã‚ÄãWertebereich es sich lohnt, weiter zu suchen, k√∂nnen wir leicht einen Datenrahmen erhalten, der die Ergebnisse des RandomizedSearchCV-Algorithmus enth√§lt.</font></font><br>
<br>
<pre><code class="python hljs">rs_df = pd.DataFrame(rs.cv_results_).sort_values(<span class="hljs-string">'rank_test_score'</span>).reset_index(drop=<span class="hljs-literal">True</span>)<font></font>
rs_df = rs_df.drop([<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'mean_fit_time'</span>,&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'std_fit_time'</span>,&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'mean_score_time'</span>,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'std_score_time'</span>,&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'params'</span>,&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'split0_test_score'</span>,&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'split1_test_score'</span>,&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'split2_test_score'</span>,&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'std_test_score'</span>],<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;axis=<span class="hljs-number">1</span>)<font></font>
rs_df.head(<span class="hljs-number">10</span>)</code></pre><br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/617/b8c/20b/617b8c20b787acc3c76c23d9235b4b5a.png"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ergebnisse des RandomizedSearchCV-Algorithmus</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Nun erstellen wir Balkendiagramme, auf denen auf der X-Achse die Hyperparameterwerte und auf der Y-Achse die von den Modellen angezeigten Durchschnittswerte angegeben sind. </font><font style="vertical-align: inherit;">Auf diese Weise k√∂nnen Sie nachvollziehen, welche Werte von Hyperparametern im Durchschnitt ihre beste Leistung zeigen.</font></font><br>
<br>
<pre><code class="python hljs">fig, axs = plt.subplots(ncols=<span class="hljs-number">3</span>, nrows=<span class="hljs-number">2</span>)<font></font>
sns.set(style=<span class="hljs-string">"whitegrid"</span>, color_codes=<span class="hljs-literal">True</span>, font_scale = <span class="hljs-number">2</span>)<font></font>
fig.set_size_inches(<span class="hljs-number">30</span>,<span class="hljs-number">25</span>)<font></font>
sns.barplot(x=<span class="hljs-string">'param_n_estimators'</span>, y=<span class="hljs-string">'mean_test_score'</span>, data=rs_df, ax=axs[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>], color=<span class="hljs-string">'lightgrey'</span>)<font></font>
axs[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>].set_ylim([<span class="hljs-number">.83</span>,<span class="hljs-number">.93</span>])axs[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>].set_title(label = <span class="hljs-string">'n_estimators'</span>, size=<span class="hljs-number">30</span>, weight=<span class="hljs-string">'bold'</span>)<font></font>
sns.barplot(x=<span class="hljs-string">'param_min_samples_split'</span>, y=<span class="hljs-string">'mean_test_score'</span>, data=rs_df, ax=axs[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>], color=<span class="hljs-string">'coral'</span>)<font></font>
axs[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>].set_ylim([<span class="hljs-number">.85</span>,<span class="hljs-number">.93</span>])axs[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>].set_title(label = <span class="hljs-string">'min_samples_split'</span>, size=<span class="hljs-number">30</span>, weight=<span class="hljs-string">'bold'</span>)<font></font>
sns.barplot(x=<span class="hljs-string">'param_min_samples_leaf'</span>, y=<span class="hljs-string">'mean_test_score'</span>, data=rs_df, ax=axs[<span class="hljs-number">0</span>,<span class="hljs-number">2</span>], color=<span class="hljs-string">'lightgreen'</span>)<font></font>
axs[<span class="hljs-number">0</span>,<span class="hljs-number">2</span>].set_ylim([<span class="hljs-number">.80</span>,<span class="hljs-number">.93</span>])axs[<span class="hljs-number">0</span>,<span class="hljs-number">2</span>].set_title(label = <span class="hljs-string">'min_samples_leaf'</span>, size=<span class="hljs-number">30</span>, weight=<span class="hljs-string">'bold'</span>)<font></font>
sns.barplot(x=<span class="hljs-string">'param_max_features'</span>, y=<span class="hljs-string">'mean_test_score'</span>, data=rs_df, ax=axs[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>], color=<span class="hljs-string">'wheat'</span>)<font></font>
axs[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>].set_ylim([<span class="hljs-number">.88</span>,<span class="hljs-number">.92</span>])axs[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>].set_title(label = <span class="hljs-string">'max_features'</span>, size=<span class="hljs-number">30</span>, weight=<span class="hljs-string">'bold'</span>)<font></font>
sns.barplot(x=<span class="hljs-string">'param_max_depth'</span>, y=<span class="hljs-string">'mean_test_score'</span>, data=rs_df, ax=axs[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>], color=<span class="hljs-string">'lightpink'</span>)<font></font>
axs[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>].set_ylim([<span class="hljs-number">.80</span>,<span class="hljs-number">.93</span>])axs[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>].set_title(label = <span class="hljs-string">'max_depth'</span>, size=<span class="hljs-number">30</span>, weight=<span class="hljs-string">'bold'</span>)<font></font>
sns.barplot(x=<span class="hljs-string">'param_bootstrap'</span>,y=<span class="hljs-string">'mean_test_score'</span>, data=rs_df, ax=axs[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>], color=<span class="hljs-string">'skyblue'</span>)<font></font>
axs[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>].set_ylim([<span class="hljs-number">.88</span>,<span class="hljs-number">.92</span>])<font></font>
axs[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>].set_title(label = <span class="hljs-string">'bootstrap'</span>, size=<span class="hljs-number">30</span>, weight=<span class="hljs-string">'bold'</span>)<font></font>
plt.show()</code></pre><br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/418/311/ba6/418311ba6c38bfcebbf152af810d6b58.jpg"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Analyse der Werte von Hyperparametern</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Wenn wir die obigen Diagramme analysieren, k√∂nnen wir einige interessante Dinge feststellen, die dar√ºber sprechen, wie sich durchschnittlich jeder Wert eines Hyperparameters auf das Modell auswirkt.</font></font><br>
<br>
<ul>
<li><code>n_estimators</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">: Werte von 300, 500, 700 zeigen anscheinend die besten durchschnittlichen Ergebnisse.</font></font></li>
<li><code>min_samples_split</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">: Kleine Werte wie 2 und 7 scheinen die besten Ergebnisse zu zeigen. </font><font style="vertical-align: inherit;">Der Wert 23 sieht auch gut aus. Sie k√∂nnen mehrere Werte dieses Hyperparameters √ºber 2 sowie mehrere Werte von etwa 23 untersuchen.</font></font></li>
<li><code>min_samples_leaf</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">: Es besteht das Gef√ºhl, dass kleine Werte dieses Hyperparameters bessere Ergebnisse liefern. </font><font style="vertical-align: inherit;">Dies bedeutet, dass wir Werte zwischen 2 und 7 erfahren k√∂nnen.</font></font></li>
<li><code>max_features</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">: Option </font></font><code>sqrt</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">gibt das h√∂chste durchschnittliche Ergebnis.</font></font></li>
<li><code>max_depth</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">: Es gibt keine klare Beziehung zwischen dem Wert des Hyperparameters und dem Ergebnis des Modells, aber es besteht das Gef√ºhl, dass die Werte 2, 3, 7, 11, 15 gut aussehen.</font></font></li>
<li><code>bootstrap</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">: Der Wert </font></font><code>False</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">zeigt das beste Durchschnittsergebnis.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Mit diesen Erkenntnissen k√∂nnen wir nun zur zweiten Runde der Optimierung von Hyperparametern √ºbergehen. </font><font style="vertical-align: inherit;">Dies wird den Wertebereich einschr√§nken, an dem wir interessiert sind.</font></font><br>
<br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">8. Optimierung von Hyperparametern. </font><font style="vertical-align: inherit;">Runde 2: GridSearchCV (endg√ºltige Vorbereitung der Parameter f√ºr Modell Nr. 3, RF + PCA + HT)</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nach der Anwendung des RandomizedSearchCV-Algorithmus verwenden wir den GridSearchCV-Algorithmus, um eine genauere Suche nach der besten Kombination von Hyperparametern durchzuf√ºhren. Die gleichen Hyperparameter werden hier untersucht, aber jetzt wenden wir eine "gr√ºndlichere" Suche nach ihrer besten Kombination an. Mit dem GridSearchCV-Algorithmus wird jede Kombination von Hyperparametern untersucht. Dies erfordert viel mehr Rechenressourcen als die Verwendung des RandomizedSearchCV-Algorithmus, wenn wir die Anzahl der Suchiterationen unabh√§ngig voneinander festlegen. Zum Beispiel erfordert die Untersuchung von 10 Werten f√ºr jeden der 6 Hyperparameter mit Kreuzvalidierung in 3 Bl√∂cken 10‚Å∂ x 3 oder 3.000.000 Modellschulungen. Aus diesem Grund verwenden wir den GridSearchCV-Algorithmus, nachdem wir nach der Anwendung von RandomizedSearchCV die Wertebereiche der untersuchten Parameter eingegrenzt haben.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Mit dem, was wir mithilfe von RandomizedSearchCV herausgefunden haben, untersuchen wir die Werte der Hyperparameter, die sich am besten gezeigt haben:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> GridSearchCV<font></font>
n_estimators = [<span class="hljs-number">300</span>,<span class="hljs-number">500</span>,<span class="hljs-number">700</span>]<font></font>
max_features = [<span class="hljs-string">'sqrt'</span>]<font></font>
max_depth = [<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">7</span>,<span class="hljs-number">11</span>,<span class="hljs-number">15</span>]<font></font>
min_samples_split = [<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">22</span>,<span class="hljs-number">23</span>,<span class="hljs-number">24</span>]<font></font>
min_samples_leaf = [<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>,<span class="hljs-number">7</span>]<font></font>
bootstrap = [<span class="hljs-literal">False</span>]<font></font>
param_grid = {<span class="hljs-string">'n_estimators'</span>: n_estimators,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'max_features'</span>: max_features,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'max_depth'</span>: max_depth,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'min_samples_split'</span>: min_samples_split,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'min_samples_leaf'</span>: min_samples_leaf,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'bootstrap'</span>: bootstrap}<font></font>
gs = GridSearchCV(rfc_2, param_grid, cv = <span class="hljs-number">3</span>, verbose = <span class="hljs-number">1</span>, n_jobs=<span class="hljs-number">-1</span>)<font></font>
gs.fit(X_train_scaled_pca, y_train)<font></font>
rfc_3 = gs.best_estimator_<font></font>
gs.best_params_<font></font>
<span class="hljs-comment"># {'bootstrap': False,</span>
<span class="hljs-comment"># 'max_depth': 7,</span>
<span class="hljs-comment"># 'max_features': 'sqrt',</span>
<span class="hljs-comment"># 'min_samples_leaf': 3,</span>
<span class="hljs-comment"># 'min_samples_split': 2,</span>
<span class="hljs-comment"># 'n_estimators': 500}</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Hier wenden wir die Kreuzvalidierung in 3 Bl√∂cken f√ºr 540 (3 x 1 x 5 x 6 x 6 x 1) Modellschulungen an, was 1620 Modellschulungen ergibt. </font><font style="vertical-align: inherit;">Nachdem wir RandomizedSearchCV und GridSearchCV verwendet haben, k√∂nnen wir uns nun dem Attribut zuwenden, </font></font><code>best_params_</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">um herauszufinden, welche Werte von Hyperparametern es dem Modell erm√∂glichen, am besten mit dem untersuchten Datensatz zu arbeiten (diese Werte sind am Ende des vorherigen Codeblocks zu sehen). . </font><font style="vertical-align: inherit;">Diese Parameter werden verwendet, um Modellnummer 3 zu erstellen.</font></font><br>
<br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">9. Bewertung der Qualit√§t der Modelle anhand der Verifizierungsdaten</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Jetzt k√∂nnen Sie die erstellten Modelle anhand der Verifizierungsdaten auswerten. </font><font style="vertical-align: inherit;">Wir sprechen n√§mlich √ºber diese drei Modelle, die ganz am Anfang des Materials beschrieben wurden. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Schauen Sie sich diese Modelle an:</font></font><br>
<br>
<pre><code class="python hljs">y_pred = rfc.predict(X_test_scaled)<font></font>
y_pred_pca = rfc.predict(X_test_scaled_pca)<font></font>
y_pred_gs = gs.best_estimator_.predict(X_test_scaled_pca)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Erstellen Sie Fehlermatrizen f√ºr die Modelle und finden Sie heraus, wie gut jeder von ihnen Brustkrebs vorhersagen kann:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> confusion_matrix<font></font>
conf_matrix_baseline = pd.DataFrame(confusion_matrix(y_test, y_pred), index = [<span class="hljs-string">'actual 0'</span>, <span class="hljs-string">'actual 1'</span>], columns = [<span class="hljs-string">'predicted 0'</span>, <span class="hljs-string">'predicted 1'</span>])<font></font>
conf_matrix_baseline_pca = pd.DataFrame(confusion_matrix(y_test, y_pred_pca), index = [<span class="hljs-string">'actual 0'</span>, <span class="hljs-string">'actual 1'</span>], columns = [<span class="hljs-string">'predicted 0'</span>, <span class="hljs-string">'predicted 1'</span>])<font></font>
conf_matrix_tuned_pca = pd.DataFrame(confusion_matrix(y_test, y_pred_gs), index = [<span class="hljs-string">'actual 0'</span>, <span class="hljs-string">'actual 1'</span>], columns = [<span class="hljs-string">'predicted 0'</span>, <span class="hljs-string">'predicted 1'</span>])<font></font>
display(conf_matrix_baseline)<font></font>
display(<span class="hljs-string">'Baseline Random Forest recall score'</span>, recall_score(y_test, y_pred))<font></font>
display(conf_matrix_baseline_pca)<font></font>
display(<span class="hljs-string">'Baseline Random Forest With PCA recall score'</span>, recall_score(y_test, y_pred_pca))<font></font>
display(conf_matrix_tuned_pca)<font></font>
display(<span class="hljs-string">'Hyperparameter Tuned Random Forest With PCA Reduced Dimensionality recall score'</span>, recall_score(y_test, y_pred_gs))</code></pre><br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f48/a9e/92f/f48a9e92fd5fdca613d6073e00bae2c6.png"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ergebnisse der Arbeit der drei Modelle.</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Hier wird die Metrik als "R√ºckruf" bezeichnet. </font><font style="vertical-align: inherit;">Tatsache ist, dass es sich um eine Krebsdiagnose handelt. </font><font style="vertical-align: inherit;">Daher sind wir √§u√üerst daran interessiert, falsch negative Prognosen von Modellen zu minimieren. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In Anbetracht dessen k√∂nnen wir den Schluss ziehen, dass das grundlegende HF-Modell die besten Ergebnisse lieferte. </font><font style="vertical-align: inherit;">Die Vollst√§ndigkeitsrate betrug 94,97%. </font><font style="vertical-align: inherit;">Im Testdatensatz wurden 179 krebskranke Patienten erfasst. </font><font style="vertical-align: inherit;">Das Modell fand 170 von ihnen.</font></font><br>
<br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zusammenfassung</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Diese Studie liefert eine wichtige Beobachtung. </font><font style="vertical-align: inherit;">Manchmal funktioniert das RF-Modell, das die Hauptkomponentenmethode und die Optimierung von Hyperparametern in gro√üem Ma√üstab verwendet, m√∂glicherweise nicht so gut wie das √ºblichste Modell mit Standardeinstellungen. </font><font style="vertical-align: inherit;">Dies ist jedoch kein Grund, sich nur auf die einfachsten Modelle zu beschr√§nken. </font><font style="vertical-align: inherit;">Ohne verschiedene Modelle auszuprobieren, ist es unm√∂glich zu sagen, welches das beste Ergebnis zeigt. </font><font style="vertical-align: inherit;">Und bei Modellen, mit denen das Vorhandensein von Krebs bei Patienten vorhergesagt wird, k√∂nnen wir sagen, dass je besser das Modell ist, desto mehr Leben k√∂nnen gerettet werden. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Liebe Leser! </font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Welche Aufgaben l√∂sen Sie mit maschinellen Lernmethoden?</font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/files/1ba/550/d25/1ba550d25e8846ce8805de564da6aa63.png"></a></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de488330/index.html">Englisch mit George Karlin: Wir analysieren den genialen Stand-up √ºber phrasenbezogene Einheiten</a></li>
<li><a href="../de488332/index.html">Null, eins, zwei, Freddy wird dich abholen</a></li>
<li><a href="../de488336/index.html">Tipps zur Verwendung des Wave Function Collapse-Algorithmus</a></li>
<li><a href="../de488338/index.html">Google-Praktika: Z√ºrich, London und Silicon Valley</a></li>
<li><a href="../de488340/index.html">Beruf: Backend-Entwickler</a></li>
<li><a href="../de488346/index.html">Installieren von or-tools mit SCIP und GLPK in einer virtuellen Python 3.7-Umgebung unter Linux</a></li>
<li><a href="../de488348/index.html">Webinar ‚ÄûZehn agilste Herausforderungen und M√∂glichkeiten, sie in einer Stunde zu √ºberwinden‚Äú 17. Februar um 20:00 Uhr Moskauer Zeit</a></li>
<li><a href="../de488352/index.html">VDI-Kostenvergleich: On-Premise versus Public Cloud</a></li>
<li><a href="../de488356/index.html">Schulung f√ºr Dassault Syst√®mes-Produkte an der Staatlichen Marine Technischen Universit√§t St. Petersburg</a></li>
<li><a href="../de488360/index.html">Big Data Mythen und digitale Kultur</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>