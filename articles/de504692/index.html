<!doctype html>
<html class="no-js" lang="de">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>💃🏼 🐫 🖐🏽 ZFS-Grundlagen: Speicher und Leistung 👁️ ⚠️ 🎼</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In diesem Frühjahr haben wir bereits einige einführende Themen besprochen, z. B. wie Sie die Geschwindigkeit Ihrer Laufwerke überprüfen und was RAID i...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>ZFS-Grundlagen: Speicher und Leistung</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/504692/"><img src="https://habrastorage.org/getpro/habr/post_images/abf/883/e96/abf883e96b01dbc78420e0dc1a158460.jpg"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In diesem Frühjahr haben wir bereits einige einführende Themen besprochen, z. </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">B. wie Sie die Geschwindigkeit Ihrer Laufwerke überprüfen</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> und </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">was RAID ist</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">In der zweiten haben wir sogar versprochen, die Leistung verschiedener Multi-Disk-Topologien in ZFS weiter zu untersuchen. </font><font style="vertical-align: inherit;">Dies ist das Dateisystem der nächsten Generation, das überall implementiert wird: von </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Apple</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> bis </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ubuntu</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nun, heute ist der beste Tag, um ZFS kennenzulernen, neugierige Leser. </font><font style="vertical-align: inherit;">Beachten Sie jedoch, dass es nach einer konservativen Einschätzung des OpenZFS-Entwicklers Matt Arens "sehr kompliziert" ist. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Aber bevor wir zu den Zahlen kommen - und ich verspreche es -, müssen Sie für alle Varianten der vosmidiskovoy ZFS-Konfiguration darüber sprechen, </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">wie</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ZFS Daten auf der Festplatte speichert.</font></font><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zpool, vdev und Gerät</font></font></h1><br>
<img src="https://habrastorage.org/getpro/habr/post_images/674/1c6/ab3/6741c6ab310f4e0edf2adf7e2ca4c6bb.png"><br>
<i><font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dieses vollständige Pooldiagramm enthält drei Hilfs-VDEVs, einen für jede Klasse und vier für RAIDz2. Normalerweise </font></font></font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/b9f/82c/887/b9f82c88748c44d1f86cc412a053bf94.png"><br>
<i><font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">gibt es keinen Grund, einen Pool unangemessener </font></font></font></i><font style="vertical-align: inherit;"><i><font color="gray"><font style="vertical-align: inherit;">VDEV- </font></font></i><i><font color="gray"><font style="vertical-align: inherit;">Typen und -Größen zu erstellen. Wenn Sie möchten, hindert Sie nichts daran</font></font></i></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
, das ZFS-Dateisystem wirklich zu verstehen müssen Sie sich die tatsächliche Struktur genau ansehen. </font><font style="vertical-align: inherit;">Erstens kombiniert ZFS traditionelle Ebenen der Datenträgerverwaltung und das Dateisystem. </font><font style="vertical-align: inherit;">Zweitens wird beim Schreiben ein Transaktionskopiermechanismus verwendet. </font><font style="vertical-align: inherit;">Diese Merkmale bedeuten, dass sich das System strukturell stark von normalen Dateisystemen und RAID-Arrays unterscheidet. </font><font style="vertical-align: inherit;">Die ersten grundlegenden Bausteine, die zu verstehen sind: ein Speicherpool (zpool), ein virtuelles Gerät (vdev) und ein reales Gerät (Gerät).</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">zpool</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Der zpool-Speicherpool ist die oberste ZFS-Struktur. Jeder Pool enthält ein oder mehrere virtuelle Geräte. Jedes von ihnen enthält wiederum ein oder mehrere reale Geräte (Geräte). Virtuelle Pools sind autonome Blöcke. Ein physischer Computer kann zwei oder mehr separate Pools enthalten, aber jeder ist völlig unabhängig von den anderen. Pools können keine virtuellen Geräte gemeinsam nutzen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die Redundanz von ZFS erfolgt auf der Ebene der virtuellen Geräte, jedoch nicht auf der Ebene der Pools. Auf Poolebene gibt es absolut keine Redundanz. Wenn ein vdev-Laufwerk oder ein spezielles vdev verloren geht, geht der gesamte Pool verloren.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Moderne Speicherpools können den Verlust eines Cache- oder virtuellen Geräteprotokolls überleben - obwohl sie eine kleine Menge schmutziger Daten verlieren können, wenn sie das vdev-Protokoll während eines Stromausfalls oder eines Systemabsturzes verlieren. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Es gibt ein weit verbreitetes Missverständnis, dass „Datenbänder“ (Streifen) von ZFS über den gesamten Pool aufgezeichnet werden. Das ist nicht wahr. Zpool ist überhaupt kein lustiges RAID0, sondern ein lustiges </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">JBOD</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> mit einem komplexen veränderbaren Verteilungsmechanismus.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die Datensätze werden größtenteils entsprechend dem verfügbaren Speicherplatz auf die verfügbaren virtuellen Geräte verteilt, sodass sie theoretisch alle gleichzeitig gefüllt werden. In späteren Versionen von ZFS wird die aktuelle Verwendung (Auslastung) von vdev berücksichtigt. Wenn ein virtuelles Gerät erheblich stärker als das andere geladen ist (z. B. aufgrund der Leselast), wird es trotz des höchsten freien Speicherplatzkoeffizienten vorübergehend zum Schreiben übersprungen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ein in moderne ZFS-Datensatzverteilungsmethoden integrierter Mechanismus zur Erkennung von Recycling kann die Latenz verringern und den Durchsatz in Zeiten ungewöhnlich hoher Last erhöhen - dies ist jedoch kein </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Freibrief</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">unwillkürliches Mischen langsamer Festplatten und schneller SSDs in einem Pool. </font><font style="vertical-align: inherit;">Solch ein ungleicher Pool arbeitet immer noch mit der Geschwindigkeit des langsamsten Geräts, das heißt, als ob es vollständig aus solchen Geräten zusammengesetzt wäre.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">vdev</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Jeder Speicherpool besteht aus einem oder mehreren virtuellen Geräten (virtuelles Gerät, vdev). </font><font style="vertical-align: inherit;">Jedes vdev enthält wiederum ein oder mehrere reale Geräte. </font><font style="vertical-align: inherit;">Die meisten virtuellen Geräte werden zum einfachen Speichern von Daten verwendet, es gibt jedoch mehrere vdev-Hilfsklassen, darunter CACHE, LOG und SPECIAL. </font><font style="vertical-align: inherit;">Jeder dieser vdev-Typen kann eine von fünf Topologien haben: Einzelgerät, RAIDz1, RAIDz2, RAIDz3 oder Spiegel.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
RAIDz1, RAIDz2 und RAIDz3 sind spezielle Variationen der sogenannten RAID-Doppelparität (Diagonale). 1, 2 und 3 beziehen sich darauf, wie viele Paritätsblöcke für jedes Datenband zugewiesen sind. Anstelle separater Festplatten für die Parität verteilen virtuelle RAIDz-Geräte diese Parität gleichmäßig auf die Festplatten. Ein RAIDz-Array kann so viele Festplatten verlieren, wie es Paritätsblöcke enthält. Wenn er einen anderen verliert, wird er scheitern und den Speicherpool mitnehmen.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In gespiegelten virtuellen Geräten (Spiegel vdev) wird jeder Block auf jedem Gerät in vdev gespeichert. Obwohl es sich um die gängigsten Spiegel mit zwei Breiten handelt, kann sich eine beliebige Anzahl von Geräten im Spiegel befinden. In großen Installationen werden häufig dreifache Geräte verwendet, um die Leseleistung und die Fehlertoleranz zu erhöhen. Der vdev-Spiegel kann jeden Fehler überstehen, während mindestens ein Gerät in vdev weiterhin funktioniert. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Einzelne vdevs sind von Natur aus gefährlich. Ein solches virtuelles Gerät überlebt einen einzelnen Fehler nicht - und wenn es als Speicher oder als spezielles vdev verwendet wird, führt sein Ausfall zur Zerstörung des gesamten Pools. Sei hier sehr, sehr vorsichtig.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Virtuelle CACHE-, LOG- und SPECIAL-Appliances können mit einer der oben genannten Topologien erstellt werden. Beachten Sie jedoch, dass der Verlust einer virtuellen SPECIAL-Appliance den Verlust eines Pools bedeutet. Daher wird eine übermäßige Topologie dringend empfohlen.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Gerät</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dies ist wahrscheinlich der am einfachsten zu verstehende Begriff in ZFS - es handelt sich buchstäblich um ein Block-Direktzugriffsgerät. Denken Sie daran, dass virtuelle Geräte aus einzelnen Geräten bestehen und der Pool aus virtuellen Geräten besteht. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Festplatten - magnetisch oder Festkörper - sind die am häufigsten verwendeten Blockgeräte, die als vdev-Bausteine ​​verwendet werden. Es ist jedoch jedes Gerät mit einem Handle in / dev geeignet, sodass Sie ganze Hardware-RAID-Arrays als separate Geräte verwenden können. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Eine einfache Rohdatei ist eines der wichtigsten alternativen Blockgeräte, aus denen vdev erstellt werden kann. Testen Sie Pools aus </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dateien mit geringer Dichte</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">&nbsp;- Eine sehr bequeme Möglichkeit, Poolbefehle zu überprüfen und festzustellen, wie viel Speicherplatz im Pool oder auf dem virtuellen Gerät dieser Topologie verfügbar ist. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/5cf/aa5/62c/5cfaa562cb208b654af113f7535b8f57.png"><br>
<i><font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sie können in wenigen Sekunden einen Testpool aus Dateien mit geringer Dichte erstellen. Vergessen Sie jedoch nicht, den gesamten Pool und seine Komponenten später zu löschen.</font></font></font></i> <br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Angenommen, Sie möchten einen Server auf acht Festplatten installieren und planen, 10-TB-Festplatten (~ 9300 GiB) zu verwenden. Sie sind sich jedoch nicht sicher, welche Die Topologie entspricht am besten Ihren Anforderungen. Im obigen Beispiel erstellen wir in Sekundenschnelle einen Testpool aus Dateien mit geringer Dichte - und jetzt wissen wir, dass RAIDz2 vdev von acht 10-TB-Laufwerken eine nützliche Kapazität von 50 TiB bietet.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Eine weitere spezielle Geräteklasse ist SPARE (Ersatz). Hot-Swap-fähige Geräte gehören im Gegensatz zu herkömmlichen Geräten zum gesamten Pool und nicht nur zu einem virtuellen Gerät. Wenn ein vdev im Pool ausfällt und das Ersatzgerät mit dem Pool verbunden und verfügbar ist, wird es automatisch dem betroffenen vdev beitreten. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nach dem Herstellen einer Verbindung zum betroffenen vdev empfängt das Ersatzgerät Kopien oder die Rekonstruktion von Daten, die sich auf dem fehlenden Gerät befinden sollten. In herkömmlichem RAID wird dies als Neuerstellung bezeichnet, während es in ZFS als "Resilvering" bezeichnet wird.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Es ist wichtig zu beachten, dass Ersatzgeräte ausgefallene Geräte nicht dauerhaft ersetzen. </font><font style="vertical-align: inherit;">Dies ist nur ein vorübergehender Ersatz, um die Zeit zu verkürzen, in der eine vdev-Verschlechterung beobachtet wird. </font><font style="vertical-align: inherit;">Nachdem der Administrator das ausgefallene vdev-Gerät ersetzt hat, wird die Redundanz auf diesem permanenten Gerät wiederhergestellt, und SPARE trennt sich von vdev und kehrt als Ersatz für den gesamten Pool zurück.</font></font><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Datensätze, Blöcke und Sektoren</font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die nächsten Bausteine, die Sie auf unserer Reise durch ZFS verstehen müssen, sind nicht so sehr die Hardware, sondern die Organisation und Speicherung der Daten. </font><font style="vertical-align: inherit;">Wir überspringen hier mehrere Ebenen - wie z. B. Metaslab -, um die Details nicht zu häufen und gleichzeitig das Verständnis der Gesamtstruktur zu bewahren.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Datensatz</font></font></h3><br>
<img src="https://habrastorage.org/getpro/habr/post_images/bd3/c48/d9d/bd3c48d9dff6e0f493a5d90d1dca6d1d.png"><br>
<i><font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wenn wir zum ersten Mal ein Dataset erstellen, wird der gesamte verfügbare Poolbereich angezeigt. Dann legen wir das Kontingent fest - und ändern den Einhängepunkt. Magie! </font></font></font></i> <br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/a18/3de/210/a183de210cdc57cd1421652201cbf2c3.png"><br>
<i><font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zvol ist größtenteils nur ein Datensatz ohne Dateisystemschicht, den wir hier durch ein völlig normales ext4-</font></font></font></i> <br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Dateisystem ersetzen. Der ZFS-Datensatz entspricht in etwa einem standardmäßig bereitgestellten Dateisystem. Wie ein normales Dateisystem scheint es auf den ersten Blick „nur ein weiterer Ordner“ zu sein. Wie bei herkömmlichen gemounteten Dateisystemen verfügt auch jedes ZFS-Dataset über eigene grundlegende Eigenschaften.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Zunächst kann einem Datensatz ein Kontingent zugewiesen werden. Wenn installiert</font></font><code>zfs set quota=100G poolname/datasetname</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, können Sie nicht</font></font><code>/poolname/datasetname</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">mehr als 100 GiB</font><font style="vertical-align: inherit;">in den bereitgestellten Ordner schreiben</font><font style="vertical-align: inherit;">.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Beachten Sie das Vorhandensein - und Fehlen - von Schrägstrichen am Anfang jeder Zeile? Jeder Datensatz hat seinen eigenen Platz sowohl in der ZFS-Hierarchie als auch in der System-Mount-Hierarchie. In der ZFS-Hierarchie gibt es keinen führenden Schrägstrich. Sie beginnen mit dem Namen des Pools und dann mit dem Pfad von einem Datensatz zum nächsten. Zum Beispiel </font></font><code>pool/parent/child</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">für ein Dataset, das </font></font><code>child</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">unter dem übergeordneten Dataset </font></font><code>parent</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">in einem Pool mit einem Creative-Namen benannt ist </font></font><code>pool</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Standardmäßig wird die Mount - </font><font style="vertical-align: inherit;">Punkt des Datensatz auf seinen Namen in der ZFS - </font><font style="vertical-align: inherit;">Hierarchie entspricht, mit einem Schrägstrich am Anfang - der Pool mit dem Namen wird </font></font><code>pool</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">montiert , </font><font style="vertical-align: inherit;">wie </font></font><code>/pool</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">der Datensatz wird </font></font><code>parent</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">in montiert </font></font><code>/pool/parent</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, und der Kind - </font><font style="vertical-align: inherit;">Datensatz </font><font style="vertical-align: inherit;">wird montiert </font></font><code>child</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">in </font></font><code>/pool/parent/child</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Der System-Mount-Punkt für das Dataset kann jedoch geändert werden. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wenn wir angeben</font></font><code>zfs set mountpoint=/lol pool/parent/child</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, dann wird der Datensatz </font></font><code>pool/parent/child</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">im System als gemountet </font></font><code>/lol</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Zusätzlich zu Datensätzen sollten wir Volumes (zvols) erwähnen. </font><font style="vertical-align: inherit;">Ein Volume ähnelt ungefähr einem Datensatz, außer dass es tatsächlich kein Dateisystem hat - es ist nur ein Blockgerät. </font><font style="vertical-align: inherit;">Sie können beispielsweise </font></font><code>zvol</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">einen Namen </font><font style="vertical-align: inherit;">erstellen </font></font><code>mypool/myzvol</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, ihn dann mit dem ext4-Dateisystem formatieren und dann dieses Dateisystem bereitstellen - jetzt haben Sie das ext4-Dateisystem, aber mit Unterstützung für alle ZFS-Sicherheitsfunktionen! </font><font style="vertical-align: inherit;">Dies mag auf einem Computer albern erscheinen, ist jedoch als Backend beim Exportieren eines iSCSI-Geräts viel sinnvoller.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Blöcke</font></font></h3><br>
<img src="https://habrastorage.org/getpro/habr/post_images/74b/4dd/d00/74b4ddd009e67db1b1b6c4467bcf6fa3.png"><br>
<i><font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Eine Datei wird durch einen oder mehrere Blöcke dargestellt. Jeder Block wird auf einem virtuellen Gerät gespeichert. Die Blockgröße entspricht normalerweise dem Parameter </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">recordsize</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , kann jedoch auf </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2 ^ ashift</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> reduziert werden, </font><font style="vertical-align: inherit;">wenn sie Metadaten oder eine kleine Datei enthält. </font></font></font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/8d1/7fd/ad2/8d17fdad2eda641c801e5e6a302f6e38.png"><br>
<i><font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wir </font><font style="vertical-align: inherit;">scherzen </font><font style="vertical-align: inherit;">wirklich, </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">wirklich</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> nicht über den großen Leistungsschaden, wenn Sie zu wenig Ashift installieren.</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Im ZFS-Pool werden alle Daten, einschließlich Metadaten, in Blöcken gespeichert. Die maximale Blockgröße für jeden Datensatz wird in der Eigenschaft</font></font><code>recordsize</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(Datensatzgröße) definiert. Die Größe des Datensatzes kann variieren, dies ändert jedoch nicht die Größe oder Position von Blöcken, die bereits in das Dataset geschrieben wurden. Sie gelten nur für neue Blöcke, während sie geschrieben werden.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Sofern nicht anders angegeben, beträgt die aktuelle Aufzeichnungsgröße standardmäßig 128 KB. Dies ist eine Art schwieriger Kompromiss, bei dem die Leistung nicht ideal, aber in den meisten Fällen nicht schrecklich ist. </font></font><code>Recordsize</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">kann auf einen beliebigen Wert von 4K bis 1M eingestellt werden (mit zusätzlichen Einstellungen können </font></font><code>recordsize</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sie noch mehr einstellen, dies ist jedoch selten eine gute Idee). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Jeder Block bezieht sich auf die Daten nur einer Datei. Sie können nicht zwei verschiedene Dateien in einem Block zusammenfassen. Jede Datei besteht je nach Größe aus einem oder mehreren Blöcken. Wenn die Dateigröße kleiner als die Datensatzgröße ist, wird sie in einem kleineren Block gespeichert. Beispielsweise belegt ein Block mit einer 2-KiB-Datei nur einen 4-KiB-Sektor auf der Festplatte. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wenn die Datei groß genug ist und mehrere Blöcke erfordert, haben alle Datensätze mit dieser Datei eine Größe</font></font><code>recordsize</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">&nbsp;- einschließlich des letzten Eintrags, von dem sich der größte Teil als </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ungenutzter Speicherplatz</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> herausstellen </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;">kann</font></a><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Zvol-Volumes haben keine Eigenschaft, </font></font><code>recordsize</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">&nbsp;sondern eine entsprechende Eigenschaft </font></font><code>volblocksize</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sektoren</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Der letzte, grundlegendste Baustein ist der Sektor. Dies ist die kleinste physische Einheit, die in die Basiseinheit geschrieben oder von dieser gelesen werden kann. Mehrere Jahrzehnte lang verwendeten die meisten Festplatten 512-Byte-Sektoren. In letzter Zeit sind die meisten Laufwerke für 4-KiB-Sektoren und in einigen - insbesondere SSDs - 8-KiB-Sektoren oder sogar mehr konfiguriert. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ZFS verfügt über eine Eigenschaft, mit der Sie die Sektorgröße manuell festlegen können. Dies ist eine Eigenschaft </font></font><code>ashift</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Es ist etwas verwirrend, dass Ashift eine Zweierpotenz ist. Zum Beispiel </font></font><code>ashift=9</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">bedeutet dies eine Sektorgröße von 2 ^ 9 oder 512 Bytes.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ZFS fragt das Betriebssystem nach detaillierten Informationen zu jedem Blockgerät, wenn es dem neuen vdev hinzugefügt wird, und stellt die Verschiebung basierend auf diesen Informationen theoretisch automatisch richtig ein. Leider lügen viele Festplatten über ihre Sektorgröße, um die Kompatibilität mit Windows XP aufrechtzuerhalten (das Festplatten mit anderen Sektorgrößen nicht verstehen konnte). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dies bedeutet, dass dem ZFS-Administrator dringend empfohlen wird, die tatsächliche Sektorgröße seiner Geräte zu kennen und manuell zu installieren</font></font><code>ashift</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Wenn eine zu kleine Verschiebung eingestellt ist, nimmt die Anzahl der Lese- / Schreibvorgänge astronomisch zu. Das Schreiben von 512-Byte-Sektoren in den realen 4-KiB-Sektor bedeutet also, den ersten „Sektor“ zu schreiben, dann den 4-KiB-Sektor zu lesen, ihn durch den zweiten 512-Byte-Sektor zu ändern, ihn in den neuen 4-KiB-Sektor zurückzuschreiben und so weiter für jeden Eintrag. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In der realen Welt schlägt eine solche Strafe Samsung EVO- </font></font><code>ashift=13</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SSDs </font><font style="vertical-align: inherit;">, für die sie handeln muss </font><font style="vertical-align: inherit;">, aber diese SSDs liegen in Bezug auf ihre Sektorgröße und sind daher standardmäßig festgelegt </font></font><code>ashift=9</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Wenn ein erfahrener Systemadministrator diese Einstellung nicht ändert, ist diese SSD </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">langsamer als eine</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> normale magnetische Festplatte. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Zum Vergleich für eine zu große Größe</font></font><code>ashift</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Es gibt praktisch keine Strafe. </font><font style="vertical-align: inherit;">Es gibt keine wirkliche Abnahme der Produktivität, und die Zunahme des nicht genutzten Speicherplatzes ist unendlich gering (oder gleich Null bei aktivierter Komprimierung). </font><font style="vertical-align: inherit;">Wir empfehlen daher dringend, auch Laufwerke zu installieren, die wirklich 512-Byte-Sektoren verwenden, </font></font><code>ashift=12</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">oder sogar </font></font><code>ashift=13</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">sicher in die Zukunft zu schauen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die Eigenschaft wird </font></font><code>ashift</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">für jedes virtuelle vdev-Gerät festgelegt und </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">nicht für den Pool</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , wie viele fälschlicherweise denken - und ändert sich nach der Installation nicht. </font><font style="vertical-align: inherit;">Wenn Sie </font></font><code>ashift</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">beim Hinzufügen eines neuen vdev zum Pool </font><font style="vertical-align: inherit;">versehentlich heruntergefahren wurden </font><font style="vertical-align: inherit;">, haben Sie diesen Pool unwiderruflich mit einem Gerät mit geringer Leistung kontaminiert. In der Regel gibt es keine andere Möglichkeit, als den Pool zu zerstören und von vorne zu beginnen. </font><font style="vertical-align: inherit;">Selbst das Entfernen von vdev rettet Sie nicht vor einem fehlerhaften Setup</font></font><code>ashift</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">!</font></font><br>
<br>
<h3>   </h3><br>
<img src="https://habrastorage.org/getpro/habr/post_images/38b/a1e/4a8/38ba1e4a8fa0e255081ed8db259a302f.gif"><br>
<i><font color="gray">      &nbsp;—     ,   </font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/90d/4cb/a35/90d4cba35ffa5a3e44a7ca5f61d4491b.gif"><br>
<i><font color="gray">         ,     </font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/c8b/2af/ffb/c8b2afffbc46f63f6a7fe1167edf5dcb.gif"><br>
<i><font color="gray">  ,      ,   « »   « »,        </font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/4c1/e2b/818/4c1e2b818077cb07d651527e214363fe.gif"><br>
<i><font color="gray">     ,       —      ,     ,       </font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Copy on Write (CoW) ist die grundlegende Grundlage dafür, was ZFS so großartig macht. Das Grundkonzept ist einfach: Wenn Sie das herkömmliche Dateisystem bitten, die Datei zu ändern, wird es genau das tun, was Sie angefordert haben. Wenn Sie das Dateisystem mit dem Kopieren während der Aufnahme auffordern, dasselbe zu tun, wird "gut" angezeigt - aber es wird Sie anlügen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Stattdessen schreibt das Copy-Write-Dateisystem die neue Version des geänderten Blocks und aktualisiert dann die Dateimetadaten, um die Verbindung zum alten Block zu trennen und den neuen Block zuzuordnen, den Sie gerade geschrieben haben.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das Trennen des alten Geräts und das Verbinden des neuen Geräts erfolgt in einem Arbeitsgang, sodass es nicht unterbrochen werden kann. Wenn Sie danach die Stromversorgung zurücksetzen, haben Sie eine neue Version der Datei. Wenn Sie die Stromversorgung früher zurücksetzen, haben Sie die alte Version. </font><font style="vertical-align: inherit;">In jedem Fall liegt kein Konflikt im Dateisystem vor. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das Kopieren beim Schreiben in ZFS erfolgt nicht nur auf Dateisystemebene, sondern auch auf Datenträgerverwaltungsebene. </font><font style="vertical-align: inherit;">Dies bedeutet, dass ZFS keinem Leerzeichen im Datensatz unterliegt (einem </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Loch im RAID</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ) - ein Phänomen, bei dem der Strip vor dem Systemabsturz nur teilweise aufzeichnen konnte und das Array nach einem Neustart beschädigt wurde. </font><font style="vertical-align: inherit;">Hier ist der Streifen atomar, vdev ist immer konsistent und </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bob ist dein Onkel</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ZIL: ZFS-Absichtsprotokoll</font></font></h3><br>
<img src="https://habrastorage.org/getpro/habr/post_images/567/71c/73f/56771c73f9a28ebaed161e02313deadb.png"><br>
<i><font color="gray"> ZFS     &nbsp;—  ,      ZIL,            </font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/cec/7c5/437/cec7c5437087f6816f9cdea5f6829820.png"><br>
<i><font color="gray"> ,   ZIL,    .      </font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/075/65a/d6b/07565ad6b2f431db3e6bc20cd24a653b.png"><br>
<i><font color="gray">SLOG,   LOG-, —   &nbsp;— , ,  &nbsp;—&nbsp;vdev,  ZIL      </font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/927/0f7/539/9270f7539b759aa37896d41e04c4ec47.png"><br>
<i><font color="gray">      ZIL &nbsp;—    ZIL   SLOG,      </font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Es gibt zwei Hauptkategorien von Schreibvorgängen - synchron (synchron) und asynchron (asynchron). Bei den meisten Workloads ist die überwiegende Mehrheit der Schreibvorgänge asynchron. Mit dem Dateisystem können Sie sie aggregieren und stapelweise bereitstellen, wodurch die Fragmentierung verringert und der Durchsatz erheblich erhöht wird. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Synchrone Aufnahmen sind eine ganz andere Sache. Wenn eine Anwendung ein synchrones Schreiben anfordert, teilt sie dem Dateisystem mit: "Sie müssen dies </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">jetzt in den</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> nichtflüchtigen Speicher übertragen </font><font style="vertical-align: inherit;">, und bis dahin kann ich nichts mehr tun." Daher sollten synchrone Aufzeichnungen sofort auf die Festplatte übertragen werden - und wenn dies die Fragmentierung erhöht oder die Bandbreite verringert, ist dies auch der Fall.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ZFS verarbeitet synchrone Datensätze anders als normale Dateisysteme. Anstatt sie sofort in den regulären Speicher hochzuladen, zeichnet ZFS sie in einem speziellen Speicherbereich auf, der als ZFS-Absichtsprotokoll - ZFS-Absichtsprotokoll oder ZIL - bezeichnet wird. Der Trick besteht darin, dass diese Datensätze </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">auch</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> im Speicher verbleiben und zusammen mit regulären asynchronen Schreibanforderungen aggregiert werden, um später als ganz normale TXGs (Transaktionsgruppen, Transaktionsgruppen) gespeichert zu werden. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Im Normalbetrieb wird ZIL aufgezeichnet und nie wieder gelesen. Wenn nach einigen Augenblicken Aufzeichnungen von ZIL im Hauptspeicher in normalem TXG aus dem RAM fixiert werden, werden sie von ZIL getrennt. Das einzige, was aus ZIL gelesen wird, ist das Importieren des Pools.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wenn ZFS abstürzt - Betriebssystemabstürze oder Stromausfälle -, wenn Daten in ZIL vorhanden sind, werden diese Daten beim nächsten Poolimport gelesen (z. B. beim Neustart des Notfallsystems). Alles, was sich in der ZIL befindet, wird gelesen, in TXG-Gruppen zusammengefasst, in den Hauptspeicher übernommen und dann während des Importvorgangs von der ZIL getrennt. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Eine der vdev-Hilfsklassen heißt LOG oder SLOG, das sekundäre LOG-Gerät. Er hat eine Aufgabe - dem Pool ein separates und vorzugsweise viel schnelleres vdev-Gerät mit sehr hohem Schreibwiderstand zum Speichern von ZIL zur Verfügung zu stellen, anstatt ZIL im vdev-Hauptspeicher zu speichern. ZIL selbst verhält sich unabhängig vom Speicherort gleich. Wenn jedoch vdev mit LOG eine sehr hohe Schreibleistung aufweist, sind synchrone Schreibvorgänge schneller.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das Hinzufügen von vdev mit LOG zum Pool </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">kann</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> die asynchrone Schreibleistung </font><b><font style="vertical-align: inherit;">nicht</font></b><font style="vertical-align: inherit;"> verbessern. Selbst wenn Sie alle Schreibvorgänge in ZIL erzwingen </font></font><code>zfs set sync=always</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, werden sie auf dieselbe Weise und im gleichen Tempo wie ohne Protokoll an das Hauptrepository in TXG gebunden. </font><font style="vertical-align: inherit;">Die einzige direkte Leistungsverbesserung ist die Verzögerung bei der synchronen Aufzeichnung (da eine höhere Protokollgeschwindigkeit den Betrieb beschleunigt </font></font><code>sync</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In einer Umgebung, in der bereits eine große Anzahl synchroner Schreibvorgänge erforderlich ist, kann vdev LOG jedoch indirekt asynchrone Schreibvorgänge und nicht zwischengespeicherte Lesevorgänge beschleunigen. </font><font style="vertical-align: inherit;">Das Hochladen von ZIL-Datensätzen in ein separates vdev-LOG bedeutet weniger Konkurrenz für IOPS im Primärspeicher, was die Leistung aller Lese- und Schreibvorgänge in gewissem Maße verbessert.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Schnappschüsse</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Der Schreibkopiermechanismus ist auch eine wesentliche Grundlage für atomare ZFS-Snapshots und inkrementelle asynchrone Replikation. </font><font style="vertical-align: inherit;">Das aktive Dateisystem verfügt über einen Zeigerbaum, der alle Datensätze mit aktuellen Daten markiert. Wenn Sie einen Snapshot erstellen, erstellen Sie einfach eine Kopie dieses Zeigerbaums. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wenn ein Datensatz im aktiven Dateisystem überschrieben wird, schreibt ZFS zuerst die neue Version des Blocks in den nicht verwendeten Speicherplatz. </font><font style="vertical-align: inherit;">Anschließend wird die alte Version des Blocks vom aktuellen Dateisystem getrennt. </font><font style="vertical-align: inherit;">Wenn sich ein Schnappschuss jedoch auf den alten Block bezieht, bleibt er unverändert. </font><font style="vertical-align: inherit;">Der alte Block wird erst dann als freier Speicherplatz wiederhergestellt, wenn alle Snapshots, die mit diesem Block verknüpft sind, zerstört wurden!</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Reproduzieren</font></font></h3><br>
<img src="https://habrastorage.org/getpro/habr/post_images/e69/167/01d/e6916701d4aa3ff27bb42efc43be60da.png"><br>
<i><font color="gray">  Steam  2015   158&nbsp;   126&nbsp;927 .        rsync&nbsp;—  ZFS    « »  750% .</font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/25f/376/0ab/25f3760ab64d6647571b9c02804b39f0.png"><br>
<i><font color="gray">      40-     Windows 7&nbsp;—   .  ZFS   289  ,  rsync&nbsp;—  «»  161  ,    ,   rsync   --inplace.</font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/776/46b/a3a/77646ba3ac20eeb0933d7dc7d644296c.png"><br>
<i><font color="gray">    ,  rsync    .  1,9         &nbsp;—    ,   ZFS   1148  ,  rsync,    rsync --inplace</font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Sobald Sie die Funktionsweise von Snapshots verstanden haben, können Sie die Essenz der Replikation leicht erfassen. Da ein Snapshot nur ein Baum von Zeigern auf Datensätze ist, </font></font><code>zfs send</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">senden wir diesen Baum und alle damit verbundenen Datensätze </font><font style="vertical-align: inherit;">, wenn wir einen </font><font style="vertical-align: inherit;">Snapshot erstellen. Wenn wir dies passieren </font></font><code>zfs send</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">in </font></font><code>zfs receive</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">auf das Zielobjekt, schreibt sie sowohl den eigentlichen Inhalt des Blocks und den Baum von Zeigern, die die Blöcke auf den Zieldatensatz verweisen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Im zweiten wird alles noch interessanter </font></font><code>zfs send</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Jetzt haben wir zwei Systeme, von denen jedes enthält </font></font><code>poolname/datasetname@1</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, und Sie schießen einen neuen Schnappschuss </font></font><code>poolname/datasetname@2</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Daher haben Sie im Quellpool </font></font><code>datasetname@1</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">und </font></font><code>datasetname@2</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">und im Zielpool bisher nur den ersten Snapshot </font></font><code>datasetname@1</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Da haben wir einen gemeinsamen Schnappschuss zwischen der Quelle und dem Ziel</font></font><code>datasetname@1</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">können wir </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">inkrementell</font></font></i> <font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> tun </font></font><code>zfs send</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Wenn wir dem System mitteilen </font></font><code>zfs send -i poolname/datasetname@1 poolname/datasetname@2</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, werden zwei Zeigerbäume verglichen. Alle Zeiger, die nur in vorhanden </font></font><code>@2</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">sind, beziehen sich offensichtlich auf neue Blöcke - daher benötigen wir den Inhalt dieser Blöcke. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Auf einem Remote-System ist die inkrementelle Verarbeitung </font></font><code>send</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">genauso einfach. Zuerst zeichnen wir alle neuen Einträge auf, die im Stream enthalten sind </font></font><code>send</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, und fügen dann Zeiger zu diesen Blöcken hinzu. Voila, in unserem </font></font><code>@2</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">neuen System! </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die asynchrone inkrementelle ZFS-Replikation ist eine enorme Verbesserung gegenüber früheren Nicht-Snapshot-Methoden wie rsync. In beiden Fällen werden nur geänderte Daten übertragen - rsync muss jedoch zuerst </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">gelesen werden</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">von der Festplatte alle Daten auf beiden Seiten, um die Menge zu überprüfen und zu vergleichen. </font><font style="vertical-align: inherit;">Im Gegensatz dazu liest die ZFS-Replikation nur Zeigerbäume - und alle Blöcke, die im allgemeinen Snapshot nicht dargestellt sind.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Inline-Komprimierung</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Der Copy-on-Write-Mechanismus vereinfacht auch das integrierte Komprimierungssystem. In einem herkömmlichen Dateisystem ist die Komprimierung problematisch - sowohl die alte als auch die neue Version der geänderten Daten befinden sich im selben Bereich. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wenn Sie ein Datenelement in der Mitte einer Datei betrachten, das sein Leben als Megabyte von Nullen ab 0x00000000 usw. beginnt, ist es sehr einfach, es auf einen Sektor auf der Festplatte zu komprimieren. Aber was passiert, wenn wir dieses Megabyte Nullen durch ein Megabyte inkompressibler Daten wie JPEG oder pseudozufälliges Rauschen ersetzen? Plötzlich benötigt dieses Megabyte an Daten nicht einen, sondern 256 Sektoren mit 4 KB, und an dieser Stelle auf der Festplatte ist nur ein Sektor reserviert.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ZFS hat kein solches Problem, da geänderte Datensätze immer in nicht verwendeten Speicherplatz geschrieben werden - der ursprüngliche Block belegt nur einen 4-KiB-Sektor, und ein neuer Datensatz benötigt 256, dies ist jedoch kein Problem - ein kürzlich geändertes Fragment aus der Mitte der Datei würde in nicht verwendeten Speicherplatz geschrieben Unabhängig davon, ob sich die Größe geändert hat oder nicht, ist dies für ZFS eine normale Situation. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die integrierte ZFS-Komprimierung ist standardmäßig deaktiviert und das System bietet Plug-In-Algorithmen - darunter LZ4, gzip (1-9), LZJB und ZLE.</font></font><br>
<br>
<ul>
<li><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">LZ4</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ist ein Streaming-Algorithmus, der für die meisten Anwendungsfälle extrem schnelle Komprimierung und Dekomprimierung sowie Leistungssteigerungen bietet - selbst auf relativ langsamen CPUs.</font></font><br>
</li>
<li><b>GZIP</b> —  ,       Unix-.        1-9,       CPU      9.       (   )  ,    &nbsp;   c CPU&nbsp;—    ,     .<br>
</li>
<li><b>LZJB</b> —    ZFS.       , LZ4     .<br>
</li>
<li><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ZLE</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - Zero Level Coding, Zero Level Coding. </font><font style="vertical-align: inherit;">Es berührt überhaupt keine normalen Daten, komprimiert jedoch große Folgen von Nullen. </font><font style="vertical-align: inherit;">Nützlich für vollständig inkompressible Datensätze (z. B. JPEG, MP4 oder andere bereits komprimierte Formate), da inkompressible Daten ignoriert werden, jedoch nicht verwendeter Speicherplatz in den resultierenden Datensätzen komprimiert wird.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wir empfehlen die LZ4-Komprimierung für fast alle Anwendungsfälle. </font><font style="vertical-align: inherit;">Die Leistungseinbuße für inkompressible Daten zu </font><font style="vertical-align: inherit;">begegnen ist sehr klein, und die </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Leistungsverstärkung</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> für typische Daten ist signifikant. </font><font style="vertical-align: inherit;">Das Kopieren eines Images einer virtuellen Maschine für eine Neuinstallation des Windows-Betriebssystems (frisch installiertes Betriebssystem, noch keine Daten darin) </font><font style="vertical-align: inherit;">wurde in </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;">diesem Test 2015</font></a></font><code>compression=lz4</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 27% schneller als mit </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;">bestanden</font></a><font style="vertical-align: inherit;"> .</font></font><code>compression=none</code><font style="vertical-align: inherit;"></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"></font><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ARC - adaptiver Ersatzcache</font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ZFS ist das einzige uns bekannte moderne Dateisystem, das einen eigenen Lese-Caching-Mechanismus verwendet und nicht auf den Seiten-Cache des Betriebssystems angewiesen ist, um Kopien kürzlich gelesener Blöcke im RAM zu speichern. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Obwohl der eigene Cache nicht ohne Probleme ist, kann ZFS nicht so schnell wie der Kernel auf neue Speicherzuweisungsanforderungen reagieren, sodass ein neuer </font></font><code>malloc()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Speicherzuweisungsaufruf möglicherweise fehlschlägt, wenn RAM benötigt wird, das derzeit von ARC belegt ist. </font><font style="vertical-align: inherit;">Aber es gibt gute Gründe, zumindest vorerst einen eigenen Cache zu verwenden.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Alle bekannten modernen Betriebssysteme, einschließlich MacOS, Windows, Linux und BSD, verwenden den LRU-Algorithmus (Least Recent Used), um den Seitencache zu implementieren. Dies ist ein primitiver Algorithmus, der den zwischengespeicherten Block nach jedem Lesen in die Warteschlange stellt und die Blöcke in der Warteschlange nach Bedarf verschiebt, um neue Cache-Fehler (Blöcke, die von der Festplatte und nicht vom Cache gelesen werden sollten) hinzuzufügen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Normalerweise funktioniert der Algorithmus einwandfrei, aber auf Systemen mit großen Arbeitsdatensätzen führt LRU leicht zu Thrashing - Verdrängung häufig benötigter Blöcke, um Platz für Blöcke zu schaffen, die nie wieder aus dem Cache gelesen werden. </font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">BOGEN</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">&nbsp;- ein viel weniger naiver Algorithmus, der als "gewichteter" Cache betrachtet werden kann. Nach jedem Lesen des zwischengespeicherten Blocks wird es etwas „schwerer“ und es wird schwieriger, sich zu verdrängen - und selbst nach dem Verdrängen wird der Block </font><font style="vertical-align: inherit;">für einen bestimmten Zeitraum </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">verfolgt</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Ein Block, der herausgedrückt wurde, dann aber in den Cache zurückgelesen werden muss, wird ebenfalls "schwerer".</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das Endergebnis all dessen ist ein Cache mit einer viel größeren Trefferquote - dem Verhältnis zwischen Treffern im Cache (aus dem Cache gelesen) und Fehlern (von der Festplatte gelesen). </font><font style="vertical-align: inherit;">Dies ist eine äußerst wichtige Statistik. Der Cache trifft nicht nur selbst Service-Größenordnungen schneller, Cache-Misses können auch schneller bedient werden. Je mehr Cache-Hits, desto weniger gleichzeitige Festplattenanforderungen und desto geringer die Verzögerung für die verbleibenden Misses, die bedient werden sollen Fahrt.</font></font><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fazit</font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nachdem wir uns mit der grundlegenden Semantik von ZFS befasst haben - wie das Kopieren beim Schreiben funktioniert - sowie mit den Beziehungen zwischen Speicherpools, virtuellen Geräten, Blöcken, Sektoren und Dateien, können wir die tatsächliche Leistung mit realen Zahlen diskutieren. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Im nächsten Teil werden wir die tatsächliche Leistung von Pools mit gespiegeltem vdev und RAIDz im Vergleich untereinander sowie im Vergleich zu herkömmlichen Linux-Kernel-RAID-Topologien untersuchen, die wir </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">zuvor</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> untersucht </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;">haben</font></a><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Zuerst wollten wir nur die Grundlagen betrachten - die ZFS - </font><font style="vertical-align: inherit;">Topologien selbst - aber nach </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">dieser</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> werden wir bereit sein , </font><font style="vertical-align: inherit;">über Fortgeschrittene ZFS - </font><font style="vertical-align: inherit;">Tuning und Tuning zu sprechen, einschließlich der Verwendung von Hilfs vdev Typen wie L2ARC, SLOG und Sonder Allocation.</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de504680/index.html">Übersicht über die NLP-Bibliothek von SpaL</a></li>
<li><a href="../de504682/index.html">Nostalgie Post: j2me, Schwerkraft trotzt, 64kb</a></li>
<li><a href="../de504686/index.html">Wie zeichnet man eine Katze?</a></li>
<li><a href="../de504688/index.html">Masken sind nutzlos: wissenschaftliche Kritik an der Sozialpolitik bei KOVID-19</a></li>
<li><a href="../de504690/index.html">Die Geschichte, wie ich Azure AD B2C für React and React Native konfiguriert habe Teil 3 (Tutorial)</a></li>
<li><a href="../de504694/index.html">So kompilieren Sie einen Dekorator - C ++, Python und seine eigene Implementierung. Teil 1</a></li>
<li><a href="../de504696/index.html">Nachrichten aus der Welt von OpenStreetMap Nr. 513 (12.05.2020-18.05.2020)</a></li>
<li><a href="../de504698/index.html">Onboarding an einem Remote-Standort</a></li>
<li><a href="../de504700/index.html">Sowjetische Grafiktafel "Skizze"</a></li>
<li><a href="../de504702/index.html">Die Leute wollen kein Englisch</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>