<!doctype html>
<html class="no-js" lang="ja">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🚲 🍶 🤓 TensorRT 6.xxx-ディープラーニングモデルの高性能推論（オブジェクト検出とセグメンテーション） 🤞🏾 🐉 🤾🏻</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="初めて痛い！
 
 みなさん、こんにちは！親愛なる友人、この記事では、github.com / aidonchuk / retinanet-examplesリポジトリに基づくTensorRT、RetinaNetを使用した私の経験を共有したいと思います（これは、nvidiaからの公式のターンキーのフォ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>TensorRT 6.xxx-ディープラーニングモデルの高性能推論（オブジェクト検出とセグメンテーション）</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ods/blog/483074/"><img src="https://habrastorage.org/webt/qr/je/yu/qrjeyup390z-iv5uyduultmmxcs.png" alt="画像"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">初めて痛い！</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
みなさん、こんにちは！親愛なる友人、この記事では、</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com / aidonchuk / retinanet-examples</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">リポジトリに基づくTensorRT、RetinaNetを使用した私の経験を共有したいと</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">思い</font></a><font style="vertical-align: inherit;">ます</font><font style="vertical-align: inherit;">（これは、</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">nvidia</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">からの公式のターンキーのフォークであり</font><font style="vertical-align: inherit;">、これにより、最短の時間で最適化されたモデルを本番</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">環境で</font></a><font style="vertical-align: inherit;">使用できるようになります）。</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ods.ai</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">コミュニティ</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">チャネルをスクロールすると、TensorRTの</font></a><font style="vertical-align: inherit;">使用に関する質問に出くわし、ほとんどの質問が繰り返されるため</font><font style="vertical-align: inherit;">、TensorRT、RetinaNet、Unet、およびDockerに基づいて高速推論を使用</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">する</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ための</font><i><font style="vertical-align: inherit;">可能な限り包括的な</font></i><font style="vertical-align: inherit;">ガイド</font><font style="vertical-align: inherit;">を書くことにしました</font><font style="vertical-align: inherit;">。</font></font><br>
<a name="habracut"></a><br>
<b><font style="vertical-align: inherit;"></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
この方法でタスクを設定するため</font><b><font style="vertical-align: inherit;">に</font></b><font style="vertical-align: inherit;">提案する</font><b><font style="vertical-align: inherit;">タスクの説明</font></b><font style="vertical-align: inherit;">：データセットをマークアップし、Pytorch1.3 +でRetinaNet / Unetネットワークをトレーニングし、受信した重みをONNXに変換してから、TensorRTエンジンに変換して、Dockerでこの全体を実行します（できればUbuntu 18）。 ARM（Jetson）*アーキテクチャでは非常に望ましいため、環境の手動展開を最小限に抑えることができます。その結果、RetinaNet / Unetのエクスポートとトレーニングだけでなく、必要なすべてのバインディングを使用した分類、セグメンテーションの完全な開発とトレーニングにも対応できるコンテナが得られます。</font></font><br>
<cut></cut><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ステージ1.環境の準備</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
ここでは、デスクトップマシンとdevboxで、少なくとも一部のライブラリの使用と展開を完全に中止したことに注意してください。作成してインストールする必要があるのは、Python仮想環境とcuda 10.2（1つのnvidiaドライバーに制限できます）だけです。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
新しくインストールしたUbuntu 18があるとします。cuda10.2（deb）をインストールします。インストールプロセスについては詳しく説明しません。公式ドキュメントで十分です。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ここで、Dockerをインストールします。Dockerのインストールガイドは簡単に見つかります。例は、</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">www.digitalocean.com / community / tutorials / docker-ubuntu-18-04-1-enです。</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、19 +バージョンはすでに利用可能です-入れてください。さて、sudoなしでdockerを使用できるようにすることを忘れないでください、それはより便利になります。すべてが判明した後、次のようにします。</font></font><br>
<br>
<pre><code class="bash hljs">distribution=$(. /etc/os-release;<span class="hljs-built_in">echo</span> <span class="hljs-variable">$ID</span><span class="hljs-variable">$VERSION_ID</span>)<font></font>
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -<font></font>
curl -s -L https://nvidia.github.io/nvidia-docker/<span class="hljs-variable">$distribution</span>/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list<font></font>
<font></font>
sudo apt-get update &amp;&amp; sudo apt-get install -y nvidia-container-toolkit<font></font>
sudo systemctl restart docker<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
また、公式リポジトリ</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/NVIDIA/nvidia-docker</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">を調べる必要もありません</font><font style="vertical-align: inherit;">。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
次にgit clone </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/aidonchuk/retinanet-examplesを実行し</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
それはほんの少し残っていますが、nvidia-imageでdockerを使い始めるためには、NGC Cloudに登録してログインする必要があります。ここで</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ngc.nvidia.comにアクセス</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">して登録し、NGC Cloudの内部に入ったら、画面の左上隅にあるSETUPを押すか、このリンク</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ngc.nvidia.com/setup/api-keyをたどり</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ます。 [キーを生成]をクリックします。保存することをお勧めします。保存しない場合は、次回アクセスするときに再生成する必要があるため、新しい車に展開して、この操作を再実行してください。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
実行：</font></font><br>
<br>
<pre><code class="bash hljs">docker login nvcr.io<font></font>
Username: <span class="hljs-variable">$oauthtoken</span><font></font>
Password: &lt;Your Key&gt; -  <font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ユーザー名だけをコピーします。</font><font style="vertical-align: inherit;">さて、考慮して、環境が展開されています！</font></font><br>
<cut></cut><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ステージ2. Dockerコンテナーの</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
組み立て作業の第2ステージでは、Dockerを組み立てて、Dockerの内部を理解します。</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
retina-examplesプロジェクトに関連するルートフォルダーに移動して実行します</font></font><br>
<br>
<pre><code class="bash hljs">docker build --build-arg USER=<span class="hljs-variable">$USER</span> --build-arg UID=<span class="hljs-variable">$UID</span> --build-arg GID=<span class="hljs-variable">$GID</span> --build-arg PW=alex -t retinanet:latest retinanet/
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
現在のユーザーを投下してdockerを収集します-これは、現在のユーザーの権限でマウントされたVOLUMEに何かを書き込む場合に非常に役立ちます。そうでない場合、ルートと痛みが発生します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
dockerの実行中に、Dockerfileを調べてみましょう。</font></font><br>
<br>
<pre><code class="powershell hljs">FROM nvcr.io/nvidia/pytorch:<span class="hljs-number">19.10</span><span class="hljs-literal">-py3</span><font></font>
<font></font>
ARG USER=alex<font></font>
ARG UID=<span class="hljs-number">1000</span>
ARG GID=<span class="hljs-number">1000</span><font></font>
ARG PW=alex<font></font>
RUN useradd <span class="hljs-literal">-m</span> <span class="hljs-variable">$</span>{USER} -<span class="hljs-literal">-uid</span>=<span class="hljs-variable">$</span>{UID} &amp;&amp; <span class="hljs-built_in">echo</span> <span class="hljs-string">"<span class="hljs-variable">$</span>{USER}:<span class="hljs-variable">$</span>{PW}"</span> | chpasswd<font></font>
<font></font>
RUN apt<span class="hljs-literal">-get</span> <span class="hljs-literal">-y</span> update &amp;&amp; apt<span class="hljs-literal">-get</span> <span class="hljs-literal">-y</span> upgrade &amp;&amp; apt<span class="hljs-literal">-get</span> <span class="hljs-literal">-y</span> install <span class="hljs-built_in">curl</span> &amp;&amp; apt<span class="hljs-literal">-get</span> <span class="hljs-literal">-y</span> install <span class="hljs-built_in">wget</span> &amp;&amp; apt<span class="hljs-literal">-get</span> <span class="hljs-literal">-y</span> install git &amp;&amp; apt<span class="hljs-literal">-get</span> <span class="hljs-literal">-y</span> install automake &amp;&amp; apt<span class="hljs-literal">-get</span> install <span class="hljs-literal">-y</span> sudo &amp;&amp; adduser <span class="hljs-variable">$</span>{USER} sudo<font></font>
RUN pip install git+https://github.com/bonlime/pytorch<span class="hljs-literal">-tools</span>.git@master<font></font>
<font></font>
<span class="hljs-built_in">COPY</span> . retinanet/<font></font>
RUN pip install -<span class="hljs-literal">-no</span><span class="hljs-literal">-cache</span><span class="hljs-literal">-dir</span> <span class="hljs-literal">-e</span> retinanet/<font></font>
RUN pip install /workspace/retinanet/extras/tensorrt<span class="hljs-literal">-6</span>.<span class="hljs-number">0.1</span>.<span class="hljs-number">5</span><span class="hljs-literal">-cp36</span><span class="hljs-literal">-none</span><span class="hljs-literal">-linux_x86_64</span>.whl<font></font>
RUN pip install tensorboardx<font></font>
RUN pip install albumentations<font></font>
RUN pip install setproctitle<font></font>
RUN pip install paramiko<font></font>
RUN pip install flask<font></font>
RUN pip install mem_top<font></font>
RUN pip install arrow<font></font>
RUN pip install pycuda<font></font>
RUN pip install torchvision<font></font>
RUN pip install pretrainedmodels<font></font>
RUN pip install efficientnet<span class="hljs-literal">-pytorch</span><font></font>
RUN pip install git+https://github.com/qubvel/segmentation_models.pytorch<font></font>
RUN pip install pytorch_toolbelt<font></font>
<font></font>
RUN chown <span class="hljs-literal">-R</span> <span class="hljs-variable">$</span>{USER}:<span class="hljs-variable">$</span>{USER} retinanet/<font></font>
<font></font>
RUN <span class="hljs-built_in">cd</span> /workspace/retinanet/extras/cppapi &amp;&amp; mkdir build &amp;&amp; <span class="hljs-built_in">cd</span> build &amp;&amp; cmake <span class="hljs-literal">-DCMAKE_CUDA_FLAGS</span>=<span class="hljs-string">"--expt-extended-lambda -std=c++14"</span> .. &amp;&amp; make &amp;&amp; <span class="hljs-built_in">cd</span> /workspace<font></font>
<font></font>
RUN apt<span class="hljs-literal">-get</span> install <span class="hljs-literal">-y</span> openssh<span class="hljs-literal">-server</span> &amp;&amp; apt install <span class="hljs-literal">-y</span> tmux &amp;&amp; apt<span class="hljs-literal">-get</span> <span class="hljs-literal">-y</span> install bison flex &amp;&amp; apt<span class="hljs-literal">-cache</span> search pcre &amp;&amp; apt<span class="hljs-literal">-get</span> <span class="hljs-literal">-y</span> install net<span class="hljs-literal">-tools</span> &amp;&amp; apt<span class="hljs-literal">-get</span> <span class="hljs-literal">-y</span> install nmap<font></font>
RUN apt<span class="hljs-literal">-get</span> <span class="hljs-literal">-y</span> install libpcre3 libpcre3<span class="hljs-literal">-dev</span> &amp;&amp; apt<span class="hljs-literal">-get</span> <span class="hljs-literal">-y</span> install iputils<span class="hljs-literal">-ping</span><font></font>
<font></font>
RUN mkdir /var/run/sshd<font></font>
RUN <span class="hljs-built_in">echo</span> <span class="hljs-string">'root:pass'</span> | chpasswd<font></font>
RUN sed <span class="hljs-literal">-i</span> <span class="hljs-string">'s/PermitRootLogin prohibit-password/PermitRootLogin yes/'</span> /etc/ssh/sshd_config<font></font>
RUN sed <span class="hljs-string">'s@session\s*required\s*pam_loginuid.so@session optional pam_loginuid.so@g'</span> <span class="hljs-literal">-i</span> /etc/pam.d/sshd<font></font>
<font></font>
ENV NOTVISIBLE <span class="hljs-string">"in users profile"</span>
RUN <span class="hljs-built_in">echo</span> <span class="hljs-string">"export VISIBLE=now"</span> &gt;&gt; /etc/profile<font></font>
CMD [<span class="hljs-string">"/usr/sbin/sshd"</span>, <span class="hljs-string">"-D"</span>]
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
テキストからわかるように、私たちはお気に入りのものをすべて採用し、retinanetをコンパイルし、Ubuntuでの作業に便利な基本的なツールを配布し、opensshサーバーを構成します。</font><font style="vertical-align: inherit;">最初の行はnvidiaイメージの継承にすぎません。NGCCloudにログインし、Pytorch1.3、TensorRT6.xxx、および検出器のcppソースコードをコンパイルできるようにする多数のライブラリが含まれています。</font></font><br>
<cut></cut><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ステージ3. dockerコンテナーの起動とデバッグ</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
コンテナーと開発環境を使用する主なケースに進みましょうまず、nvidia dockerを実行します。</font><font style="vertical-align: inherit;">実行：</font></font><br>
<br>
<pre><code class="bash hljs">docker run --gpus all --net=host -v /home/&lt;your_user_name&gt;:/workspace/mounted_vol -d -P --rm --ipc=host -it retinanet:latest</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
これで、コンテナーはssh &lt;curr_user_name&gt; @localhostで使用できるようになりました。</font><font style="vertical-align: inherit;">正常に起動したら、PyCharmでプロジェクトを開きます。</font><font style="vertical-align: inherit;">次に、開く</font></font><br>
<br>
<pre><code class="bash hljs">Settings-&gt;Project Interpreter-&gt;Add-&gt;Ssh Interpreter</code></pre><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ステップ1 </font></font></i><br>
<img src="https://habrastorage.org/webt/g0/qc/e4/g0qce4xw2pe0arglt4b4iu4jfle.png" alt="画像"><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ステップ2 </font></font></i><br>
<img src="https://habrastorage.org/webt/nf/m_/cu/nfm_cuj84kymlgofajbo-go8dwy.png" alt="画像"><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ステップ3</font></font></i><br>
<img src="https://habrastorage.org/webt/w6/nn/eg/w6nnegihsdou1fhoy75p7g9evhi.png" alt="画像"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
スクリーンショットのようにすべてを選択し、</font></font><br>
<br>
<pre><code class="bash hljs">Interpreter -&gt; /opt/conda/bin/python</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 -これはPython3.6ではlnであり、 </font></font><br>
<br>
<pre><code class="bash hljs">Sync folder -&gt; /workspace/retinanet</code></pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
フィニッシュラインを押して、インデックス作成を期待します。これで、環境を使用する準備が整いました。</font></font><br>
<br>
<i><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">重要！！！</font></font></b></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">インデックス作成の直後に、DockerからRetinanetのコンパイル済みファイルを抽出します。プロジェクトのルートにあるコンテキストメニューで、</font></font><br>
<br>
<pre><code class="bash hljs">Deployment-&gt;Download</code></pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
1つのファイルと2つのビルドフォルダーretinanet.egg-infoと_soが表示され</font></font><br>
<br>
<img src="https://habrastorage.org/webt/ti/uo/pp/tiuoppum1j_wx2yfxlkl8i-gdle.png" alt="画像"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ます</font><font style="vertical-align: inherit;">。</font><font style="vertical-align: inherit;">プロジェクトが次のようになっている場合、環境は必要なすべてのファイルを認識しており、RetinaNetを学習する準備ができています。</font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ステージ4.データをマークアップし、検出器をトレーニングします。</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
マークアップには、主に</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">supervise.ly</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">を使用します</font><font style="vertical-align: inherit;">-快適で便利なツールです。前回、一連の側枠が修正され、動作が改善されました。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
データセットをマークアップしてダウンロードしたが、それが独自のフォーマットであり、このためにCOCOに変換する必要があるため、RetinaNetにデータを挿入してもすぐには機能しないとします。</font><font style="vertical-align: inherit;">変換ツールは次の場所にあります：</font></font><br>
<br>
<pre><code class="bash hljs">markup_utils/supervisly_to_coco.py</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
スクリプトのカテゴリは例であり、独自のものを挿入する必要があることに注意してください（背景カテゴリを追加する必要はありません）。</font></font><br>
<br>
<pre><code class="json hljs">categories = [{'id': <span class="hljs-number">1</span>, 'name': '<span class="hljs-number">1</span>'}, <font></font>
                  {'id': <span class="hljs-number">2</span>, 'name': '<span class="hljs-number">2</span>'}, <font></font>
                  {'id': <span class="hljs-number">3</span>, 'name': '<span class="hljs-number">3</span>'},<font></font>
                  {'id': <span class="hljs-number">4</span>, 'name': '<span class="hljs-number">4</span>'}] </code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
なんらかの理由で、元のリポジトリの作成者は、COCO / VOC以外は検出用にトレーニングしないことにしたので、ソースファイルを少し変更する必要がありました </font></font><br>
<br>
<pre><code class="bash hljs">retinanet/dataset.py</code></pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
お気に入りのalbumentations.readthedocs.io/en/latestの拡張機能を</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">そこに</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">追加し、</font><font style="vertical-align: inherit;">COCOからステッチされたカテゴリを切り取ります。</font><font style="vertical-align: inherit;">大きな画像で小さなオブジェクトを探していて、小さなデータセット=）を持っている場合、検出の大きな領域をまき散らすことも可能です。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
一般に、列車のループも弱く、最初はチェックポイントを保存しませんでした。ある種のひどいスケジューラなどを使用していました。</font><font style="vertical-align: inherit;">しかし今必要なのはバックボーンを選択して実行することだけです</font></font><br>
<br>
<pre><code class="bash hljs">/opt/conda/bin/python retinanet/main.py</code></pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
パラメータ付き：</font></font><br>
<br>
<pre><code class="bash hljs">train retinanet_rn34fpn.pth<font></font>
--backbone ResNet34FPN<font></font>
--classes 12<font></font>
--val-iters 10<font></font>
--images /workspace/mounted_vol/dataset/train/images<font></font>
--annotations /workspace/mounted_vol/dataset/train_12_class.json<font></font>
--val-images /workspace/mounted_vol/dataset/<span class="hljs-built_in">test</span>/images_small<font></font>
--val-annotations /workspace/mounted_vol/dataset/val_10_class_cropped.json<font></font>
--jitter 256 512<font></font>
--max-size 512<font></font>
--batch 32<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
コンソールに次が表示されます：</font></font><br>
<br>
<pre><code class="plaintext hljs">Initializing model...<font></font>
     model: RetinaNet<font></font>
  backbone: ResNet18FPN<font></font>
   classes: 2, anchors: 9<font></font>
Selected optimization level O0:  Pure FP32 training.<font></font>
<font></font>
Defaults for this optimization level are:<font></font>
enabled                : True<font></font>
opt_level              : O0<font></font>
cast_model_type        : torch.float32<font></font>
patch_torch_functions  : False<font></font>
keep_batchnorm_fp32    : None<font></font>
master_weights         : False<font></font>
loss_scale             : 1.0<font></font>
Processing user overrides (additional kwargs that are not None)...<font></font>
After processing overrides, optimization options are:<font></font>
enabled                : True<font></font>
opt_level              : O0<font></font>
cast_model_type        : torch.float32<font></font>
patch_torch_functions  : False<font></font>
keep_batchnorm_fp32    : None<font></font>
master_weights         : False<font></font>
loss_scale             : 128.0<font></font>
Preparing dataset...<font></font>
    loader: pytorch<font></font>
    resize: [1024, 1280], max: 1280<font></font>
    device: 4 gpus<font></font>
    batch: 4, precision: mixed<font></font>
Training model for 20000 iterations...<font></font>
[    1/20000] focal loss: 0.95619, box loss: 0.51584, 4.042s/4-batch (fw: 0.698s, bw: 0.459s), 1.0 im/s, lr: 0.0001<font></font>
[   12/20000] focal loss: 0.76191, box loss: 0.31794, 0.187s/4-batch (fw: 0.055s, bw: 0.133s), 21.4 im/s, lr: 0.0001<font></font>
[   24/20000] focal loss: 0.65036, box loss: 0.30269, 0.173s/4-batch (fw: 0.045s, bw: 0.128s), 23.1 im/s, lr: 0.0001<font></font>
[   36/20000] focal loss: 0.46425, box loss: 0.23141, 0.178s/4-batch (fw: 0.047s, bw: 0.131s), 22.4 im/s, lr: 0.0001<font></font>
[   48/20000] focal loss: 0.45115, box loss: 0.23505, 0.180s/4-batch (fw: 0.047s, bw: 0.133s), 22.2 im/s, lr: 0.0001<font></font>
[   59/20000] focal loss: 0.38958, box loss: 0.25373, 0.184s/4-batch (fw: 0.049s, bw: 0.134s), 21.8 im/s, lr: 0.0001<font></font>
[   71/20000] focal loss: 0.37733, box loss: 0.23988, 0.174s/4-batch (fw: 0.049s, bw: 0.125s), 22.9 im/s, lr: 0.0001<font></font>
[   83/20000] focal loss: 0.39514, box loss: 0.23878, 0.181s/4-batch (fw: 0.048s, bw: 0.133s), 22.1 im/s, lr: 0.0001<font></font>
[   94/20000] focal loss: 0.39947, box loss: 0.23817, 0.185s/4-batch (fw: 0.050s, bw: 0.134s), 21.6 im/s, lr: 0.0001<font></font>
[  105/20000] focal loss: 0.37343, box loss: 0.20238, 0.182s/4-batch (fw: 0.048s, bw: 0.134s), 22.0 im/s, lr: 0.0001<font></font>
[  116/20000] focal loss: 0.19689, box loss: 0.17371, 0.183s/4-batch (fw: 0.050s, bw: 0.132s), 21.8 im/s, lr: 0.0001<font></font>
[  128/20000] focal loss: 0.20368, box loss: 0.16538, 0.178s/4-batch (fw: 0.046s, bw: 0.131s), 22.5 im/s, lr: 0.0001<font></font>
[  140/20000] focal loss: 0.22763, box loss: 0.15772, 0.176s/4-batch (fw: 0.050s, bw: 0.126s), 22.7 im/s, lr: 0.0001<font></font>
[  148/20000] focal loss: 0.21997, box loss: 0.18400, 0.585s/4-batch (fw: 0.047s, bw: 0.144s), 6.8 im/s, lr: 0.0001<font></font>
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.52674<font></font>
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.91450<font></font>
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.35172<font></font>
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.61881<font></font>
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.00000<font></font>
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.00000<font></font>
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.58824<font></font>
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.61765<font></font>
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.61765<font></font>
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.61765<font></font>
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.00000<font></font>
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.00000<font></font>
Saving model: 148</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
パラメータセット全体を調べるには </font></font><br>
<br>
<pre><code class="bash hljs">retinanet/main.py</code></pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
一般に、これらは検出の標準であり、説明があります。</font><font style="vertical-align: inherit;">トレーニングを実行し、結果を待ちます。</font><font style="vertical-align: inherit;">推論の例は次の場所にあります。</font></font><br>
<br>
<pre><code class="bash hljs">retinanet/infer_example.py</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 または次のコマンドを実行します：</font></font><br>
<br>
<pre><code class="bash hljs">/opt/conda/bin/python retinanet/main.py infer retinanet_rn34fpn.pth <font></font>
--images /workspace/mounted_vol/dataset/<span class="hljs-built_in">test</span>/images <font></font>
--annotations /workspace/mounted_vol/dataset/val.json <font></font>
--output result.json <font></font>
--resize 256 <font></font>
--max-size 512 <font></font>
--batch 32<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Focal Lossといくつかのバックボーンはすでにリポジトリに組み込まれており、</font></font><br>
<br>
<pre><code class="bash hljs">retinanet/backbones/*.py</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
著者はプレートにいくつかの特徴を与えています：</font></font><br>
<br>
<img src="https://habrastorage.org/webt/z2/rj/3c/z2rj3cuo4rvxgnqfdx-rntclfm0.png" alt="画像"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
torchvisionから取られたバックボーンResNeXt50_32x4dFPNとResNeXt101_32x8dFPNもあります。</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
検出が少し</font></font><i><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">わかったと</font></font></b></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">思いますが、公式ドキュメントを必ず読んで</font><i><b><font style="vertical-align: inherit;">、エクスポートモードとロギングモード</font></b></i><font style="vertical-align: inherit;">を</font><i><b><font style="vertical-align: inherit;">理解してください</font></b></i><font style="vertical-align: inherit;">。</font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ステージ5. Resnetエンコーダーを使用したUnetモデルのエクスポートと推論</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
おそらく</font><b><font style="vertical-align: inherit;">お気付きのように、</font></b><font style="vertical-align: inherit;">セグメンテーション用のライブラリはDockerfileにインストールされ、特に素晴らしいlib </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/qubvel/segmentation_models.pytorchに</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">インストールされました</font><font style="vertical-align: inherit;">。 Yunetパッケージには、TensorRTエンジンでのpytorchチェックポイントの推論とエクスポートの例があります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ONNXのようなモデルをONNXからTensoRTにエクスポートするときの主な問題は、固定のアップサンプルサイズを設定するか、ConvTranspose2Dを使用する必要があることです。</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">import</span> torch.onnx.symbolic_opset9 <span class="hljs-keyword">as</span> onnx_symbolic
        <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">upsample_nearest2d</span>(<span class="hljs-params">g, input, output_size</span>):</span>
            <span class="hljs-comment"># Currently, TRT 5.1/6.0 ONNX Parser does not support all ONNX ops</span>
            <span class="hljs-comment"># needed to support dynamic upsampling ONNX forumlation</span>
            <span class="hljs-comment"># Here we hardcode scale=2 as a temporary workaround</span>
            scales = g.op(<span class="hljs-string">"Constant"</span>, value_t=torch.tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>]))
            <span class="hljs-keyword">return</span> g.op(<span class="hljs-string">"Upsample"</span>, input, scales, mode_s=<span class="hljs-string">"nearest"</span>)<font></font>
<font></font>
        onnx_symbolic.upsample_nearest2d = upsample_nearest2d<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
この変換を使用すると、ONNXにエクスポートするときにこれを自動的に実行できますが、TensorRTのバージョン7ではすでにこの問題が解決されており、ほとんど待つ必要がありませんでした。</font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">結論</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
dockerを使い始めたとき、自分のタスクに対するそのパフォーマンスに疑問がありました。私のユニットの1つで、複数のカメラによって生成されるかなりのネットワークトラフィックが発生しています。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/3p/b9/1q/3pb91qmq_vcanxwqlskw3aervtu.png" alt="画像"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
インターネットでのさまざまなテストにより、ネットワークの相互作用とVOLUMEでの記録に比較的大きなオーバーヘッドと、未知のひどいGILが明らかになりました。フレームの撮影以来、ドライバーの操作とネットワークを介したフレームの送信は、</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ハードリアルタイム</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">モード</font><font style="vertical-align: inherit;">での</font><font style="vertical-align: inherit;">アトミック操作であり</font><font style="vertical-align: inherit;">、遅延オンラインは私にとって非常に重要です。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
しかし、何も起こらなかった=）</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
PSそれでも、セグメント化と生産のためにお気に入りの列車ループを追加します！</font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">おかげで</font></font></b><font style="vertical-align: inherit;"><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">ods.ai</font></a></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
コミュニティのおかげで</font><font style="vertical-align: inherit;">、それなしで開発することは不可能です！</font><font style="vertical-align: inherit;">どうもありがとうございます</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" class="user_link"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">n01z3</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、彼がかけがえのないアドバイスと並外れたプロフェッショナリズムのためにDLに取り組むことを私に望んだDL！</font></font><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">本番環境で最適化されたモデルを使用してください！</font></font></i><br>
<br>
<img src="https://habrastorage.org/webt/xv/x3/yv/xvx3yvszd_twqjbrtk3rjaxrxpk.png"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">オーロライ、LLC</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../ja483056/index.html">データベースの論理フィールド、解毒剤はありますか？</a></li>
<li><a href="../ja483058/index.html">休日調査結果</a></li>
<li><a href="../ja483064/index.html">すべてのカノンで最小の別名小さなブログのVue</a></li>
<li><a href="../ja483066/index.html">ビル・ゲイツの伝統的な5冊の本</a></li>
<li><a href="../ja483068/index.html">人気のアンチチートBattlEyeのリバースエンジニアリング</a></li>
<li><a href="../ja483078/index.html">深層強化学習：クモに歩き方を教える方法</a></li>
<li><a href="../ja483082/index.html">脆弱性ハンティングは7％効果的</a></li>
<li><a href="../ja483084/index.html">トラッキング機能付きカメラ</a></li>
<li><a href="../ja483086/index.html">2019年の結果：ロシアの投資家にとって最も収益性の高い資産はどれか</a></li>
<li><a href="../ja483094/index.html">マイクロソフトのロシアのオフィスを訪問したとき、どのようにして私の夢を実現しましたか</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>