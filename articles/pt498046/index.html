<!doctype html>
<html class="no-js" lang="pt">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèº‚Äçüéì ‚ö†Ô∏è üå§Ô∏è Pesquisadores est√£o desenvolvendo uma abordagem para reduzir o vi√©s nos conjuntos de dados de vis√£o computacional üëÉ ü•Ñ üìé</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Uma tradu√ß√£o do artigo foi preparada especificamente para os alunos do curso Vis√£o Computacional . 
 
 14 de fevereiro de 2020 
 Universidade de Princ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Pesquisadores est√£o desenvolvendo uma abordagem para reduzir o vi√©s nos conjuntos de dados de vis√£o computacional</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/498046/"><b><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Uma tradu√ß√£o do artigo foi preparada especificamente para os alunos do curso </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Vis√£o Computacional</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font></i></b><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">14 de fevereiro de 2020 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Universidade de Princeton, Departamento de Engenharia.</font></font></i><br>
<br>
<img src="https://habrastorage.org/webt/7l/rc/oe/7lrcoe52n2bvvynvhjwhmhhprx4.png"><br>
<hr><br>
<blockquote><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Resumo:</font></font></i><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Para resolver os problemas de vi√©s na intelig√™ncia artificial, os cientistas da computa√ß√£o desenvolveram m√©todos para obter conjuntos de dados mais confi√°veis ‚Äã‚Äãcontendo imagens de pessoas. Os pesquisadores est√£o oferecendo aprimoramentos ao ImageNet, um banco de dados com mais de 14 milh√µes de imagens, que desempenhou um papel fundamental no desenvolvimento da vis√£o computacional na √∫ltima d√©cada.</font></font></blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O ImageNet, que inclui imagens de objetos, paisagens e, em particular, pessoas, serve como fonte de dados de treinamento para pesquisadores que criam algoritmos de aprendizado de m√°quina que classificam imagens ou reconhecem elementos individuais neles. A escala sem precedentes da ImageNet exigia coleta e anota√ß√£o automatizadas de imagens usando o crowdsourcing. Embora a categoria de imagens de pessoas do banco de dados raramente tenha sido usada pela comunidade de pesquisa, a equipe do ImageNet trabalhou para eliminar o vi√©s e v√°rios outros problemas associados √†s imagens de pessoas, que s√£o conseq√º√™ncias n√£o intencionais do design do ImageNet.</font></font><a name="habracut"></a><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
"Hoje, a vis√£o computacional funciona bem o suficiente para ser implementada em todos os lugares em uma variedade de contextos", disse a coautora Olga Russakovskaya, professora associada de ci√™ncia da computa√ß√£o em Princeton. "Isso significa que agora √© a hora de conversar sobre como isso afeta o mundo e pensar sobre as quest√µes de credibilidade".</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Em um novo artigo, a equipe do ImageNet identificou sistematicamente conceitos n√£o visuais e categorias ofensivas, como caracter√≠sticas raciais e sexuais, para categorias de imagens humanas do ImageNet e sugeriu remov√™-los do banco de dados. Os pesquisadores tamb√©m desenvolveram uma ferramenta que permite aos usu√°rios identificar e obter conjuntos de imagens de pessoas equilibradas por idade, sexo e cor da pele, a fim de facilitar algoritmos apropriados para classificar de maneira mais confi√°vel o rosto das pessoas e suas a√ß√µes nas imagens. Os pesquisadores apresentaram seu trabalho em 30 de janeiro em uma confer√™ncia sobre a veracidade, confiabilidade e transpar√™ncia da Associa√ß√£o de Tecnologia da Computa√ß√£o em Barcelona, ‚Äã‚ÄãEspanha.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
"√â muito importante trazer para a discuss√£o a aten√ß√£o de laborat√≥rios e pesquisadores com experi√™ncia t√©cnica fundamental", continua Russakovskaya. ‚ÄúDado o fato de que precisamos coletar dados em uma escala colossal, e o fato de que isso ser√° realizado por meio do crowdsourcing (porque √© o pipeline mais eficiente e comprovado), surge a pergunta - como fazemos isso para garantir o melhor confiabilidade sem pisar em um ancinho familiar? Este artigo se concentra principalmente em solu√ß√µes de design. ‚Äù</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Um grupo de cientistas da computa√ß√£o em Princeton e Stanford lan√ßou o ImageNet em 2009 como um recurso para pesquisadores e educadores. O professor e graduado em Princeton Fay-Fay Lee, atualmente professor de ci√™ncia da computa√ß√£o em Stanford, liderou a iniciativa. Para incentivar os pesquisadores a criar melhores algoritmos de vis√£o computacional usando o ImageNet, a equipe tamb√©m lan√ßou o Desafio de reconhecimento visual do ImageNet em larga escala. A competi√ß√£o foi focada principalmente no reconhecimento de objetos usando 1000 categorias de imagens, das quais apenas tr√™s apresentavam pessoas.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Alguns dos problemas de confiabilidade do ImageNet decorrem do pipeline usado para criar o banco de dados. Suas categorias de imagens s√£o extra√≠das do WordNet, um antigo banco de dados de palavras em ingl√™s usado para pesquisas em processamento de linguagem natural. Os criadores do ImageNet pegaram nomes emprestados do WordNet - alguns dos quais, embora sejam termos verbais bem definidos, s√£o mal traduzidos para um dicion√°rio visual. Por exemplo, os termos que descrevem a religi√£o ou a origem geogr√°fica de uma pessoa podem extrair apenas os resultados de pesquisa de imagens mais importantes, o que pode resultar em algoritmos que refor√ßam estere√≥tipos.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Um projeto de arte recente chamado ImageNet Roulette chamou a aten√ß√£o para esses problemas. O projeto, lan√ßado em setembro de 2019 como parte de uma exposi√ß√£o de arte dedicada a sistemas de reconhecimento de imagem, usou as imagens de pessoas da ImageNet para treinar um modelo de intelig√™ncia artificial que categorizou pessoas com palavras com base na imagem apresentada. Os usu√°rios podem fazer upload de sua imagem e obter uma tag com base nesse modelo. Muitas das classifica√ß√µes eram ofensivas ou simplesmente infundadas.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A principal inova√ß√£o que permitiu aos criadores do ImageNet acumular um banco de dados t√£o grande de imagens marcadas foi o uso de crowdsourcing, em particular a plataforma Amazon Mechanical Turk (MTurk), na qual os funcion√°rios foram pagos para verificar as imagens candidatas. Apesar de revolucion√°ria, essa abordagem era imperfeita, o que levou a algumas categorias tendenciosas e inadequadas. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
"Quando voc√™ pede √†s pessoas que verifiquem imagens selecionando entre uma ampla variedade de candidatos, as pessoas sentem a press√£o para escolher algo, e essas imagens tendem a ter caracter√≠sticas distintas ou estereotipadas", diz o principal autor Kayu Young, formado em ci√™ncia da computa√ß√£o .</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
No decorrer do estudo, Jan e seus colegas filtraram primeiro categorias potencialmente abusivas ou sens√≠veis de pessoas da ImageNet. Consideraram ofensivas as categorias que contenham palavr√µes ou insultos raciais ou de g√™nero; categorias sens√≠veis inclu√≠am, por exemplo, classifica√ß√£o de pessoas com base em orienta√ß√£o sexual ou religi√£o. Para anotar as categorias, eles recrutaram 12 estudantes de p√≥s-gradua√ß√£o de diferentes esferas da vida, instruindo-os a marcar a categoria como sens√≠vel se n√£o tiverem certeza. Portanto, eles exclu√≠ram 1593 categorias - cerca de 54% das 2932 categorias de pessoas no ImageNet.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Em seguida, os pesquisadores procuraram os funcion√°rios do MTurk para obter ajuda, para que eles classificassem as "imagens" das categorias aceit√°veis ‚Äã‚Äãrestantes em uma escala de 1 a 5. A sele√ß√£o de categorias com uma classifica√ß√£o de imagens igual ou superior a 4 levou ao fato de que apenas 158 categorias foram classificadas como aceit√°veis ‚Äã‚Äãe suficientemente figurativas. Mesmo esse conjunto cuidadosamente filtrado de categorias continha mais de 133.000 imagens - um grande n√∫mero de exemplos para o ensino de algoritmos de vis√£o computacional.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dentro dessas 158 categorias, os pesquisadores estudaram a representa√ß√£o demogr√°fica de pessoas em imagens para avaliar o n√≠vel de vi√©s no ImageNet e desenvolver uma abordagem para criar conjuntos de dados mais apropriados. O conte√∫do do ImageNet vem principalmente de mecanismos de pesquisa segmentados por imagem, como o Flickr. Os mecanismos de pesquisa, em geral, tendem a retornar resultados que representam homens, pessoas de pele clara e adultos com idades entre 18 e 40 anos em uma extens√£o muito maior. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
‚ÄúAs pessoas descobriram que os resultados da pesquisa de imagens s√£o altamente tendenciosos em termos de distribui√ß√£o demogr√°fica; portanto, o ImageNet tamb√©m possui uma distribui√ß√£o tendenciosa‚Äù, diz Young. "Neste artigo, tentamos avaliar o n√≠vel de vi√©s e tamb√©m propusemos um m√©todo que equilibrasse a distribui√ß√£o".</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Os pesquisadores identificaram e revisaram tr√™s atributos protegidos pelas leis antidiscriminat√≥rias dos EUA: cor da pele, sexo e idade. Os trabalhadores do MTurk foram solicitados a anotar cada atributo de cada pessoa na imagem. Eles classificaram a cor da pele como clara, m√©dia ou escura; e por idade quando crian√ßas (menores de 18 anos), adultos entre 18 e 40 anos, adultos entre 40 e 65 anos ou adultos com mais de 65 anos. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A classifica√ß√£o de g√™nero incluiu homens, mulheres e g√™nero indefinido - uma maneira de incluir pessoas com diferentes express√µes de g√™nero, bem como anotar imagens nas quais o g√™nero n√£o pode ser percebido por sinais visuais (como imagens de muitas crian√ßas ou mergulhadores).</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Uma an√°lise das anota√ß√µes mostrou que, como nos resultados da pesquisa, o conte√∫do do ImageNet reflete um vi√©s significativo. Pessoas marcadas como negras, mulheres e adultos com mais de 40 anos estavam sub-representadas na maioria das categorias.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Embora o processo de anota√ß√£o inclua controle de qualidade e exija que os anotadores cheguem a um consenso, devido a preocupa√ß√µes com os poss√≠veis danos de anota√ß√µes incorretas, os pesquisadores optaram por n√£o emitir anota√ß√µes demogr√°ficas para imagens individuais. Em vez disso, eles desenvolveram uma ferramenta baseada na Web que permite aos usu√°rios recuperar um conjunto de imagens que s√£o equilibradas demograficamente da maneira especificada pelo usu√°rio. Por exemplo, uma cole√ß√£o completa de imagens na categoria programador pode incluir cerca de 90% dos homens e 10% das mulheres, enquanto nos Estados Unidos cerca de 20% dos programadores s√£o mulheres. O pesquisador pode usar a nova ferramenta para obter um conjunto de imagens de programadores representando 80% dos homens e 20% das mulheres - ou mesmo individualmente, dependendo dos objetivos do pesquisador.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
"N√£o queremos falar sobre como equilibrar dados demogr√°ficos, porque n√£o √© um problema muito simples", diz Young. ‚ÄúA distribui√ß√£o pode ser diferente em diferentes partes do mundo - por exemplo, a distribui√ß√£o de cores de pele nos EUA √© diferente da distribui√ß√£o nos pa√≠ses asi√°ticos. Portanto, deixamos essa pergunta para o usu√°rio e simplesmente fornecemos uma ferramenta para extrair um subconjunto equilibrado de imagens. " </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A equipe do ImageNet est√° atualmente trabalhando em atualiza√ß√µes t√©cnicas de seus equipamentos e do pr√≥prio banco de dados, al√©m de implementar a filtragem de faces e a ferramenta de reequil√≠brio desenvolvida neste estudo. O ImageNet ser√° reeditado em breve com essas atualiza√ß√µes e um pedido de feedback da comunidade de pesquisadores de vis√£o computacional.</font></font><br>
 <br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Princeton Ph.D. Clint Kinami e professor associado de ci√™ncia da computa√ß√£o, Jia Dang, co-autor de Young, Lee e Russakovskaya. </font><font style="vertical-align: inherit;">O estudo foi apoiado pela National Science Foundation. </font></font><br>
 <br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fonte: </font></font></b><br>
 <br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Materiais</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> fornecidos pelo </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Departamento de Engenharia da Universidade de Princeton</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Original escrito por Molly Charlach. </font><font style="vertical-align: inherit;">P </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nota: O conte√∫do pode ser modificado por estilo e comprimento. </font></font></i><br>
 <br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Link:</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Kaiyu Yang, Klint Qinami, Li Fei-Fei, Jia Deng, Olga Russakovsky. </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Em dire√ß√£o a conjuntos de dados mais justos: filtrando e equilibrando a distribui√ß√£o da sub√°rvore de pessoas na hierarquia do ImageNet. </font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Anais da Confer√™ncia 2020 sobre Justi√ßa, Responsabilidade e Transpar√™ncia, 2020 DOI: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">10.1145 / 3351095.3375709</font></font></a><br>
<br>
<hr><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Saiba mais sobre o curso</font></font></a><br>
<br>
<hr></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt498032/index.html">C√°lculo r√°pido de f√≥rmulas do Excel em C #</a></li>
<li><a href="../pt498034/index.html">As modernas aeronaves projetadas s√£o protegidas contra amea√ßas biol√≥gicas (COVID-19) melhor do que voc√™ pensa</a></li>
<li><a href="../pt498036/index.html">Mark Andriessen: √â hora de criar para n√≥s mesmos (√© hora de construir)</a></li>
<li><a href="../pt498038/index.html">O resumo de materiais frescos do mundo do front-end da √∫ltima semana n ¬∞ 411 (13-19 de abril de 2020)</a></li>
<li><a href="../pt498042/index.html">Analisar, n√£o validar</a></li>
<li><a href="../pt498050/index.html">Cultura como base para escalar a equipe x2 todos os anos. Sobre erros de contrata√ß√£o e adequa√ß√£o √† cultura</a></li>
<li><a href="../pt498052/index.html">Zabbix 5.0 ou O que h√° de novo no servidor de modelos da IPMI</a></li>
<li><a href="../pt498054/index.html">Derrote o Dragon News Feed: Como garantir uma vida boa</a></li>
<li><a href="../pt498056/index.html">Eventos digitais em Moscou, de 20 a 26 de abril</a></li>
<li><a href="../pt498060/index.html">Abordagem de ajuste industrial do PostgreSQL: experi√™ncias com bancos de dados Nikolay Samokhvalov</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>