<!doctype html>
<html class="no-js" lang="pt">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👨🏼‍🎓 ⚠️ 🌤️ Pesquisadores estão desenvolvendo uma abordagem para reduzir o viés nos conjuntos de dados de visão computacional 👃 🥄 📎</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Uma tradução do artigo foi preparada especificamente para os alunos do curso Visão Computacional . 
 
 14 de fevereiro de 2020 
 Universidade de Princ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Pesquisadores estão desenvolvendo uma abordagem para reduzir o viés nos conjuntos de dados de visão computacional</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/498046/"><b><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Uma tradução do artigo foi preparada especificamente para os alunos do curso </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Visão Computacional</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font></i></b><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">14 de fevereiro de 2020 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Universidade de Princeton, Departamento de Engenharia.</font></font></i><br>
<br>
<img src="https://habrastorage.org/webt/7l/rc/oe/7lrcoe52n2bvvynvhjwhmhhprx4.png"><br>
<hr><br>
<blockquote><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Resumo:</font></font></i><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Para resolver os problemas de viés na inteligência artificial, os cientistas da computação desenvolveram métodos para obter conjuntos de dados mais confiáveis ​​contendo imagens de pessoas. Os pesquisadores estão oferecendo aprimoramentos ao ImageNet, um banco de dados com mais de 14 milhões de imagens, que desempenhou um papel fundamental no desenvolvimento da visão computacional na última década.</font></font></blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O ImageNet, que inclui imagens de objetos, paisagens e, em particular, pessoas, serve como fonte de dados de treinamento para pesquisadores que criam algoritmos de aprendizado de máquina que classificam imagens ou reconhecem elementos individuais neles. A escala sem precedentes da ImageNet exigia coleta e anotação automatizadas de imagens usando o crowdsourcing. Embora a categoria de imagens de pessoas do banco de dados raramente tenha sido usada pela comunidade de pesquisa, a equipe do ImageNet trabalhou para eliminar o viés e vários outros problemas associados às imagens de pessoas, que são conseqüências não intencionais do design do ImageNet.</font></font><a name="habracut"></a><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
"Hoje, a visão computacional funciona bem o suficiente para ser implementada em todos os lugares em uma variedade de contextos", disse a coautora Olga Russakovskaya, professora associada de ciência da computação em Princeton. "Isso significa que agora é a hora de conversar sobre como isso afeta o mundo e pensar sobre as questões de credibilidade".</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Em um novo artigo, a equipe do ImageNet identificou sistematicamente conceitos não visuais e categorias ofensivas, como características raciais e sexuais, para categorias de imagens humanas do ImageNet e sugeriu removê-los do banco de dados. Os pesquisadores também desenvolveram uma ferramenta que permite aos usuários identificar e obter conjuntos de imagens de pessoas equilibradas por idade, sexo e cor da pele, a fim de facilitar algoritmos apropriados para classificar de maneira mais confiável o rosto das pessoas e suas ações nas imagens. Os pesquisadores apresentaram seu trabalho em 30 de janeiro em uma conferência sobre a veracidade, confiabilidade e transparência da Associação de Tecnologia da Computação em Barcelona, ​​Espanha.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
"É muito importante trazer para a discussão a atenção de laboratórios e pesquisadores com experiência técnica fundamental", continua Russakovskaya. “Dado o fato de que precisamos coletar dados em uma escala colossal, e o fato de que isso será realizado por meio do crowdsourcing (porque é o pipeline mais eficiente e comprovado), surge a pergunta - como fazemos isso para garantir o melhor confiabilidade sem pisar em um ancinho familiar? Este artigo se concentra principalmente em soluções de design. ”</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Um grupo de cientistas da computação em Princeton e Stanford lançou o ImageNet em 2009 como um recurso para pesquisadores e educadores. O professor e graduado em Princeton Fay-Fay Lee, atualmente professor de ciência da computação em Stanford, liderou a iniciativa. Para incentivar os pesquisadores a criar melhores algoritmos de visão computacional usando o ImageNet, a equipe também lançou o Desafio de reconhecimento visual do ImageNet em larga escala. A competição foi focada principalmente no reconhecimento de objetos usando 1000 categorias de imagens, das quais apenas três apresentavam pessoas.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Alguns dos problemas de confiabilidade do ImageNet decorrem do pipeline usado para criar o banco de dados. Suas categorias de imagens são extraídas do WordNet, um antigo banco de dados de palavras em inglês usado para pesquisas em processamento de linguagem natural. Os criadores do ImageNet pegaram nomes emprestados do WordNet - alguns dos quais, embora sejam termos verbais bem definidos, são mal traduzidos para um dicionário visual. Por exemplo, os termos que descrevem a religião ou a origem geográfica de uma pessoa podem extrair apenas os resultados de pesquisa de imagens mais importantes, o que pode resultar em algoritmos que reforçam estereótipos.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Um projeto de arte recente chamado ImageNet Roulette chamou a atenção para esses problemas. O projeto, lançado em setembro de 2019 como parte de uma exposição de arte dedicada a sistemas de reconhecimento de imagem, usou as imagens de pessoas da ImageNet para treinar um modelo de inteligência artificial que categorizou pessoas com palavras com base na imagem apresentada. Os usuários podem fazer upload de sua imagem e obter uma tag com base nesse modelo. Muitas das classificações eram ofensivas ou simplesmente infundadas.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A principal inovação que permitiu aos criadores do ImageNet acumular um banco de dados tão grande de imagens marcadas foi o uso de crowdsourcing, em particular a plataforma Amazon Mechanical Turk (MTurk), na qual os funcionários foram pagos para verificar as imagens candidatas. Apesar de revolucionária, essa abordagem era imperfeita, o que levou a algumas categorias tendenciosas e inadequadas. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
"Quando você pede às pessoas que verifiquem imagens selecionando entre uma ampla variedade de candidatos, as pessoas sentem a pressão para escolher algo, e essas imagens tendem a ter características distintas ou estereotipadas", diz o principal autor Kayu Young, formado em ciência da computação .</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
No decorrer do estudo, Jan e seus colegas filtraram primeiro categorias potencialmente abusivas ou sensíveis de pessoas da ImageNet. Consideraram ofensivas as categorias que contenham palavrões ou insultos raciais ou de gênero; categorias sensíveis incluíam, por exemplo, classificação de pessoas com base em orientação sexual ou religião. Para anotar as categorias, eles recrutaram 12 estudantes de pós-graduação de diferentes esferas da vida, instruindo-os a marcar a categoria como sensível se não tiverem certeza. Portanto, eles excluíram 1593 categorias - cerca de 54% das 2932 categorias de pessoas no ImageNet.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Em seguida, os pesquisadores procuraram os funcionários do MTurk para obter ajuda, para que eles classificassem as "imagens" das categorias aceitáveis ​​restantes em uma escala de 1 a 5. A seleção de categorias com uma classificação de imagens igual ou superior a 4 levou ao fato de que apenas 158 categorias foram classificadas como aceitáveis ​​e suficientemente figurativas. Mesmo esse conjunto cuidadosamente filtrado de categorias continha mais de 133.000 imagens - um grande número de exemplos para o ensino de algoritmos de visão computacional.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dentro dessas 158 categorias, os pesquisadores estudaram a representação demográfica de pessoas em imagens para avaliar o nível de viés no ImageNet e desenvolver uma abordagem para criar conjuntos de dados mais apropriados. O conteúdo do ImageNet vem principalmente de mecanismos de pesquisa segmentados por imagem, como o Flickr. Os mecanismos de pesquisa, em geral, tendem a retornar resultados que representam homens, pessoas de pele clara e adultos com idades entre 18 e 40 anos em uma extensão muito maior. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
“As pessoas descobriram que os resultados da pesquisa de imagens são altamente tendenciosos em termos de distribuição demográfica; portanto, o ImageNet também possui uma distribuição tendenciosa”, diz Young. "Neste artigo, tentamos avaliar o nível de viés e também propusemos um método que equilibrasse a distribuição".</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Os pesquisadores identificaram e revisaram três atributos protegidos pelas leis antidiscriminatórias dos EUA: cor da pele, sexo e idade. Os trabalhadores do MTurk foram solicitados a anotar cada atributo de cada pessoa na imagem. Eles classificaram a cor da pele como clara, média ou escura; e por idade quando crianças (menores de 18 anos), adultos entre 18 e 40 anos, adultos entre 40 e 65 anos ou adultos com mais de 65 anos. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A classificação de gênero incluiu homens, mulheres e gênero indefinido - uma maneira de incluir pessoas com diferentes expressões de gênero, bem como anotar imagens nas quais o gênero não pode ser percebido por sinais visuais (como imagens de muitas crianças ou mergulhadores).</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Uma análise das anotações mostrou que, como nos resultados da pesquisa, o conteúdo do ImageNet reflete um viés significativo. Pessoas marcadas como negras, mulheres e adultos com mais de 40 anos estavam sub-representadas na maioria das categorias.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Embora o processo de anotação inclua controle de qualidade e exija que os anotadores cheguem a um consenso, devido a preocupações com os possíveis danos de anotações incorretas, os pesquisadores optaram por não emitir anotações demográficas para imagens individuais. Em vez disso, eles desenvolveram uma ferramenta baseada na Web que permite aos usuários recuperar um conjunto de imagens que são equilibradas demograficamente da maneira especificada pelo usuário. Por exemplo, uma coleção completa de imagens na categoria programador pode incluir cerca de 90% dos homens e 10% das mulheres, enquanto nos Estados Unidos cerca de 20% dos programadores são mulheres. O pesquisador pode usar a nova ferramenta para obter um conjunto de imagens de programadores representando 80% dos homens e 20% das mulheres - ou mesmo individualmente, dependendo dos objetivos do pesquisador.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
"Não queremos falar sobre como equilibrar dados demográficos, porque não é um problema muito simples", diz Young. “A distribuição pode ser diferente em diferentes partes do mundo - por exemplo, a distribuição de cores de pele nos EUA é diferente da distribuição nos países asiáticos. Portanto, deixamos essa pergunta para o usuário e simplesmente fornecemos uma ferramenta para extrair um subconjunto equilibrado de imagens. " </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A equipe do ImageNet está atualmente trabalhando em atualizações técnicas de seus equipamentos e do próprio banco de dados, além de implementar a filtragem de faces e a ferramenta de reequilíbrio desenvolvida neste estudo. O ImageNet será reeditado em breve com essas atualizações e um pedido de feedback da comunidade de pesquisadores de visão computacional.</font></font><br>
 <br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Princeton Ph.D. Clint Kinami e professor associado de ciência da computação, Jia Dang, co-autor de Young, Lee e Russakovskaya. </font><font style="vertical-align: inherit;">O estudo foi apoiado pela National Science Foundation. </font></font><br>
 <br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fonte: </font></font></b><br>
 <br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Materiais</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> fornecidos pelo </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Departamento de Engenharia da Universidade de Princeton</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Original escrito por Molly Charlach. </font><font style="vertical-align: inherit;">P </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nota: O conteúdo pode ser modificado por estilo e comprimento. </font></font></i><br>
 <br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Link:</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Kaiyu Yang, Klint Qinami, Li Fei-Fei, Jia Deng, Olga Russakovsky. </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Em direção a conjuntos de dados mais justos: filtrando e equilibrando a distribuição da subárvore de pessoas na hierarquia do ImageNet. </font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Anais da Conferência 2020 sobre Justiça, Responsabilidade e Transparência, 2020 DOI: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">10.1145 / 3351095.3375709</font></font></a><br>
<br>
<hr><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Saiba mais sobre o curso</font></font></a><br>
<br>
<hr></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt498032/index.html">Cálculo rápido de fórmulas do Excel em C #</a></li>
<li><a href="../pt498034/index.html">As modernas aeronaves projetadas são protegidas contra ameaças biológicas (COVID-19) melhor do que você pensa</a></li>
<li><a href="../pt498036/index.html">Mark Andriessen: É hora de criar para nós mesmos (é hora de construir)</a></li>
<li><a href="../pt498038/index.html">O resumo de materiais frescos do mundo do front-end da última semana n ° 411 (13-19 de abril de 2020)</a></li>
<li><a href="../pt498042/index.html">Analisar, não validar</a></li>
<li><a href="../pt498050/index.html">Cultura como base para escalar a equipe x2 todos os anos. Sobre erros de contratação e adequação à cultura</a></li>
<li><a href="../pt498052/index.html">Zabbix 5.0 ou O que há de novo no servidor de modelos da IPMI</a></li>
<li><a href="../pt498054/index.html">Derrote o Dragon News Feed: Como garantir uma vida boa</a></li>
<li><a href="../pt498056/index.html">Eventos digitais em Moscou, de 20 a 26 de abril</a></li>
<li><a href="../pt498060/index.html">Abordagem de ajuste industrial do PostgreSQL: experiências com bancos de dados Nikolay Samokhvalov</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>