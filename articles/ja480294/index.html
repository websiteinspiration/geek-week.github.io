<!doctype html>
<html class="no-js" lang="ja">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🙈 👨‍❤️‍👨 👨‍👧‍👦 汎用性の高いLEGO部品分類装置での高速マシンビジョン 🧔🏼 🐉 🕹️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="過去数年間、私はレゴの部品を認識して分類できる機械を設計、製造してきました。マシンの最も重要な部分は、コンベヤーベルト、照明、カメラがあり、ほぼ完全に密閉された小さなコンパートメントであるキャプチャユニットです。
 
 
 照明は少し低く見えます。
 
 カメラは、コンベヤーを通過するLEGOパーツ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>汎用性の高いLEGO部品分類装置での高速マシンビジョン</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/480294/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">過去数年間、私はレゴの部品を認識して分類できる機械を設計、製造してきました。</font><font style="vertical-align: inherit;">マシンの最も重要な部分</font><font style="vertical-align: inherit;">は、コンベヤーベルト、照明、カメラがあり、ほぼ完全に密閉された小さなコンパートメントである</font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">キャプチャユニット</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">です。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/641/6c7/5d8/6416c75d85738aceecf1162f1d48718d.jpg"></div><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">照明は少し低く見えます。</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
カメラは、コンベヤーを通過するLEGOパーツの写真を撮り、人工知能アルゴリズムを実行するサーバーにワイヤレスで画像を転送して、数千の可能なLEGO要素の中からパーツを認識します。 AIアルゴリズムについては、今後の記事で詳しく説明します。この記事では、ビデオカメラの生の出力とニューラルネットワークへの入力との間で実行される処理に焦点を当てます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
私が解決する必要があった主な問題は、コンベヤからのビデオストリームを、ニューラルネットワークが使用できるパーツの個別の画像に変換することでした。</font></font><br>
<a name="habracut"></a><br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ca8/feb/2ea/ca8feb2eafab64d7d835d50e090e7bc5.gif"></div><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">最終的な目標は、生のビデオ（左側）から同じサイズの画像セット（右側）に切り替えて、ニューラルネットワークに転送することです。 （実際の作業と比較すると、gifは約半分の速度です）</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
これは、表面上は単純に見えるが、実際には多くのユニークで興味深い障害を引き起こし、その多くはマシンビジョンプラットフォームに固有のタスクの優れた例です。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
この方法で画像の正しい部分を取得することは、オブジェクト検出と呼ばれることがよくあります。それがまさに私がしなければならないことです。オブジェクトの存在、その位置とサイズを認識し、</font><font style="vertical-align: inherit;">各フレームの各パーツに</font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">外接する長方形</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">を</font><font style="vertical-align: inherit;">生成できるようにします</font><font style="vertical-align: inherit;">。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/cfc/265/c31/cfc265c31250940a6170ad404f89de08.gif"></div><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">最も重要なことは、適切な境界の長方形を見つけることです（上記の緑色で表示）。</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
問題を解決するための3つの側面を検討します。</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">不要な変数を削除するための準備</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">単純なマシンビジョン操作からプロセスを作成する</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">リソースが限られているRaspberry Piプラットフォームで十分なパフォーマンスを維持する</font></font></li>
</ul><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">不要な変数の排除</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
このようなタスクの場合は、マシンビジョンテクニックを使用する前に、できるだけ多くの変数を削除することをお勧めします。たとえば、環境条件、カメラの位置の違い、他の部品との一部の重なりによる情報の損失について心配する必要はありません。もちろん、これらの変数をすべてプログラムで解決することは（非常に困難ではありますが）可能ですが、幸い、私にとってこのマシンはゼロから作成されています。私自身も、コードを書き始める前から、すべての干渉を排除して、成功するソリューションの準備をすることができます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
最初のステップは、カメラの位置、角度、焦点をしっかりと固定することです。これにより、すべてが簡単です。システムでは、カメラはコンベヤの上に取り付けられています。他の部品からの干渉を心配する必要はありません。不要なオブジェクトがキャプチャユニットに入る可能性はほとんどありません。少し難しいが、確認することが非常に重要</font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">一定の照明条件</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。テープに沿って移動するパーツの影を物理的なオブジェクトとして誤って解釈するために、オブジェクト認識機能が必要ありません。幸い、キャプチャユニットは非常に小さいので（カメラの視野全体がパンの塊よりも小さいため）、周囲の状況を十分に制御できました。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/8ec/9da/652/8ec9da6525ba0a6f906d3e1ef6309647.jpg"></div><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">キャプチャユニット、内部ビュー。カメラはフレームの上3分の1にあります。</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
1つの解決策は、コンパートメントを完全に密閉して、外部照明が入らないようにすることです。光源としてLEDストリップを使用してこのアプローチを試しました。残念ながら、システムは非常に不機嫌であることが判明しました-ケースの1つの小さな穴で十分であり、光がコンパートメントに浸透し、オブジェクトを認識できなくなりました。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
結局、最良の解決策は、小さなコンパートメントを強い光で満たすことにより、他のすべての光源を「詰まらせる」ことでした。住宅の建物を照らすために使用できる光源は非常に安価で使いやすいことがわかりました。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/16a/031/025/16a031025c08531ff3ef2f0ef20a2394.jpg"></div><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">影をゲット！</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
光源が小さなコンパートメントに向けられると、すべての潜在的な外光干渉が完全に詰まります。</font><font style="vertical-align: inherit;">このようなシステムには便利な副作用もあります。カメラには大量の光があるため、非常に高速のシャッター速度を使用でき、コンベヤーに沿ってすばやく移動する場合でも、部品の完全に鮮明な画像を取得できます。</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">物体認識</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
どのようにして、均一な照明のあるこの美しいビデオを必要な境界の長方形に変換できましたか？</font><font style="vertical-align: inherit;">AIを使用している場合、</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">YOLO</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">や</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Faster R-CNN</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">などの</font><font style="vertical-align: inherit;">オブジェクト認識用のニューラルネットワークを実装することを提案できます</font><font style="vertical-align: inherit;">。</font><font style="vertical-align: inherit;">これらのニューラルネットワークは、タスクに簡単に対処できます。</font><font style="vertical-align: inherit;">残念ながら、</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"><font style="vertical-align: inherit;">Raspberry piで</font></a><font style="vertical-align: inherit;">オブジェクト認識コードを実行しています</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。強力なコンピューターでも、約90FPSが必要な周波数でこれらのたたみ込みニューラルネットワークを実行すると問題が発生します。そして、AI互換のGPUを持たないRaspberry piは、そのようなAIアルゴリズムの非常に簡略化されたバージョンの1つに対応できませんでした。 Piから別のコンピューターにビデオをストリーミングできますが、リアルタイムのビデオ送信は非常に不快なプロセスであり、遅延や帯域幅の制限により、特に高いデータ転送速度が必要な場合に深刻な問題が発生します。</font></font><br>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/K9a6mGNmhbc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">YOLOはとてもクールです！しかし、私はそのすべての機能を必要としません。</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
幸い、私は「昔ながらの」マシンビジョン技術を使用して、難しいAIベースのソリューションを回避できました。最初の手法は</font><font style="vertical-align: inherit;">、画像のすべての変更部分を分離しようとする</font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">背景減算</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">です。私の場合、カメラの視野内で動く唯一のものはレゴの詳細です。 （もちろん、テープも移動しますが、色が均一であるため、カメラに対して静止しているように見えます）。これらのLEGOの詳細を背景から分離すると、問題の半分が解決されます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
背景の減算が機能するためには、前景オブジェクトが背景と大きく異なる必要があります。レゴのディテールは色の範囲が広いので、背景色をレゴの色からできるだけ遠ざけるように注意深く選択する必要がありました。そのため、カメラの下のテープは紙で作られています。非常に均一であるだけでなく、レゴで構成することもできません。そうでなければ、私が認識する必要のあるパーツの色になります。私は淡いピンクを選びましたが、他のパステルカラーは、通常のレゴカラーとは異なり、使えます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
すばらしいOpenCVライブラリには、すでにバックグラウンド減算用のアルゴリズムがいくつかあります。 MOG2バックグラウンド減算器は、それらの中で最も複雑であり、同時にラズベリーpiでも非常に高速に動作します。ただし、ビデオフレームをMOG2に直接フィードすることは、あまりうまく機能しません。薄い灰色と白の数字は、薄い背景の明るさに近すぎて失われます。テープを細部からより明確に分離する方法を考え、背景の減算器</font><font style="vertical-align: inherit;">が</font><i><font style="vertical-align: inherit;">明るさ</font></i><font style="vertical-align: inherit;">ではなく</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">色</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">をより詳しく見るように命令する必要が</font><font style="vertical-align: inherit;">ありました</font><font style="vertical-align: inherit;">。これを行うには、背景の減算器に転送する前に画像の彩度を上げるだけで十分でした。結果は大幅に改善されました。</font></font><i><font style="vertical-align: inherit;"></font></i><font style="vertical-align: inherit;"></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
背景を差し引いた後、モルフォロジー演算を使用して、できるだけ多くのノイズを取り除く必要がありました。</font><font style="vertical-align: inherit;">白い領域の輪郭を見つけるには、OpenCVライブラリのfindContours（）関数を使用できます。</font><font style="vertical-align: inherit;">さまざまなヒューリスティックを適用してノイズを含むループを偏向させることにより、これらのループを事前定義された境界ボックスに簡単に変換できます。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/214/785/f54/214785f5468b2b84ae29f212a66d5abd.gif"></div><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">パフォーマンス</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ニューラルネットワークは貪欲な生き物です。</font><font style="vertical-align: inherit;">分類で最良の結果を得るには、彼女は最大解像度の画像をできるだけ多く必要とします。</font><font style="vertical-align: inherit;">つまり、画像の品質と解像度を維持しながら、非常に高いフレームレートで撮影する必要があります。</font><font style="vertical-align: inherit;">私はカメラとGPU Raspberry PIから可能な限り最大を絞る必要があります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
非常に詳細な</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">picameraドキュメント</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">V2カメラチップは、最大周波数が毎秒90フレームのサイズで1280x720ピクセルの画像を生成できると書かれています。これは信じられないほどの量のデータであり、カメラはそれを生成できますが、これはコンピュータがそれを処理できることを意味するものではありません。生の24ビットRGBイメージを処理する場合、データを約237 MB /秒の速度で転送する必要があります。これは、Piコンピューターの貧弱なGPUやSDRAMにとっては大きすぎます。 JPEGでGPUアクセラレーション圧縮を使用する場合でも、90fpsは実現できません。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Raspberry Piは、フィルタリングされていない生のYUV画像を表示できます。 RGBよりも操作が難しいですが、YUVには実際には多くの便利なプロパティがあります。それらの中で最も重要なのは、ピクセルあたり12ビットしか格納しないことです（RGBの場合は24ビットです）。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d50/28a/686/d5028a6862f79b8caf4d6f895dd46d84.png"></div><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Yの4バイトごとに1バイトUと1バイトVがあります。つまり、ピクセルあたり1.5バイトです。</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
これは、RGBフレームと比較して、</font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2倍の</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> YUVフレームを</font><font style="vertical-align: inherit;">処理できることを意味します。これは、</font><font style="vertical-align: inherit;">RGBイメージへの変換時にGPUが節約する追加の時間をカウントしていません。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ただし、このアプローチでは、処理プロセスに固有の制限が課されます。フルサイズのビデオフレームでの操作のほとんどは、非常に大量のメモリとCPUリソースを消費します。私の厳しい時間制限内では、フルスクリーンYUVフレームをデコードすることさえ不可能です。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
幸い、フレーム全体を処理する必要はありません。オブジェクトの認識では、境界の長方形は正確である必要はなく、おおよその精度で十分なので、オブジェクトを認識するプロセス全体をはるかに小さなフレームで実行できます。ズームアウト操作では、フルサイズのフレームのすべてのピクセルを考慮する必要がないため、フレームを非常にすばやく、コストをかけずに削減できます。次に、結果の境界矩形のスケールが再び増加し、フルサイズのYUVフレームからオブジェクトを切り取るために使用されます。これにより、高解像度のフレーム全体をデコードしたり、処理したりする必要がなくなりました。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fcd/d6a/9e4/fcdd6a9e466f0b87be4dbb349f19e402.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
幸いなことに、このYUV形式（上記を参照）の保存方法のおかげで、YUV形式で直接動作する高速クロッピングおよびズーム操作を実装するのは非常に簡単です。</font><font style="vertical-align: inherit;">さらに、プロセス全体を4つのPiコアに問題なく並列化できます。</font><font style="vertical-align: inherit;">ただし、すべてのコアがそれらの能力を最大限に活用しているわけではないことがわかりました。これは、メモリ帯域幅がボトルネックのままであることを示しています。</font><font style="vertical-align: inherit;">それでも、実際には70〜80 FPSを達成することができました。</font><font style="vertical-align: inherit;">メモリ使用量のより深い分析は、物事をさらにスピードアップするのに役立つかもしれません。</font></font><br>
<br>
<hr><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
プロジェクトについて詳しく知りたい場合は、以前の記事</font></font><a rel="nofollow" href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">「学習のために10万を超えるラベル付きLEGOイメージを作成した方法」を</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">参照してください</font><font style="vertical-align: inherit;">。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
選別機全体の操作のビデオ：</font></font><br>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/04JkdHEX3Yk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../ja480276/index.html">ゲーム面接：採用戦略。パートI</a></li>
<li><a href="../ja480280/index.html">TurboConfセキュリティ調査</a></li>
<li><a href="../ja480282/index.html">フィットネススタートアップのPelotonの歴史：80億ドルの評価から広告の不成功、85％の在庫減少の予測まで</a></li>
<li><a href="../ja480284/index.html">Yandex.Maps APIでの（ナノ）経験、または手順が必要な理由</a></li>
<li><a href="../ja480290/index.html">16のZ80の自家製ZedRipperノートブック</a></li>
<li><a href="../ja480296/index.html">リアクティブ開発Telegramボット</a></li>
<li><a href="../ja480300/index.html">2011年、NginxがIgor SysoevとRamblerのどちらに属しているかの問題</a></li>
<li><a href="../ja480302/index.html">パイオニアとパイオニア。3Dビルディングサーキュラープリンター-すべての始まり</a></li>
<li><a href="../ja480304/index.html">jscodeshiftとTypeScriptでの型推論</a></li>
<li><a href="../ja480306/index.html">なぜ閉じた扉を打ち負かしたのですか？</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>