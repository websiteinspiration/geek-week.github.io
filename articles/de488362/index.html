<!doctype html>
<html class="no-js" lang="de">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🚔 👨🏻‍🏫 👷🏼 Klassifizierung mit mehreren Tags 🌡️ 🔪 👨🏽‍⚕️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo habrozhiteli! Wir haben beschlossen, einen Auszug aus dem Buch von Andrei Burkov , Maschinelles Lernen ohne zusätzliche Wörter , zu zitieren , d...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Klassifizierung mit mehreren Tags</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/488362/"><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/webt/hm/vf/c_/hmvfc_yyxepplv1mj0crw1vu7pw.jpeg" align="left" alt="Bild"></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hallo habrozhiteli! </font><font style="vertical-align: inherit;">Wir haben beschlossen, einen Auszug aus dem Buch von Andrei Burkov </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, Maschinelles Lernen ohne zusätzliche Wörter</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , </font><font style="vertical-align: inherit;">zu zitieren </font><font style="vertical-align: inherit;">, das der Klassifizierung gewidmet ist.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Zur Beschreibung des Bildes in der Abbildung können mehrere Bezeichnungen gleichzeitig verwendet werden: „Nadelwald“, „Berge“, „Straße“. Wenn die Anzahl der möglichen Werte für Beschriftungen groß ist, sie jedoch alle dieselbe Natur wie Tags haben, kann jedes markierte Beispiel in mehrere markierte Daten konvertiert werden, eines für jedes Tag. Alle diese neuen Daten haben dieselben Merkmalsvektoren und nur eine Bezeichnung. Infolgedessen wird die Aufgabe zu einem Klassifizierungsproblem für mehrere Klassen. Es kann mit der Strategie „Eins gegen alle“ gelöst werden. Der einzige Unterschied zum üblichen Problem der Klassifizierung mehrerer Klassen besteht im Auftreten eines neuen Hyperparameters: der Schwelle. Wenn die Ähnlichkeitsbewertung für ein Etikett über einem Schwellenwert liegt, wird dieses Etikett dem Eingabe-Feature-Vektor zugewiesen. In diesem Szenario können einem charakteristischen Vektor mehrere Beschriftungen zugewiesen werden.Der Schwellenwert wird mit dem Regelsatz ausgewählt.</font></font><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Um das Klassifizierungsproblem mit vielen Bezeichnungen zu lösen, kann man auf ähnliche Weise Algorithmen anwenden, die natürlich in mehrere Klassen konvertiert werden (Entscheidungsbäume, logistische Regression, neuronale Netze usw.). Sie geben eine Schätzung für jede Klasse zurück, sodass wir einen Schwellenwert definieren und dann einem Merkmalsvektor mehrere Bezeichnungen zuweisen können, für die der Proximity-Score diesen Schwellenwert überschreitet. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Neuronale Netze können natürlich in Multi-Label-Klassifikationen unter Verwendung der binären Kreuzentropie als Kostenfunktion trainiert werden. Die Ausgabeschicht des neuronalen Netzwerks hat in diesem Fall einen Knoten pro Etikett. Jeder Knoten in der Ausgabeschicht verfügt über eine Sigmoid-Aktivierungsfunktion. Dementsprechend ist jedes Etikett l binär</font></font><img src="https://habrastorage.org/webt/2n/_n/mg/2n_nmgvuciuadryomnq3urrg1xw.jpeg" alt="Bild"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">wobei l = 1, ..., L und i = 1, ..., N. Die binäre Kreuzentropie der Wahrscheinlichkeit, </font></font><img src="https://habrastorage.org/webt/lb/le/cf/lblecfgyuy23k8zvlasuajncsds.jpeg" alt="Bild"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">dass die Probe xi mit l bezeichnet wird, definiert als das </font></font><img src="https://habrastorage.org/webt/qd/1e/fx/qd1efxpmi6mmuhv_fhmsqpjgpi4.jpeg" alt="Bild"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Kriterium der Minimierung - der einfache Durchschnitt aller Mitglieder der binären Kreuzentropie in allen Trainingsproben und alle ihre Tags. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In Fällen, in denen die Anzahl der möglichen Etikettenwerte gering ist, können Sie versuchen, das Klassifizierungsproblem mit vielen Etiketten in ein Klassifizierungsproblem für mehrere Klassen umzuwandeln. </font><font style="vertical-align: inherit;">Stellen Sie sich das folgende Problem vor. </font><font style="vertical-align: inherit;">Sie müssen Bildern zwei Arten von Beschriftungen zuweisen. </font><font style="vertical-align: inherit;">Etiketten des ersten Typs können zwei mögliche Bedeutungen haben: { </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Foto, Malerei</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> }; </font><font style="vertical-align: inherit;">Markierungen des zweiten Typs können drei mögliche Bedeutungen haben: { </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Porträt, Landschaft, andere</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">}. </font><font style="vertical-align: inherit;">Für jede Kombination von zwei Quellklassen können Sie eine neue Dummy-Klasse erstellen, zum Beispiel:</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/mz/px/zn/mzpxzn0rlrumwoql7gkk3no7ihk.jpeg" alt="Bild"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Jetzt haben wir die gleichen markierten Daten, aber wir haben den Satz der echten Beschriftungen durch eine Dummy-Beschriftung mit Werten von 1 bis 6 ersetzt. In der Praxis liefert dieser Ansatz gute Ergebnisse, wenn nicht zu viele mögliche Kombinationen von Klassen vorhanden sind. Andernfalls müssen viel mehr Trainingsdaten verwendet werden, um die Zunahme der Klassen zu kompensieren. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Der Hauptvorteil dieses letzteren Ansatzes besteht darin, dass die Markierungen im Gegensatz zu den oben beschriebenen Methoden, die jede Markierung unabhängig voneinander vorhersagen, korreliert bleiben. Bei vielen Aufgaben kann die Korrelation zwischen Etiketten ein wesentlicher Faktor sein. Stellen Sie sich beispielsweise vor, Sie möchten E-Mails als </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Spam</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> und </font><i><font style="vertical-align: inherit;">Nicht- </font></i><i><font style="vertical-align: inherit;">Spam</font></i><font style="vertical-align: inherit;"> klassifizieren</font></font><i><font style="vertical-align: inherit;"></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">und gleichzeitig wie gewöhnlich und wichtig. </font><font style="vertical-align: inherit;">Sie möchten wahrscheinlich Prognosen wie [ </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Spam, wichtig</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ] ausschließen.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">7.5. </font><font style="vertical-align: inherit;">Ensemble-Training</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die grundlegenden Algorithmen, die wir in Kapitel 3 behandelt haben, haben ihre Grenzen. Aufgrund seiner Einfachheit können sie manchmal kein Modell erstellen, das für Ihre Aufgabe effektiv genug ist. In solchen Fällen können Sie versuchen, tiefe neuronale Netze zu verwenden. In der Praxis erfordern tiefe neuronale Netze jedoch eine erhebliche Menge an beschrifteten Daten, über die Sie möglicherweise nicht verfügen. Eine andere Möglichkeit, die Effektivität einfacher Lernalgorithmen zu erhöhen, ist das </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ensemble-Training</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ensemble-Training ist ein Trainingsparadigma, das darauf basiert, nicht nur ein superkorrektes Modell, sondern eine große Anzahl von Modellen mit geringer Genauigkeit zu trainieren und die Prognosen dieser </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">schwachen</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Modelle zu kombinieren, um ein korrekteres </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Metamodell zu erhalten</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Modelle mit geringer Genauigkeit werden normalerweise durch </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">schwache Lernalgorithmen</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> trainiert </font><font style="vertical-align: inherit;">, die keine komplexen Modelle trainieren können und daher in der Trainings- und Prognosephase eine hohe Geschwindigkeit aufweisen. Am häufigsten wird der Entscheidungsbaum-Lernalgorithmus als schwacher Algorithmus verwendet, der normalerweise nach mehreren Iterationen aufhört, den Trainingssatz zu unterbrechen. Das Ergebnis sind kleine und nicht sehr regelmäßige Bäume, aber wie die Idee, das Ensemble zu lernen, besagt, können wir durch die Kombination einer großen Anzahl solcher Bäume eine hohe Genauigkeit erzielen, wenn die Bäume nicht identisch sind und jeder Baum zumindest geringfügig besser als zufällige Vermutungen ist. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Um die endgültige Prognose für Eintrag </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x zu erhalten</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Prognosen aller schwachen Modelle werden mit einer gewichteten Abstimmungsmethode kombiniert. </font><font style="vertical-align: inherit;">Die spezifische Form der Gewichtung der Stimmen hängt vom Algorithmus ab, aber das Wesentliche hängt nicht davon ab: Wenn insgesamt schwache Modelle vorhersagen, dass es sich bei der E-Mail um Spam handelt, weisen wir Probe </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> das </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Spam-</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Label </font><font style="vertical-align: inherit;">zu </font><font style="vertical-align: inherit;">. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die beiden Hauptmethoden für das Training von Ensembles sind </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Boosten</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> und </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Absacken</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (Aggregation). </font><font style="vertical-align: inherit;">Übersetzungen der Begriffe Boosten und Absacken sind ungenau und nicht gewohnt.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">7.5.1. </font><font style="vertical-align: inherit;">Boosten und Absacken</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die Boosting-Methode besteht darin, die anfänglichen Trainingsdaten zu verwenden und iterativ mehrere Modelle unter Verwendung eines schwachen Algorithmus zu erstellen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Jedes neue Modell unterscheidet sich von den vorherigen darin, dass ein schwacher Algorithmus bei seiner Konstruktion versucht, die von früheren Modellen gemachten Fehler zu „beheben“. </font><font style="vertical-align: inherit;">Das endgültige Ensemblemodell ist eine Kombination dieser vielen schwachen iterativ konstruierten Modelle. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das Wesentliche beim Absacken besteht darin, viele „Kopien“ von Trainingsdaten zu erstellen (jede Kopie unterscheidet sich geringfügig von den anderen) und dann auf jede Kopie einen schwachen Algorithmus anzuwenden, um mehrere schwache Modelle zu erhalten, und diese dann zu kombinieren. </font><font style="vertical-align: inherit;">Ein weit verbreiteter und effizienter Algorithmus für maschinelles Lernen, der auf der Idee des Absackens basiert, ist eine </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">zufällige Gesamtstruktur</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">7.5.2. </font><font style="vertical-align: inherit;">Zufälliger Wald</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Der "klassische" Absackalgorithmus funktioniert wie folgt. </font><font style="vertical-align: inherit;">B Stichproben aus dem bestehenden Trainingssatz erzeugt </font></font><img src="https://habrastorage.org/webt/-d/6r/gn/-d6rgnsn5wk-yzl3dbk0i7x2ole.jpeg" alt="Bild"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(für jeden b = 1, ..., B) und ein </font><font style="vertical-align: inherit;">Entscheidungsbaum - </font></font><img src="https://habrastorage.org/webt/-d/6r/gn/-d6rgnsn5wk-yzl3dbk0i7x2ole.jpeg" alt="Bild"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Modell ist aufgebaut </font><font style="vertical-align: inherit;">auf der Basis von jeder Probe </font></font><img src="https://habrastorage.org/webt/nd/ew/4z/ndew4zvx7r0jpeilfknakojmrbs.jpeg" alt="Bild"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Um eine Probe </font></font><img src="https://habrastorage.org/webt/-d/6r/gn/-d6rgnsn5wk-yzl3dbk0i7x2ole.jpeg" alt="Bild"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">für einige b zu erhalten, wird eine </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Probe mit Ersatz erstellt</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Das heißt, zuerst wird eine leere Stichprobe erstellt, und dann wird eine zufällige Stichprobe aus dem Trainingssatz ausgewählt und ihre genaue Kopie wird abgelegt </font></font><img src="https://habrastorage.org/webt/-d/6r/gn/-d6rgnsn5wk-yzl3dbk0i7x2ole.jpeg" alt="Bild"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, während die Stichprobe selbst im ursprünglichen Trainingssatz verbleibt. </font><font style="vertical-align: inherit;">Die Auswahl der Daten wird fortgesetzt, bis die Bedingung erfüllt ist. </font></font><img src="https://habrastorage.org/webt/se/au/-5/seau-5gwous1c1cshmrx8rwubig.jpeg" alt="Bild"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Als Ergebnis des Trainings werden </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">B-</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Entscheidungsbäume </font><font style="vertical-align: inherit;">erhalten </font><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Die Prognose für die neue Stichprobe </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> im Falle einer Regression wird als Durchschnitt von </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">B bestimmt</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Prognosen</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/re/zp/mj/rezpmjqa9lo7w4dxvqd6njwwqcm.jpeg" alt="Bild"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
oder mit Stimmenmehrheit bei Einstufung.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Zufälliger Wald hat nur einen Unterschied zum klassischen Absacken. Es wird ein modifizierter Baumlernalgorithmus verwendet, der bei jeder Aufteilung im Lernprozess eine zufällige Teilmenge von Merkmalen überprüft. Dies geschieht, um die Korrelation zwischen Bäumen zu beseitigen: Wenn ein oder mehrere Features eine große Vorhersagefähigkeit aufweisen, werden sie von vielen Bäumen zum Aufteilen von Daten ausgewählt. Dies wird dazu führen, dass im "Wald" eine große Anzahl korrelierter Bäume erscheint. Die Vorzeichenkorrelation mit hoher Vorhersagefähigkeit verhindert, dass die Vorhersagegenauigkeit zunimmt. Die hohe Effizienz des Modellensembles erklärt sich aus der Tatsache, dass gute Modelle höchstwahrscheinlich mit derselben Prognose übereinstimmen und schlechte Modelle wahrscheinlich nicht übereinstimmen und unterschiedliche Prognosen liefern. Durch die Korrelation werden schlechte Modelle eher zustimmen.Dies verzerrt das Abstimmungsmuster oder beeinflusst den Durchschnitt.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die wichtigsten Hyperparameter für die Optimierung sind die Anzahl der Bäume B und die Größe einer zufälligen Teilmenge von Merkmalen, die bei jeder Aufteilung berücksichtigt werden müssen. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Random Forest ist einer der am häufigsten verwendeten Ensemble-Lernalgorithmen. Was bestimmt seine Wirksamkeit? Der Grund dafür ist, dass wir durch die Verwendung mehrerer Stichproben aus dem Originaldatensatz die </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Varianz des</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> endgültigen Modells </font><font style="vertical-align: inherit;">reduzieren </font><font style="vertical-align: inherit;">. Denken Sie daran, dass eine geringe Varianz eine schwache Veranlagung zur </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Umschulung bedeutet</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Eine Umschulung erfolgt, wenn das Modell versucht, kleine Abweichungen im Datensatz zu erklären, da der Datensatz nur eine kleine Auswahl aller möglichen Beispiele für das Phänomen ist, das wir simulieren möchten. </font><font style="vertical-align: inherit;">Im Falle eines erfolglosen Ansatzes zur Bildung des Trainingssatzes können einige unerwünschte (aber unvermeidliche) Artefakte in den Satz fallen: Rauschen, abnormale und übermäßig oder unzureichend repräsentative Daten. </font><font style="vertical-align: inherit;">Indem wir mit dem Ersetzen des Trainingssatzes mehrere Zufallsstichproben erstellen, reduzieren wir den Einfluss dieser Artefakte.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">7.5.3. </font><font style="vertical-align: inherit;">Gradientenverstärkung</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ein weiterer effektiver Ensemble-Trainingsalgorithmus, der auf der Idee des Boostings basiert, ist das Gradienten-Boosting. </font><font style="vertical-align: inherit;">Betrachten Sie zunächst die Verwendung der Gradientenverstärkung bei der Regression. </font><font style="vertical-align: inherit;">Wir werden mit dem Aufbau eines effektiven Regressionsmodells mit einem konstanten Modell beginnen </font></font><img src="https://habrastorage.org/webt/7u/7x/jd/7u7xjdufljpsjwwu45j9_gkc3r0.jpeg" alt="Bild"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(wie wir es in ID3 getan haben):</font></font><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/em/vj/bv/emvjbvtmptxl_d3bihzev4wbh7o.jpeg" alt="Bild"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ändern Sie dann die Beschriftungen in allen Proben i = 1, ..., N im Trainingssatz:</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/va/wc/ah/vawcahk0zsdpgnumh_bfjuozw_e.jpeg" alt="Bild"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
wo </font></font><img src="https://habrastorage.org/webt/rf/1x/pk/rf1xpkgfxmcivy-1tqpwv2vgroi.jpeg" alt="Bild"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">die aufgerufen wird , </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Rest</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> und ist das neue Label der Probe </font></font><img src="https://habrastorage.org/webt/dk/ey/2r/dkey2rj3yf-zkei2029wfa2ujso.jpeg" alt="Bild"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
verwenden wir nun das modifizierte Trainingssatz mit den anstelle der Originaletiketten bleibt ein neues Modell des Entscheidungsbaums zu bauen. </font></font><img src="https://habrastorage.org/webt/mx/fw/um/mxfwumzpc5tq1wjpdate2rxra48.jpeg" alt="Bild"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Verstärkungs Modell jetzt ist definiert als </font></font><img src="https://habrastorage.org/webt/cy/c2/vz/cyc2vz0tmrrihcm6kta_7rnwmui.jpeg" alt="Bild"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">wobei α die Lerngeschwindigkeit (Hyper). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dann berechnen wir die Residuen unter Verwendung von Gleichung 7.2 neu, ersetzen die Beschriftungen in den Trainingsdaten erneut, lehren ein neues Modell des Entscheidungsbaums, </font></font><img src="https://habrastorage.org/webt/p4/wk/nl/p4wknlhlvwiqtr7zz7lx_y5oq3s.jpeg" alt="Bild"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">definieren das Boost-Modell neu, während </font></font><img src="https://habrastorage.org/webt/n-/mn/ht/n-mnhtck0rar7pz4anzlbdc-bmo.jpeg" alt="Bild"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">wir den Prozess wiederholen, bis wir die vorbestimmte maximale Anzahl </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">M von</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Bäumen </font><font style="vertical-align: inherit;">kombinieren </font><font style="vertical-align: inherit;">.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Lassen Sie uns intuitiv verstehen, was hier passiert. Durch Berechnung der Residuen bestimmen wir, wie gut (oder schlecht) das Ziel jeder Trainingsstichprobe vom aktuellen Modell f vorhergesagt wird. Dann trainieren wir einen anderen Baum, um die Fehler des aktuellen Modells zu korrigieren (weshalb wir Reste anstelle der tatsächlichen Beschriftungen verwenden) und fügen dem vorhandenen Modell einen neuen Baum mit einem gewissen Gewicht α hinzu. Infolgedessen korrigiert jeder neue Baum, der dem Modell hinzugefügt wird, teilweise die Fehler, die von vorherigen Bäumen gemacht wurden. Der Prozess wird fortgesetzt, bis die maximale Anzahl M (ein weiterer Hyperparameter) der Bäume kombiniert ist.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Versuchen wir nun, die Frage zu beantworten, warum dieser Algorithmus als Gradientenverstärkung bezeichnet wird. Bei der Gradientenverstärkung berechnen wir den Gradienten nicht, anders als in Kapitel 4, um das Problem der linearen Regression zu lösen. Um die Ähnlichkeiten zwischen Gradientenverstärkung und Gradientenabstieg zu erkennen, denken Sie daran, warum wir den Gradienten in linearer Regression berechnet haben: Um die Richtung der Parameterwerte herauszufinden, um die MSE-Kostenfunktion zu minimieren. Der Gradient zeigt die Richtung an, zeigt jedoch nicht, wie weit in diese Richtung zu gehen ist. Daher haben wir in jeder Iteration einen kleinen Schritt gemacht und dann erneut die Richtung bestimmt. Dasselbe passiert bei der Gradientenverstärkung, aber anstatt den Gradienten direkt zu berechnen, verwenden wir seine Schätzung in Form von Residuen: Sie zeigen, wie das Modell angepasst werden sollte, um den Fehler (Residuum) zu reduzieren.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bei der Gradientenverstärkung stehen drei Haupthyperparameter zur Abstimmung zur Verfügung: die Anzahl der Bäume, die Lerngeschwindigkeit und die Tiefe der Bäume. Alle drei beeinflussen die Genauigkeit des Modells. Die Tiefe der Bäume beeinflusst auch die Lern- und Prognosegeschwindigkeit: Je kleiner die Tiefe, desto schneller. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Es kann gezeigt werden, dass das Lernen durch Residuen das Gesamtmodell f für den Standardfehlerstandard optimiert. Hier sehen Sie den Unterschied zum Absacken: Durch das Boosten werden Verzerrungen (oder mangelnde Bildung) anstelle von Varianz verringert. Infolgedessen muss das Boosten umgeschult werden. Durch Anpassen der Tiefe und Anzahl der Bäume kann jedoch eine Umschulung weitgehend vermieden werden.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die Gradientenverstärkung ist für Bewertungsaufgaben ähnlich, die Schritte unterscheiden sich jedoch geringfügig. Betrachten Sie den Fall der binären Klassifizierung. Angenommen, es gibt M Regressionsentscheidungsbäume. In Analogie zur logistischen Regression wird die Vorhersage des Ensembles von Entscheidungsbäumen mithilfe der Sigmoid-Funktion modelliert:</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/wy/pd/gw/wypdgwjjgpzuojrelehnadtdggc.jpeg" alt="Bild"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wo </font></font><img src="https://habrastorage.org/webt/w6/3d/kb/w63dkbj3f9j-ik9hrhqbexyxtem.jpeg" alt="Bild"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ist der Regressionsbaum? </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Und wieder wird, wie bei der logistischen Regression, beim Versuch, ein Modell für die Maximierung zu finden </font></font><img src="https://habrastorage.org/webt/c4/lv/tt/c4lvttexfzr8disbsv1ph_drzka.jpeg" alt="Bild"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, das Prinzip der maximalen Wahrscheinlichkeit angewendet. Um einen numerischen Überlauf zu vermeiden, maximieren wir in ähnlicher Weise die Summe der Wahrscheinlichkeitslogarithmen und nicht das Produkt der Wahrscheinlichkeit. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Der Algorithmus beginnt mit dem anfänglichen konstanten Modell, </font></font><img src="https://habrastorage.org/webt/hn/7j/ep/hn7jepdxhudnxvjjgbulvfwtpyw.jpeg" alt="Bild"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">wobei </font></font><img src="https://habrastorage.org/webt/gj/6s/f6/gj6sf6i874m3gq3_fbbxbkrudae.jpeg" alt="Bild"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(Es kann gezeigt werden, dass eine solche Initialisierung für die Sigmoidfunktion optimal ist.) Dann wird bei jeder Iteration m ein neuer Baum fm zum Modell hinzugefügt. Um den besten Baum </font></font><img src="https://habrastorage.org/webt/b0/5w/rv/b05wrvcuk5hzvnrmkpebdnk_kxu.jpeg" alt="Bild"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">zu finden </font><font style="vertical-align: inherit;">Um den besten Baum </font><font style="vertical-align: inherit;">zu finden </font></font><img src="https://habrastorage.org/webt/xm/u4/dp/xmu4dpm1hi3-podtuiayxggydti.jpeg" alt="Bild"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, wird zuerst die partielle Ableitung des </font></font><img src="https://habrastorage.org/webt/ep/hx/gu/ephxgusen4ou8pc8atm4ksqathq.jpeg" alt="Bild"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">aktuellen Modells für jedes i = 1, ..., N </font><font style="vertical-align: inherit;">berechnet </font><font style="vertical-align: inherit;">:</font></font><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/wg/v_/oo/wgv_oogvmupu4q5j6g3rq7dphvk.jpeg" alt="Bild"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dabei ist f das Modell des Ensemble-Klassifikators, der auf der vorherigen Iteration m - 1 basiert. Um zu berechnen </font></font><img src="https://habrastorage.org/webt/ep/hx/gu/ephxgusen4ou8pc8atm4ksqathq.jpeg" alt="Bild"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, müssen wir die Ableitungen von in </font></font><img src="https://habrastorage.org/webt/pv/pe/lb/pvpelb2jplsmq_jmkbdfyl-pzom.jpeg" alt="Bild"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bezug auf f für alle i finden. </font><font style="vertical-align: inherit;">Es ist zu beachten, dass die </font></font><img src="https://habrastorage.org/webt/nk/zn/0l/nkzn0lh0l0brnzl_vh0xxlmttrq.jpeg" alt="Bild"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ableitung in Bezug auf f des richtigen Terms in der vorherigen Gleichung ist</font></font><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/xx/en/iu/xxeniu2qr35ln17asbfk2fqb3sc.jpeg" alt="Bild"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dann wird der Trainingssatz transformiert, indem das ursprüngliche Etikett der </font></font><img src="https://habrastorage.org/webt/a1/fv/cu/a1fvcukqqvsu5wpv3x8i1zj5smc.jpeg" alt="Bild"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">entsprechenden partiellen Ableitung ersetzt wird </font></font><img src="https://habrastorage.org/webt/ep/hx/gu/ephxgusen4ou8pc8atm4ksqathq.jpeg" alt="Bild"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, und ein neuer Baum wird auf der Basis des konvertierten Trainingssatzes erstellt. </font></font><img src="https://habrastorage.org/webt/w1/8f/41/w18f41gto37doyvyakgs4np_fyy.jpeg" alt="Bild"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Als nächstes wird der optimale Aktualisierungsschritt wie folgt bestimmt </font></font><img src="https://habrastorage.org/webt/1n/cf/sj/1ncfsjxfe-tao3ep_csuw-s-arw.jpeg" alt="Bild"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">:</font></font><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/co/qc/hw/coqchwpctxgy2ukbdwaybkouxx0.jpeg" alt="Bild"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Am Ende von Iteration m aktualisieren wir das Ensemble-Modell, </font></font><img src="https://habrastorage.org/webt/r7/ox/vy/r7oxvyc5mesbifpwfjtkcrtdr2q.jpeg" alt="Bild"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">indem </font><font style="vertical-align: inherit;">wir </font><font style="vertical-align: inherit;">einen neuen Baum hinzufügen</font></font><img src="https://habrastorage.org/webt/s2/vp/u0/s2vpu0-7pmzuzv0n55f1z1fgktu.jpeg" alt="Bild"><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/vg/ar/qn/vgarqnddik0vjhxxfsesr1t5qs8.jpeg" alt="Bild"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die Iterationen werden fortgesetzt, bis die Bedingung m = M erfüllt ist. Danach stoppt das Training und das Ensemblemodell f wird erhalten. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Gradient Boosting ist einer der leistungsstärksten Algorithmen für maschinelles Lernen. </font><font style="vertical-align: inherit;">Nicht nur, weil es sehr genaue Modelle erstellt, sondern auch, weil es in der Lage ist, große Datenmengen mit Millionen von Daten und Funktionen zu verarbeiten. </font><font style="vertical-align: inherit;">In der Regel ist es einem zufälligen Wald in seiner Genauigkeit überlegen, aber aufgrund der konsistenten Natur kann es viel langsamer lernen.</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de488346/index.html">Installieren von or-tools mit SCIP und GLPK in einer virtuellen Python 3.7-Umgebung unter Linux</a></li>
<li><a href="../de488348/index.html">Webinar „Zehn agilste Herausforderungen und Möglichkeiten, sie in einer Stunde zu überwinden“ 17. Februar um 20:00 Uhr Moskauer Zeit</a></li>
<li><a href="../de488352/index.html">VDI-Kostenvergleich: On-Premise versus Public Cloud</a></li>
<li><a href="../de488356/index.html">Schulung für Dassault Systèmes-Produkte an der Staatlichen Marine Technischen Universität St. Petersburg</a></li>
<li><a href="../de488360/index.html">Big Data Mythen und digitale Kultur</a></li>
<li><a href="../de488366/index.html">Und noch einmal zu "Falsche Zeitzoneninformationen für russische Zeitzonen" [.Net-Fehler, ID: 693286]</a></li>
<li><a href="../de488368/index.html">Was ich bei der Arbeit an meinem ersten Großprojekt gelernt habe</a></li>
<li><a href="../de488370/index.html">TDD für Mikrocontroller. Teil 2: Wie Spione Abhängigkeiten loswerden</a></li>
<li><a href="../de488374/index.html">Telegramm + 1C + Webhooks + Apache + Selbstsigniertes Zertifikat</a></li>
<li><a href="../de488376/index.html">Wenn das Prinzip "zur Hölle mit allem, nimm es und mach es!" funktioniert nicht: Zaudernotizen</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>