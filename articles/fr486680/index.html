<!doctype html>
<html class="no-js" lang="fr">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🤷🏽 🙏 🚯 ML, VR & Robots (et un peu de cloud) 🛌🏿 😟 🧞</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bonjour à tous! 
 
 Je veux parler d'un projet très ennuyeux où la robotique, le Machine Learning (et ensemble c'est le Robot Learning), la réalité vi...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>ML, VR & Robots (et un peu de cloud)</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/recognitor/blog/486680/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bonjour à tous! </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Je veux parler d'un projet très ennuyeux où la robotique, le Machine Learning (et ensemble c'est le Robot Learning), la réalité virtuelle et un peu de technologie cloud se sont croisés. </font><font style="vertical-align: inherit;">Et tout cela a du sens. </font><font style="vertical-align: inherit;">Après tout, il est très pratique de passer à un robot, de montrer quoi faire, puis de former des poids sur le serveur ML en utilisant les données stockées. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Sous la coupe, nous dirons comment cela fonctionne maintenant, et quelques détails sur chacun des aspects qui devaient être développés.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/t0/kx/wi/t0kxwiicqakswvomnrwz-iycc2o.jpeg"><br>
<a name="habracut"></a><br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pourquoi</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pour commencer, cela vaut la peine d'être révélé. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Il semble que des robots armés de Deep Learning soient sur le point d'évincer les gens de leur travail partout. </font><font style="vertical-align: inherit;">En fait, tout n'est pas si fluide. </font><font style="vertical-align: inherit;">Là où les actions sont strictement répétées, les processus sont déjà très bien automatisés. </font><font style="vertical-align: inherit;">Si nous parlons de «robots intelligents», c'est-à-dire d'applications où la vision par ordinateur et les algorithmes suffisent déjà. </font><font style="vertical-align: inherit;">Mais il y a aussi beaucoup d'histoires extrêmement compliquées. </font><font style="vertical-align: inherit;">Les robots peuvent difficilement faire face à la variété des objets à traiter et à la diversité de l'environnement.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Points clés</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Il y a 3 éléments clés en termes de mise en œuvre qui ne se trouvent pas encore partout: </font></font><br>
<br>
<ul>
<li>       (data-driven learning). ..   ,    ,     ,    . ,     .</li>
<li>   ()    </li>
<li>  -  (Human-machine collaboration) </li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La seconde est également importante car en ce moment, nous observerons un changement dans les approches de l'apprentissage, des algorithmes, derrière eux et des outils informatiques. </font><font style="vertical-align: inherit;">Les algorithmes de perception et de contrôle deviendront plus flexibles. </font><font style="vertical-align: inherit;">Une mise à niveau du robot coûte de l'argent. </font><font style="vertical-align: inherit;">Et la calculatrice peut être utilisée plus efficacement si elle sert plusieurs robots à la fois. </font><font style="vertical-align: inherit;">Ce concept est appelé «robotique cloud». </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Avec ce dernier, tout est simple - l'IA n'est pas suffisamment développée à l'heure actuelle pour fournir une fiabilité et une précision à 100% dans toutes les situations requises par les entreprises. </font><font style="vertical-align: inherit;">Par conséquent, l'opérateur superviseur, qui peut parfois aider les robots des salles, ne sera pas blessé.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Schème</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pour commencer, sur une plate-forme logicielle / réseau qui fournit toutes les fonctionnalités décrites: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/fd/6y/fi/fd6yfi2svby-3l7hkn9lxxt3neo.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Composants:</font></font><br>
<br>
<ol>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Le robot envoie un flux vidéo 3D au serveur et reçoit le contrôle en réponse. </font></font></li>
<li>    :  - ,      (, , , )</li>
<li>  ML  ( ),        ,   ,  .      —  3D   ,     . </li>
<li> -  ,  3D       ,   UI   .   — . </li>
</ol><br>
<h4> </h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Il existe 2 modes de fonctionnement du robot: automatique et manuel. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En mode manuel, le robot fonctionne si le service ML n'est pas encore formé. </font><font style="vertical-align: inherit;">Ensuite, le robot passe de l'automatique au manuel soit à la demande de l'opérateur (j'ai vu des comportements étranges en regardant le robot), soit lorsque les services ML eux-mêmes détectent une anomalie. </font><font style="vertical-align: inherit;">À propos de la détection des anomalies sera plus tard - c'est une partie très importante, sans laquelle il est impossible d'appliquer l'approche proposée. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
L'évolution du contrôle est la suivante:</font></font><br>
<br>
<ol>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La tâche du robot est formulée en termes lisibles par l'homme et des indicateurs de performance sont décrits.</font></font><br>
</li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">L'opérateur se connecte au robot en VR et exécute la tâche dans le flux de travail existant pendant un certain temps</font></font><br>
</li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La partie ML est formée sur les données reçues</font></font><br>
</li>
<li>      ,     ML             <br>
</li>
<li>              <br>
</li>
</ol><br>
<h4>       3D</h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Très souvent, les robots utilisent l'environnement ROS (robot operating system), qui est en fait un cadre de gestion des «nœuds» (nœuds), chacun fournissant une partie des fonctionnalités du robot. En général, il s'agit d'une façon relativement pratique de programmer des robots, qui ressemble à certains égards à l'architecture de microservices des applications Web. Le principal avantage de ROS est la norme de l'industrie et il existe déjà un grand nombre de modules nécessaires pour créer un robot. Même les bras robotiques industriels peuvent avoir un module d'interface ROS. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La chose la plus simple est de créer un modèle de pont entre notre partie serveur et ROS. Par exemple, par </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">exemple</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Maintenant, notre projet utilise une version plus développée du «nœud» ROS, qui se connecte et interroge le microservice du registre auquel le serveur de relais un robot particulier peut se connecter. Le code source n'est donné qu'à titre d'exemple d'instructions pour l'installation du module ROS. Au début, lorsque vous maîtrisez ce framework (ROS), tout semble assez hostile, mais la documentation est assez bonne, et après quelques semaines, les développeurs commencent à utiliser ses fonctionnalités en toute confiance. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
D'intéressant - le problème de la compression du flux de données 3D, qui doit être produit directement sur le robot.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ce n'est pas si facile de compresser la carte de profondeur. </font><font style="vertical-align: inherit;">Même avec un petit degré de compression du flux RVB, une distorsion locale très grave de la luminosité par rapport au vrai en pixels aux frontières ou lors du déplacement d'objets est autorisée. </font><font style="vertical-align: inherit;">L'œil ne le remarque presque pas, mais dès que les mêmes distorsions sont permises dans la carte de profondeur, lors du rendu 3D tout devient très mauvais: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/kv/1l/6-/kv1l6-qgxaqxhsf1a-zl67hymtk.png"><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
(à partir de l' </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">article</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ) </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ces défauts sur les bords gâchent grandement la scène 3D, car </font><font style="vertical-align: inherit;">il y a juste beaucoup de déchets dans l'air.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nous avons commencé à utiliser la compression image par image - JPEG pour RVB et PNG pour une carte de profondeur avec de petits hacks. </font><font style="vertical-align: inherit;">Cette méthode comprime le flux 30FPS pour une résolution de scanner 3D de 640x480 à 25 Mbps. </font><font style="vertical-align: inherit;">Une meilleure compression peut également être fournie si le trafic est critique pour l'application. </font><font style="vertical-align: inherit;">Il existe des codecs de flux 3D commerciaux qui peuvent également être utilisés pour compresser ce flux.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Contrôle de réalité virtuelle</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Après avoir calibré le cadre de référence de la caméra et du robot (et nous avons déjà écrit </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">un article sur le calibrage</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ), le bras du robot peut être contrôlé en réalité virtuelle. </font><font style="vertical-align: inherit;">Le contrôleur définit à la fois la position en 3D XYZ et l'orientation. </font><font style="vertical-align: inherit;">Pour certains roboruk, seules 3 coordonnées suffiront, mais avec un grand nombre de degrés de liberté, l'orientation de l'outil spécifiée par le contrôleur doit également être transmise. </font><font style="vertical-align: inherit;">De plus, il y a suffisamment de commandes sur les contrôleurs pour exécuter les commandes du robot telles que la mise sous / hors tension de la pompe, le contrôle de la pince, etc. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Initialement, il a été décidé d'utiliser le framework JavaScript pour la réalité virtuelle A-frame, basé sur le moteur WebVR. </font><font style="vertical-align: inherit;">Et les premiers résultats (démonstration vidéo à la fin de l'article pour le bras à 4 coordonnées) ont été obtenus sur le cadre A.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En fait, il s'est avéré que WebVR (ou A-frame) était une solution infructueuse pour plusieurs raisons:</font></font><br>
<br>
<ul>
<li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">compatibilité principalement avec FireFox</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , et c'est dans FireFox que le cadre A-frame n'a pas libéré de ressources de texture (le reste des navigateurs a copié) jusqu'à ce que la consommation de mémoire atteigne 16 Go.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">interaction limitée avec les contrôleurs VR et le casque. </font><font style="vertical-align: inherit;">Ainsi, par exemple, il n'a pas été possible d'ajouter des marques supplémentaires avec lesquelles vous pouvez définir la position, par exemple, des coudes de l'opérateur.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">L'application nécessitait du multithreading ou plusieurs processus. </font><font style="vertical-align: inherit;">Dans un thread / processus, il était nécessaire de déballer les images vidéo, dans un autre - dessiner. </font><font style="vertical-align: inherit;">En conséquence, tout a été organisé par des travailleurs, mais le temps de déballage a atteint 30 ms, et le rendu en VR devrait être effectué à une fréquence de 90FPS.</font></font><br>
</li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Toutes ces lacunes ont entraîné le fait que le rendu du cadre n'a pas eu le temps dans les 10 ms alloués et qu'il y a eu des secousses très désagréables en VR. Probablement, tout pouvait être surmonté, mais l'identité de chaque navigateur était un peu agaçante. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nous avons maintenant décidé de partir pour le port C #, OpenTK et C # de la bibliothèque OpenVR. Il existe encore une alternative: l'unité. Ils écrivent que Unity est pour les débutants ... mais difficile. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La chose la plus importante qui devait être trouvée et connue pour gagner en liberté:</font></font><br>
<br>
<pre><code class="cs hljs">VRTextureBounds_t bounds = <span class="hljs-keyword">new</span> VRTextureBounds_t() { uMin = <span class="hljs-number">0</span>, vMin = <span class="hljs-number">0</span>, uMax = <span class="hljs-number">1f</span>, vMax = <span class="hljs-number">1f</span> }; <font></font>
OpenVR.Compositor.Submit(EVREye.Eye_Left, <span class="hljs-keyword">ref</span> leftTexture, <span class="hljs-keyword">ref</span> bounds, EVRSubmitFlags.Submit_Default);<font></font>
OpenVR.Compositor.Submit(EVREye.Eye_Right, <span class="hljs-keyword">ref</span> rightTexture, <span class="hljs-keyword">ref</span> bounds, EVRSubmitFlags.Submit_Default);
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
(c'est le code pour envoyer deux textures aux yeux gauche et droit du casque) </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
i.e. </font><font style="vertical-align: inherit;">dessinez dans OpenGL dans la texture ce que différents yeux voient et envoyez-le dans des verres. </font><font style="vertical-align: inherit;">La joie ne connaissait pas de limites lorsqu'elle s'est avérée remplir l'œil gauche de rouge et la droite de bleu. </font><font style="vertical-align: inherit;">Juste quelques jours et maintenant la profondeur et la carte RVB provenant de webSocket ont été transférées au modèle polygonal en 10 ms au lieu de 30 sur JS. </font><font style="vertical-align: inherit;">Et puis il suffit d'interroger les coordonnées et les boutons des contrôleurs, d'entrer dans le système d'événements pour les boutons, de traiter les clics des utilisateurs, d'entrer dans la State Machine pour l'interface utilisateur, et maintenant vous pouvez attraper un verre de l'espresso:</font></font><br>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/EuBNaGZctf8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Maintenant, la qualité du Realsense D435 est quelque peu déprimante, mais elle passera dès que nous installerons au moins </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">un scanner 3D aussi intéressant de Microsoft</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , dont le nuage de points est beaucoup plus précis.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Du côté serveur</font></font></h4><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Serveur de relais</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
L'élément fonctionnel principal est le relais de serveur (serveur au milieu), qui reçoit un flux vidéo du robot avec des images 3D et des lectures de capteur et l'état du robot et le distribue aux consommateurs. Entrer des trames et des lectures de capteur emballées par TCP / IP. La distribution aux consommateurs est effectuée par des sockets Web (un mécanisme très pratique pour le streaming vers plusieurs consommateurs, y compris un navigateur). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
De plus, le serveur intermédiaire stocke le flux de données dans le stockage cloud S3 afin qu'il puisse ensuite être utilisé pour la formation. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Chaque serveur relais prend en charge l'API http, qui vous permet de connaître son état actuel, ce qui est pratique pour surveiller les connexions actuelles.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La tâche de relais est assez difficile, à la fois du point de vue de l'informatique et du point de vue du trafic. Par conséquent, nous avons suivi ici la logique selon laquelle les serveurs relais sont déployés sur une variété de serveurs cloud. Et cela signifie que vous devez garder une trace de qui se connecte où (en particulier si les robots et les opérateurs sont dans des régions différentes). </font></font><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">S'inscrire</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Le plus fiable maintenant sera difficile de définir pour chaque robot les serveurs auxquels il peut se connecter (la redondance ne nuira pas). Le service de gestion ML est associé au robot, il interroge le serveur relais pour déterminer celui auquel le robot est connecté et est connecté à celui correspondant, à condition, bien sûr, qu'il dispose de droits suffisants pour cela. L'application de l'opérateur fonctionne de manière similaire.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Le plus agréable! Du fait que la formation des robots est un service, le service n'est visible que pour nous à l'intérieur. Ainsi, son frontal peut être aussi pratique que possible pour nous! Ceux. c'est une console dans le navigateur (il y a une bibliothèque </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">terminalJS</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> avec une grande simplicité </font><font style="vertical-align: inherit;">, qui est très facile à modifier si vous voulez des fonctions supplémentaires, telles que l'auto-complétion TAB ou la lecture de l'historique des appels) et cela ressemble à ceci: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/wl/vl/yo/wlvlyojuytiepjvhzexlxr-ixcq.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ceci, bien sûr, est un sujet distinct pour la discussion, pourquoi la ligne de commande Tellement confortable. Soit dit en passant, il est particulièrement pratique de faire des tests unitaires d'un tel frontend.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En plus de l'API http, ce service implémente un mécanisme d'enregistrement des utilisateurs avec des jetons temporaires, des opérateurs de connexion / déconnexion, des administrateurs et des robots, un support de session, des clés de cryptage de session pour le cryptage du trafic entre le serveur relais et le robot. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Tout cela se fait en Python avec Flask - une pile très proche pour les développeurs ML (c'est-à-dire nous). </font><font style="vertical-align: inherit;">Oui, en outre, l'infrastructure CI / CD existante pour les microservices est en bons termes avec Flask.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Problème de retard</font></font></h4> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Si nous voulons contrôler les manipulateurs en temps réel, le délai minimum est extrêmement utile. </font><font style="vertical-align: inherit;">Si le retard devient trop important (plus de 300 ms), il est alors très difficile de contrôler les manipulateurs en fonction de l'image dans le casque virtuel. </font><font style="vertical-align: inherit;">Dans notre solution, en raison de la compression image par image (c'est-à-dire qu'il n'y a pas de mise en mémoire tampon) et de l'absence d'outils standard comme GStreamer, le retard, même en tenant compte du serveur intermédiaire, est d'environ 150-200 ms. </font><font style="vertical-align: inherit;">Le temps de transmission sur le réseau d'entre eux est d'environ 80 ms. </font><font style="vertical-align: inherit;">Le reste du retard est causé par la caméra Realsense D435 et la fréquence de capture limitée.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bien sûr, il s'agit d'un problème de pleine hauteur qui se pose en mode «tracking», lorsque le manipulateur dans sa réalité suit constamment le contrôleur de l'opérateur en réalité virtuelle. </font><font style="vertical-align: inherit;">Dans le mode de déplacement vers un point XYZ donné, le retard ne pose aucun problème à l'opérateur.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Partie ML</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Il existe 2 types de services: la gestion et la formation. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Le service de formation collecte les données stockées dans le stockage S3 et commence la ré-formation des poids du modèle. </font><font style="vertical-align: inherit;">A la fin de la formation, des poids sont envoyés au service de gestion. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Le service de gestion n'est pas différent en termes de données d'entrée et de sortie de l'application de l'opérateur. </font><font style="vertical-align: inherit;">De même, le flux d'entrée RGBD (RGB + Depth), les lectures des capteurs et l'état du robot, les commandes de contrôle de sortie. </font><font style="vertical-align: inherit;">En raison de cette identité, il semble possible de se former dans le cadre du concept de «formation par les données».</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
L'état du robot (et les lectures des capteurs) est une histoire clé pour ML. Il définit le contexte. Par exemple, un robot aura une machine d'état qui est caractéristique de son fonctionnement, ce qui détermine en grande partie le type de contrôle nécessaire. Ces 2 valeurs sont transmises avec chaque trame: le mode de fonctionnement et le vecteur d'état du robot. </font></font><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Et un peu sur la formation:</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
dans la démonstration à la fin de l'article était la tâche de trouver un objet (un cube pour enfants) sur une scène 3D. Il s'agit d'une tâche de base pour les applications pick &amp; place. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La formation était basée sur une paire de cadres «avant et après» et une désignation de cible obtenue avec un contrôle manuel: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/n6/v2/s3/n6v2s3v59u7eumxhdh3gxucngxq.jpeg"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En raison de la présence de deux cartes de profondeur, il était facile de calculer le masque de l'objet déplacé dans le cadre: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/6a/vm/k-/6avmk-ue2jfndgwzy08zdb0qtqe.jpeg"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
De plus, xyz sont projetés sur le plan de la caméra et vous pouvez sélectionner le voisinage de l'objet capturé:</font></font><br>
<br>
<img src="https://habrastorage.org/webt/ha/_s/o-/ha_so-yh4xmxwsp4dkoo42tct9i.jpeg"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En fait avec ce quartier et fonctionnera. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nous obtenons d'abord XY en entraînant Unet sur un réseau convolutionnel pour la segmentation des cubes. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ensuite, nous devons déterminer la profondeur et comprendre si l'image est anormale devant nous. </font><font style="vertical-align: inherit;">Cela se fait en utilisant un encodeur automatique en RVB et un encodeur automatique conditionnel en profondeur. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Architecture de modèle pour la formation d'encodeur automatique: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/xf/q4/ex/xfq4exzon5b63cucrz6aqi8kflm.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En conséquence, la logique de travail:</font></font><br>
<br>
<ol>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">rechercher un maximum sur la «heat map» (déterminer les coordonnées angulaires u = x / zv = y / z de l'objet) qui dépasse le seuil</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">puis l'auto-encodeur reconstruit le voisinage du point trouvé pour toutes les hypothèses en profondeur (avec un pas donné de min_depth à max_depth) et sélectionne la profondeur à laquelle l'écart entre la reconstruction et l'entrée est minime</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ayant les coordonnées angulaires u, v et la profondeur, vous pouvez obtenir les coordonnées x, y, z</font></font></li>
</ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Un exemple de reconstruction d'auto-encodeur d'une carte de profondeurs de cube avec une profondeur correctement définie: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/av/1t/bu/av1tbu5dyvflda-_-lpbdw0t0qs.jpeg"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En partie, l'idée d'une méthode de recherche de profondeur est basée sur un </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">article sur les ensembles d'auto-encodeurs</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Cette approche fonctionne bien pour des objets de formes diverses. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Mais, en général, il existe de nombreuses approches différentes pour trouver un objet XYZ à partir d'une image RGBD. Bien entendu, il est nécessaire en pratique et sur une grande quantité de données de choisir la méthode la plus précise. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Il y avait aussi la tâche de détecter les anomalies, pour cela nous avons besoin d'un réseau convolutionnel de segmentation pour apprendre des masques disponibles. Ensuite, selon ce masque, vous pouvez évaluer la précision de la reconstruction de l'encodeur automatique dans la carte de profondeur et RVB. En raison de cet écart, on peut décider de la présence d'une anomalie.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Grâce à cette méthode, il est possible de détecter l'apparition d'objets précédemment invisibles dans la trame, qui sont néanmoins détectés par l'algorithme de recherche principal.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Manifestation</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La vérification et le débogage de l'ensemble de la plateforme logicielle créée ont été effectués sur le stand:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Caméra 3D Realsense D435</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">4 coordonnées Dobot Magicien</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Casque VR HTC Vive</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Serveurs sur Yandex Cloud (réduit la latence par rapport au cloud AWS)</font></font></li>
</ul><br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/G5rBhaxHL_E" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dans la vidéo, nous enseignons comment trouver un cube dans une scène 3D en effectuant une tâche dans VR pick &amp; place. </font><font style="vertical-align: inherit;">Une cinquantaine d'exemples suffisaient pour s'entraîner sur un cube. </font><font style="vertical-align: inherit;">Ensuite, l'objet change et environ 30 autres exemples sont affichés. </font><font style="vertical-align: inherit;">Après le recyclage, le robot peut trouver un nouvel objet. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
L'ensemble du processus a pris environ 15 minutes, dont environ la moitié du poids du modèle d'entraînement. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Et dans cette vidéo, YuMi contrôle en VR. </font><font style="vertical-align: inherit;">Pour apprendre à manipuler des objets, vous devez évaluer l'orientation et l'emplacement de l'outil. </font><font style="vertical-align: inherit;">Les mathématiques reposent sur un principe similaire, mais en sont maintenant au stade des tests et du développement.</font></font><br>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/Q0yTey1srBM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Conclusion</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Le Big Data et le Deep Learning ne sont pas tout. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nous modifions l'approche de l'apprentissage, évoluant vers la façon dont les gens apprennent de nouvelles choses - en répétant ce qu'ils voient. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
L'appareil mathématique «sous le capot», que nous développerons sur des applications réelles, vise le problème de l'interprétation et du contrôle contextuels. </font><font style="vertical-align: inherit;">Le contexte ici est des informations naturelles disponibles à partir de capteurs de robot ou des informations externes sur le processus actuel. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Et, plus nous maîtrisons les processus technologiques, plus la structure du «cerveau dans les nuages» sera développée et ses parties individuelles seront formées. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Points forts de cette approche:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">la possibilité d'apprendre à manipuler des objets variables </font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">apprendre dans un environnement en évolution (par exemple, robots mobiles)</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">tâches mal structurées</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">délai de mise sur le marché court; </font><font style="vertical-align: inherit;">Vous pouvez effectuer la cible même en mode manuel en utilisant les opérateurs</font></font><br>
</li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Limitation:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">besoin d'Internet fiable et bon</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">des méthodes supplémentaires sont nécessaires pour atteindre une grande précision, par exemple, des caméras dans le manipulateur lui-même</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nous travaillons actuellement sur l'application de notre approche à la tâche de sélection et de placement standard de divers objets. </font><font style="vertical-align: inherit;">Mais il nous semble (naturellement!) Qu'il est capable de plus. </font><font style="vertical-align: inherit;">Avez-vous d'autres idées pour vous essayer? </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Merci de votre attention!</font></font></div>
      
    </div><p class="reference-to-source js-reference-to-source">Source: https://habr.com/ru/post/undefined/</p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr486670/index.html">Passeport électronique de la Fédération de Russie, 2020e partie du ballet Marleson</a></li>
<li><a href="../fr486672/index.html">OpenVINO Hackathon: reconnaître la voix et l'émotion sur le Raspberry Pi</a></li>
<li><a href="../fr486674/index.html">Le condensé de matières fraîches du monde du front-end de la dernière semaine n ° 400 (27 janvier - 2 février 2020)</a></li>
<li><a href="../fr486676/index.html">Inévitabilité de la pénétration du FPGA dans les centres de données</a></li>
<li><a href="../fr486678/index.html">Quartz dans ASP.NET Core</a></li>
<li><a href="../fr486682/index.html">Docker Compose: simplifiez l'utilisation de Makefile</a></li>
<li><a href="../fr486684/index.html">Ma réponse à ceux qui croient que la valeur du TDD est exagérée</a></li>
<li><a href="../fr486686/index.html">À propos de l'implémentation d'une bibliothèque d'apprentissage en profondeur dans Python</a></li>
<li><a href="../fr486688/index.html">Node.js, Tor, Puppeteer et Cheerio: grattage web anonyme</a></li>
<li><a href="../fr486690/index.html">5 conseils pour écrire des fonctions fléchées de qualité</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>