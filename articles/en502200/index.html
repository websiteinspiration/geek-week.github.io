<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§±üèø üë∏üèæ üèóÔ∏è Entropy: how Decision Trees make decisions ‚ùå üê¢ üë®üèæ‚Äçüéì</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="A translation of the article was prepared ahead of the start of the Machine Learning course .
 
 
 
 You are a Data Science Specialist who is currentl...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Entropy: how Decision Trees make decisions</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/502200/"><i><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A translation of the article was prepared ahead of the start of </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">the Machine Learning</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> course </font><font style="vertical-align: inherit;">.</font></font></b></i><br>
<br>
<img src="https://habrastorage.org/webt/az/2h/3e/az2h3eq1jejcxtd0g4wi4gmamki.png"><br>
<hr><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
You are a Data Science Specialist who is currently following a learning path. And you have come a long way since you wrote your first line of code in Python or R. You know Scikit-Learn like the back of your hand. Now you are more sitting on Kaggle than on Facebook. You are not new to creating stunning random forests and other ensemble models of decision trees that do an excellent job. Nevertheless, you know that you will not achieve anything if you do not develop comprehensively. You want to dig deeper and understand the intricacies and concepts underlying the popular machine learning models. Well, me too.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Today I will talk about the concept of entropy - one of the most important topics in statistics, and later we will talk about the concept of Information Gain (information gain) and find out why these fundamental concepts form the basis of how decision trees are built from the data obtained.</font></font><a name="habracut"></a><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Good. Now let‚Äôs transgress. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
What is entropy? In simple terms, entropy is nothing but a measure of disorder. (It can also be considered a measure of purity, and soon you will see why. But I like the mess more because it sounds cooler.) The </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
mathematical formula of entropy is as follows: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ey/wa/u-/eywau-ntm5stedcuyrelbhhoipu.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Entropy. It is sometimes written as H.</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Here p </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">i</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> is the frequency probability of an element / class i of our data. For simplicity, suppose we have only two classes: positive and negative. Then i will take the value of either "+" or "-". If we had a total of 100 points in our dataset, 30 of which would belong to the positive class and 70 to the negative, then p </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> would be 3/10, and p</font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">-</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> will be 7/10. Everything is simple here. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
If I calculate the entropy of the classes from this example, then this is what I get using the formula above: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/5_/hh/20/5_hh20bihmp119n_5vzmlq_vuyw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Entropy is about 0.88. This value is considered quite high, that is, we have a high level of entropy or disorder (that is, a low value of purity). Entropy is measured in the range from 0 to 1. Depending on the number of classes in your dataset, the value of entropy may be more than 1, but it will mean the same as the level of disorder is extremely high. For simplicity of explanation, in today's article we will have entropy ranging from 0 to 1. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Take a look at the chart below.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/jt/sx/zs/jtsxzsfwwbstp10fqo-rd0ndddw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
On the X axis, the number of points from the positive class in each circle is reflected, and on the Y axis, the corresponding entropies. You can immediately notice the inverted U-shape of the graph. Entropy will be the smallest at extrema when there are no positive elements in the circle in principle, or when there are only positive elements in them. That is, when the same elements in the circle - the disorder will be 0. The entropy will be highest in the middle of the graph, where positive and negative elements will be evenly distributed inside the circle. Here the greatest entropy or disorder will be achieved, since there will be no predominant elements.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Is there any reason that the entropy is measured using the base 2 logarithm, or why is the entropy measured between 0 and 1, and not in a different range? No, there is no reason. This is just a metric. It‚Äôs not so important to understand why this is happening. It is important to know how what we got above is calculated and how it works. Entropy is a measure of confusion or uncertainty, and the goal of machine learning models and Data Science specialists in general is to reduce this uncertainty. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Now we know how mess is measured. Next, we need a value to measure the reduction of this disorder in the additional information (attributes / independent variables) of the target variable / class. This is where the Information Gain or Information Gain comes into play. From the point of view of mathematics, it can be written as follows:</font></font><br>
<br>
<img src="https://habrastorage.org/webt/bn/el/t4/bnelt40yxay8hkbanig088mpk6a.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 We simply subtract the entropy Y from X, from the entropy Y, in order to calculate the decrease in uncertainty about Y, provided that there is information about X about Y. The stronger the uncertainty decreases, the more information can be obtained from Y about X. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Let's look at a simple example of the contingency table so that get closer to the question of how decision trees use entropy and information gain to decide on what basis to break nodes in the learning process on data. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Example: Conjugation table</font></font></b><br>
<br>
<img src="https://habrastorage.org/webt/s4/ea/e5/s4eae57mpuehp3mk_mjpmikuan0.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
 Here, our target variable will be </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Liability</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , which can take only two values: </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ÄúNormal‚Äù</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> and </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ÄúHigh‚Äù</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. We also have only one sign, which is called Credit Rating, it distributes the values ‚Äã‚Äãinto three categories: </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ÄúExcellent‚Äù</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ÄúGood‚Äù</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> and </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ÄúPoor‚Äù</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . A total of 14 observations were made. 7 of them belong to the </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Normal Liability</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> class </font><font style="vertical-align: inherit;">, and 7 more to the </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">High Liability</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> class </font><font style="vertical-align: inherit;">. This is a division in itself. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
If we look at the total sum of the values ‚Äã‚Äãin the first row, we will see that we have 4 observations with </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Excellent</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> value </font><font style="vertical-align: inherit;">based on </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Credit Rating</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Moreover, I can even say that my target variable is broken by </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ÄúExcellent‚Äù Credit Rating</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Among the observations with the value </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ÄúExcellent‚Äù</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> by attribute</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Credit Rating</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , there are 3 that belong to the </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Normal Liability</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> class </font><font style="vertical-align: inherit;">and 1 that belongs to the </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">High Liability</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> class </font><font style="vertical-align: inherit;">. Similarly, I can calculate similar results for other </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Credit Rating</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> values </font><font style="vertical-align: inherit;">from the contingency table. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
For example, I use the contingency table above to independently calculate the entropy of our target variable, and then calculate its entropy, taking into account additional information from the </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Credit Rating</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> attribute </font><font style="vertical-align: inherit;">. So I can calculate how much additional information </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Credit Rating</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> will give me </font><font style="vertical-align: inherit;">for the </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Liability</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> target variable </font><font style="vertical-align: inherit;">. </font></font><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 So let's get started.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/2v/2y/lg/2v2ylghvtk-f-e0eom6aeocsb1q.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 The entropy of our target variable is 1, which means maximum clutter due to the even distribution of elements between </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ÄúNormal‚Äù</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> and </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ÄúHigh‚Äù</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . The next step is to calculate the entropy of the </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Liability</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> target variable </font><font style="vertical-align: inherit;">, taking into account additional information from </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Credit Rating</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . To do this, we calculate the </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Liability</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> entropy </font><font style="vertical-align: inherit;">for each </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Credit Rating</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> value </font><font style="vertical-align: inherit;">and add them using the average weighted observation ratio for each value. Why we use the weighted average will become clearer when we talk about decision trees. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/rt/_5/fh/rt_5fhldx4dfjcioh7d_uiori6e.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 We obtained the entropy of our target variable with the </font><i><font style="vertical-align: inherit;">Credit Rating</font></i><font style="vertical-align: inherit;"> attribute</font></font><i><font style="vertical-align: inherit;"></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Now we can calculate the informational </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Liability</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> gain </font><font style="vertical-align: inherit;">from </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Credit Rating</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> to understand how informative this feature is. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/9b/d8/xv/9bd8xviw3mxbiqj28zmdqbiaazs.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 Knowing </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Credit Rating has</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> helped us reduce the uncertainty of our </font><i><font style="vertical-align: inherit;">Liability</font></i><font style="vertical-align: inherit;"> target variable.</font></font><i><font style="vertical-align: inherit;"></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Isn't that a good sign that should work? Give us information about the target variable? Well, for this very reason, decision trees use entropy and informational gain. They determine by what criterion to break the nodes into branches, in order to approach the target variable with each subsequent partition, and also to understand when the tree construction needs to be completed! (in addition to hyperparameters such as maximum depth, of course). Let's see how this all works in the following example using decision trees. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Example: Decision Tree</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Let's look at an example of building a decision tree to predict whether a person‚Äôs credit will be written off or not. The population will be 30 copies. 16 will belong to the </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">write-off</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> class </font><font style="vertical-align: inherit;">, and the other 14 will</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ÄúNon-write-off‚Äù</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . We will have two signs, namely </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ÄúBalance‚Äù</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , which can take two values: ‚Äú&lt;50K‚Äù or ‚Äú&gt; 50K‚Äù, and </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ÄúResidence‚Äù</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , which takes three values: </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ÄúOWN‚Äù</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ÄúRENT‚Äù</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> or </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ÄúOTHER‚Äù</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . I will demonstrate how the decision tree algorithm will decide which attribute to break first and which attribute will be more informative, that is, it best eliminates the uncertainty of the target variable by using the concept of entropy and information gain. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Symptom 1: Balance</font></font></b><br>
<br>
<img src="https://habrastorage.org/webt/9b/d8/xv/9bd8xviw3mxbiqj28zmdqbiaazs.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
 Here the circles belong to the </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚Äúwrite-off‚Äù</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> class, and the stars correspond to the </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚Äúnon-write-off‚Äù</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> class </font><font style="vertical-align: inherit;">. Partitioning a Parent Root by Attribute</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Balance</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> will give us 2 heir nodes. In the left node there will be 13 observations, where 12/13 (probability 0.92) of observations from the </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚Äúwrite-off‚Äù</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> class </font><font style="vertical-align: inherit;">, and only 1/13 (probability 0.08) of observations from the class </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚Äúnon-write-off‚Äù</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . In the right node there will be 17 out of 30 observations, where 13/17 (probability 0.76) of observations from the </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚Äúwrite-off‚Äù</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> class </font><font style="vertical-align: inherit;">and 4/17 (probability 0.24) of observations from the class </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚Äúnon-write-off‚Äù</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Let's calculate the entropy of the root and see how much the tree can reduce the uncertainty by using a partition based on </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Balance</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br>
<img src="https://habrastorage.org/webt/yq/ke/do/yqkedojc2s80__h-vqqcptzewai.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 A split based on </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Balance</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> will give an informational gain of 0.37. Let's count the same for the </font><i><font style="vertical-align: inherit;">Residence</font></i><font style="vertical-align: inherit;"> sign</font></font><i><font style="vertical-align: inherit;"></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">and compare the results. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Symptom 2: Residence</font></font></b><br>
<br>
<img src="https://habrastorage.org/webt/mx/tm/sd/mxtmsdt2hm0mamxkdxzqnb9v7mg.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
 Splitting a tree based on </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Residence</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> will give you 3 heir nodes. The left descendant node will receive 8 observations, where 7/8 (probability 0.88) of observations from the </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">write-off</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> class </font><font style="vertical-align: inherit;">and only 1/8 (probability 0.12) of observations from the </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">non-write-off</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> class </font><font style="vertical-align: inherit;">. The average successor node will receive 10 observations, where 4/10 (probability 0.4) of observations from the </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">write-off</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> class </font><font style="vertical-align: inherit;">and 6/10 (probability 0.6) of observations from the </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">non-write-off</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> class </font><font style="vertical-align: inherit;">. The right heir will receive 12 observations, where 5/12 (probability 0.42) of observations from the </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">write-off</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> class </font><font style="vertical-align: inherit;">and 7/12 (probability 0.58) of observations from the </font><i><font style="vertical-align: inherit;">non-write-off</font></i><font style="vertical-align: inherit;"> class</font></font><i><font style="vertical-align: inherit;"></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. We already know the entropy of the parent node, so we simply calculate the entropy after the partition to understand the informational gain from the </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Residence</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> attribute </font><font style="vertical-align: inherit;">. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/cb/zt/zf/cbztzffw12-wkj6cjfayt_jzlcq.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 The informational gain from the </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Balance</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> attribute is </font><font style="vertical-align: inherit;">almost 3 times more than from the </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Residence</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ! If you look at the graphs again, you will see that partitioning according to </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Balance</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> will give cleaner descendant nodes than according to </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Residence</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . However, the leftmost node in </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Residence is</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> also quite clean, but it is here that the weighted average comes into play. Despite the fact that the node is clean, it has the least number of observations, and its result is lost in the general recalculation and calculation of the total entropy according to </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Residence</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. This is important because we are looking for the general informational content of the attribute and do not want the final result to be distorted by the rare value of the attribute. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Balance</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> attribute itself </font><font style="vertical-align: inherit;">provides more information about the target variable than </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Residence</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Thus, the entropy of our target variable is reduced. The decision tree algorithm uses this result to make the first split according to </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Balance</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">to later decide on what basis to break the following nodes. In the real world, when there are more than two features, the first breakdown occurs according to the most informative feature, and then, with each subsequent breakup, the information gain will be recounted for each additional feature, since it will not be the same as the information gain from each feature individually. Entropy and informational gain should be calculated after one or several partitions have occurred, which will affect the final result. The decision tree will repeat this process as it grows in depth, until it either reaches a certain depth or some kind of splitting leads to a higher informational gain beyond a certain threshold, which can also be specified as a hyperparameter!</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
That's all! </font><font style="vertical-align: inherit;">Now you know what entropy, information gain and how they are calculated. </font><font style="vertical-align: inherit;">Now you understand how the decision tree, by itself or as part of an ensemble, makes decisions about the best order of partitioning by attributes and decides when to stop when learning the available data. </font><font style="vertical-align: inherit;">Well, if you have to explain to someone how decision trees work, I hope you will adequately cope with this task. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
I hope you have learned something useful for yourself from this article. </font><font style="vertical-align: inherit;">If I missed something or expressed myself inaccurately, write to me about it. </font><font style="vertical-align: inherit;">I'll be very grateful to you! </font><font style="vertical-align: inherit;">Thank.</font></font><br>
<br>
<hr><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Learn more about the course.</font></font></a><br>
<br>
<hr></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../en502178/index.html">oVirt in 2 hours. Part 3. Advanced settings</a></li>
<li><a href="../en502180/index.html">The day the perimeter disappeared. Security Solutions from Microsoft and Partners</a></li>
<li><a href="../en502182/index.html">Again about MikroTik or the long-awaited SOCKS5</a></li>
<li><a href="../en502186/index.html">Webinar. Information Security: Quarantined SOC</a></li>
<li><a href="../en502196/index.html">–í –ø–æ–¥—Ö–æ–¥–µ –∫ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ —Å—Ç–æ–ª–µ—Ç–Ω–µ–π –¥–∞–≤–Ω–æ—Å—Ç–∏ –Ω–∞–π–¥–µ–Ω—ã –Ω–æ–≤—ã–µ –∫–ª—é—á–∏ –∫ —Ä–∞–∑–≥–∞–¥–∫–µ –ø—Ä–∏—Ä–æ–¥—ã –≤—Ä–µ–º–µ–Ω–∏</a></li>
<li><a href="../en502202/index.html">The development of unmanned technology in railway transport</a></li>
<li><a href="../en502204/index.html">Writing @SpringBootTest tests when using Spring Shell in an application</a></li>
<li><a href="../en502206/index.html">Yandex recorded the sounds of retrocomputers</a></li>
<li><a href="../en502208/index.html">Chrome extension to hide distracting recommendations on YouTube</a></li>
<li><a href="../en502234/index.html">–ò–Ω—Å–∞–π–¥—ã –æ—Ç —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞ Facebook: –∫–∞–∫ –ø–æ–ø–∞—Å—Ç—å –Ω–∞ —Å—Ç–∞–∂–∏—Ä–æ–≤–∫—É, –ø–æ–ª—É—á–∏—Ç—å –æ—Ñ—Ñ–µ—Ä –∏ –≤—Å–µ –æ —Ä–∞–±–æ—Ç–µ –≤ –∫–æ–º–ø–∞–Ω–∏–∏</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>