<!doctype html>
<html class="no-js" lang="ja">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👨‍💼 👱🏿 🦇 Kerasでのダミーと実装への注意 ↖️ 🔦 🙃</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="ロシア語の人工知能に関する記事について 
 注意メカニズムが英語の文献で説明されているという事実にもかかわらず、私はまだロシア語圏でこの技術のまともな説明を見ていません。私たちの言語の人工知能（AI）に関する記事はたくさんあります。ただし、見つかった記事では、たたみ込みネットワーク、生成ネットワーク...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Kerasでのダミーと実装への注意</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/458992/"><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ロシア語の人工知能に関する記事について </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
注意メカニズムが英語の文献で説明されているという事実にもかかわらず、私はまだロシア語圏でこの技術のまともな説明を見ていません。私たちの言語の人工知能（AI）に関する記事はたくさんあります。ただし、見つかった記事では、たたみ込みネットワーク、生成ネットワークなど、最も単純なAIモデルしか明らかにされていません。ただし、AIの分野における最先端の最新動向によれば、ロシア語圏の記事はほとんどありません。</font></font><br>
<br>
<a name="habracut"></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">私がトピックを入力し、AIの分野の現状を調査したとき、最新の進展に関するロシア語の記事の欠如が私にとって問題になりました。私は英語をよく知っています。AIトピックについて英語で記事を読みます。しかし、AIの新しい概念や新しい原則が出てくると、外国語での理解は苦痛で長くなる可能性があります。英語を知っている、複雑なオブジェクトで非ネイティブに侵入することは、まだはるかに多くの努力と時間の価値があります。説明を読んだ後、あなたは自分自身に質問をします：あなたは何パーセント理解しましたか？ロシア語の記事があれば、最初に読んだ後で100％理解できます。これは、一連の優れた記事がある生成ネットワークで起こりました。すべてを読んだ後、すべてが明らかになりました。しかし、ネットワークの世界では、英語でしか説明されておらず、何日も対処しなければならなかった多くのアプローチがあります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
私は母国語で定期的に記事を書き、私たちの言語分野に知識をもたらします。</font><font style="vertical-align: inherit;">ご存知のように、トピックを理解する最良の方法は、誰かにそれを説明することです。</font><font style="vertical-align: inherit;">それで、他に誰が私が最も近代的で複雑な、高度な建築AIに関する一連の記事を始めるべきか。</font><font style="vertical-align: inherit;">記事の終わりまでに、私自身が100％のアプローチを理解し、理解を読み、改善する人に役立ちます（ちなみに、私はGesserが大好きですが、** Blanche de bruxelles **）。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
あなたが主題を理解するとき、理解の4つのレベルがあります：</font></font><br>
<br>
<ol>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">アルゴリズム/レベルの原理と入力と出力を理解している</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ギャザリングの出口と、一般的な用語でどのように機能するかを理解している</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">上記のすべて、および各ネットワークレベルのデバイスを理解している（たとえば、VAEモデルでは、原理を理解しており、再パラメーター化トリックの本質も理解している）</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">各レベルを含むすべてを理解し、すべてが学習する理由も理解しました。同時に、既製のソリューションをコピーアンドペーストするのではなく、タスクにハイパーパラメータを選択できるようになりました。</font></font></li>
</ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
新しいアーキテクチャの場合、レベル1からレベル4への移行はしばしば困難です。著者は、さまざまな重要な詳細を表面的に詳しく説明していることを強調します（彼ら自身が理解したのでしょうか？）。</font><font style="vertical-align: inherit;">または、あなたの脳には構造が含まれていないため、説明を読んでも解読されず、スキルに変化しませんでした。</font><font style="vertical-align: inherit;">これは、学生時代に、あなたが適切なマットを与えたナイトパーティーafterの後に、同じマタンレッスンで眠った場合に起こります。</font><font style="vertical-align: inherit;">装置。</font><font style="vertical-align: inherit;">そしてここに、各操作のニュアンスと微妙さを明らかにする母国語の記事が必要です。</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">注意の概念と応用</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
上記は理解度のシナリオです。注意を解析するには、レベル1から始めましょう。入力と出力を説明する前に、本質を分析します。つまり、子供でも理解できる基本的な概念は、この概念に基づいています。この記事では、英語の用語であるAttentionを使用します。これは、この形式ではKerasライブラリ関数の呼び出しでもあるためです（直接実装されていないため、追加のモジュールが必要ですが、以下で詳しく説明します）。さらに読むには、ソースコードが提供されるため、Kerasおよびpythonライブラリについて理解している必要があります。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/lf/nv/-a/lfnv-ayy8tlpfgkiwcrgiinkme4.png" align="right"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">注意は英語から「注意」に変換されます。この用語は、アプローチの本質を正しく説明しています。あなたが運転手で、交通警察の将軍が写真に写っている場合、写真のコンテキストに関係なく、直感的にそれを重視します。将軍をよく見ると思います。あなたは目を疲れさせ、ショルダーストラップを注意深く見ます：彼が特にそこに持っている星の数。将軍があまり背が高くないなら、彼を無視してください。それ以外の場合は、意思決定の重要な要素としてそれを考慮に入れてください。これが私たちの脳の働きです。ロシアの文化では、私たちは世代によって高いランクに注意を払うように訓練されてきました、私たちの脳は自動的にそのようなオブジェクトに高い優先順位を置きます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
注意とは、ネットワークに、もっと注意すべき点を伝える方法です。つまり、ニューロンの状態と入力データに応じて、特定の結果の可能性を報告します。 Keras自体に実装されたアテンションレイヤーは、トレーニングサンプルに基づいて要素を識別します。これにより、ネットワークエラーが減少します。重要な要素の識別は、畳み込みネットワークでこれを行う方法と同様に、エラーの逆伝播の方法で実行されます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
トレーニングでは、注意がその確率的な性質を示します。メカニズム自体が重要度スケールのマトリックスを形成します。注意力を訓練していなかった場合、たとえば経験的に重要度を設定できた可能性があります（一般的なものは、旗よりも重要です）。しかし、データでネットワークをトレーニングする場合、重要性は、ネットワークの入力で受信したデータに応じて、結果の確率の関数になります。たとえば、ロシア帝国に住んでいるときに将軍に会った場合、ガントレットを手に入れる確率は高くなります。これを確認したら、統計を収集するいくつかの個人的な会議を通じてそれが可能になるでしょう。その後、私たちの脳は、この主題に会うという事実に適切な重みを置き、ショルダーストラップとストライプにマーカーを置きます。セットマーカーは確率ではないことに注意してください：将軍の会合はあなたにとってそれとはまったく異なる結果を伴います、さらに、重量は複数になる場合があります。ただし、正規化することで、重みを確率まで下げることができます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
学習における注意メカニズムの確率的な性質は、機械翻訳タスクで明らかになります。たとえば、ロシア語から英語に翻訳する場合、Loveという単語は90％の場合はLoveとして、9％の場合はSexとして、1％の場合はその他として翻訳されることをネットワークに通知します。ネットワークはすぐに多くのオプションをマークし、最高のトレーニング品質を示します。翻訳するときは、ネットワークに次のように伝えます。「愛という言葉を翻訳するときは、英語の愛に特別な注意を払い、それがまだセックスであるかどうかも確認してください。」</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
アテンションアプローチは、テキストおよびサウンドと時系列の操作に適用されます。テキスト処理には、リカレントニューラルネットワーク（RNN、LSTM、GRU）が広く使用されています。注意はそれらを補完するか、またはそれらを置き換えることができ、ネットワークをより単純でより高速なアーキテクチャに移動します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
注意の最も有名な用途の1つは、それを使用して、反復ネットワークを放棄し、完全に接続されたモデルに移行することです。リカレントネットワークには一連の欠点があります。GPUでトレーニングを提供できないこと、高速で近づいてくるリトレーニングです。注意メカニズムを使用して、完全に接続されたネットワークに基づいてシーケンスを学習できるネットワークを構築し、GPUでトレーニングし、droputを使用できます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
注意は、たとえば言語から言語への翻訳の分野で、反復ネットワークのパフォーマンスを向上させるために広く使用されています。エンコーディング/デコーディングアプローチを使用する場合、これは最近のAI（バリエーションオートエンコーダーなど）でよく使用されます。エンコーダーとデコーダーの間にアテンションレイヤーを追加すると、ネットワークのパフォーマンスが著しく向上します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
この記事では、アテンションを使用した特定のネットワークアーキテクチャについては触れませんが、これは別の作業の対象になります。</font><font style="vertical-align: inherit;">注意のすべての可能な使用法のリストは、別の記事に値する。</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">すぐに使えるKerasでの注意の実装</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
どのようなアプローチを理解したら、基本原則を学ぶことは非常に役立ちます。</font><font style="vertical-align: inherit;">しかし、多くの場合、完全な理解は技術的な実装を見ることによってのみ得られます。</font><font style="vertical-align: inherit;">操作の機能を構成するデータストリームが表示され、正確に計算されるものが明確になります。</font><font style="vertical-align: inherit;">しかし、最初にそれを実行して、「注意こんにちは」と書く必要があります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
現在、Keras自体には注意が実装されていません。</font><font style="vertical-align: inherit;">しかし</font><font style="vertical-align: inherit;">、githubでインストールできる</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">アテンションケラ</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">などのサードパーティの実装はすでに存在してい</font><font style="vertical-align: inherit;">ます。</font><font style="vertical-align: inherit;">その後、コードは非常に単純になります。</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> attention_keras.layers.attention <span class="hljs-keyword">import</span> AttentionLayer<font></font>
attn_layer = AttentionLayer(name=<span class="hljs-string">'attention_layer'</span>)<font></font>
attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
この実装は、注意スケールの視覚化機能をサポートしています。</font><font style="vertical-align: inherit;">アテンションをトレーニングすると、マトリックスシグナリングを取得できます。これは、ネットワークによれば、このタイプについて特に重要です（attention-kerasライブラリページのgithubからの画像）。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/ju/hr/yj/juhryjvb8eedahp2h77q07p_pwm.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
基本的に、他には何も必要ありません。このコードをレベルの1つとしてネットワークに接続し、ネットワークの学習を楽しんでください。</font><font style="vertical-align: inherit;">ネットワークやアルゴリズムは、最初の段階で概念的なレベルで設計され（ちなみにデータベースなど）、その後、実装は実装前に論理的および物理的な表現で指定されます。</font><font style="vertical-align: inherit;">ニューラルネットワークの場合、この設計方法はまだ開発されていません（そうですね、これは次の記事のトピックになります）。</font><font style="vertical-align: inherit;">畳み込みレイヤーが内部でどのように機能するか理解していませんか？</font><font style="vertical-align: inherit;">原理を説明し、使用します。</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">注目度の低いKeras実装</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
最後にトピックを理解するために、以下では、フードの下でアテンションの実装を詳細に分析します。</font><font style="vertical-align: inherit;">コンセプトは良いですが、それはどのように正確に機能し、なぜ結果は述べたとおり正確なのですか？</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Kerasでのアテンションメカニズムの最も単純な実装は、たった3行です。</font></font><br>
<br>
<pre><code class="python hljs">inputs = Input(shape=(input_dims,))<font></font>
attention_probs = Dense(input_dims, activation=<span class="hljs-string">'softmax'</span>, name=<span class="hljs-string">'attention_probs'</span>)(inputs)<font></font>
attention_mul = merge([inputs, attention_probs], output_shape=<span class="hljs-number">32</span>, name=<span class="hljs-string">'attention_mul'</span>, mode=<span class="hljs-string">'mul'</span>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
この場合、入力層は最初の行で宣言され、次に、最初の層の要素の数に等しいニューロンの数を持つsoftmaxアクティベーション関数を持つ完全に接続された層になります。</font><font style="vertical-align: inherit;">3番目のレイヤーは、完全に接続されたレイヤーの結果に、入力データを要素ごとに乗算します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
以下は、少し複雑な自己注意メカニズムを実装するAttentionクラス全体です。これは、モデルの本格的なレベルとして使用でき、クラスはKerasレイヤークラスを継承します。</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-comment"># Attention</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Attention</span>(<span class="hljs-params">Layer</span>):</span>
      <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, step_dim,
                   W_regularizer=None, b_regularizer=None,
                   W_constraint=None, b_constraint=None,
                   bias=True, **kwargs</span>):</span>
          self.supports_masking = <span class="hljs-literal">True</span>
          self.init = initializers.get(<span class="hljs-string">'glorot_uniform'</span>)<font></font>
<font></font>
          self.W_regularizer = regularizers.get(W_regularizer)<font></font>
          self.b_regularizer = regularizers.get(b_regularizer)<font></font>
<font></font>
          self.W_constraint = constraints.get(W_constraint)<font></font>
          self.b_constraint = constraints.get(b_constraint)<font></font>
<font></font>
          self.bias = bias<font></font>
          self.step_dim = step_dim<font></font>
          self.features_dim = <span class="hljs-number">0</span><font></font>
          super(Attention, self).__init__(**kwargs)<font></font>
<font></font>
      <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">build</span>(<span class="hljs-params">self, input_shape</span>):</span>
          <span class="hljs-keyword">assert</span> len(input_shape) == <span class="hljs-number">3</span><font></font>
<font></font>
          self.W = self.add_weight((input_shape[<span class="hljs-number">-1</span>],),<font></font>
                                   initializer=self.init,<font></font>
                                   name=<span class="hljs-string">'{}_W'</span>.format(self.name),<font></font>
                                   regularizer=self.W_regularizer,<font></font>
                                   constraint=self.W_constraint)<font></font>
          self.features_dim = input_shape[<span class="hljs-number">-1</span>]<font></font>
<font></font>
          <span class="hljs-keyword">if</span> self.bias:<font></font>
              self.b = self.add_weight((input_shape[<span class="hljs-number">1</span>],),<font></font>
                                       initializer=<span class="hljs-string">'zero'</span>,<font></font>
                                       name=<span class="hljs-string">'{}_b'</span>.format(self.name),<font></font>
                                       regularizer=self.b_regularizer,<font></font>
                                       constraint=self.b_constraint)<font></font>
          <span class="hljs-keyword">else</span>:<font></font>
              self.b = <span class="hljs-literal">None</span><font></font>
<font></font>
          self.built = <span class="hljs-literal">True</span><font></font>
<font></font>
      <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">compute_mask</span>(<span class="hljs-params">self, input, input_mask=None</span>):</span>
          <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><font></font>
<font></font>
      <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">call</span>(<span class="hljs-params">self, x, mask=None</span>):</span><font></font>
          features_dim = self.features_dim<font></font>
          step_dim = self.step_dim<font></font>
          eij = K.reshape(K.dot(K.reshape(x, (<span class="hljs-number">-1</span>, features_dim)),                           K.reshape(self.W, (features_dim, <span class="hljs-number">1</span>))), (<span class="hljs-number">-1</span>, step_dim))
          <span class="hljs-keyword">if</span> self.bias:<font></font>
              eij += self.b<font></font>
          eij = K.tanh(eij)<font></font>
          a = K.exp(eij)<font></font>
<font></font>
          <span class="hljs-keyword">if</span> mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<font></font>
              a *= K.cast(mask, K.floatx())<font></font>
			<font></font>
          a /= K.cast(K.sum(a, axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-literal">True</span>) + K.epsilon(), K.floatx())<font></font>
<font></font>
          a = K.expand_dims(a)<font></font>
          weighted_input = x * a <font></font>
          <span class="hljs-keyword">return</span> K.sum(weighted_input, axis=<span class="hljs-number">1</span>)<font></font>
<font></font>
      <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">compute_output_shape</span>(<span class="hljs-params">self, input_shape</span>):</span>
          <span class="hljs-keyword">return</span> input_shape[<span class="hljs-number">0</span>],  self.features_dim
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ここでは、完全に接続されたKerasレイヤーを介して上記で実装されたものとほぼ同じものが表示され、下位レベルのより深いロジックによってのみ実行されます。関数内にパラメトリックレベル（self.W）が作成され、それが入力ベクトルとスカラー倍（K.dot）されます。このバリアントのワイヤードロジックはもう少し複雑です：シフト（バイアスパラメーターが開示されている場合）、双曲線正接、露出、マスク（指定されている場合）、正規化が入力ベクトルに適用され、self.Wの重みが乗算されます。得られた結果。この例で規定されているロジックの説明はありませんが、コードを読み取る操作を再現しています。ちなみに、このロジックで数学的な高レベル関数を認識した場合は、コメントに書き込んでください。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
クラスには「バイアス」というパラメータがあります。</font><font style="vertical-align: inherit;">バイアス。</font><font style="vertical-align: inherit;">パラメーターがアクティブになっている場合、高密度レイヤーを適用した後、最終パラメーターがレイヤーパラメーター「self.b」のベクターに追加されます。これにより、アテンション関数の「重み」を決定できるだけでなく、アテンションレベルを数値でシフトすることもできます。</font><font style="vertical-align: inherit;">人生の例：私たちは幽霊を恐れていますが、幽霊に会ったことはありません。</font><font style="vertical-align: inherit;">したがって、私たちは恐怖を-100ポイント修正します。</font><font style="vertical-align: inherit;">つまり、恐怖が100ポイントのスケールから外れた場合にのみ、ゴーストからの保護、ゴーストバスティング代理店への電話、反発装置の購入などを決定します。</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">結論</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
注意メカニズムにはバリエーションがあります。上記のクラスで実装されている最も単純な注意オプションは、自己注意と呼ばれます。自己注意は、各タイムスタンプのコンテキストを考慮して、シーケンシャルデータを処理するように設計されたメカニズムです。テキスト情報の操作に最もよく使用されます。 keras-self-attentionライブラリをインポートすることで、自己アテンション実装をそのまま使用できます。注意には他にもバリエーションがあります。英語の教材を研究したところ、5種類以上のバリエーションを数えることができました。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
この比較的短い記事でさえ、10以上の英語の記事を勉強しました。</font><font style="vertical-align: inherit;">もちろん、これらすべての記事のすべてのデータを5ページにダウンロードすることはできませんでした。「ダミーのガイド」を作成するためにスクイーズを作成しました。</font><font style="vertical-align: inherit;">注意メカニズムのすべてのニュアンスを理解するには、150〜200ページの本が必要です。</font><font style="vertical-align: inherit;">機械学習を理解し始めたばかりの人がこれがどのように機能するかを理解できるように、このメカニズムの基本的な本質を明らかにできたことを本当に望んでいます。</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">出典</font></font></h2><br>
<ol>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Kerasを使用したニューラルネットワークの注意メカニズム</font></font></a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Kerasによるディープネットワークでの注意</font></font></a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Kerasにおける注意ベースのシーケンス間</font></font></a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Kerasでの注意メカニズムを使用したテキスト分類</font></font></a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">広範な注意：シーケンス間予測のための2D畳み込みニューラルネットワーク</font></font></a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Kerasでアテンションレイヤーを実装する方法？</font></font></a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">注意？</font><font style="vertical-align: inherit;">注意！</font></font></a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">注目のニューラル機械翻訳</font></font></a></li>
</ol></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../ja458982/index.html">Nginxレシピ：HTMLおよびURLからPDFおよびPSへの変換</a></li>
<li><a href="../ja458984/index.html">取引所で取引するための最初のアプリケーションを作成する方法：3つの初期ステップ</a></li>
<li><a href="../ja458986/index.html">PostgreSQLレシピ：HTMLおよびURLからPDFおよびPSへの変換</a></li>
<li><a href="../ja458988/index.html">テクスチャリング、またはSurfaceアーティストになるために知っておくべきこと。パート4.モデル、法線、スイープ</a></li>
<li><a href="../ja458990/index.html">コード内のコメントで熱心に停止する</a></li>
<li><a href="../ja458994/index.html">Raspberry Pi + CentOS = Wi-Fi Hotspot（またはRed HatのRaspberry Router）</a></li>
<li><a href="../ja458996/index.html">ユーザーインターフェース-ユーザーを苦しめない方法</a></li>
<li><a href="../ja459000/index.html">Halo 2を改善しようとしたが、ほとんど台無しにされた方法</a></li>
<li><a href="../ja459002/index.html">HTTPSの構成方法-SSL構成ジェネレーターが役立ちます</a></li>
<li><a href="../ja459004/index.html">グラスホッパー暗号アルゴリズム：ほぼ複雑</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>