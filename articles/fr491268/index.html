<!doctype html>
<html class="no-js" lang="fr">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üèáüèø üë®üèæ‚Äçüè≠ üöå M√©thodes de Monte Carlo pour les cha√Ænes de Markov (MCMC). introduction üë≤üèæ ü§≥üèΩ üôáüèø</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bonjour, Habr! 
 
 Nous vous rappelons que nous avons annonc√© plus t√¥t le livre " Machine Learning Without Extra Words " - et maintenant il est d√©j√† e...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>M√©thodes de Monte Carlo pour les cha√Ænes de Markov (MCMC). introduction</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/491268/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bonjour, Habr! </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nous vous rappelons que nous avons annonc√© plus t√¥t le livre " </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Machine Learning Without Extra Words</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> " - et maintenant il est d√©j√† </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">en vente</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Malgr√© le fait que pour les d√©butants en MO, le livre puisse en effet devenir un </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ordinateur de bureau</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , certains sujets n'y √©taient toujours pas abord√©s. Par cons√©quent, nous proposons √† toutes les personnes int√©ress√©es une traduction d'un article de Simon Kerstens sur l'essence des algorithmes </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">MCMC</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> avec la mise en ≈ìuvre d'un tel algorithme en Python.</font></font><br>
<a name="habracut"></a> <br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Les m√©thodes de Monte Carlo pour les cha√Ænes de Markov</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (MCMC) sont une classe puissante de m√©thodes d'√©chantillonnage √† partir de distributions de probabilit√©s connues uniquement jusqu'√† une certaine constante de normalisation (inconnue). </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Cependant, avant de plonger dans le MCMC, voyons pourquoi vous pourriez m√™me avoir besoin de faire une telle s√©lection. La r√©ponse est: vous pouvez √™tre int√©ress√© soit par les √©chantillons eux-m√™mes de l'√©chantillon (par exemple, pour d√©terminer des param√®tres inconnus en utilisant la m√©thode de d√©rivation bay√©sienne), soit pour approximer les valeurs attendues des fonctions par rapport √† la distribution de probabilit√© (par exemple, pour calculer les quantit√©s thermodynamiques √† partir de la distribution des √©tats en physique statistique). Parfois, nous ne nous int√©ressons qu'au mode de distribution des probabilit√©s. Dans ce cas, nous l'obtenons par la m√©thode d'optimisation num√©rique, il n'est donc pas n√©cessaire de faire une s√©lection compl√®te.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Il s'av√®re que l'√©chantillonnage de toutes les distributions de probabilit√©s, √† l'exception des plus primitives, est une t√¢che difficile. </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La m√©thode de transformation inverse</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> est une technique √©l√©mentaire d'√©chantillonnage √† partir de distributions de probabilit√©s, qui n√©cessite cependant l'utilisation d'une fonction de distribution cumulative, et pour l'utiliser, √† son tour, vous devez conna√Ætre la constante de normalisation, qui est g√©n√©ralement inconnue. En principe, une constante de normalisation peut √™tre obtenue par int√©gration num√©rique, mais cette m√©thode devient rapidement impraticable avec une augmentation du nombre de dimensions. </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">√âchantillonnage par d√©viation</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Il ne n√©cessite pas une distribution normalis√©e, mais pour le mettre en ≈ìuvre efficacement, il faut beaucoup de choses sur la distribution qui nous int√©resse. De plus, cette m√©thode souffre gravement de la mal√©diction des dimensions - cela signifie que son efficacit√© diminue rapidement avec une augmentation du nombre de variables. C'est pourquoi vous devez organiser intelligemment la r√©ception d'√©chantillons repr√©sentatifs de votre distribution - sans avoir besoin de conna√Ætre la constante de normalisation. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Les algorithmes MCMC sont une classe de m√©thodes con√ßues sp√©cifiquement pour cela. Ils reviennent √† l' </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">article historique de Metropolis et d'autres</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ; Metropolis a d√©velopp√© le premier algorithme MCMC nomm√© d' </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">apr√®s lui</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">et con√ßu pour calculer l'√©tat d'√©quilibre d'un syst√®me bidimensionnel de sph√®res dures. En fait, les chercheurs recherchaient une m√©thode universelle qui nous permettrait de calculer les valeurs attendues en physique statistique. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Cet article couvrira les bases de l'√©chantillonnage MCMC. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">CHA√éNES DE MARKOV</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Maintenant que nous comprenons pourquoi nous devons √©chantillonner, passons au c≈ìur de MCMC: les cha√Ænes de Markov. Qu'est-ce qu'une cha√Æne Markov? Sans entrer dans les d√©tails techniques, nous pouvons dire qu'une cha√Æne de Markov est une s√©quence al√©atoire d'√©tats dans un certain espace d'√©tat, o√π la probabilit√© de choisir un certain √©tat d√©pend uniquement de l'√©tat actuel de la cha√Æne, mais pas de son histoire pass√©e: cette cha√Æne est d√©pourvue de m√©moire. Dans certaines conditions, une cha√Æne de Markov a une distribution stationnaire unique d'√©tats, vers laquelle elle converge, surmontant un certain nombre d'√©tats. Apr√®s un tel nombre d'√©tats dans une cha√Æne de Markov, une distribution invariante est obtenue. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pour √©chantillonner √† partir d'une distribution, </font></font><img src="https://habrastorage.org/webt/wb/sl/yf/wbslyf0r1hu5bjl1gbu7pyr3u48.png"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l'algorithme MCMC cr√©e et simule une cha√Æne de Markov dont la distribution stationnaire est</font></font><img src="https://habrastorage.org/webt/wb/sl/yf/wbslyf0r1hu5bjl1gbu7pyr3u48.png"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">; cela signifie qu'apr√®s la p√©riode initiale de ¬´graine¬ª, les √©tats d'une telle cha√Æne de Markov sont distribu√©s selon le principe </font></font><img src="https://habrastorage.org/webt/wb/sl/yf/wbslyf0r1hu5bjl1gbu7pyr3u48.png"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Par cons√©quent, nous n'aurons qu'√† enregistrer l'√©tat afin d'en obtenir des √©chantillons </font></font><img src="https://habrastorage.org/webt/wb/sl/yf/wbslyf0r1hu5bjl1gbu7pyr3u48.png"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
√Ä des fins √©ducatives, consid√©rons √† la fois un espace d'√©tat discret et un ¬´temps¬ª discret. La quantit√© cl√© caract√©risant une cha√Æne de Markov est un op√©rateur de transition </font></font><img src="https://habrastorage.org/webt/5q/fu/fx/5qfufxlugigeusphytdet38q580.png"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">indiquant la probabilit√© d'√™tre dans un √©tat </font></font><img src="https://habrastorage.org/webt/gs/gz/hz/gsgzhzrwlbmvbexpmq21j8u1dbu.png"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">√† un moment </font></font><img src="https://habrastorage.org/webt/cj/ut/o7/cjuto7hmum9a2ki6gyy23a86jb0.png"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, √† condition que la cha√Æne soit dans un √©tat </font></font><img src="https://habrastorage.org/webt/cj/ut/o7/cjuto7hmum9a2ki6gyy23a86jb0.png"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">au moment </font></font><code>i</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Maintenant juste pour le plaisir (et comme d√©monstration) tissons rapidement une cha√Æne Markov avec une distribution stationnaire unique. Commen√ßons par quelques importations et param√®tres pour les graphiques:</font></font><br>
<br>
<pre><code class="python hljs">%matplotlib notebook<font></font>
%matplotlib inline<font></font>
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<font></font>
plt.rcParams[<span class="hljs-string">'figure.figsize'</span>] = [<span class="hljs-number">10</span>, <span class="hljs-number">6</span>]<font></font>
np.random.seed(<span class="hljs-number">42</span>)</code></pre><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La cha√Æne de Markov fera le tour de l'espace d'√©tat discret form√© par trois conditions m√©t√©orologiques: </font></font><br>
<br>
<pre><code class="python hljs">state_space = (<span class="hljs-string">"sunny"</span>, <span class="hljs-string">"cloudy"</span>, <span class="hljs-string">"rainy"</span>)</code></pre><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dans un espace d'√©tats discret, l'op√©rateur de transition n'est qu'une matrice. </font><font style="vertical-align: inherit;">Dans notre cas, les colonnes et les lignes correspondent √† un temps ensoleill√©, nuageux et pluvieux. </font><font style="vertical-align: inherit;">Choisissons des valeurs relativement raisonnables pour les probabilit√©s de toutes les transitions:</font></font><br>
<br>
<pre><code class="python hljs">transition_matrix = np.array(((<span class="hljs-number">0.6</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">0.1</span>),<font></font>
                              (<span class="hljs-number">0.3</span>, <span class="hljs-number">0.4</span>, <span class="hljs-number">0.3</span>),<font></font>
                              (<span class="hljs-number">0.2</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">0.5</span>)))</code></pre><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Les lignes indiquent les √©tats dans lesquels le circuit peut actuellement √™tre situ√©, et les colonnes indiquent les √©tats dans lesquels le circuit peut aller. </font><font style="vertical-align: inherit;">Si nous prenons le pas de ¬´temps¬ª de la cha√Æne de Markov dans une heure, alors, √† condition qu'il soit ensoleill√© maintenant, il y a 60% de chances que le temps ensoleill√© continue pendant l'heure suivante. </font><font style="vertical-align: inherit;">Il y a √©galement 30% de chances que le temps soit nuageux au cours de la prochaine heure et 10% de chances qu'il pleuve imm√©diatement apr√®s un temps ensoleill√©. </font><font style="vertical-align: inherit;">Cela signifie √©galement que les valeurs de chaque ligne totalisent un. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Conduisons un peu notre cha√Æne Markov:</font></font><br>
<br>
<pre><code class="python hljs">n_steps = <span class="hljs-number">20000</span>
states = [<span class="hljs-number">0</span>]
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(n_steps):<font></font>
    states.append(np.random.choice((<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>), p=transition_matrix[states[<span class="hljs-number">-1</span>]]))<font></font>
states = np.array(states)</code></pre><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nous pouvons observer comment la cha√Æne de Markov converge vers une distribution stationnaire, calculant la probabilit√© empirique de chacun des √©tats en fonction de la longueur de la cha√Æne:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">despine</span>(<span class="hljs-params">ax, spines=(<span class="hljs-params"><span class="hljs-string">'top'</span>, <span class="hljs-string">'left'</span>, <span class="hljs-string">'right'</span></span>)</span>):</span>
    <span class="hljs-keyword">for</span> spine <span class="hljs-keyword">in</span> spines:<font></font>
        ax.spines[spine].set_visible(<span class="hljs-literal">False</span>)<font></font>
<font></font>
fig, ax = plt.subplots()<font></font>
width = <span class="hljs-number">1000</span>
offsets = range(<span class="hljs-number">1</span>, n_steps, <span class="hljs-number">5</span>)
<span class="hljs-keyword">for</span> i, label <span class="hljs-keyword">in</span> enumerate(state_space):<font></font>
    ax.plot(offsets, [np.sum(states[:offset] == i) / offset <font></font>
            <span class="hljs-keyword">for</span> offset <span class="hljs-keyword">in</span> offsets], label=label)<font></font>
ax.set_xlabel(<span class="hljs-string">"number of steps"</span>)<font></font>
ax.set_ylabel(<span class="hljs-string">"likelihood"</span>)<font></font>
ax.legend(frameon=<span class="hljs-literal">False</span>)<font></font>
despine(ax, (<span class="hljs-string">'top'</span>, <span class="hljs-string">'right'</span>))<font></font>
plt.show()</code></pre><br>
<br>
<img src="https://habrastorage.org/webt/vu/tg/xv/vutgxvc3lq1-sang725fmek1ibm.png"><br>
 <br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">HONNEUR DE TOUS LES MCMC: ALGORITHME DE M√âTROPOLE-HASTINGS</font></font></b> <br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Bien s√ªr, tout cela est tr√®s int√©ressant, mais revenons au processus d'√©chantillonnage d'une distribution de probabilit√© arbitraire </font></font><img src="https://habrastorage.org/webt/r2/lm/5b/r2lm5bcuhr-rdtxeg90gjnogcza.png"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Elle peut √™tre soit discr√®te, auquel cas nous continuerons √† parler de la matrice de transition </font></font><img src="https://habrastorage.org/webt/sz/nh/au/sznhau4xxghwlc7ma-eixqb9jok.png"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, soit continue, auquel cas ce </font></font><img src="https://habrastorage.org/webt/sz/nh/au/sznhau4xxghwlc7ma-eixqb9jok.png"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">sera un noyau de transition. Nous parlerons ci-apr√®s de distributions continues, mais tous les concepts que nous consid√©rons ici sont √©galement applicables √† des cas discrets. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Si nous pouvions concevoir le noyau de transition de mani√®re √† ce que l'√©tat suivant soit d√©j√† d√©duit </font></font><img src="https://habrastorage.org/webt/r2/lm/5b/r2lm5bcuhr-rdtxeg90gjnogcza.png"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, cela pourrait √™tre limit√©, car notre cha√Æne de Markov ... serait directement √©chantillonn√©e </font></font><img src="https://habrastorage.org/webt/r2/lm/5b/r2lm5bcuhr-rdtxeg90gjnogcza.png"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Malheureusement, pour y parvenir, nous devons pouvoir √©chantillonner</font></font><img src="https://habrastorage.org/webt/r2/lm/5b/r2lm5bcuhr-rdtxeg90gjnogcza.png"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ce que nous ne pouvons pas faire - sinon vous ne liriez pas cela, non? </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La solution consiste √† diviser le noyau de transition </font></font><img src="https://habrastorage.org/webt/7_/os/ja/7_osja2wwom8x6f29sp5ecxnrwc.png"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">en deux parties: l'√©tape de proposition et l'√©tape d'acceptation / rejet. Une distribution auxiliaire appara√Æt √† l'√©tape d'√©chantillonnage</font></font><img src="https://habrastorage.org/webt/qf/bl/m0/qfblm0cpxxgzun1zq6i9qv_zmfq.png"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">parmi lesquels les prochains √©tats possibles de la cha√Æne sont s√©lectionn√©s. Non seulement nous pouvons faire une s√©lection √† partir de cette distribution, mais nous pouvons choisir arbitrairement la distribution elle-m√™me. Cependant, lors de la conception, il faut s'efforcer de parvenir √† une configuration dans laquelle les √©chantillons pr√©lev√©s dans cette distribution seraient en corr√©lation minimale avec l'√©tat actuel et auraient en m√™me temps de bonnes chances de passer par la phase de r√©ception. L'√©tape de r√©ception / rejet ci-dessus est la deuxi√®me partie du noyau de transition; √† ce stade, les erreurs contenues dans les √©tats d'essai s√©lectionn√©s sont corrig√©es </font></font><img src="https://habrastorage.org/webt/dq/wo/ba/dqwoba_pfa-ktifmo9zviggtesm.png"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Ici, la probabilit√© de r√©ception r√©ussie est calcul√©e </font></font><img src="https://habrastorage.org/webt/hz/f4/rv/hzf4rviqapfrbg3mphv1na6kao0.png"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">et un √©chantillon est pr√©lev√© </font></font><img src="https://habrastorage.org/webt/gs/gz/hz/gsgzhzrwlbmvbexpmq21j8u1dbu.png"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">avec une probabilit√© telle que l'√©tat suivant dans la cha√Æne. Obtenir le prochain √©tat </font></font><img src="https://habrastorage.org/webt/gs/gz/hz/gsgzhzrwlbmvbexpmq21j8u1dbu.png"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">de</font></font><img src="https://habrastorage.org/webt/5q/fu/fx/5qfufxlugigeusphytdet38q580.png"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">puis effectu√© comme suit: d'abord, l'√©tat d'essai </font></font><img src="https://habrastorage.org/webt/gs/gz/hz/gsgzhzrwlbmvbexpmq21j8u1dbu.png"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">est tir√© de </font></font><img src="https://habrastorage.org/webt/qf/bl/m0/qfblm0cpxxgzun1zq6i9qv_zmfq.png"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Ensuite, il est consid√©r√© comme le prochain √©tat avec probabilit√© </font></font><img src="https://habrastorage.org/webt/hz/f4/rv/hzf4rviqapfrbg3mphv1na6kao0.png"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ou rejet√© avec probabilit√© </font></font><img src="https://habrastorage.org/webt/cr/_w/sd/cr_wsdxfgphyie2qbzl977pvfgq.png"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, et dans ce dernier cas, l'√©tat actuel est copi√© et utilis√© comme prochain. Par </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
cons√©quent, nous avons </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ty/vg/w-/tyvgw-g_ij7vgrpzoxtt7t3enmw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
condition suffisante pour la cha√Æne de </font><font style="vertical-align: inherit;">Markov a </font></font><img src="https://habrastorage.org/webt/r2/lm/5b/r2lm5bcuhr-rdtxeg90gjnogcza.png"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">une distribution stationnaire est la suivante: Le noyau de </font><font style="vertical-align: inherit;">transition doit pr√©senter un </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">√©quilibre d√©taill√©</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ou √©crire dans la litt√©rature physique, la </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">r√©versibilit√© microscopique</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : </font></font><br>
<br>
<img src="https://habrastorage.org/webt/hf/bg/oy/hfbgoyr944ikp5ce_menl-sg0da.png"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Cela signifie que la probabilit√© d'√™tre dans un √©tat </font></font><img src="https://habrastorage.org/webt/mt/2y/jr/mt2yjrl958wkwmuhsrpm3h-4uko.png"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">et de passer de l√† √†</font></font><img src="https://habrastorage.org/webt/4t/py/hc/4tpyhcj70euwoys5id2rrqxa_ac.png"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">doit √™tre √©gal √† la probabilit√© du processus inverse, c'est-√†-dire pouvoir </font></font><img src="https://habrastorage.org/webt/4t/py/hc/4tpyhcj70euwoys5id2rrqxa_ac.png"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">et entrer dans un √©tat </font></font><img src="https://habrastorage.org/webt/mt/2y/jr/mt2yjrl958wkwmuhsrpm3h-4uko.png"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Les noyaux de transition de la plupart des algorithmes MCMC remplissent cette condition. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pour que le noyau de transition en deux parties ob√©isse √† l'√©quilibre d√©taill√©, il est n√©cessaire de choisir correctement </font></font><img src="https://habrastorage.org/webt/oc/hr/x1/ochrx15_pe9awrsow_fygffjego.png"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, c'est-√†-dire de s'assurer qu'il vous permet de corriger les asym√©tries dans le flux de probabilit√© de / vers </font></font><img src="https://habrastorage.org/webt/4t/py/hc/4tpyhcj70euwoys5id2rrqxa_ac.png"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ou </font></font><img src="https://habrastorage.org/webt/mt/2y/jr/mt2yjrl958wkwmuhsrpm3h-4uko.png"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Algorithme de </font><font style="vertical-align: inherit;">Metropolis-Hastings utilise crit√®re de recevabilit√© Metropolis: </font></font><br>
 <br>
<img src="https://habrastorage.org/webt/mh/dm/z2/mhdmz27voos90zkz5_-zexopsdc.png"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Et ici, la magie commence: </font></font><img src="https://habrastorage.org/webt/r2/lm/5b/r2lm5bcuhr-rdtxeg90gjnogcza.png"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">nous ne connaissons qu'une constante, mais cela n'a pas d'importance, car cette constante inconnue annule l'expression de</font></font><img src="https://habrastorage.org/webt/oc/hr/x1/ochrx15_pe9awrsow_fygffjego.png"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">! C'est cette propri√©t√© paccpacc qui assure le fonctionnement d'algorithmes bas√©s sur l'algorithme Metropolis-Hastings sur des distributions non normalis√©es. Des distributions auxiliaires sym√©triques c sont souvent utilis√©es </font></font><img src="https://habrastorage.org/webt/b2/ss/pz/b2sspzujgmdn_uwizxwjjokkg5w.png"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, auquel cas l'algorithme Metropolis-Hastings est r√©duit √† l'algorithme Metropolis original (moins g√©n√©ral) d√©velopp√© en 1953. Dans l'algorithme d'origine </font></font><br>
<br>
<img src="https://habrastorage.org/webt/hd/3k/vq/hd3kvqg9l2jqltan97_gfmzutsi.png"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dans ce cas, le noyau de transition complet de Metropolis-Hastings peut s'√©crire </font></font><br>
<br>
<img src="https://habrastorage.org/webt/tl/ic/rd/tlicrdfto9zvzlooid7cdt4ii-m.png"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">NOUS METTONS EN ≈íUVRE L'ALGORITHME METROPOLIS-HASTINGS √Ä PYTHON</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Eh bien, maintenant que nous avons compris comment fonctionne l'algorithme Metropolis-Hastings, passons √† sa mise en ≈ìuvre. Premi√®rement, nous √©tablissons la probabilit√© logarithmique de la distribution √† partir de laquelle nous allons faire une s√©lection - sans constantes de normalisation; on suppose que nous ne les connaissons pas. Ensuite, nous travaillons avec la distribution normale standard:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">log_prob</span>(<span class="hljs-params">x</span>):</span>
     <span class="hljs-keyword">return</span> <span class="hljs-number">-0.5</span> * np.sum(x ** <span class="hljs-number">2</span>)</code></pre><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ensuite, nous choisissons une distribution auxiliaire sym√©trique. </font><font style="vertical-align: inherit;">En g√©n√©ral, les performances de l'algorithme Metropolis-Hastings peuvent √™tre am√©lior√©es si vous incluez des informations d√©j√† connues de la distribution √† partir de laquelle vous souhaitez effectuer une s√©lection dans la distribution auxiliaire. </font><font style="vertical-align: inherit;">Une approche simplifi√©e ressemble √† ceci: nous prenons l'√©tat actuel </font></font><code>x</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">et s√©lectionnons un √©chantillon √† partir de </font></font><img src="https://habrastorage.org/webt/tl/ic/rd/tlicrdfto9zvzlooid7cdt4ii-m.png"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, c'est-√†-dire, d√©finissons une certaine taille de pas </font></font><code>Œî</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">et allons √† gauche ou √† droite de notre √©tat actuel par pas plus de </font></font><img src="https://habrastorage.org/webt/q1/n2/d_/q1n2d_lbzpbqtvbmbgiha503cmo.png"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">proposal</span>(<span class="hljs-params">x, stepsize</span>):</span>
    <span class="hljs-keyword">return</span> np.random.uniform(low=x - <span class="hljs-number">0.5</span> * stepsize, <font></font>
                             high=x + <span class="hljs-number">0.5</span> * stepsize, <font></font>
                             size=x.shape)</code></pre><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Enfin, nous calculons la probabilit√© d'acceptation de la proposition:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">p_acc_MH</span>(<span class="hljs-params">x_new, x_old, log_prob</span>):</span>
    <span class="hljs-keyword">return</span> min(<span class="hljs-number">1</span>, np.exp(log_prob(x_new) - log_prob(x_old)))</code></pre><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Maintenant, nous mettons tout cela ensemble dans une mise en ≈ìuvre vraiment br√®ve de la phase d'√©chantillonnage pour l'algorithme Metropolis-Hastings:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sample_MH</span>(<span class="hljs-params">x_old, log_prob, stepsize</span>):</span><font></font>
    x_new = proposal(x_old, stepsize)<font></font>
    <span class="hljs-comment">#   ,     :</span>
    <span class="hljs-comment">#       [0,1]  </span>
    <span class="hljs-comment">#     </span><font></font>
    accept = np.random.random() &lt; p_acc(x_new, x_old, log_prob)<font></font>
    <span class="hljs-keyword">if</span> accept:
        <span class="hljs-keyword">return</span> accept, x_new
    <span class="hljs-keyword">else</span>:
        <span class="hljs-keyword">return</span> accept, x_old</code></pre><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En plus du prochain √©tat de la cha√Æne de Markov, </font></font><code>x_new</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ou </font></font><code>x_old</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, nous renvoyons √©galement des informations pour savoir si l'√©tape MCMC a √©t√© adopt√©e. </font><font style="vertical-align: inherit;">Cela nous permettra de suivre la dynamique de la collecte d'√©chantillons. </font><font style="vertical-align: inherit;">En conclusion de cette impl√©mentation, nous √©crivons une fonction qui appellera it√©rativement </font></font><code>sample_MH</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">et donc construira une cha√Æne de Markov:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">build_MH_chain</span>(<span class="hljs-params">init, stepsize, n_total, log_prob</span>):</span><font></font>
<font></font>
    n_accepted = <span class="hljs-number">0</span><font></font>
    chain = [init]<font></font>
<font></font>
    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> range(n_total):<font></font>
        accept, state = sample_MH(chain[<span class="hljs-number">-1</span>], log_prob, stepsize)<font></font>
        chain.append(state)<font></font>
        n_accepted += accept<font></font>
    <font></font>
    acceptance_rate = n_accepted / float(n_total)<font></font>
    <font></font>
    <span class="hljs-keyword">return</span> chain, acceptance_rate</code></pre><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">TESTER NOTRE ALGORITHME METROPOLIS-HASTINGS ET RECHERCHER SON COMPORTEMENT</font></font></b> <br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Probablement, maintenant, vous avez h√¢te de voir tout cela en action. </font><font style="vertical-align: inherit;">Nous le ferons, nous prendrons des d√©cisions √©clair√©es sur les arguments </font></font><code>stepsize</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">et </font></font><code>n_total</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">:</font></font><br>
<br>
<pre><code class="python hljs">chain, acceptance_rate = build_MH_chain(np.array([<span class="hljs-number">2.0</span>]), <span class="hljs-number">3.0</span>, <span class="hljs-number">10000</span>, log_prob)<font></font>
chain = [state <span class="hljs-keyword">for</span> state, <span class="hljs-keyword">in</span> chain]<font></font>
print(<span class="hljs-string">"Acceptance rate: {:.3f}"</span>.format(acceptance_rate))<font></font>
last_states = <span class="hljs-string">", "</span>.join(<span class="hljs-string">"{:.5f}"</span>.format(state) 
                        <span class="hljs-keyword">for</span> state <span class="hljs-keyword">in</span> chain[<span class="hljs-number">-10</span>:])<font></font>
print(<span class="hljs-string">"Last ten states of chain: "</span> + last_states)<font></font>
Acceptance rate: <span class="hljs-number">0.722</span>
Last ten states of chain: <span class="hljs-number">-0.84962</span>, <span class="hljs-number">-0.84962</span>, <span class="hljs-number">-0.84962</span>, <span class="hljs-number">-0.08692</span>, <span class="hljs-number">0.92728</span>, <span class="hljs-number">-0.46215</span>, <span class="hljs-number">0.08655</span>, <span class="hljs-number">-0.33841</span>, <span class="hljs-number">-0.33841</span>, <span class="hljs-number">-0.33841</span></code></pre><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Les choses sont bonnes. Alors √ßa a march√©? Nous avons r√©ussi √† pr√©lever des √©chantillons dans environ 71% des cas et nous avons une cha√Æne d'√©tats. Les premiers √âtats dans lesquels la cha√Æne n'a pas encore converg√© vers sa distribution stationnaire doivent √™tre √©cart√©s. V√©rifions si les conditions que nous avons choisies ont bien une distribution normale:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_samples</span>(<span class="hljs-params">chain, log_prob, ax, orientation=<span class="hljs-string">'vertical'</span>, normalize=True,
                 xlims=(<span class="hljs-params"><span class="hljs-number">-5</span>, <span class="hljs-number">5</span></span>), legend=True</span>):</span>
    <span class="hljs-keyword">from</span> scipy.integrate <span class="hljs-keyword">import</span> quad<font></font>
    <font></font>
    ax.hist(chain, bins=<span class="hljs-number">50</span>, density=<span class="hljs-literal">True</span>, label=<span class="hljs-string">"MCMC samples"</span>,<font></font>
           orientation=orientation)<font></font>
    <span class="hljs-comment">#     PDF</span>
    <span class="hljs-keyword">if</span> normalize:<font></font>
        Z, _ = quad(<span class="hljs-keyword">lambda</span> x: np.exp(log_prob(x)), -np.inf, np.inf)
    <span class="hljs-keyword">else</span>:<font></font>
        Z = <span class="hljs-number">1.0</span>
    xses = np.linspace(xlims[<span class="hljs-number">0</span>], xlims[<span class="hljs-number">1</span>], <span class="hljs-number">1000</span>)<font></font>
    yses = [np.exp(log_prob(x)) / Z <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> xses]
    <span class="hljs-keyword">if</span> orientation == <span class="hljs-string">'horizontal'</span>:<font></font>
        (yses, xses) = (xses, yses)<font></font>
    ax.plot(xses, yses, label=<span class="hljs-string">"true distribution"</span>)
    <span class="hljs-keyword">if</span> legend:<font></font>
        ax.legend(frameon=<span class="hljs-literal">False</span>)<font></font>
    <font></font>
fig, ax = plt.subplots()<font></font>
plot_samples(chain[<span class="hljs-number">500</span>:], log_prob, ax)<font></font>
despine(ax)<font></font>
ax.set_yticks(())<font></font>
plt.show()</code></pre><br>
<br>
<img src="https://habrastorage.org/webt/yo/rx/59/yorx59lirnkyju_dymptouaaokw.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Cela semble tr√®s bien! </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Qu'en est-il des param√®tres </font></font><code>stepsize</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">et </font></font><code>n_total</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">? Nous discutons d'abord de la taille de l'√©tape: elle d√©termine dans quelle mesure l'√©tat d'essai peut √™tre retir√© de l'√©tat actuel du circuit. Par cons√©quent, il s'agit d'un param√®tre de distribution auxiliaire </font></font><code>q</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">qui contr√¥le la taille des pas al√©atoires de la cha√Æne de Markov. Si la taille du pas est trop grande, les √©tats d'essai se retrouvent souvent dans la queue de la distribution, o√π les valeurs de probabilit√© sont faibles. Le m√©canisme d'√©chantillonnage de Metropolis-Hastings ignore la plupart de ces √©tapes, ce qui entra√Æne une r√©duction des taux de r√©ception et un ralentissement significatif de la convergence. Voir par vous-m√™me:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sample_and_display</span>(<span class="hljs-params">init_state, stepsize, n_total, n_burnin, log_prob</span>):</span><font></font>
    chain, acceptance_rate = build_MH_chain(init_state, stepsize, n_total, log_prob)<font></font>
    print(<span class="hljs-string">"Acceptance rate: {:.3f}"</span>.format(acceptance_rate))<font></font>
    fig, ax = plt.subplots()<font></font>
    plot_samples([state <span class="hljs-keyword">for</span> state, <span class="hljs-keyword">in</span> chain[n_burnin:]], log_prob, ax)<font></font>
    despine(ax)<font></font>
    ax.set_yticks(())<font></font>
    plt.show()<font></font>
    <font></font>
sample_and_display(np.array([<span class="hljs-number">2.0</span>]), <span class="hljs-number">30</span>, <span class="hljs-number">10000</span>, <span class="hljs-number">500</span>, log_prob)<font></font>
Acceptance rate: <span class="hljs-number">0.116</span></code></pre><br>
<br>
<img src="https://habrastorage.org/webt/0g/4-/-r/0g4--rezypxzaqta-pkugq3gi3w.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pas tr√®s cool, non? Il semble maintenant qu'il est pr√©f√©rable de d√©finir une taille de pas minuscule. Il s'av√®re que ce n'est pas non plus une d√©cision intelligente, car la cha√Æne de Markov √©tudiera la distribution de probabilit√© tr√®s lentement et ne convergera donc pas aussi rapidement qu'avec une taille de pas bien choisie:</font></font><br>
<br>
<pre><code class="python hljs">sample_and_display(np.array([<span class="hljs-number">2.0</span>]), <span class="hljs-number">0.1</span>, <span class="hljs-number">10000</span>, <span class="hljs-number">500</span>, log_prob)<font></font>
Acceptance rate: <span class="hljs-number">0.992</span></code></pre><br>
<br>
<img src="https://habrastorage.org/webt/bf/qh/xi/bfqhxistqravnn7xug43ubowgmu.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Quelle que soit la fa√ßon dont vous choisissez le param√®tre de taille de pas, la cha√Æne de Markov converge finalement vers une distribution stationnaire. Mais cela peut prendre beaucoup de temps. Le temps pendant lequel nous simulerons la cha√Æne de Markov est </font></font><code>n_total</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">d√©termin√© </font><font style="vertical-align: inherit;">par le param√®tre </font><font style="vertical-align: inherit;">- il d√©termine simplement le nombre d'√©tats de la cha√Æne de Markov (et, par cons√©quent, les √©chantillons s√©lectionn√©s) que nous aurons √©ventuellement. Si la cha√Æne converge lentement, alors elle doit √™tre augment√©e </font></font><code>n_total</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">pour que la cha√Æne de Markov ait le temps ¬´d'oublier¬ª l'√©tat initial. Par cons√©quent, nous allons laisser la taille de l'√©tape minuscule et augmenter le nombre d'√©chantillons en augmentant le param√®tre </font></font><code>n_total</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">:</font></font><br>
<br>
<pre><code class="python hljs">sample_and_display(np.array([<span class="hljs-number">2.0</span>]), <span class="hljs-number">0.1</span>, <span class="hljs-number">500000</span>, <span class="hljs-number">25000</span>, log_prob)<font></font>
Acceptance rate: <span class="hljs-number">0.990</span></code></pre><br>
<br>
<img src="https://habrastorage.org/webt/fs/ba/27/fsba27vppyvfqnwdnnr0ifdp3w4.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nous progressons plus lentement vers l'objectif ... </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">CONCLUSIONS</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Compte tenu de tout ce qui pr√©c√®de, j'esp√®re que vous avez maintenant compris intuitivement l'essence de l'algorithme Metropolis-Hastings, ses param√®tres, et comprenez pourquoi il s'agit d'un outil extr√™mement utile pour s√©lectionner des distributions de probabilit√©s non standard que vous pouvez rencontrer dans la pratique. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Je vous recommande fortement d'exp√©rimenter avec le code donn√© </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ici.</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- pour vous habituer au comportement de l'algorithme dans diverses circonstances et le comprendre plus profond√©ment. </font><font style="vertical-align: inherit;">Essayez la distribution auxiliaire asym√©trique! </font><font style="vertical-align: inherit;">Que se passera-t-il si vous ne d√©finissez pas correctement les crit√®res d'acceptation? </font><font style="vertical-align: inherit;">Que se passe-t-il si vous essayez d'√©chantillonner √† partir d'une distribution bimodale? </font><font style="vertical-align: inherit;">Pouvez-vous trouver un moyen d'ajuster automatiquement la taille du pas? </font><font style="vertical-align: inherit;">Quels sont les pi√®ges ici? </font><font style="vertical-align: inherit;">R√©pondez √† ces questions vous-m√™me!</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr491258/index.html">IT girls, d'o√π venez-vous? Construisons une carte</a></li>
<li><a href="../fr491260/index.html">Normalisation du texte dans les t√¢ches de reconnaissance vocale</a></li>
<li><a href="../fr491262/index.html">Oeil pour oeil. Probl√®mes de biom√©trie</a></li>
<li><a href="../fr491264/index.html">Introduction au SSD. Partie 4. Physique</a></li>
<li><a href="../fr491266/index.html">SurfingAttack: compromettre les smartphones avec des assistants sonores [+ vid√©o]</a></li>
<li><a href="../fr491272/index.html">D√©veloppement de site web en pascal (backend)</a></li>
<li><a href="../fr491276/index.html">Comment j'ai contourn√© l'interdiction de l'API Messages via la documentation de Vkontakte</a></li>
<li><a href="../fr491278/index.html">CLRium # 7: Rapports, pratique, mentors</a></li>
<li><a href="../fr491280/index.html">L'histoire de la fa√ßon dont j'ai d√©velopp√© un langage de programmation</a></li>
<li><a href="../fr491282/index.html">Comment augmenter la productivit√© de l'√©quipe (et r√©duire les erreurs) √† l'aide de rallyes</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>