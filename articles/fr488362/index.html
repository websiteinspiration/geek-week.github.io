<!doctype html>
<html class="no-js" lang="fr">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üòí üìπ üç¢ Classification multi-tagu√©e üë©üèª‚Äçüíº üêÖ üßë</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bonjour, habrozhiteli! Nous avons d√©cid√© de citer un extrait du livre d'Andrei Burkov , Machine Learning Without Extra Words , consacr√© √† la classific...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Classification multi-tagu√©e</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/488362/"><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><img src="https://habrastorage.org/webt/hm/vf/c_/hmvfc_yyxepplv1mj0crw1vu7pw.jpeg" align="left" alt="image"></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bonjour, habrozhiteli! </font><font style="vertical-align: inherit;">Nous avons d√©cid√© de citer un extrait du livre d'Andrei Burkov </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, Machine Learning Without Extra Words</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , consacr√© √† la classification.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pour d√©crire l'image sur la figure, plusieurs √©tiquettes peuvent √™tre utilis√©es simultan√©ment: ¬´for√™t de conif√®res¬ª, ¬´montagnes¬ª, ¬´route¬ª. Si le nombre de valeurs possibles pour les √©tiquettes est important, mais qu'elles ont toutes la m√™me nature que les √©tiquettes, chaque √©chantillon √©tiquet√© peut √™tre converti en plusieurs donn√©es √©tiquet√©es, une pour chaque √©tiquette. Toutes ces nouvelles donn√©es auront les m√™mes vecteurs de caract√©ristiques et une seule √©tiquette. Par cons√©quent, la t√¢che devient un probl√®me de classification multiclasse. Il peut √™tre r√©solu en utilisant la strat√©gie ¬´un contre tous¬ª. La seule diff√©rence avec le probl√®me habituel de classification multiclasse est l'apparition d'un nouveau hyperparam√®tre: le seuil. Si le score de similitude pour une √©tiquette est sup√©rieur √† une valeur seuil, cette √©tiquette est affect√©e au vecteur d'entit√© en entr√©e. Dans ce sc√©nario, plusieurs √©tiquettes peuvent √™tre affect√©es √† un vecteur caract√©ristique.La valeur seuil est s√©lectionn√©e √† l'aide de l'ensemble de contr√¥le.</font></font><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pour r√©soudre le probl√®me de classification avec de nombreuses √©tiquettes, on peut √©galement appliquer des algorithmes qui sont naturellement transform√©s en multiclasses (arbres de d√©cision, r√©gression logistique, r√©seaux de neurones, etc.). Ils renvoient une estimation pour chaque classe, afin que nous puissions d√©finir un seuil puis attribuer plusieurs √©tiquettes √† un vecteur d'entit√© pour lequel le score de proximit√© d√©passe ce seuil. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Les r√©seaux de neurones peuvent naturellement √™tre form√©s aux classifications multi-√©tiquettes en utilisant l'entropie crois√©e binaire comme fonction de co√ªt. La couche de sortie du r√©seau neuronal dans ce cas a un n≈ìud par √©tiquette. Chaque n≈ìud de la couche de sortie a une fonction d'activation sigmo√Øde. En cons√©quence, chaque √©tiquette l est binaire</font></font><img src="https://habrastorage.org/webt/2n/_n/mg/2n_nmgvuciuadryomnq3urrg1xw.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">o√π l = 1, ..., L et i = 1, ..., N.L'entropie crois√©e binaire d√©termine la probabilit√© </font></font><img src="https://habrastorage.org/webt/lb/le/cf/lblecfgyuy23k8zvlasuajncsds.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">que l'√©chantillon xi ait le label l, est d√©fini comme le </font></font><img src="https://habrastorage.org/webt/qd/1e/fx/qd1efxpmi6mmuhv_fhmsqpjgpi4.jpeg" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
crit√®re de minimisation - une moyenne simple de tous les membres de l'entropie crois√©e binaire dans tous les √©chantillons d'apprentissage. et toutes leurs balises. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dans les cas o√π le nombre de valeurs d'√©tiquette possibles est petit, vous pouvez essayer de convertir le probl√®me de classification avec de nombreuses √©tiquettes en un probl√®me de classification multiclasse. </font><font style="vertical-align: inherit;">Imaginez le probl√®me suivant. </font><font style="vertical-align: inherit;">Vous devez attribuer deux types d'√©tiquettes aux images. </font><font style="vertical-align: inherit;">Les √©tiquettes du premier type peuvent avoir deux significations possibles: { </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">photo, peinture</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> }; </font><font style="vertical-align: inherit;">les marques du deuxi√®me type peuvent avoir trois significations possibles: { </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">portrait, paysage, autre</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">}. </font><font style="vertical-align: inherit;">Pour chaque combinaison de deux classes source, vous pouvez cr√©er une nouvelle classe factice, par exemple:</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/mz/px/zn/mzpxzn0rlrumwoql7gkk3no7ihk.jpeg" alt="image"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Maintenant, nous avons les m√™mes donn√©es balis√©es, mais nous avons remplac√© l'ensemble de vraies √©tiquettes par une √©tiquette fictive avec des valeurs de 1 √† 6. En pratique, cette approche donne de bons r√©sultats lorsqu'il n'y a pas trop de combinaisons possibles de classes. Sinon, beaucoup plus de donn√©es de formation doivent √™tre utilis√©es pour compenser l'augmentation de l'ensemble des classes. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Le principal avantage de cette derni√®re approche est que les √©tiquettes restent corr√©l√©es, contrairement aux m√©thodes d√©crites ci-dessus, qui pr√©disent chaque √©tiquette ind√©pendamment les unes des autres. Dans de nombreuses t√¢ches, la corr√©lation entre les √©tiquettes peut √™tre un facteur important. Par exemple, imaginez que vous souhaitez classer les e-mails comme </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">spam</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> et </font><i><font style="vertical-align: inherit;">non- </font></i><i><font style="vertical-align: inherit;">spam</font></i></font><i><font style="vertical-align: inherit;"></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, et en m√™me temps que ordinaire et important. </font><font style="vertical-align: inherit;">Vous voudrez probablement exclure des pr√©visions telles que [ </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">spam, important</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ].</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">7.5. </font><font style="vertical-align: inherit;">Formation d'ensemble</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Les algorithmes fondamentaux que nous avons trait√©s au chapitre 3 ont leurs limites. En raison de sa simplicit√©, ils ne peuvent parfois pas cr√©er un mod√®le suffisamment efficace pour votre t√¢che. Dans de tels cas, vous pouvez essayer d'utiliser des r√©seaux de neurones profonds. Cependant, dans la pratique, les r√©seaux de neurones profonds n√©cessitent une quantit√© importante de donn√©es √©tiquet√©es, que vous n'avez peut-√™tre pas. Une autre fa√ßon d'augmenter l'efficacit√© d'algorithmes d'apprentissage simples consiste √† utiliser la </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">formation d'ensemble</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La formation d'ensemble est un paradigme de formation qui est bas√© sur la formation non seulement d'un mod√®le super-correct, mais d'un grand nombre de mod√®les de faible pr√©cision et combinant les pr√©visions donn√©es par ces </font><font style="vertical-align: inherit;">mod√®les </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">faibles</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> pour obtenir un </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">m√©tamod√®le</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> plus correct </font><font style="vertical-align: inherit;">.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Les mod√®les de faible pr√©cision sont g√©n√©ralement form√©s par </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">des algorithmes d'apprentissage faibles</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> qui ne sont pas en mesure de former des mod√®les complexes et, par cons√©quent, affichent une vitesse √©lev√©e aux √©tapes de formation et de pr√©vision. Le plus souvent, l'algorithme d'apprentissage de l'arbre de d√©cision est utilis√© comme algorithme faible, ce qui arr√™te g√©n√©ralement de casser l'ensemble d'apprentissage apr√®s plusieurs it√©rations. Le r√©sultat est des arbres petits et pas tr√®s r√©guliers, mais, comme le dit l'id√©e de former l'ensemble, si les arbres ne sont pas identiques et que chaque arbre est au moins l√©g√®rement meilleur que la supposition al√©atoire, nous pouvons obtenir une grande pr√©cision en combinant un grand nombre de ces arbres. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pour obtenir les pr√©visions finales pour l'entr√©e </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, les pr√©visions de tous les mod√®les faibles sont combin√©es √† l'aide d'une m√©thode de vote pond√©r√©e. </font><font style="vertical-align: inherit;">La forme sp√©cifique de pond√©ration des votes d√©pend de l'algorithme, mais l'essentiel n'en d√©pend pas: si, collectivement, les mod√®les faibles pr√©disent que l'e-mail est du spam, nous attribuons </font><font style="vertical-align: inherit;">l'√©tiquette de </font><i><font style="vertical-align: inherit;">spam</font></i><font style="vertical-align: inherit;"> √† l'√©chantillon </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">
Les deux principales m√©thodes de formation des ensembles sont le </font><b><font style="vertical-align: inherit;">boosting</font></b><font style="vertical-align: inherit;"> et l' </font><b><font style="vertical-align: inherit;">ensachage</font></b><font style="vertical-align: inherit;"> (agr√©gation). </font><font style="vertical-align: inherit;">Les traductions des termes boosting et bagging sont inexactes et peu habituelles.</font></font><i><font style="vertical-align: inherit;"></font></i><font style="vertical-align: inherit;"></font><br>
<br><font style="vertical-align: inherit;"></font><b><font style="vertical-align: inherit;"></font></b><font style="vertical-align: inherit;"></font><b><font style="vertical-align: inherit;"></font></b><font style="vertical-align: inherit;"></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">7.5.1. </font><font style="vertical-align: inherit;">Boosting et ensachage</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La m√©thode de boosting consiste √† utiliser les donn√©es de formation initiales et √† cr√©er de mani√®re it√©rative plusieurs mod√®les √† l'aide d'un algorithme faible. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Chaque nouveau mod√®le diff√®re des pr√©c√©dents en ce que, en le construisant, un algorithme faible tente de ¬´corriger¬ª les erreurs commises par les mod√®les pr√©c√©dents. </font><font style="vertical-align: inherit;">Le mod√®le d'ensemble final est une combinaison de ces nombreux mod√®les de construction it√©rative faibles. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
L'essence de l'ensachage est de cr√©er de nombreuses ¬´copies¬ª des donn√©es d'entra√Ænement (chaque copie est l√©g√®rement diff√©rente des autres), puis d'appliquer un algorithme faible √† chaque copie afin d'obtenir plusieurs mod√®les faibles, puis de les combiner. </font><font style="vertical-align: inherit;">Un algorithme d'apprentissage automatique largement utilis√© et efficace bas√© sur l'id√©e de l'ensachage est une </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">for√™t al√©atoire</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">7.5.2. </font><font style="vertical-align: inherit;">For√™t al√©atoire</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
L'algorithme d'ensachage ¬´classique¬ª fonctionne comme suit. </font><font style="vertical-align: inherit;">B √©chantillons al√©atoires sont cr√©√©s √† partir de l'ensemble d'apprentissage existant </font></font><img src="https://habrastorage.org/webt/-d/6r/gn/-d6rgnsn5wk-yzl3dbk0i7x2ole.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(pour chaque b = 1, ..., B) et un </font></font><img src="https://habrastorage.org/webt/-d/6r/gn/-d6rgnsn5wk-yzl3dbk0i7x2ole.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">mod√®le d' </font></font><img src="https://habrastorage.org/webt/nd/ew/4z/ndew4zvx7r0jpeilfknakojmrbs.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">arbre de d√©cision </font><font style="vertical-align: inherit;">est construit </font><font style="vertical-align: inherit;">sur la base de chaque √©chantillon </font><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Pour obtenir un √©chantillon </font></font><img src="https://habrastorage.org/webt/-d/6r/gn/-d6rgnsn5wk-yzl3dbk0i7x2ole.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">pour certains b, un </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">√©chantillon est fait avec remplacement</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Autrement dit, un √©chantillon vide est d'abord cr√©√©, puis un √©chantillon al√©atoire est s√©lectionn√© dans l'ensemble d'apprentissage, et sa copie exacte est plac√©e dans </font></font><img src="https://habrastorage.org/webt/-d/6r/gn/-d6rgnsn5wk-yzl3dbk0i7x2ole.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, tandis que l'√©chantillon lui-m√™me reste dans l'ensemble d'apprentissage d'origine. </font><font style="vertical-align: inherit;">La s√©lection des donn√©es se poursuit jusqu'√† ce que la condition soit remplie. </font></font><img src="https://habrastorage.org/webt/se/au/-5/seau-5gwous1c1cshmrx8rwubig.jpeg" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Suite √† la formation, </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">des</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> arbres de d√©cision </font><i><font style="vertical-align: inherit;">B</font></i><font style="vertical-align: inherit;"> sont </font><font style="vertical-align: inherit;">obtenus </font><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">La pr√©vision du nouvel √©chantillon </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , en cas de r√©gression, est d√©termin√©e comme la moyenne de </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">B</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> pr√©visions</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/re/zp/mj/rezpmjqa9lo7w4dxvqd6njwwqcm.jpeg" alt="image"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ou par un vote majoritaire en cas de classement.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La for√™t al√©atoire n'a qu'une seule diff√©rence avec l'ensachage classique. Il utilise un algorithme d'apprentissage d'arbre modifi√© qui, √† chaque fractionnement du processus d'apprentissage, v√©rifie un sous-ensemble al√©atoire de fonctionnalit√©s. Ceci est fait afin d'√©liminer la corr√©lation entre les arbres: si une ou plusieurs entit√©s ont une grande capacit√© pr√©dictive, de nombreux arbres les choisiront pour fractionner les donn√©es. Cela conduira √† l'apparition dans la "for√™t" d'un grand nombre d'arbres corr√©l√©s. La corr√©lation des signes avec une capacit√© pr√©dictive √©lev√©e emp√™che la pr√©cision de la pr√©diction d'augmenter. La grande efficacit√© de l'ensemble des mod√®les s'explique par le fait que les bons mod√®les sont plus susceptibles d'√™tre d'accord avec la m√™me pr√©vision, et les mauvais mod√®les ne sont pas susceptibles d'√™tre d'accord et de donner des pr√©visions diff√©rentes. La corr√©lation rendra les mod√®les pauvres plus susceptibles d'√™tre d'accord,ce qui faussera le sch√©ma de vote ou affectera la moyenne.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Les hyperparam√®tres les plus importants pour le r√©glage sont le nombre d'arbres B et la taille d'un sous-ensemble al√©atoire d'entit√©s qui doivent √™tre prises en compte pour chaque fractionnement. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La for√™t al√©atoire est l'un des algorithmes d'apprentissage d'ensemble les plus utilis√©s. Qu'est-ce qui d√©termine son efficacit√©? La raison en est qu'en utilisant plusieurs √©chantillons de l'ensemble de donn√©es d'origine, nous r√©duisons la </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">variance du</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> mod√®le final. N'oubliez pas qu'une faible variance signifie une faible pr√©disposition √† se </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">recycler</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Le recyclage se produit lorsque le mod√®le essaie d'expliquer de petites variations dans l'ensemble de donn√©es, car l'ensemble de donn√©es n'est qu'un petit √©chantillon de tous les exemples possibles du ph√©nom√®ne que nous essayons de simuler. </font><font style="vertical-align: inherit;">Dans le cas d'une approche infructueuse de la formation de l'ensemble d'entra√Ænement, certains artefacts ind√©sirables (mais in√©vitables) peuvent y tomber: du bruit, des donn√©es anormales et excessivement ou insuffisamment repr√©sentatives. </font><font style="vertical-align: inherit;">En cr√©ant plusieurs √©chantillons al√©atoires avec le remplacement de l'ensemble d'apprentissage, nous r√©duisons l'influence de ces artefacts.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">7.5.3. </font><font style="vertical-align: inherit;">Augmentation du gradient</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Un autre algorithme de formation d'ensemble efficace bas√© sur l'id√©e de boosting est le boosting de gradient. </font><font style="vertical-align: inherit;">Tout d'abord, consid√©rez l'utilisation de l'augmentation du gradient dans la r√©gression. </font><font style="vertical-align: inherit;">Nous allons commencer √† construire un mod√®le de r√©gression efficace avec un mod√®le constant </font></font><img src="https://habrastorage.org/webt/7u/7x/jd/7u7xjdufljpsjwwu45j9_gkc3r0.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(comme nous l'avons fait dans ID3):</font></font><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/em/vj/bv/emvjbvtmptxl_d3bihzev4wbh7o.jpeg" alt="image"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Modifiez ensuite les √©tiquettes dans tous les √©chantillons i = 1, ..., N dans le jeu d'apprentissage:</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/va/wc/ah/vawcahk0zsdpgnumh_bfjuozw_e.jpeg" alt="image"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
o√π </font></font><img src="https://habrastorage.org/webt/rf/1x/pk/rf1xpkgfxmcivy-1tqpwv2vgroi.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">est appel√© le </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">reste</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> et est le nouveau libell√© de l'√©chantillon </font></font><img src="https://habrastorage.org/webt/dk/ey/2r/dkey2rj3yf-zkei2029wfa2ujso.jpeg" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
. Nous utilisons maintenant l'ensemble d'apprentissage modifi√© avec les restes au lieu des libell√©s d'origine pour cr√©er un nouveau mod√®le de l'arbre de d√©cision. </font></font><img src="https://habrastorage.org/webt/mx/fw/um/mxfwumzpc5tq1wjpdate2rxra48.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Le mod√®le de stimulation est maintenant d√©fini comme </font></font><img src="https://habrastorage.org/webt/cy/c2/vz/cyc2vz0tmrrihcm6kta_7rnwmui.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">o√π Œ± est la vitesse d'apprentissage (hyperparam√®tre). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ensuite, nous recalculons les r√©sidus √† l'aide de l'√©quation 7.2, rempla√ßons √† nouveau les √©tiquettes dans les donn√©es d'apprentissage, enseignons un nouveau mod√®le de l'arbre de d√©cision, </font></font><img src="https://habrastorage.org/webt/p4/wk/nl/p4wknlhlvwiqtr7zz7lx_y5oq3s.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">red√©finissons le mod√®le de boost pendant que </font></font><img src="https://habrastorage.org/webt/n-/mn/ht/n-mnhtck0rar7pz4anzlbdc-bmo.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">nous r√©p√©tons le processus, jusqu'√† ce que nous combinions le nombre maximum pr√©d√©termin√© </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">M d'</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> arbres.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Comprenons intuitivement ce qui se passe ici. En calculant les r√©sidus, nous d√©terminons dans quelle mesure (ou mal) l'objectif de chaque √©chantillon d'apprentissage est pr√©dit par le mod√®le actuel f. Ensuite, nous formons un autre arbre pour corriger les erreurs du mod√®le actuel (c'est pourquoi nous utilisons des restes au lieu d'√©tiquettes r√©elles) et ajoutons un nouvel arbre au mod√®le existant avec un certain poids Œ±. Par cons√©quent, chaque nouvel arbre ajout√© au mod√®le corrige partiellement les erreurs faites par les arbres pr√©c√©dents. Le processus se poursuit jusqu'√† ce que le nombre maximum M (un autre hyperparam√®tre) des arbres soit combin√©.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Essayons maintenant de r√©pondre √† la question de savoir pourquoi cet algorithme est appel√© renforcement du gradient. En boosting de gradient, nous ne calculons pas le gradient, contrairement √† ce que nous avons fait au chapitre 4, en r√©solvant le probl√®me de r√©gression lin√©aire. Pour voir les similitudes entre l'augmentation du gradient et la descente du gradient, rappelez-vous pourquoi nous avons calcul√© le gradient en r√©gression lin√©aire: pour trouver la direction des valeurs des param√®tres afin de minimiser la fonction de co√ªt MSE. Le gradient montre la direction, mais ne montre pas jusqu'o√π aller dans cette direction, donc √† chaque it√©ration, nous avons fait un petit pas, puis d√©termin√© √† nouveau la direction. La m√™me chose se produit en boosting de gradient, mais au lieu de calculer directement le gradient, nous utilisons son estimation sous forme de r√©sidus: ils montrent comment le mod√®le doit √™tre ajust√© pour r√©duire l'erreur (r√©siduelle).</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En boosting de gradient, trois hyperparam√®tres principaux sont disponibles pour le r√©glage: le nombre d'arbres, la vitesse d'apprentissage et la profondeur des arbres. Les trois affectent la pr√©cision du mod√®le. La profondeur des arbres affecte √©galement la vitesse d'apprentissage et de pr√©vision: plus la profondeur est petite, plus vite. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
On peut montrer que l'apprentissage par r√©sidus optimise le mod√®le global f pour la norme d'erreur standard. Ici, vous pouvez voir la diff√©rence par rapport √† l'ensachage: le renforcement r√©duit le biais (ou le manque d'√©ducation) au lieu de la variance. En cons√©quence, le boosting est soumis √† un recyclage. Cependant, en ajustant la profondeur et le nombre d'arbres, le recyclage peut √™tre largement √©vit√©.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
L'amplification du d√©grad√© est similaire pour les t√¢ches de notation, mais les √©tapes sont l√©g√®rement diff√©rentes. Prenons le cas de la classification binaire. Supposons qu'il existe M arbres de d√©cision de r√©gression. Par analogie avec la r√©gression logistique, la pr√©vision de l'ensemble des arbres de d√©cision est mod√©lis√©e √† l'aide de la fonction sigmo√Øde:</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/wy/pd/gw/wypdgwjjgpzuojrelehnadtdggc.jpeg" alt="image"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
o√π </font></font><img src="https://habrastorage.org/webt/w6/3d/kb/w63dkbj3f9j-ik9hrhqbexyxtem.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">est l'arbre de r√©gression. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Et encore une fois, comme dans la r√©gression logistique, lorsque l'on essaie de trouver un mod√®le f maximisant </font></font><img src="https://habrastorage.org/webt/c4/lv/tt/c4lvttexfzr8disbsv1ph_drzka.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, le principe du maximum de vraisemblance est appliqu√©. De m√™me, pour √©viter un d√©bordement num√©rique, nous maximisons la somme des logarithmes de vraisemblance, plut√¥t que le produit de la vraisemblance. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
L'algorithme commence par le mod√®le constant initial </font></font><img src="https://habrastorage.org/webt/hn/7j/ep/hn7jepdxhudnxvjjgbulvfwtpyw.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">o√π </font></font><img src="https://habrastorage.org/webt/gj/6s/f6/gj6sf6i874m3gq3_fbbxbkrudae.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(on peut montrer qu'une telle initialisation est optimale pour la fonction sigmo√Øde.) Ensuite, √† chaque it√©ration m, un nouvel arbre fm est ajout√© au mod√®le. Pour trouver le meilleur arbre </font></font><img src="https://habrastorage.org/webt/b0/5w/rv/b05wrvcuk5hzvnrmkpebdnk_kxu.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pour trouver le meilleur arbre </font></font><img src="https://habrastorage.org/webt/xm/u4/dp/xmu4dpm1hi3-podtuiayxggydti.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, la d√©riv√©e partielle du </font></font><img src="https://habrastorage.org/webt/ep/hx/gu/ephxgusen4ou8pc8atm4ksqathq.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">mod√®le actuel est d' </font><font style="vertical-align: inherit;">abord calcul√©e </font><font style="vertical-align: inherit;">pour chaque i = 1, ..., N:</font></font><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/wg/v_/oo/wgv_oogvmupu4q5j6g3rq7dphvk.jpeg" alt="image"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
o√π f est le mod√®le du classificateur d'ensemble construit sur l'it√©ration pr√©c√©dente m - 1. Pour calculer </font></font><img src="https://habrastorage.org/webt/ep/hx/gu/ephxgusen4ou8pc8atm4ksqathq.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, nous devons trouver les d√©riv√©es de par </font></font><img src="https://habrastorage.org/webt/pv/pe/lb/pvpelb2jplsmq_jmkbdfyl-pzom.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">rapport √† f pour tout i. </font><font style="vertical-align: inherit;">Notez que la </font></font><img src="https://habrastorage.org/webt/nk/zn/0l/nkzn0lh0l0brnzl_vh0xxlmttrq.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">d√©riv√©e par rapport √† f du bon terme dans l'√©quation pr√©c√©dente est</font></font><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/xx/en/iu/xxeniu2qr35ln17asbfk2fqb3sc.jpeg" alt="image"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ensuite, l'ensemble d'apprentissage est transform√© en rempla√ßant l'√©tiquette d'origine de la </font></font><img src="https://habrastorage.org/webt/a1/fv/cu/a1fvcukqqvsu5wpv3x8i1zj5smc.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">d√©riv√©e partielle correspondante </font></font><img src="https://habrastorage.org/webt/ep/hx/gu/ephxgusen4ou8pc8atm4ksqathq.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, et une nouvelle arborescence est construite sur la base de l'ensemble d'apprentissage converti. </font></font><img src="https://habrastorage.org/webt/w1/8f/41/w18f41gto37doyvyakgs4np_fyy.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ensuite, l'√©tape de mise √† jour optimale est d√©termin√©e </font></font><img src="https://habrastorage.org/webt/1n/cf/sj/1ncfsjxfe-tao3ep_csuw-s-arw.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">comme </font><font style="vertical-align: inherit;">suit </font><font style="vertical-align: inherit;">:</font></font><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/co/qc/hw/coqchwpctxgy2ukbdwaybkouxx0.jpeg" alt="image"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A la fin de l'it√©ration m, nous mettons √† jour le mod√®le d'ensemble en </font></font><img src="https://habrastorage.org/webt/r7/ox/vy/r7oxvyc5mesbifpwfjtkcrtdr2q.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ajoutant un nouvel arbre</font></font><img src="https://habrastorage.org/webt/s2/vp/u0/s2vpu0-7pmzuzv0n55f1z1fgktu.jpeg" alt="image"><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/vg/ar/qn/vgarqnddik0vjhxxfsesr1t5qs8.jpeg" alt="image"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Les it√©rations se poursuivent jusqu'√† ce que la condition m = M soit remplie, apr√®s quoi la formation est termin√©e et le mod√®le d'ensemble f est obtenu. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
L'amplification du gradient est l'un des algorithmes d'apprentissage automatique les plus puissants. </font><font style="vertical-align: inherit;">Non seulement parce qu'il cr√©e des mod√®les tr√®s pr√©cis, mais aussi parce qu'il est capable de traiter d'√©normes ensembles de donn√©es avec des millions de donn√©es et de fonctionnalit√©s. </font><font style="vertical-align: inherit;">En r√®gle g√©n√©rale, sa pr√©cision est sup√©rieure √† celle d'une for√™t al√©atoire, mais en raison de sa nature coh√©rente, elle peut apprendre beaucoup plus lentement.</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr488346/index.html">Installation de ou-tools avec SCIP et GLPK dans un environnement virtuel Python 3.7 sous Linux</a></li>
<li><a href="../fr488348/index.html">Webinaire ¬´Les dix principaux d√©fis agiles et les moyens de les surmonter en une heure¬ª 17 f√©vrier √† 20 h, heure de Moscou</a></li>
<li><a href="../fr488352/index.html">Comparaison des co√ªts VDI: sur site et cloud public</a></li>
<li><a href="../fr488356/index.html">Formation √† l'Universit√© technique maritime de Saint-P√©tersbourg pour les produits Dassault Syst√®mes</a></li>
<li><a href="../fr488360/index.html">Mythes du Big Data et culture num√©rique</a></li>
<li><a href="../fr488366/index.html">Et encore une fois sur ¬´Informations de fuseau horaire incorrectes pour les fuseaux horaires russes¬ª [bogue .Net, ID: 693286]</a></li>
<li><a href="../fr488368/index.html">Ce que j'ai appris en travaillant sur mon premier projet √† grande √©chelle</a></li>
<li><a href="../fr488370/index.html">TDD pour microcontr√¥leurs. Partie 2: Comment les espions se d√©barrassent des d√©pendances</a></li>
<li><a href="../fr488374/index.html">T√©l√©gramme + 1C + Webhooks + Apache + Certificat auto-sign√©</a></li>
<li><a href="../fr488376/index.html">Quand le principe "au diable tout, prenez-le et faites-le!" ne fonctionne pas: notes du procrastinateur</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>