<!doctype html>
<html class="no-js" lang="fr">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>😒 📹 🍢 Classification multi-taguée 👩🏻‍💼 🐅 🧑</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bonjour, habrozhiteli! Nous avons décidé de citer un extrait du livre d'Andrei Burkov , Machine Learning Without Extra Words , consacré à la classific...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Classification multi-taguée</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/488362/"><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><img src="https://habrastorage.org/webt/hm/vf/c_/hmvfc_yyxepplv1mj0crw1vu7pw.jpeg" align="left" alt="image"></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bonjour, habrozhiteli! </font><font style="vertical-align: inherit;">Nous avons décidé de citer un extrait du livre d'Andrei Burkov </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, Machine Learning Without Extra Words</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , consacré à la classification.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pour décrire l'image sur la figure, plusieurs étiquettes peuvent être utilisées simultanément: «forêt de conifères», «montagnes», «route». Si le nombre de valeurs possibles pour les étiquettes est important, mais qu'elles ont toutes la même nature que les étiquettes, chaque échantillon étiqueté peut être converti en plusieurs données étiquetées, une pour chaque étiquette. Toutes ces nouvelles données auront les mêmes vecteurs de caractéristiques et une seule étiquette. Par conséquent, la tâche devient un problème de classification multiclasse. Il peut être résolu en utilisant la stratégie «un contre tous». La seule différence avec le problème habituel de classification multiclasse est l'apparition d'un nouveau hyperparamètre: le seuil. Si le score de similitude pour une étiquette est supérieur à une valeur seuil, cette étiquette est affectée au vecteur d'entité en entrée. Dans ce scénario, plusieurs étiquettes peuvent être affectées à un vecteur caractéristique.La valeur seuil est sélectionnée à l'aide de l'ensemble de contrôle.</font></font><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pour résoudre le problème de classification avec de nombreuses étiquettes, on peut également appliquer des algorithmes qui sont naturellement transformés en multiclasses (arbres de décision, régression logistique, réseaux de neurones, etc.). Ils renvoient une estimation pour chaque classe, afin que nous puissions définir un seuil puis attribuer plusieurs étiquettes à un vecteur d'entité pour lequel le score de proximité dépasse ce seuil. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Les réseaux de neurones peuvent naturellement être formés aux classifications multi-étiquettes en utilisant l'entropie croisée binaire comme fonction de coût. La couche de sortie du réseau neuronal dans ce cas a un nœud par étiquette. Chaque nœud de la couche de sortie a une fonction d'activation sigmoïde. En conséquence, chaque étiquette l est binaire</font></font><img src="https://habrastorage.org/webt/2n/_n/mg/2n_nmgvuciuadryomnq3urrg1xw.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">où l = 1, ..., L et i = 1, ..., N.L'entropie croisée binaire détermine la probabilité </font></font><img src="https://habrastorage.org/webt/lb/le/cf/lblecfgyuy23k8zvlasuajncsds.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">que l'échantillon xi ait le label l, est défini comme le </font></font><img src="https://habrastorage.org/webt/qd/1e/fx/qd1efxpmi6mmuhv_fhmsqpjgpi4.jpeg" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
critère de minimisation - une moyenne simple de tous les membres de l'entropie croisée binaire dans tous les échantillons d'apprentissage. et toutes leurs balises. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dans les cas où le nombre de valeurs d'étiquette possibles est petit, vous pouvez essayer de convertir le problème de classification avec de nombreuses étiquettes en un problème de classification multiclasse. </font><font style="vertical-align: inherit;">Imaginez le problème suivant. </font><font style="vertical-align: inherit;">Vous devez attribuer deux types d'étiquettes aux images. </font><font style="vertical-align: inherit;">Les étiquettes du premier type peuvent avoir deux significations possibles: { </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">photo, peinture</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> }; </font><font style="vertical-align: inherit;">les marques du deuxième type peuvent avoir trois significations possibles: { </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">portrait, paysage, autre</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">}. </font><font style="vertical-align: inherit;">Pour chaque combinaison de deux classes source, vous pouvez créer une nouvelle classe factice, par exemple:</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/mz/px/zn/mzpxzn0rlrumwoql7gkk3no7ihk.jpeg" alt="image"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Maintenant, nous avons les mêmes données balisées, mais nous avons remplacé l'ensemble de vraies étiquettes par une étiquette fictive avec des valeurs de 1 à 6. En pratique, cette approche donne de bons résultats lorsqu'il n'y a pas trop de combinaisons possibles de classes. Sinon, beaucoup plus de données de formation doivent être utilisées pour compenser l'augmentation de l'ensemble des classes. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Le principal avantage de cette dernière approche est que les étiquettes restent corrélées, contrairement aux méthodes décrites ci-dessus, qui prédisent chaque étiquette indépendamment les unes des autres. Dans de nombreuses tâches, la corrélation entre les étiquettes peut être un facteur important. Par exemple, imaginez que vous souhaitez classer les e-mails comme </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">spam</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> et </font><i><font style="vertical-align: inherit;">non- </font></i><i><font style="vertical-align: inherit;">spam</font></i></font><i><font style="vertical-align: inherit;"></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, et en même temps que ordinaire et important. </font><font style="vertical-align: inherit;">Vous voudrez probablement exclure des prévisions telles que [ </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">spam, important</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ].</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">7.5. </font><font style="vertical-align: inherit;">Formation d'ensemble</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Les algorithmes fondamentaux que nous avons traités au chapitre 3 ont leurs limites. En raison de sa simplicité, ils ne peuvent parfois pas créer un modèle suffisamment efficace pour votre tâche. Dans de tels cas, vous pouvez essayer d'utiliser des réseaux de neurones profonds. Cependant, dans la pratique, les réseaux de neurones profonds nécessitent une quantité importante de données étiquetées, que vous n'avez peut-être pas. Une autre façon d'augmenter l'efficacité d'algorithmes d'apprentissage simples consiste à utiliser la </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">formation d'ensemble</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La formation d'ensemble est un paradigme de formation qui est basé sur la formation non seulement d'un modèle super-correct, mais d'un grand nombre de modèles de faible précision et combinant les prévisions données par ces </font><font style="vertical-align: inherit;">modèles </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">faibles</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> pour obtenir un </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">métamodèle</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> plus correct </font><font style="vertical-align: inherit;">.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Les modèles de faible précision sont généralement formés par </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">des algorithmes d'apprentissage faibles</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> qui ne sont pas en mesure de former des modèles complexes et, par conséquent, affichent une vitesse élevée aux étapes de formation et de prévision. Le plus souvent, l'algorithme d'apprentissage de l'arbre de décision est utilisé comme algorithme faible, ce qui arrête généralement de casser l'ensemble d'apprentissage après plusieurs itérations. Le résultat est des arbres petits et pas très réguliers, mais, comme le dit l'idée de former l'ensemble, si les arbres ne sont pas identiques et que chaque arbre est au moins légèrement meilleur que la supposition aléatoire, nous pouvons obtenir une grande précision en combinant un grand nombre de ces arbres. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pour obtenir les prévisions finales pour l'entrée </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, les prévisions de tous les modèles faibles sont combinées à l'aide d'une méthode de vote pondérée. </font><font style="vertical-align: inherit;">La forme spécifique de pondération des votes dépend de l'algorithme, mais l'essentiel n'en dépend pas: si, collectivement, les modèles faibles prédisent que l'e-mail est du spam, nous attribuons </font><font style="vertical-align: inherit;">l'étiquette de </font><i><font style="vertical-align: inherit;">spam</font></i><font style="vertical-align: inherit;"> à l'échantillon </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">
Les deux principales méthodes de formation des ensembles sont le </font><b><font style="vertical-align: inherit;">boosting</font></b><font style="vertical-align: inherit;"> et l' </font><b><font style="vertical-align: inherit;">ensachage</font></b><font style="vertical-align: inherit;"> (agrégation). </font><font style="vertical-align: inherit;">Les traductions des termes boosting et bagging sont inexactes et peu habituelles.</font></font><i><font style="vertical-align: inherit;"></font></i><font style="vertical-align: inherit;"></font><br>
<br><font style="vertical-align: inherit;"></font><b><font style="vertical-align: inherit;"></font></b><font style="vertical-align: inherit;"></font><b><font style="vertical-align: inherit;"></font></b><font style="vertical-align: inherit;"></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">7.5.1. </font><font style="vertical-align: inherit;">Boosting et ensachage</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La méthode de boosting consiste à utiliser les données de formation initiales et à créer de manière itérative plusieurs modèles à l'aide d'un algorithme faible. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Chaque nouveau modèle diffère des précédents en ce que, en le construisant, un algorithme faible tente de «corriger» les erreurs commises par les modèles précédents. </font><font style="vertical-align: inherit;">Le modèle d'ensemble final est une combinaison de ces nombreux modèles de construction itérative faibles. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
L'essence de l'ensachage est de créer de nombreuses «copies» des données d'entraînement (chaque copie est légèrement différente des autres), puis d'appliquer un algorithme faible à chaque copie afin d'obtenir plusieurs modèles faibles, puis de les combiner. </font><font style="vertical-align: inherit;">Un algorithme d'apprentissage automatique largement utilisé et efficace basé sur l'idée de l'ensachage est une </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">forêt aléatoire</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">7.5.2. </font><font style="vertical-align: inherit;">Forêt aléatoire</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
L'algorithme d'ensachage «classique» fonctionne comme suit. </font><font style="vertical-align: inherit;">B échantillons aléatoires sont créés à partir de l'ensemble d'apprentissage existant </font></font><img src="https://habrastorage.org/webt/-d/6r/gn/-d6rgnsn5wk-yzl3dbk0i7x2ole.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(pour chaque b = 1, ..., B) et un </font></font><img src="https://habrastorage.org/webt/-d/6r/gn/-d6rgnsn5wk-yzl3dbk0i7x2ole.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">modèle d' </font></font><img src="https://habrastorage.org/webt/nd/ew/4z/ndew4zvx7r0jpeilfknakojmrbs.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">arbre de décision </font><font style="vertical-align: inherit;">est construit </font><font style="vertical-align: inherit;">sur la base de chaque échantillon </font><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Pour obtenir un échantillon </font></font><img src="https://habrastorage.org/webt/-d/6r/gn/-d6rgnsn5wk-yzl3dbk0i7x2ole.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">pour certains b, un </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">échantillon est fait avec remplacement</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Autrement dit, un échantillon vide est d'abord créé, puis un échantillon aléatoire est sélectionné dans l'ensemble d'apprentissage, et sa copie exacte est placée dans </font></font><img src="https://habrastorage.org/webt/-d/6r/gn/-d6rgnsn5wk-yzl3dbk0i7x2ole.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, tandis que l'échantillon lui-même reste dans l'ensemble d'apprentissage d'origine. </font><font style="vertical-align: inherit;">La sélection des données se poursuit jusqu'à ce que la condition soit remplie. </font></font><img src="https://habrastorage.org/webt/se/au/-5/seau-5gwous1c1cshmrx8rwubig.jpeg" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Suite à la formation, </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">des</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> arbres de décision </font><i><font style="vertical-align: inherit;">B</font></i><font style="vertical-align: inherit;"> sont </font><font style="vertical-align: inherit;">obtenus </font><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">La prévision du nouvel échantillon </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , en cas de régression, est déterminée comme la moyenne de </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">B</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> prévisions</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/re/zp/mj/rezpmjqa9lo7w4dxvqd6njwwqcm.jpeg" alt="image"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ou par un vote majoritaire en cas de classement.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La forêt aléatoire n'a qu'une seule différence avec l'ensachage classique. Il utilise un algorithme d'apprentissage d'arbre modifié qui, à chaque fractionnement du processus d'apprentissage, vérifie un sous-ensemble aléatoire de fonctionnalités. Ceci est fait afin d'éliminer la corrélation entre les arbres: si une ou plusieurs entités ont une grande capacité prédictive, de nombreux arbres les choisiront pour fractionner les données. Cela conduira à l'apparition dans la "forêt" d'un grand nombre d'arbres corrélés. La corrélation des signes avec une capacité prédictive élevée empêche la précision de la prédiction d'augmenter. La grande efficacité de l'ensemble des modèles s'explique par le fait que les bons modèles sont plus susceptibles d'être d'accord avec la même prévision, et les mauvais modèles ne sont pas susceptibles d'être d'accord et de donner des prévisions différentes. La corrélation rendra les modèles pauvres plus susceptibles d'être d'accord,ce qui faussera le schéma de vote ou affectera la moyenne.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Les hyperparamètres les plus importants pour le réglage sont le nombre d'arbres B et la taille d'un sous-ensemble aléatoire d'entités qui doivent être prises en compte pour chaque fractionnement. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La forêt aléatoire est l'un des algorithmes d'apprentissage d'ensemble les plus utilisés. Qu'est-ce qui détermine son efficacité? La raison en est qu'en utilisant plusieurs échantillons de l'ensemble de données d'origine, nous réduisons la </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">variance du</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> modèle final. N'oubliez pas qu'une faible variance signifie une faible prédisposition à se </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">recycler</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Le recyclage se produit lorsque le modèle essaie d'expliquer de petites variations dans l'ensemble de données, car l'ensemble de données n'est qu'un petit échantillon de tous les exemples possibles du phénomène que nous essayons de simuler. </font><font style="vertical-align: inherit;">Dans le cas d'une approche infructueuse de la formation de l'ensemble d'entraînement, certains artefacts indésirables (mais inévitables) peuvent y tomber: du bruit, des données anormales et excessivement ou insuffisamment représentatives. </font><font style="vertical-align: inherit;">En créant plusieurs échantillons aléatoires avec le remplacement de l'ensemble d'apprentissage, nous réduisons l'influence de ces artefacts.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">7.5.3. </font><font style="vertical-align: inherit;">Augmentation du gradient</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Un autre algorithme de formation d'ensemble efficace basé sur l'idée de boosting est le boosting de gradient. </font><font style="vertical-align: inherit;">Tout d'abord, considérez l'utilisation de l'augmentation du gradient dans la régression. </font><font style="vertical-align: inherit;">Nous allons commencer à construire un modèle de régression efficace avec un modèle constant </font></font><img src="https://habrastorage.org/webt/7u/7x/jd/7u7xjdufljpsjwwu45j9_gkc3r0.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(comme nous l'avons fait dans ID3):</font></font><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/em/vj/bv/emvjbvtmptxl_d3bihzev4wbh7o.jpeg" alt="image"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Modifiez ensuite les étiquettes dans tous les échantillons i = 1, ..., N dans le jeu d'apprentissage:</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/va/wc/ah/vawcahk0zsdpgnumh_bfjuozw_e.jpeg" alt="image"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
où </font></font><img src="https://habrastorage.org/webt/rf/1x/pk/rf1xpkgfxmcivy-1tqpwv2vgroi.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">est appelé le </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">reste</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> et est le nouveau libellé de l'échantillon </font></font><img src="https://habrastorage.org/webt/dk/ey/2r/dkey2rj3yf-zkei2029wfa2ujso.jpeg" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
. Nous utilisons maintenant l'ensemble d'apprentissage modifié avec les restes au lieu des libellés d'origine pour créer un nouveau modèle de l'arbre de décision. </font></font><img src="https://habrastorage.org/webt/mx/fw/um/mxfwumzpc5tq1wjpdate2rxra48.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Le modèle de stimulation est maintenant défini comme </font></font><img src="https://habrastorage.org/webt/cy/c2/vz/cyc2vz0tmrrihcm6kta_7rnwmui.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">où α est la vitesse d'apprentissage (hyperparamètre). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ensuite, nous recalculons les résidus à l'aide de l'équation 7.2, remplaçons à nouveau les étiquettes dans les données d'apprentissage, enseignons un nouveau modèle de l'arbre de décision, </font></font><img src="https://habrastorage.org/webt/p4/wk/nl/p4wknlhlvwiqtr7zz7lx_y5oq3s.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">redéfinissons le modèle de boost pendant que </font></font><img src="https://habrastorage.org/webt/n-/mn/ht/n-mnhtck0rar7pz4anzlbdc-bmo.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">nous répétons le processus, jusqu'à ce que nous combinions le nombre maximum prédéterminé </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">M d'</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> arbres.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Comprenons intuitivement ce qui se passe ici. En calculant les résidus, nous déterminons dans quelle mesure (ou mal) l'objectif de chaque échantillon d'apprentissage est prédit par le modèle actuel f. Ensuite, nous formons un autre arbre pour corriger les erreurs du modèle actuel (c'est pourquoi nous utilisons des restes au lieu d'étiquettes réelles) et ajoutons un nouvel arbre au modèle existant avec un certain poids α. Par conséquent, chaque nouvel arbre ajouté au modèle corrige partiellement les erreurs faites par les arbres précédents. Le processus se poursuit jusqu'à ce que le nombre maximum M (un autre hyperparamètre) des arbres soit combiné.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Essayons maintenant de répondre à la question de savoir pourquoi cet algorithme est appelé renforcement du gradient. En boosting de gradient, nous ne calculons pas le gradient, contrairement à ce que nous avons fait au chapitre 4, en résolvant le problème de régression linéaire. Pour voir les similitudes entre l'augmentation du gradient et la descente du gradient, rappelez-vous pourquoi nous avons calculé le gradient en régression linéaire: pour trouver la direction des valeurs des paramètres afin de minimiser la fonction de coût MSE. Le gradient montre la direction, mais ne montre pas jusqu'où aller dans cette direction, donc à chaque itération, nous avons fait un petit pas, puis déterminé à nouveau la direction. La même chose se produit en boosting de gradient, mais au lieu de calculer directement le gradient, nous utilisons son estimation sous forme de résidus: ils montrent comment le modèle doit être ajusté pour réduire l'erreur (résiduelle).</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En boosting de gradient, trois hyperparamètres principaux sont disponibles pour le réglage: le nombre d'arbres, la vitesse d'apprentissage et la profondeur des arbres. Les trois affectent la précision du modèle. La profondeur des arbres affecte également la vitesse d'apprentissage et de prévision: plus la profondeur est petite, plus vite. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
On peut montrer que l'apprentissage par résidus optimise le modèle global f pour la norme d'erreur standard. Ici, vous pouvez voir la différence par rapport à l'ensachage: le renforcement réduit le biais (ou le manque d'éducation) au lieu de la variance. En conséquence, le boosting est soumis à un recyclage. Cependant, en ajustant la profondeur et le nombre d'arbres, le recyclage peut être largement évité.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
L'amplification du dégradé est similaire pour les tâches de notation, mais les étapes sont légèrement différentes. Prenons le cas de la classification binaire. Supposons qu'il existe M arbres de décision de régression. Par analogie avec la régression logistique, la prévision de l'ensemble des arbres de décision est modélisée à l'aide de la fonction sigmoïde:</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/wy/pd/gw/wypdgwjjgpzuojrelehnadtdggc.jpeg" alt="image"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
où </font></font><img src="https://habrastorage.org/webt/w6/3d/kb/w63dkbj3f9j-ik9hrhqbexyxtem.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">est l'arbre de régression. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Et encore une fois, comme dans la régression logistique, lorsque l'on essaie de trouver un modèle f maximisant </font></font><img src="https://habrastorage.org/webt/c4/lv/tt/c4lvttexfzr8disbsv1ph_drzka.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, le principe du maximum de vraisemblance est appliqué. De même, pour éviter un débordement numérique, nous maximisons la somme des logarithmes de vraisemblance, plutôt que le produit de la vraisemblance. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
L'algorithme commence par le modèle constant initial </font></font><img src="https://habrastorage.org/webt/hn/7j/ep/hn7jepdxhudnxvjjgbulvfwtpyw.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">où </font></font><img src="https://habrastorage.org/webt/gj/6s/f6/gj6sf6i874m3gq3_fbbxbkrudae.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(on peut montrer qu'une telle initialisation est optimale pour la fonction sigmoïde.) Ensuite, à chaque itération m, un nouvel arbre fm est ajouté au modèle. Pour trouver le meilleur arbre </font></font><img src="https://habrastorage.org/webt/b0/5w/rv/b05wrvcuk5hzvnrmkpebdnk_kxu.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pour trouver le meilleur arbre </font></font><img src="https://habrastorage.org/webt/xm/u4/dp/xmu4dpm1hi3-podtuiayxggydti.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, la dérivée partielle du </font></font><img src="https://habrastorage.org/webt/ep/hx/gu/ephxgusen4ou8pc8atm4ksqathq.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">modèle actuel est d' </font><font style="vertical-align: inherit;">abord calculée </font><font style="vertical-align: inherit;">pour chaque i = 1, ..., N:</font></font><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/wg/v_/oo/wgv_oogvmupu4q5j6g3rq7dphvk.jpeg" alt="image"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
où f est le modèle du classificateur d'ensemble construit sur l'itération précédente m - 1. Pour calculer </font></font><img src="https://habrastorage.org/webt/ep/hx/gu/ephxgusen4ou8pc8atm4ksqathq.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, nous devons trouver les dérivées de par </font></font><img src="https://habrastorage.org/webt/pv/pe/lb/pvpelb2jplsmq_jmkbdfyl-pzom.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">rapport à f pour tout i. </font><font style="vertical-align: inherit;">Notez que la </font></font><img src="https://habrastorage.org/webt/nk/zn/0l/nkzn0lh0l0brnzl_vh0xxlmttrq.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">dérivée par rapport à f du bon terme dans l'équation précédente est</font></font><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/xx/en/iu/xxeniu2qr35ln17asbfk2fqb3sc.jpeg" alt="image"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ensuite, l'ensemble d'apprentissage est transformé en remplaçant l'étiquette d'origine de la </font></font><img src="https://habrastorage.org/webt/a1/fv/cu/a1fvcukqqvsu5wpv3x8i1zj5smc.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">dérivée partielle correspondante </font></font><img src="https://habrastorage.org/webt/ep/hx/gu/ephxgusen4ou8pc8atm4ksqathq.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, et une nouvelle arborescence est construite sur la base de l'ensemble d'apprentissage converti. </font></font><img src="https://habrastorage.org/webt/w1/8f/41/w18f41gto37doyvyakgs4np_fyy.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ensuite, l'étape de mise à jour optimale est déterminée </font></font><img src="https://habrastorage.org/webt/1n/cf/sj/1ncfsjxfe-tao3ep_csuw-s-arw.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">comme </font><font style="vertical-align: inherit;">suit </font><font style="vertical-align: inherit;">:</font></font><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/co/qc/hw/coqchwpctxgy2ukbdwaybkouxx0.jpeg" alt="image"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A la fin de l'itération m, nous mettons à jour le modèle d'ensemble en </font></font><img src="https://habrastorage.org/webt/r7/ox/vy/r7oxvyc5mesbifpwfjtkcrtdr2q.jpeg" alt="image"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ajoutant un nouvel arbre</font></font><img src="https://habrastorage.org/webt/s2/vp/u0/s2vpu0-7pmzuzv0n55f1z1fgktu.jpeg" alt="image"><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/vg/ar/qn/vgarqnddik0vjhxxfsesr1t5qs8.jpeg" alt="image"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Les itérations se poursuivent jusqu'à ce que la condition m = M soit remplie, après quoi la formation est terminée et le modèle d'ensemble f est obtenu. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
L'amplification du gradient est l'un des algorithmes d'apprentissage automatique les plus puissants. </font><font style="vertical-align: inherit;">Non seulement parce qu'il crée des modèles très précis, mais aussi parce qu'il est capable de traiter d'énormes ensembles de données avec des millions de données et de fonctionnalités. </font><font style="vertical-align: inherit;">En règle générale, sa précision est supérieure à celle d'une forêt aléatoire, mais en raison de sa nature cohérente, elle peut apprendre beaucoup plus lentement.</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr488346/index.html">Installation de ou-tools avec SCIP et GLPK dans un environnement virtuel Python 3.7 sous Linux</a></li>
<li><a href="../fr488348/index.html">Webinaire «Les dix principaux défis agiles et les moyens de les surmonter en une heure» 17 février à 20 h, heure de Moscou</a></li>
<li><a href="../fr488352/index.html">Comparaison des coûts VDI: sur site et cloud public</a></li>
<li><a href="../fr488356/index.html">Formation à l'Université technique maritime de Saint-Pétersbourg pour les produits Dassault Systèmes</a></li>
<li><a href="../fr488360/index.html">Mythes du Big Data et culture numérique</a></li>
<li><a href="../fr488366/index.html">Et encore une fois sur «Informations de fuseau horaire incorrectes pour les fuseaux horaires russes» [bogue .Net, ID: 693286]</a></li>
<li><a href="../fr488368/index.html">Ce que j'ai appris en travaillant sur mon premier projet à grande échelle</a></li>
<li><a href="../fr488370/index.html">TDD pour microcontrôleurs. Partie 2: Comment les espions se débarrassent des dépendances</a></li>
<li><a href="../fr488374/index.html">Télégramme + 1C + Webhooks + Apache + Certificat auto-signé</a></li>
<li><a href="../fr488376/index.html">Quand le principe "au diable tout, prenez-le et faites-le!" ne fonctionne pas: notes du procrastinateur</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>