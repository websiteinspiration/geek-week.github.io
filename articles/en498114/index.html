<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚õ∞Ô∏è üë©üèª‚Äçüè´ üéÖüèΩ One pixel attack. Or how to trick a neural network üéß üë©üèª‚Äçüíº üçô</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Let's get acquainted with one of the attacks on the neural network, which leads to classification errors with minimal external influences. Imagine for...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>One pixel attack. Or how to trick a neural network</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/498114/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Let's get acquainted with one of the attacks on the neural network, which leads to classification errors with minimal external influences. Imagine for a moment that the neural network is you. And at the moment, while drinking a cup of aromatic coffee, you classify the images of cats with an accuracy of more than 90 percent without even suspecting that the ‚Äúone-pixel attack‚Äù turned all your ‚Äúcats‚Äù into trucks. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
And now we will pause, move the coffee aside, import all the libraries we need and analyze how such one pixel attacks work.</font></font><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The purpose of this attack is to make the algorithm (neural network) give an incorrect answer. </font><font style="vertical-align: inherit;">Below we will see this with several different models of convolutional neural networks. </font><font style="vertical-align: inherit;">Using one of the methods of multidimensional mathematical optimization - differential evolution, we find a special pixel that can change the image so that the neural network begins to incorrectly classify this image (despite the fact that earlier the algorithm ‚Äúrecognized‚Äù the same image correctly and with high accuracy). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Import the libraries:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-comment"># Python Libraries</span><font></font>
%matplotlib inline<font></font>
<span class="hljs-keyword">import</span> pickle
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> matplotlib
<span class="hljs-keyword">from</span> keras.datasets <span class="hljs-keyword">import</span> cifar10
<span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> backend <span class="hljs-keyword">as</span> K<font></font>
<font></font>
<span class="hljs-comment"># Custom Networks</span>
<span class="hljs-keyword">from</span> networks.lenet <span class="hljs-keyword">import</span> LeNet
<span class="hljs-keyword">from</span> networks.pure_cnn <span class="hljs-keyword">import</span> PureCnn
<span class="hljs-keyword">from</span> networks.network_in_network <span class="hljs-keyword">import</span> NetworkInNetwork
<span class="hljs-keyword">from</span> networks.resnet <span class="hljs-keyword">import</span> ResNet
<span class="hljs-keyword">from</span> networks.densenet <span class="hljs-keyword">import</span> DenseNet
<span class="hljs-keyword">from</span> networks.wide_resnet <span class="hljs-keyword">import</span> WideResNet
<span class="hljs-keyword">from</span> networks.capsnet <span class="hljs-keyword">import</span> CapsNet<font></font>
<font></font>
<span class="hljs-comment"># Helper functions</span>
<span class="hljs-keyword">from</span> differential_evolution <span class="hljs-keyword">import</span> differential_evolution
<span class="hljs-keyword">import</span> helper<font></font>
<font></font>
matplotlib.style.use(<span class="hljs-string">'ggplot'</span>)
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
For our experiment, we will load the CIFAR-10 dataset containing real-world images divided into 10 classes.</font></font><br>
<br>
<pre><code class="python hljs">(x_train, y_train), (x_test, y_test) = cifar10.load_data()<font></font>
<font></font>
class_names = [<span class="hljs-string">'airplane'</span>, <span class="hljs-string">'automobile'</span>, <span class="hljs-string">'bird'</span>, <span class="hljs-string">'cat'</span>, <span class="hljs-string">'deer'</span>, <span class="hljs-string">'dog'</span>, <span class="hljs-string">'frog'</span>, <span class="hljs-string">'horse'</span>, <span class="hljs-string">'ship'</span>, <span class="hljs-string">'truck'</span>]
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Let's look at any image by its index. </font><font style="vertical-align: inherit;">For example, here on this horse.</font></font><br>
<br>
<pre><code class="python hljs">image_id = <span class="hljs-number">99</span> <span class="hljs-comment"># Image index in the test set</span><font></font>
helper.plot_image(x_test[image_id])<font></font>
</code></pre><br>
<img src="https://habrastorage.org/webt/gp/k9/gw/gpk9gwtmh-_e8e5jcni2vhcej70.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We will have to look for the very powerful pixel that can change the response of the neural network, which means it's time to write a function to change one or more pixels of the image.</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">perturb_image</span>(<span class="hljs-params">xs, img</span>):</span>
    <span class="hljs-comment"># If this function is passed just one perturbation vector,</span>
    <span class="hljs-comment"># pack it in a list to keep the computation the same</span>
    <span class="hljs-keyword">if</span> xs.ndim &lt; <span class="hljs-number">2</span>:<font></font>
        xs = np.array([xs])<font></font>
    <font></font>
    <span class="hljs-comment"># Copy the image n == len(xs) times so that we can </span>
    <span class="hljs-comment"># create n new perturbed images</span>
    tile = [len(xs)] + [<span class="hljs-number">1</span>]*(xs.ndim+<span class="hljs-number">1</span>)<font></font>
    imgs = np.tile(img, tile)<font></font>
    <font></font>
    <span class="hljs-comment"># Make sure to floor the members of xs as int types</span><font></font>
    xs = xs.astype(int)<font></font>
    <font></font>
    <span class="hljs-keyword">for</span> x,img <span class="hljs-keyword">in</span> zip(xs, imgs):
        <span class="hljs-comment"># Split x into an array of 5-tuples (perturbation pixels)</span>
        <span class="hljs-comment"># i.e., [[x,y,r,g,b], ...]</span>
        pixels = np.split(x, len(x) // <span class="hljs-number">5</span>)
        <span class="hljs-keyword">for</span> pixel <span class="hljs-keyword">in</span> pixels:
            <span class="hljs-comment"># At each pixel's x,y position, assign its rgb value</span><font></font>
            x_pos, y_pos, *rgb = pixel<font></font>
            img[x_pos, y_pos] = rgb<font></font>
    <font></font>
    <span class="hljs-keyword">return</span> imgs
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Check it out ?! </font><font style="vertical-align: inherit;">Change one pixel of our horse with coordinates (16, 16) to yellow.</font></font><br>
<br>
<pre><code class="python hljs">image_id = <span class="hljs-number">99</span> <span class="hljs-comment"># Image index in the test set</span>
pixel = np.array([<span class="hljs-number">16</span>, <span class="hljs-number">16</span>, <span class="hljs-number">255</span>, <span class="hljs-number">255</span>, <span class="hljs-number">0</span>]) <span class="hljs-comment"># pixel = x,y,r,g,b</span>
image_perturbed = perturb_image(pixel, x_test[image_id])[<span class="hljs-number">0</span>]<font></font>
<font></font>
helper.plot_image(image_perturbed)<font></font>
</code></pre><br>
<img src="https://habrastorage.org/webt/qj/sf/li/qjsflidtymruwiuotebu0r7siby.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
To demonstrate the attack, you need to download pre-trained models of neural networks on our CIFAR-10 dataset. </font><font style="vertical-align: inherit;">We will use two models lenet and resnet, but you can use others for your experiments by uncommenting the corresponding lines of code.</font></font><br>
<br>
<pre><code class="python hljs">lenet = LeNet()<font></font>
resnet = ResNet()<font></font>
<font></font>
models = [lenet, resnet]<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 After loading the models, it is necessary to evaluate the test images of each model to make sure that we attack only images that are correctly classified. </font><font style="vertical-align: inherit;">The code below displays the accuracy and number of parameters for each model.</font></font><br>
<br>
<pre><code class="python hljs">network_stats, correct_imgs = helper.evaluate_models(models, x_test, y_test)<font></font>
<font></font>
correct_imgs = pd.DataFrame(correct_imgs, columns=[<span class="hljs-string">'name'</span>, <span class="hljs-string">'img'</span>, <span class="hljs-string">'label'</span>, <span class="hljs-string">'confidence'</span>, <span class="hljs-string">'pred'</span>])<font></font>
<font></font>
network_stats = pd.DataFrame(network_stats, columns=[<span class="hljs-string">'name'</span>, <span class="hljs-string">'accuracy'</span>, <span class="hljs-string">'param_count'</span>])<font></font>
<font></font>
network_stats<font></font>
Evaluating lenet<font></font>
Evaluating resnet<font></font>
<font></font>
Out[<span class="hljs-number">11</span>]:<font></font>
<font></font>
<font></font>
	name        accuracy    param_count<font></font>
<span class="hljs-number">0</span>      lenet        <span class="hljs-number">0.748</span>       <span class="hljs-number">62006</span>
<span class="hljs-number">1</span>      resnet       <span class="hljs-number">0.9231</span>      <span class="hljs-number">470218</span><font></font>
<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 All such attacks can be divided into two classes: WhiteBox and BlackBox. The difference between them is that in the first case, we all reliably know about the algorithm, the model with which we are dealing. In the case of the BlackBox, all we need is input (image) and output (probabilities of being assigned to one of the classes). One pixel attack refers to the BlackBox. </font></font><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In this article, we consider two options for attacking a single pixel: untargeted and targeted. In the first case, it will absolutely not matter to which class the neural network of our cat will belong to, most importantly, not to the class of cats. Targeted attack is applicable when we want our cat to become a truck and only a truck.</font></font><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
But how to find the very pixels whose change will lead to a change in the class of the image? How to find a pixel by changing which one pixel attack becomes possible and successful? Let's try to formulate this problem as an optimization problem, but only in very simple words: with an untargeted attack, we must minimize trust in the desired class, and with targeted, maximize the trust in the target class. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
When carrying out such attacks, it is difficult to optimize the function using a gradient. An optimization algorithm must be used that does not rely on the smoothness of the function.</font></font><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Recall that for our experiment we use the CIFAR-10 dataset, which contains real-world images, 32 x 32 pixels in size, divided into 10 classes. </font><font style="vertical-align: inherit;">And this means that we have integer discrete values ‚Äã‚Äãfrom 0 to 31 and color intensities from 0 to 255, and the function is not expected to be smooth, but rather jagged, as shown below: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/vs/0k/mv/vs0kmvtq35lowxplx0rsq2-kppw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
That is why we use the differential evolution algorithm. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
But back to the code and write a function that returns the probability of the reliability of the model. </font><font style="vertical-align: inherit;">If the target class is correct, then we want to minimize this function so that the model is sure of another class (which is not true).</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict_classes</span>(<span class="hljs-params">xs, img, target_class, model, minimize=True</span>):</span>
    <span class="hljs-comment"># Perturb the image with the given pixel(s) x and get the prediction of the model</span><font></font>
    imgs_perturbed = perturb_image(xs, img)<font></font>
    predictions = model.predict(imgs_perturbed)[:,target_class]<font></font>
    <span class="hljs-comment"># This function should always be minimized, so return its complement if needed</span>
    <span class="hljs-keyword">return</span> predictions <span class="hljs-keyword">if</span> minimize <span class="hljs-keyword">else</span> <span class="hljs-number">1</span> - predictions<font></font>
<font></font>
image_id = <span class="hljs-number">384</span>
pixel = np.array([<span class="hljs-number">16</span>, <span class="hljs-number">13</span>,  <span class="hljs-number">25</span>, <span class="hljs-number">48</span>, <span class="hljs-number">156</span>])<font></font>
model = resnet<font></font>
<font></font>
true_class = y_test[image_id, <span class="hljs-number">0</span>]<font></font>
prior_confidence = model.predict_one(x_test[image_id])[true_class]<font></font>
confidence = predict_classes(pixel, x_test[image_id], true_class, model)[<span class="hljs-number">0</span>]<font></font>
<font></font>
print(<span class="hljs-string">'Confidence in true class'</span>, class_names[true_class], <span class="hljs-string">'is'</span>, confidence)<font></font>
print(<span class="hljs-string">'Prior confidence was'</span>, prior_confidence)<font></font>
helper.plot_image(perturb_image(pixel, x_test[image_id])[<span class="hljs-number">0</span>])<font></font>
<font></font>
Confidence <span class="hljs-keyword">in</span> true <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">bird</span> <span class="hljs-title">is</span> 0.00018887444
<span class="hljs-title">Prior</span> <span class="hljs-title">confidence</span> <span class="hljs-title">was</span> 0.70661753
</span></code></pre><br>
<img src="https://habrastorage.org/webt/re/g3/yn/reg3ync4qxac0hxb5wrr0jus_je.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We will need the next function to confirm the criterion for the success of the attack, it will return True when the change was enough to trick the model.</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">attack_success</span>(<span class="hljs-params">x, img, target_class, model, targeted_attack=False, verbose=False</span>):</span>
    <span class="hljs-comment"># Perturb the image with the given pixel(s) and get the prediction of the model</span><font></font>
    attack_image = perturb_image(x, img)<font></font>
    confidence = model.predict(attack_image)[<span class="hljs-number">0</span>]<font></font>
    predicted_class = np.argmax(confidence)<font></font>
    <font></font>
    <span class="hljs-comment"># If the prediction is what we want (misclassification or </span>
    <span class="hljs-comment"># targeted classification), return True</span>
    <span class="hljs-keyword">if</span> verbose:<font></font>
        print(<span class="hljs-string">'Confidence:'</span>, confidence[target_class])
    <span class="hljs-keyword">if</span> ((targeted_attack <span class="hljs-keyword">and</span> predicted_class == target_class) <span class="hljs-keyword">or</span>
        (<span class="hljs-keyword">not</span> targeted_attack <span class="hljs-keyword">and</span> predicted_class != target_class)):
        <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span>
    <span class="hljs-comment"># <span class="hljs-doctag">NOTE:</span> return None otherwise (not False), due to how Scipy handles its callback function</span>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Let's look at the work of the success criterion function. </font><font style="vertical-align: inherit;">In order to demonstrate, we assume a non-target attack.</font></font><br>
<br>
<pre><code class="python hljs">image_id = <span class="hljs-number">541</span>
pixel = np.array([<span class="hljs-number">17</span>, <span class="hljs-number">18</span>, <span class="hljs-number">185</span>, <span class="hljs-number">36</span>, <span class="hljs-number">215</span>])<font></font>
model = resnet<font></font>
<font></font>
true_class = y_test[image_id, <span class="hljs-number">0</span>]<font></font>
prior_confidence = model.predict_one(x_test[image_id])[true_class]<font></font>
success = attack_success(pixel, x_test[image_id], true_class, model, verbose=<span class="hljs-literal">True</span>)<font></font>
<font></font>
print(<span class="hljs-string">'Prior confidence'</span>, prior_confidence)<font></font>
print(<span class="hljs-string">'Attack success:'</span>, success == <span class="hljs-literal">True</span>)<font></font>
helper.plot_image(perturb_image(pixel, x_test[image_id])[<span class="hljs-number">0</span>])<font></font>
Confidence: <span class="hljs-number">0.07460087</span>
Prior confidence <span class="hljs-number">0.50054216</span>
Attack success: <span class="hljs-literal">True</span>
</code></pre><br>
<img src="https://habrastorage.org/webt/vh/qb/vp/vhqbvpxy3blhf-krbwmmtezadbu.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
It's time to collect all the puzzles into one picture. </font><font style="vertical-align: inherit;">We will use a small modification of the implementation of differential evolution in Scipy.</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">attack</span>(<span class="hljs-params">img_id, model, target=None, pixel_count=<span class="hljs-number">1</span>, 
           maxiter=<span class="hljs-number">75</span>, popsize=<span class="hljs-number">400</span>, verbose=False</span>):</span>
    <span class="hljs-comment"># Change the target class based on whether this is a targeted attack or not</span>
    targeted_attack = target <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>
    target_class = target <span class="hljs-keyword">if</span> targeted_attack <span class="hljs-keyword">else</span> y_test[img_id, <span class="hljs-number">0</span>]<font></font>
    <font></font>
    <span class="hljs-comment"># Define bounds for a flat vector of x,y,r,g,b values</span>
    <span class="hljs-comment"># For more pixels, repeat this layout</span>
    bounds = [(<span class="hljs-number">0</span>,<span class="hljs-number">32</span>), (<span class="hljs-number">0</span>,<span class="hljs-number">32</span>), (<span class="hljs-number">0</span>,<span class="hljs-number">256</span>), (<span class="hljs-number">0</span>,<span class="hljs-number">256</span>), (<span class="hljs-number">0</span>,<span class="hljs-number">256</span>)] * pixel_count<font></font>
    <font></font>
    <span class="hljs-comment"># Population multiplier, in terms of the size of the perturbation vector x</span>
    popmul = max(<span class="hljs-number">1</span>, popsize // len(bounds))<font></font>
    <font></font>
    <span class="hljs-comment"># Format the predict/callback functions for the differential evolution algorithm</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict_fn</span>(<span class="hljs-params">xs</span>):</span>
        <span class="hljs-keyword">return</span> predict_classes(xs, x_test[img_id], target_class, <font></font>
                               model, target <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>)<font></font>
    <font></font>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">callback_fn</span>(<span class="hljs-params">x, convergence</span>):</span>
        <span class="hljs-keyword">return</span> attack_success(x, x_test[img_id], target_class, <font></font>
                              model, targeted_attack, verbose)<font></font>
    <font></font>
    <span class="hljs-comment"># Call Scipy's Implementation of Differential Evolution</span><font></font>
    attack_result = differential_evolution(<font></font>
        predict_fn, bounds, maxiter=maxiter, popsize=popmul,<font></font>
        recombination=<span class="hljs-number">1</span>, atol=<span class="hljs-number">-1</span>, callback=callback_fn, polish=<span class="hljs-literal">False</span>)<font></font>
<font></font>
    <span class="hljs-comment"># Calculate some useful statistics to return from this function</span>
    attack_image = perturb_image(attack_result.x, x_test[img_id])[<span class="hljs-number">0</span>]<font></font>
    prior_probs = model.predict_one(x_test[img_id])<font></font>
    predicted_probs = model.predict_one(attack_image)<font></font>
    predicted_class = np.argmax(predicted_probs)<font></font>
    actual_class = y_test[img_id, <span class="hljs-number">0</span>]<font></font>
    success = predicted_class != actual_class<font></font>
    cdiff = prior_probs[actual_class] - predicted_probs[actual_class]<font></font>
<font></font>
    <span class="hljs-comment"># Show the best attempt at a solution (successful or not)</span><font></font>
    helper.plot_image(attack_image, actual_class, class_names, predicted_class)<font></font>
<font></font>
    <span class="hljs-keyword">return</span> [model.name, pixel_count, img_id, actual_class, predicted_class, success, cdiff, prior_probs, predicted_probs, attack_result.x]
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
It's time to share the results of the study (the attack) and see how changing only one pixel will turn a frog into a dog, a cat into a frog, and a car into an airplane. </font><font style="vertical-align: inherit;">But the more image points are allowed to change, the higher the likelihood of a successful attack on any image. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ga/mc/r2/gamcr2jr7fgitczf7wevptkafwa.png"><img src="https://habrastorage.org/webt/hc/df/xg/hcdfxgdnndfgcug3bb4hn9ij6xk.png"><img src="https://habrastorage.org/webt/qn/l_/zj/qnl_zjqfyz67jemvl5fewdbvvii.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Demonstrate a successful attack on a frog image using the resnet model. </font><font style="vertical-align: inherit;">We should see confidence in the true decline in class after several iterations.</font></font><br>
<br>
<pre><code class="python hljs">image_id = <span class="hljs-number">102</span>
pixels = <span class="hljs-number">1</span> <span class="hljs-comment"># Number of pixels to attack</span><font></font>
model = resnet<font></font>
<font></font>
_ = attack(image_id, model, pixel_count=pixels, verbose=<span class="hljs-literal">True</span>)<font></font>
<font></font>
Confidence: <span class="hljs-number">0.9938618</span>
Confidence: <span class="hljs-number">0.77454716</span>
Confidence: <span class="hljs-number">0.77454716</span>
Confidence: <span class="hljs-number">0.77454716</span>
Confidence: <span class="hljs-number">0.77454716</span>
Confidence: <span class="hljs-number">0.77454716</span>
Confidence: <span class="hljs-number">0.53226393</span>
Confidence: <span class="hljs-number">0.53226393</span>
Confidence: <span class="hljs-number">0.53226393</span>
Confidence: <span class="hljs-number">0.53226393</span>
Confidence: <span class="hljs-number">0.4211318</span>
</code></pre><br>
<img src="https://habrastorage.org/webt/a2/bw/rh/a2bwrhij-hnpjshjqcpmigtisyi.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
These were examples of an untargeted attack, and now we will conduct a targeted attack and choose to which class we would like the model to classify the image. </font><font style="vertical-align: inherit;">The task is much more complicated than the previous one, because we will make the neural network classify the image of a ship as a car, and a horse as a cat. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/yr/n5/sw/yrn5swb3-4bqoyfujgxz6dfdome.png"><img src="https://habrastorage.org/webt/li/cp/iy/licpiypraj8dcsuxolvh2etbdde.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Below we will try to get lenet to classify the image of the ship as a car.</font></font><br>
<br>
<pre><code class="python hljs">image_id = <span class="hljs-number">108</span>
target_class = <span class="hljs-number">1</span> <span class="hljs-comment"># Integer in range 0-9</span>
pixels = <span class="hljs-number">3</span><font></font>
model = lenet<font></font>
<font></font>
print(<span class="hljs-string">'Attacking with target'</span>, class_names[target_class])<font></font>
_ = attack(image_id, model, target_class, pixel_count=pixels, verbose=<span class="hljs-literal">True</span>)<font></font>
Attacking <span class="hljs-keyword">with</span> target automobile<font></font>
Confidence: <span class="hljs-number">0.044409167</span>
Confidence: <span class="hljs-number">0.044409167</span>
Confidence: <span class="hljs-number">0.044409167</span>
Confidence: <span class="hljs-number">0.054611664</span>
Confidence: <span class="hljs-number">0.054611664</span>
Confidence: <span class="hljs-number">0.054611664</span>
Confidence: <span class="hljs-number">0.054611664</span>
Confidence: <span class="hljs-number">0.054611664</span>
Confidence: <span class="hljs-number">0.054611664</span>
Confidence: <span class="hljs-number">0.054611664</span>
Confidence: <span class="hljs-number">0.054611664</span>
Confidence: <span class="hljs-number">0.054611664</span>
Confidence: <span class="hljs-number">0.054611664</span>
Confidence: <span class="hljs-number">0.054611664</span>
Confidence: <span class="hljs-number">0.054611664</span>
Confidence: <span class="hljs-number">0.081972085</span>
Confidence: <span class="hljs-number">0.081972085</span>
Confidence: <span class="hljs-number">0.081972085</span>
Confidence: <span class="hljs-number">0.081972085</span>
Confidence: <span class="hljs-number">0.1537778</span>
Confidence: <span class="hljs-number">0.1537778</span>
Confidence: <span class="hljs-number">0.1537778</span>
Confidence: <span class="hljs-number">0.22246778</span>
Confidence: <span class="hljs-number">0.23916133</span>
Confidence: <span class="hljs-number">0.25238588</span>
Confidence: <span class="hljs-number">0.25238588</span>
Confidence: <span class="hljs-number">0.25238588</span>
Confidence: <span class="hljs-number">0.44560355</span>
Confidence: <span class="hljs-number">0.44560355</span>
Confidence: <span class="hljs-number">0.44560355</span>
Confidence: <span class="hljs-number">0.5711696</span>
</code></pre><br>
<img src="https://habrastorage.org/webt/xi/ym/pn/xiympn3dd-xburofpy5tl34yybu.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Having dealt with single cases of attacks, we will collect statistics using the architecture of convolutional neural networks ResNet, going through each model, changing 1, 3 or 5 pixels of each image. </font><font style="vertical-align: inherit;">In this article, we show the final conclusions without bothering the reader to familiarize themselves with each iteration, since it takes a lot of time and computational resources.</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">attack_all</span>(<span class="hljs-params">models, samples=<span class="hljs-number">500</span>, pixels=(<span class="hljs-params"><span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">5</span></span>), targeted=False, 
               maxiter=<span class="hljs-number">75</span>, popsize=<span class="hljs-number">400</span>, verbose=False</span>):</span><font></font>
    results = []<font></font>
    <span class="hljs-keyword">for</span> model <span class="hljs-keyword">in</span> models:<font></font>
        model_results = []<font></font>
        valid_imgs = correct_imgs[correct_imgs.name == model.name].img<font></font>
        img_samples = np.random.choice(valid_imgs, samples, replace=<span class="hljs-literal">False</span>)<font></font>
        <font></font>
        <span class="hljs-keyword">for</span> pixel_count <span class="hljs-keyword">in</span> pixels:
            <span class="hljs-keyword">for</span> i, img_id <span class="hljs-keyword">in</span> enumerate(img_samples):<font></font>
                print(<span class="hljs-string">'\n'</span>, model.name, <span class="hljs-string">'- image'</span>, img_id, <span class="hljs-string">'-'</span>, i+<span class="hljs-number">1</span>, <span class="hljs-string">'/'</span>, len(img_samples))<font></font>
                targets = [<span class="hljs-literal">None</span>] <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> targeted <span class="hljs-keyword">else</span> range(<span class="hljs-number">10</span>)<font></font>
                <font></font>
                <span class="hljs-keyword">for</span> target <span class="hljs-keyword">in</span> targets:
                    <span class="hljs-keyword">if</span> targeted:<font></font>
                        print(<span class="hljs-string">'Attacking with target'</span>, class_names[target])
                        <span class="hljs-keyword">if</span> target == y_test[img, <span class="hljs-number">0</span>]:
                            <span class="hljs-keyword">continue</span><font></font>
                    result = attack(img_id, model, target, pixel_count, <font></font>
                                    maxiter=maxiter, popsize=popsize, <font></font>
                                    verbose=verbose)<font></font>
                    model_results.append(result)<font></font>
                    <font></font>
        results += model_results<font></font>
        helper.checkpoint(results, targeted)<font></font>
    <span class="hljs-keyword">return</span> results<font></font>
<font></font>
untargeted = attack_all(models, samples=<span class="hljs-number">100</span>, targeted=<span class="hljs-literal">False</span>)<font></font>
<font></font>
targeted = attack_all(models, samples=<span class="hljs-number">10</span>, targeted=<span class="hljs-literal">False</span>)
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
To test the possibility of network discredit, an algorithm was developed and its effect on the forecast quality of the pattern recognition solution was measured. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Let's see the final results.</font></font><br>
<br>
<pre><code class="python hljs">untargeted, targeted = helper.load_results()<font></font>
<font></font>
columns = [<span class="hljs-string">'model'</span>, <span class="hljs-string">'pixels'</span>, <span class="hljs-string">'image'</span>, <span class="hljs-string">'true'</span>, <span class="hljs-string">'predicted'</span>, <span class="hljs-string">'success'</span>, <span class="hljs-string">'cdiff'</span>, <span class="hljs-string">'prior_probs'</span>, <span class="hljs-string">'predicted_probs'</span>, <span class="hljs-string">'perturbation'</span>]<font></font>
<font></font>
untargeted_results = pd.DataFrame(untargeted, columns=columns)<font></font>
targeted_results = pd.DataFrame(targeted, columns=columns)<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The table below shows that using the ResNet neural network with an accuracy of 0.9231, changing several pixels of the image, we got a very good percentage of successfully attacked images (attack_success_rate).</font></font><br>
<br>
<pre><code class="python hljs">helper.attack_stats(targeted_results, models, network_stats)<font></font>
Out[<span class="hljs-number">26</span>]:<font></font>
	model	accuracy   pixels	attack_success_rate<font></font>
<span class="hljs-number">0</span>	resnet	<span class="hljs-number">0.9231</span>	    <span class="hljs-number">1</span>	        <span class="hljs-number">0.144444</span>
<span class="hljs-number">1</span>	resnet	<span class="hljs-number">0.9231</span>	    <span class="hljs-number">3</span>	        <span class="hljs-number">0.211111</span>
<span class="hljs-number">2</span>	resnet	<span class="hljs-number">0.9231</span>	    <span class="hljs-number">5</span>	        <span class="hljs-number">0.222222</span><font></font>
<font></font>
helper.attack_stats(untargeted_results, models, network_stats)<font></font>
Out[<span class="hljs-number">27</span>]:<font></font>
	model	accuracy   pixels	attack_success_rate<font></font>
<span class="hljs-number">0</span>	resnet	<span class="hljs-number">0.9231</span>	   <span class="hljs-number">1</span>	        <span class="hljs-number">0.34</span>
<span class="hljs-number">1</span>	resnet	<span class="hljs-number">0.9231</span>	   <span class="hljs-number">3</span>	        <span class="hljs-number">0.79</span>
<span class="hljs-number">2</span>	resnet	<span class="hljs-number">0.9231</span>	   <span class="hljs-number">5</span>	        <span class="hljs-number">0.79</span>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In your experiments, you are free to use other architectures of artificial neural networks, as there are currently a great many of them. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ta/lw/tv/talwtvpnwkzp2od0p-8q9o28eli.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Neural networks have enveloped the modern world with invisible threads. For a long time, services have been invented where, using AI (artificial intelligence), users receive processed photos stylistically similar to the work of great artists, and today the algorithms themselves can draw pictures, create musical masterpieces, write books and even scripts for films. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Areas such as computer vision, face recognition, unmanned vehicles, diagnosis of diseases - make important decisions and do not have the right to make mistakes, and interference with the operation of the algorithms will lead to disastrous consequences.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
One pixel attack is one way to spoof attacks. </font><font style="vertical-align: inherit;">To test the possibility of network discredit, an algorithm was developed and its effect on the forecast quality of the pattern recognition solution was measured. </font><font style="vertical-align: inherit;">The result showed that the applied convolutional architectures of neural networks are vulnerable to the specially trained One pixel attack algorithm, which replaces one pixel, in order to discredit the recognition algorithm. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The article was prepared by </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Alexander Andronic</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> and </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Adrey Cherny-Tkach</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> as part of an internship at </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Data4</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../en498094/index.html">How we transferred all internal communications in the company to online</a></li>
<li><a href="../en498096/index.html">How a Memories demo fits in 256 bytes</a></li>
<li><a href="../en498100/index.html">Designing Kubernetes Clusters: How Many Should There Be?</a></li>
<li><a href="../en498104/index.html">How sites are developed from $ 100,000</a></li>
<li><a href="../en498110/index.html">‚ÄúWhat to do when dramatic changes in processes demotivate a team.‚Äù The experience of the backend engineer who became a team leader</a></li>
<li><a href="../en498118/index.html">Conferences on the remote: online transformation of the JUG Ru Group</a></li>
<li><a href="../en498120/index.html">Build Time Optimization - Part 1</a></li>
<li><a href="../en498122/index.html">How to test on a remote site so as not to ruin the product and your life</a></li>
<li><a href="../en498124/index.html">Telegram for tech support: tools and pitfalls</a></li>
<li><a href="../en498126/index.html">How Amazon is trying to get people to buy ... less</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>