<!doctype html>
<html class="no-js" lang="pt">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🍽️ 😨 🧑🏿‍🤝‍🧑🏻 Unity Machine Learning: ensinando agentes do MO a pular muros 📔 🎙️ 👩🏽‍⚕️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Houve grandes avanços no aprendizado por reforço (RL) nos últimos anos: desde o primeiro uso bem-sucedido no treinamento de pixels brutos até o treina...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Unity Machine Learning: ensinando agentes do MO a pular muros</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/pixonic/blog/492548/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Houve grandes avanços no aprendizado por reforço (RL) nos últimos anos: desde o primeiro uso bem-sucedido no treinamento de pixels brutos até o treinamento de roboristas de IA aberta, e ambientes cada vez mais sofisticados são necessários para progresso adicional, para o qual ajudar A unidade vem. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A ferramenta Unity ML-Agents é um novo plug-in no mecanismo do jogo Unity, permitindo que você use o Unity como um construtor de ambiente para treinar agentes de MO. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Desde jogar futebol a caminhar, pular de paredes e treinar cães de IA para jogar bengalas, o Unity ML-Agents Toolkit fornece uma ampla gama de condições para os agentes de treinamento. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Neste artigo, veremos como os agentes do Unity MO funcionam e, em seguida, ensinaremos um desses agentes a pular muros.</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/9c6/5ad/bdb/9c65adbdbb38b939262c10e5ac728dde.gif" alt="imagem"><br>
<br>
<a name="habracut"></a><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O que são os agentes ML da Unity?</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O Unity ML-Agents é um novo plug-in para o mecanismo de jogo Unity, que permite criar ou usar ambientes prontos para o treinamento de nossos agentes. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O plug-in consiste em três componentes: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/zo/mx/m_/zomxm_3g9svsej0iarjpw1vbnta.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O primeiro - um </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ambiente de aprendizado</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ( </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">o Ambiente de Aprendizado</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ), contendo cenas do Unity e de elementos ambientais. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A segunda é a </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">API Python</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , que contém os algoritmos RL (como PPO - Proximal Policy Optimization e SAC - Soft Actor-Critic). </font><font style="vertical-align: inherit;">Usamos essa API para iniciar treinamentos, testes, etc. Ela é conectada ao ambiente de aprendizado através do terceiro componente - </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">um comunicador externo</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Em que consiste o ambiente de aprendizagem</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O componente de treinamento consiste em vários elementos: </font></font><br>
 <br>
<img src="https://habrastorage.org/webt/hy/1u/pt/hy1upt9en2esnt-amk-iipklmn4.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O primeiro agente é o ator de palco. </font><font style="vertical-align: inherit;">É ele quem treinaremos otimizando um componente chamado "Cérebro", no qual são registradas as ações que devem ser executadas em cada um dos estados possíveis. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O terceiro elemento, a Academy, gerencia agentes e seus processos de tomada de decisão e solicitações de processos da API do Python. </font><font style="vertical-align: inherit;">Para entender melhor seu papel, vamos relembrar o processo de RL. </font><font style="vertical-align: inherit;">Pode ser representado como um ciclo que funciona da seguinte maneira: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/5y/ge/ef/5ygeefzasafou6bkwggdnaq7ows.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Suponha que um agente precise aprender a jogar um jogo de plataformas. </font><font style="vertical-align: inherit;">O processo RL neste caso será parecido com o seguinte:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O agente recebe o estado </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">S </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0</font></font></sub></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> do ambiente - este será o primeiro quadro do nosso jogo.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Com base no estado </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">S </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0, o</font></font></sub></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> agente executa a ação </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0</font></font></sub></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> e muda para a direita.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O ambiente entra em um novo estado </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">S </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O agente recebe recompensa </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">R </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> por não estar morto ( </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">recompensa positiva</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> +1).</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Esse ciclo de RL forma uma sequência de estado, ação e recompensa. </font><font style="vertical-align: inherit;">O objetivo do agente é maximizar a recompensa total esperada. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/qm/mr/ml/qmmrmlkz7urj8cywqpsg1ejajve.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Assim, a Academy envia instruções aos agentes e fornece sincronização em sua execução, a saber:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Coleção de observações;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A escolha da ação de acordo com as instruções estabelecidas;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Execução de ação;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Redefina se o número de etapas foi atingido ou a meta foi alcançada.</font></font></li>
</ul><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ensinamos o agente a pular através das paredes</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Agora que sabemos como os agentes da Unity funcionam, treinaremos um para pular paredes. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Modelos já treinados também podem ser baixados no </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GitHub</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ambiente de aprendizagem Wall Jumping</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O objetivo desse ambiente é ensinar o agente a chegar ao ladrilho verde. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Considere três casos: </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
1. Não há paredes e nosso agente só precisa chegar ao ladrilho. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/17a/8fd/da8/17a8fdda8bbe767f64a34d8d05fd9be9.png" alt="imagem"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
2. O agente precisa aprender a pular para alcançar o ladrilho verde. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/d2c/149/0d0/d2c1490d0b90c4fdefb8eb4922f446b2.png" alt="imagem"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
3. O caso mais difícil: a parede é muito alta para o agente pular, então ele precisa pular primeiro no bloco branco. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/458/175/9b6/4581759b653a175597054fcf60a0050e.png" alt="imagem"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ensinaremos ao agente dois cenários de comportamento, dependendo da altura do muro:</font></font><br>
<br>
<ul>
<li><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SmallWallJump</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> em caixas sem paredes ou em baixas alturas;</font></font></li>
<li><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">BigWallJump</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> no caso de muros altos.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
É assim que o sistema de recompensa será: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/rt/lo/hm/rtlohmnturcucyikrnhfsadztgi.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Em nossas observações, não estamos usando um quadro regular, mas 14 reykast, cada um dos quais pode detectar quatro objetos possíveis. </font><font style="vertical-align: inherit;">Nesse caso, o reykast pode ser percebido como raios laser que podem determinar se eles passam por um objeto. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Também usaremos a posição de agente global em nosso programa. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/a06/0fc/fa2/a060fcfa26e80154ef988d76dc9b8f05.png" alt="imagem"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Quatro opções são possíveis em nosso espaço: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ni/g6/zs/nig6zskfzterisy_hskg3k-rsnk.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O objetivo é obter um ladrilho verde </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">com uma recompensa média de 0,8</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Então vamos começar!</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Primeiro de tudo, abra o projeto </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">UnitySDK</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Entre os exemplos, você precisa encontrar e abrir a cena do </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">WallJump</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Como você pode ver, existem muitos agentes no palco, cada um deles retirado da mesma pré-fabricada, e todos eles têm o mesmo "cérebro". </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/bad/195/820/bad195820da3e5b2c74309e9f44e9935.png" alt="imagem"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Como no caso do clássico Deep Reinforcement Learning, depois de lançarmos várias instâncias do jogo (por exemplo, 128 ambientes paralelos), agora apenas copiamos e colamos os agentes para ter estados mais diferentes. E como queremos treinar nosso agente do zero, precisamos primeiro remover o "cérebro" do agente. Para fazer isso, vá para a pasta prefabs e abra Prefab.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Em seguida, na hierarquia de Prefab, você precisa selecionar o agente e ir para as configurações. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nos Parâmetros de comportamento, você precisa excluir o modelo. Se tivermos várias GPUs à nossa disposição, você poderá usar o Dispositivo de Inferência da CPU como uma GPU. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/d0d/bdb/6ec/d0dbdb6ec05655b6fecdf675d629d126.png" alt="imagem"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
No componente Agente de salto em parede, você deve remover o cérebro para uma caixa sem paredes, bem como para paredes baixas e altas. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/618/e81/c8c/618e81c8c3c0cb15ae5fbca791c33c8c.png" alt="imagem"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Depois disso, você pode começar a treinar seu agente do zero. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para nosso primeiro treinamento, simplesmente alteramos o número total de etapas de treinamento para dois cenários de comportamento: SmallWallJump e BigWallJump. Para que possamos atingir a meta em apenas 300 mil etapas. Para fazer isso, em </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">config / trainer config.yaml,</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> altere max_steps para 3e5 para os casos SmallWallJump e BigWallJump.</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/b80/bbb/1b2/b80bbb1b21f0f9e445fa8d2583c39d52.png" alt="imagem"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para treinar nosso agente, usaremos o </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PPO</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (Proximal Policy Optimization). O algoritmo inclui o acúmulo de experiência em interagir com o ambiente e usá-lo para atualizar políticas de tomada de decisão. Após atualizá-lo, os eventos anteriores são descartados e a coleta de dados subsequente já é realizada sob os termos da política atualizada. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Então, primeiro, usando a API Python, precisamos chamar um comunicador externo para instruir a Academia a iniciar agentes. Para fazer isso, abra o terminal onde o ml-agents-master está localizado e digite: </font></font><br>
<br>
<code>mlagents-learn config/trainer_config.yaml — run-id=”WallJump_FirstTrain” — train</code><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Este comando solicitará que você inicie a cena do Unity. Para fazer isso, pressione ► na parte superior do editor. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/212/874/4fb/2128744fb143fec6308d95938e0383f7.png" alt="imagem"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Você pode assistir ao treinamento de seus agentes no Tensorboard com o seguinte comando:</font></font><br>
<br>
<code>tensorboard — logdir=summaries</code><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Quando o treinamento terminar, você precisará mover os arquivos de modelo salvos contidos em </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ml-agents-master / models para UnitySDK / Assets / ML-Agents / examples / WallJump / TFModels</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Em seguida, abra o editor do Unity novamente e selecione a cena </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">WallJump</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , onde abrimos o objeto </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">WallJumpArea</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> finalizado </font><font style="vertical-align: inherit;">. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Depois disso, selecione o agente e, em seus parâmetros de comportamento, arraste o arquivo </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SmallWallJump.nn</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> para o </font><i><font style="vertical-align: inherit;">Espaço</font></i><font style="vertical-align: inherit;"> reservado para modelo. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/d0d/bdb/6ec/d0dbdb6ec05655b6fecdf675d629d126.png" alt="imagem"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Mova também:</font></font><br>
<br>
<ol>
<li><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SmallWallJump.nn</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> no espaço </font><i><font style="vertical-align: inherit;">reservado</font></i><font style="vertical-align: inherit;"> no cérebro da parede.</font></font></li>
<li><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SmallWallJump.nn</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> no </font><i><font style="vertical-align: inherit;">espaço</font></i><font style="vertical-align: inherit;"> reservado do cérebro de parede pequena.</font></font></li>
<li><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">BigWallJump.nn</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> no </font><i><font style="vertical-align: inherit;">espaço</font></i><font style="vertical-align: inherit;"> reservado para o cérebro sem parede.</font></font></li>
</ol><br>
<img src="https://habrastorage.org/getpro/habr/post_images/618/e81/c8c/618e81c8c3c0cb15ae5fbca791c33c8c.png" alt="imagem"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Depois disso, pressione o botão ► na parte superior do editor e pronto! </font><font style="vertical-align: inherit;">O algoritmo de configuração de treinamento do agente agora está completo.</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/9c6/5ad/bdb/9c65adbdbb38b939262c10e5ac728dde.gif" alt="imagem"><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tempo da experiência</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A melhor maneira de aprender é constantemente tentar trazer algo novo. </font><font style="vertical-align: inherit;">Agora que já alcançamos bons resultados, tentaremos colocar algumas hipóteses e testá-las.</font></font><br>
<br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Reduzindo o coeficiente de desconto para 0,95</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Então sabemos que:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Quanto maior a gama, menor o desconto. </font><font style="vertical-align: inherit;">Ou seja, o agente está mais preocupado com recompensas de longo prazo.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Por outro lado, quanto menor a gama, maior o desconto. </font><font style="vertical-align: inherit;">Nesse caso, a prioridade do agente é a compensação de curto prazo.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A idéia desse experimento é que, se aumentarmos o desconto diminuindo a gama de 0,99 para 0,95, a recompensa de curto prazo será uma prioridade para o agente - o que pode ajudá-lo a abordar rapidamente a política de comportamento ideal. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/u_/cm/hv/u_cmhvpc0dk-npolzz49loxofiq.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Curiosamente, no caso de um salto através de um muro baixo, o agente lutará pelo mesmo resultado. </font><font style="vertical-align: inherit;">Isso pode ser explicado pelo fato de que este caso é bastante simples: o agente só precisa passar para o ladrilho verde e, se necessário, pular se houver uma parede na frente. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/yc/ic/tt/ycicttb0mkwxmwn5c1ghgm2iiiu.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Por outro lado, no caso do Big Wall Jump, isso funciona pior, porque nosso agente se preocupa mais com a recompensa de curto prazo e, portanto, não entende que ele precisa subir no bloco branco para pular o muro.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Maior complexidade da rede neural</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Finalmente, hipotetizamos se nosso agente se tornará mais inteligente se aumentarmos a complexidade da rede neural. </font><font style="vertical-align: inherit;">Para fazer isso, aumente o tamanho do nível oculto de 256 para 512. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
E descobrimos que, nesse caso, o novo agente funciona pior que o nosso primeiro agente. </font><font style="vertical-align: inherit;">Isso significa que não faz sentido aumentar a complexidade da nossa rede, porque, caso contrário, o tempo de aprendizado também aumentará. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/zo/g7/qm/zog7qm1uemu7l7taqc5uuwlqauw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Então, treinamos o agente para pular os muros, e isso é tudo por hoje. </font><font style="vertical-align: inherit;">Lembre-se de que para comparar os resultados, os modelos treinados podem ser baixados </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">aqui</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/af6/f00/ce3/af6f00ce3784c60469522296276d2651.gif" alt="imagem"></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt492530/index.html">MVVMs baseadas em combinação nos aplicativos UIKit e SwiftUI para desenvolvedores do UIKit</a></li>
<li><a href="../pt492534/index.html">Então, existem furacões reais em Moscou ou não? Analisamos o caso de 13 de março de 2020 em perseguição</a></li>
<li><a href="../pt492538/index.html">Wrike TechClub: Infraestrutura de entrega - processos e ferramentas (DevOps + QAA). Artigos em inglês</a></li>
<li><a href="../pt492540/index.html">O jogo "Espere um momento!" no arduino</a></li>
<li><a href="../pt492546/index.html">Verificando a vulnerabilidade de qualquer site usando o Nikto</a></li>
<li><a href="../pt492552/index.html">Como viver e trabalhar em quarentena em Barcelona</a></li>
<li><a href="../pt492558/index.html">Olá, aqui é COVID19: O coronavírus vive na superfície de um smartphone?</a></li>
<li><a href="../pt492560/index.html">Tabela de hash simples para GPU</a></li>
<li><a href="../pt492562/index.html">Três webinars úteis do Apache Ignite em seu programa de quarentena</a></li>
<li><a href="../pt492566/index.html">Análise da combinação de um algoritmo de busca por clique ganancioso com enumeração parcial de vértices de gráfico</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>