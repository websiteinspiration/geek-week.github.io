<!doctype html>
<html class="no-js" lang="pt">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üçΩÔ∏è üò® üßëüèø‚Äçü§ù‚Äçüßëüèª Unity Machine Learning: ensinando agentes do MO a pular muros üìî üéôÔ∏è üë©üèΩ‚Äç‚öïÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Houve grandes avan√ßos no aprendizado por refor√ßo (RL) nos √∫ltimos anos: desde o primeiro uso bem-sucedido no treinamento de pixels brutos at√© o treina...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Unity Machine Learning: ensinando agentes do MO a pular muros</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/pixonic/blog/492548/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Houve grandes avan√ßos no aprendizado por refor√ßo (RL) nos √∫ltimos anos: desde o primeiro uso bem-sucedido no treinamento de pixels brutos at√© o treinamento de roboristas de IA aberta, e ambientes cada vez mais sofisticados s√£o necess√°rios para progresso adicional, para o qual ajudar A unidade vem. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A ferramenta Unity ML-Agents √© um novo plug-in no mecanismo do jogo Unity, permitindo que voc√™ use o Unity como um construtor de ambiente para treinar agentes de MO. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Desde jogar futebol a caminhar, pular de paredes e treinar c√£es de IA para jogar bengalas, o Unity ML-Agents Toolkit fornece uma ampla gama de condi√ß√µes para os agentes de treinamento. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Neste artigo, veremos como os agentes do Unity MO funcionam e, em seguida, ensinaremos um desses agentes a pular muros.</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/9c6/5ad/bdb/9c65adbdbb38b939262c10e5ac728dde.gif" alt="imagem"><br>
<br>
<a name="habracut"></a><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O que s√£o os agentes ML da Unity?</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O Unity ML-Agents √© um novo plug-in para o mecanismo de jogo Unity, que permite criar ou usar ambientes prontos para o treinamento de nossos agentes. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O plug-in consiste em tr√™s componentes: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/zo/mx/m_/zomxm_3g9svsej0iarjpw1vbnta.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O primeiro - um </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ambiente de aprendizado</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ( </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">o Ambiente de Aprendizado</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ), contendo cenas do Unity e de elementos ambientais. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A segunda √© a </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">API Python</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , que cont√©m os algoritmos RL (como PPO - Proximal Policy Optimization e SAC - Soft Actor-Critic). </font><font style="vertical-align: inherit;">Usamos essa API para iniciar treinamentos, testes, etc. Ela √© conectada ao ambiente de aprendizado atrav√©s do terceiro componente - </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">um comunicador externo</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Em que consiste o ambiente de aprendizagem</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O componente de treinamento consiste em v√°rios elementos: </font></font><br>
 <br>
<img src="https://habrastorage.org/webt/hy/1u/pt/hy1upt9en2esnt-amk-iipklmn4.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O primeiro agente √© o ator de palco. </font><font style="vertical-align: inherit;">√â ele quem treinaremos otimizando um componente chamado "C√©rebro", no qual s√£o registradas as a√ß√µes que devem ser executadas em cada um dos estados poss√≠veis. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O terceiro elemento, a Academy, gerencia agentes e seus processos de tomada de decis√£o e solicita√ß√µes de processos da API do Python. </font><font style="vertical-align: inherit;">Para entender melhor seu papel, vamos relembrar o processo de RL. </font><font style="vertical-align: inherit;">Pode ser representado como um ciclo que funciona da seguinte maneira: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/5y/ge/ef/5ygeefzasafou6bkwggdnaq7ows.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Suponha que um agente precise aprender a jogar um jogo de plataformas. </font><font style="vertical-align: inherit;">O processo RL neste caso ser√° parecido com o seguinte:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O agente recebe o estado </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">S </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0</font></font></sub></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> do ambiente - este ser√° o primeiro quadro do nosso jogo.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Com base no estado </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">S </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0, o</font></font></sub></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> agente executa a a√ß√£o </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0</font></font></sub></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> e muda para a direita.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O ambiente entra em um novo estado </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">S </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O agente recebe recompensa </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">R </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> por n√£o estar morto ( </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">recompensa positiva</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> +1).</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Esse ciclo de RL forma uma sequ√™ncia de estado, a√ß√£o e recompensa. </font><font style="vertical-align: inherit;">O objetivo do agente √© maximizar a recompensa total esperada. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/qm/mr/ml/qmmrmlkz7urj8cywqpsg1ejajve.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Assim, a Academy envia instru√ß√µes aos agentes e fornece sincroniza√ß√£o em sua execu√ß√£o, a saber:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cole√ß√£o de observa√ß√µes;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A escolha da a√ß√£o de acordo com as instru√ß√µes estabelecidas;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Execu√ß√£o de a√ß√£o;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Redefina se o n√∫mero de etapas foi atingido ou a meta foi alcan√ßada.</font></font></li>
</ul><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ensinamos o agente a pular atrav√©s das paredes</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Agora que sabemos como os agentes da Unity funcionam, treinaremos um para pular paredes. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Modelos j√° treinados tamb√©m podem ser baixados no </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GitHub</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ambiente de aprendizagem Wall Jumping</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O objetivo desse ambiente √© ensinar o agente a chegar ao ladrilho verde. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Considere tr√™s casos: </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
1. N√£o h√° paredes e nosso agente s√≥ precisa chegar ao ladrilho. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/17a/8fd/da8/17a8fdda8bbe767f64a34d8d05fd9be9.png" alt="imagem"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
2. O agente precisa aprender a pular para alcan√ßar o ladrilho verde. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/d2c/149/0d0/d2c1490d0b90c4fdefb8eb4922f446b2.png" alt="imagem"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
3. O caso mais dif√≠cil: a parede √© muito alta para o agente pular, ent√£o ele precisa pular primeiro no bloco branco. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/458/175/9b6/4581759b653a175597054fcf60a0050e.png" alt="imagem"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ensinaremos ao agente dois cen√°rios de comportamento, dependendo da altura do muro:</font></font><br>
<br>
<ul>
<li><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SmallWallJump</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> em caixas sem paredes ou em baixas alturas;</font></font></li>
<li><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">BigWallJump</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> no caso de muros altos.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
√â assim que o sistema de recompensa ser√°: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/rt/lo/hm/rtlohmnturcucyikrnhfsadztgi.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Em nossas observa√ß√µes, n√£o estamos usando um quadro regular, mas 14 reykast, cada um dos quais pode detectar quatro objetos poss√≠veis. </font><font style="vertical-align: inherit;">Nesse caso, o reykast pode ser percebido como raios laser que podem determinar se eles passam por um objeto. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Tamb√©m usaremos a posi√ß√£o de agente global em nosso programa. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/a06/0fc/fa2/a060fcfa26e80154ef988d76dc9b8f05.png" alt="imagem"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Quatro op√ß√µes s√£o poss√≠veis em nosso espa√ßo: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ni/g6/zs/nig6zskfzterisy_hskg3k-rsnk.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O objetivo √© obter um ladrilho verde </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">com uma recompensa m√©dia de 0,8</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ent√£o vamos come√ßar!</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Primeiro de tudo, abra o projeto </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">UnitySDK</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Entre os exemplos, voc√™ precisa encontrar e abrir a cena do </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">WallJump</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Como voc√™ pode ver, existem muitos agentes no palco, cada um deles retirado da mesma pr√©-fabricada, e todos eles t√™m o mesmo "c√©rebro". </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/bad/195/820/bad195820da3e5b2c74309e9f44e9935.png" alt="imagem"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Como no caso do cl√°ssico Deep Reinforcement Learning, depois de lan√ßarmos v√°rias inst√¢ncias do jogo (por exemplo, 128 ambientes paralelos), agora apenas copiamos e colamos os agentes para ter estados mais diferentes. E como queremos treinar nosso agente do zero, precisamos primeiro remover o "c√©rebro" do agente. Para fazer isso, v√° para a pasta prefabs e abra Prefab.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Em seguida, na hierarquia de Prefab, voc√™ precisa selecionar o agente e ir para as configura√ß√µes. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nos Par√¢metros de comportamento, voc√™ precisa excluir o modelo. Se tivermos v√°rias GPUs √† nossa disposi√ß√£o, voc√™ poder√° usar o Dispositivo de Infer√™ncia da CPU como uma GPU. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/d0d/bdb/6ec/d0dbdb6ec05655b6fecdf675d629d126.png" alt="imagem"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
No componente Agente de salto em parede, voc√™ deve remover o c√©rebro para uma caixa sem paredes, bem como para paredes baixas e altas. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/618/e81/c8c/618e81c8c3c0cb15ae5fbca791c33c8c.png" alt="imagem"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Depois disso, voc√™ pode come√ßar a treinar seu agente do zero. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para nosso primeiro treinamento, simplesmente alteramos o n√∫mero total de etapas de treinamento para dois cen√°rios de comportamento: SmallWallJump e BigWallJump. Para que possamos atingir a meta em apenas 300 mil etapas. Para fazer isso, em </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">config / trainer config.yaml,</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> altere max_steps para 3e5 para os casos SmallWallJump e BigWallJump.</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/b80/bbb/1b2/b80bbb1b21f0f9e445fa8d2583c39d52.png" alt="imagem"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para treinar nosso agente, usaremos o </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PPO</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (Proximal Policy Optimization). O algoritmo inclui o ac√∫mulo de experi√™ncia em interagir com o ambiente e us√°-lo para atualizar pol√≠ticas de tomada de decis√£o. Ap√≥s atualiz√°-lo, os eventos anteriores s√£o descartados e a coleta de dados subsequente j√° √© realizada sob os termos da pol√≠tica atualizada. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ent√£o, primeiro, usando a API Python, precisamos chamar um comunicador externo para instruir a Academia a iniciar agentes. Para fazer isso, abra o terminal onde o ml-agents-master est√° localizado e digite: </font></font><br>
<br>
<code>mlagents-learn config/trainer_config.yaml ‚Äî run-id=‚ÄùWallJump_FirstTrain‚Äù ‚Äî train</code><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Este comando solicitar√° que voc√™ inicie a cena do Unity. Para fazer isso, pressione ‚ñ∫ na parte superior do editor. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/212/874/4fb/2128744fb143fec6308d95938e0383f7.png" alt="imagem"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Voc√™ pode assistir ao treinamento de seus agentes no Tensorboard com o seguinte comando:</font></font><br>
<br>
<code>tensorboard ‚Äî logdir=summaries</code><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Quando o treinamento terminar, voc√™ precisar√° mover os arquivos de modelo salvos contidos em </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ml-agents-master / models para UnitySDK / Assets / ML-Agents / examples / WallJump / TFModels</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Em seguida, abra o editor do Unity novamente e selecione a cena </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">WallJump</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , onde abrimos o objeto </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">WallJumpArea</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> finalizado </font><font style="vertical-align: inherit;">. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Depois disso, selecione o agente e, em seus par√¢metros de comportamento, arraste o arquivo </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SmallWallJump.nn</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> para o </font><i><font style="vertical-align: inherit;">Espa√ßo</font></i><font style="vertical-align: inherit;"> reservado para modelo. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/d0d/bdb/6ec/d0dbdb6ec05655b6fecdf675d629d126.png" alt="imagem"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Mova tamb√©m:</font></font><br>
<br>
<ol>
<li><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SmallWallJump.nn</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> no espa√ßo </font><i><font style="vertical-align: inherit;">reservado</font></i><font style="vertical-align: inherit;"> no c√©rebro da parede.</font></font></li>
<li><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SmallWallJump.nn</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> no </font><i><font style="vertical-align: inherit;">espa√ßo</font></i><font style="vertical-align: inherit;"> reservado do c√©rebro de parede pequena.</font></font></li>
<li><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">BigWallJump.nn</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> no </font><i><font style="vertical-align: inherit;">espa√ßo</font></i><font style="vertical-align: inherit;"> reservado para o c√©rebro sem parede.</font></font></li>
</ol><br>
<img src="https://habrastorage.org/getpro/habr/post_images/618/e81/c8c/618e81c8c3c0cb15ae5fbca791c33c8c.png" alt="imagem"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Depois disso, pressione o bot√£o ‚ñ∫ na parte superior do editor e pronto! </font><font style="vertical-align: inherit;">O algoritmo de configura√ß√£o de treinamento do agente agora est√° completo.</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/9c6/5ad/bdb/9c65adbdbb38b939262c10e5ac728dde.gif" alt="imagem"><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tempo da experi√™ncia</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A melhor maneira de aprender √© constantemente tentar trazer algo novo. </font><font style="vertical-align: inherit;">Agora que j√° alcan√ßamos bons resultados, tentaremos colocar algumas hip√≥teses e test√°-las.</font></font><br>
<br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Reduzindo o coeficiente de desconto para 0,95</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ent√£o sabemos que:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Quanto maior a gama, menor o desconto. </font><font style="vertical-align: inherit;">Ou seja, o agente est√° mais preocupado com recompensas de longo prazo.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Por outro lado, quanto menor a gama, maior o desconto. </font><font style="vertical-align: inherit;">Nesse caso, a prioridade do agente √© a compensa√ß√£o de curto prazo.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A id√©ia desse experimento √© que, se aumentarmos o desconto diminuindo a gama de 0,99 para 0,95, a recompensa de curto prazo ser√° uma prioridade para o agente - o que pode ajud√°-lo a abordar rapidamente a pol√≠tica de comportamento ideal. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/u_/cm/hv/u_cmhvpc0dk-npolzz49loxofiq.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Curiosamente, no caso de um salto atrav√©s de um muro baixo, o agente lutar√° pelo mesmo resultado. </font><font style="vertical-align: inherit;">Isso pode ser explicado pelo fato de que este caso √© bastante simples: o agente s√≥ precisa passar para o ladrilho verde e, se necess√°rio, pular se houver uma parede na frente. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/yc/ic/tt/ycicttb0mkwxmwn5c1ghgm2iiiu.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Por outro lado, no caso do Big Wall Jump, isso funciona pior, porque nosso agente se preocupa mais com a recompensa de curto prazo e, portanto, n√£o entende que ele precisa subir no bloco branco para pular o muro.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Maior complexidade da rede neural</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Finalmente, hipotetizamos se nosso agente se tornar√° mais inteligente se aumentarmos a complexidade da rede neural. </font><font style="vertical-align: inherit;">Para fazer isso, aumente o tamanho do n√≠vel oculto de 256 para 512. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
E descobrimos que, nesse caso, o novo agente funciona pior que o nosso primeiro agente. </font><font style="vertical-align: inherit;">Isso significa que n√£o faz sentido aumentar a complexidade da nossa rede, porque, caso contr√°rio, o tempo de aprendizado tamb√©m aumentar√°. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/zo/g7/qm/zog7qm1uemu7l7taqc5uuwlqauw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ent√£o, treinamos o agente para pular os muros, e isso √© tudo por hoje. </font><font style="vertical-align: inherit;">Lembre-se de que para comparar os resultados, os modelos treinados podem ser baixados </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">aqui</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/af6/f00/ce3/af6f00ce3784c60469522296276d2651.gif" alt="imagem"></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt492530/index.html">MVVMs baseadas em combina√ß√£o nos aplicativos UIKit e SwiftUI para desenvolvedores do UIKit</a></li>
<li><a href="../pt492534/index.html">Ent√£o, existem furac√µes reais em Moscou ou n√£o? Analisamos o caso de 13 de mar√ßo de 2020 em persegui√ß√£o</a></li>
<li><a href="../pt492538/index.html">Wrike TechClub: Infraestrutura de entrega - processos e ferramentas (DevOps + QAA). Artigos em ingl√™s</a></li>
<li><a href="../pt492540/index.html">O jogo "Espere um momento!" no arduino</a></li>
<li><a href="../pt492546/index.html">Verificando a vulnerabilidade de qualquer site usando o Nikto</a></li>
<li><a href="../pt492552/index.html">Como viver e trabalhar em quarentena em Barcelona</a></li>
<li><a href="../pt492558/index.html">Ol√°, aqui √© COVID19: O coronav√≠rus vive na superf√≠cie de um smartphone?</a></li>
<li><a href="../pt492560/index.html">Tabela de hash simples para GPU</a></li>
<li><a href="../pt492562/index.html">Tr√™s webinars √∫teis do Apache Ignite em seu programa de quarentena</a></li>
<li><a href="../pt492566/index.html">An√°lise da combina√ß√£o de um algoritmo de busca por clique ganancioso com enumera√ß√£o parcial de v√©rtices de gr√°fico</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>