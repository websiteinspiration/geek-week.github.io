<!doctype html>
<html class="no-js" lang="zh-CN">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🦂 🤓 👩‍❤️‍💋‍👨 神经网络-在没有老师的情况下进行培训。政策梯度法 💇🏼 👨🏿‍🌾 👨🏾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="美好的一天，哈伯
 本文打开了一系列有关如何在没有老师的情况下训练神经网络的文章。
 （神经元网络的强化学习）
 
 在本周期中，我计划撰写三篇关于无老师的三种神经网络训练算法代码的理论和实现的文章。根据Actor-Critic方法，第一篇文章将介绍“策略梯度”，第二篇文章将涉及Q学习，第三篇文章将...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>神经网络-在没有老师的情况下进行培训。政策梯度法</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/506384/"><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">美好的一天，哈伯</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
本文打开了一系列有关如何在没有老师的情况下训练神经网络的文章。</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 （神经元网络的强化学习）</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
在</font><font style="vertical-align: inherit;">本周</font><font style="vertical-align: inherit;">期中，我计划撰写三篇关于无老师的三种神经网络训练算法代码的理论和实现的文章。</font><font style="vertical-align: inherit;">根据Actor-Critic方法，第一篇文章将介绍“策略梯度”，第二篇文章将涉及Q学习，第三篇文章将是最终文章。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
享受阅读。</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">第一条-没有老师的政策梯度学习</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 （强化学习的政策梯度）</font></font></h4><br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">介绍</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
在机器学习算法中，机器学习算法占据了一个特殊的位置，该算法学习算法自行解决问题而无需人工干预，直接与他学习的环境进行交互。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
此类算法已获得通用名称-在没有老师的情况下学习算法，对于此类算法，您不需要收集数据库，也不需要分类或标记它们。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
对于没有老师的学生来说，仅对他的行为或决定做出落后的回应就足够了-无论这些行动或决定是否好。</font></font><br>
<a name="habracut"></a><br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">第一章。教师培训</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
那是什么-在有或没有老师的情况下学习。我们将通过现代机器学习及其解决的任务中的示例对此进行更详细的研究。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
大多数针对分类，回归，分割问题的现代机器学习算法本质上都是在以老师本人为老师的情况下训练算法。因为是由标记数据的人告诉算法正确答案应该是什么，从而算法试图找到解决方案，以便算法在解决问题时给出的答案与为给定任务的人指示的答案匹配为正确答案。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
以Mnist数据集的分类问题为例，该人对该算法给出的正确答案是训练集中数字类别的标签。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/za/kq/b6/zakqb6ob1_xce07caywnurtsl7k.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
在Mnist数据集中，对于机器算法必须学习分类的每幅图像，人们预先设置了该图像所属类别的正确标签。在学习过程中，预测图像类别的算法将特定图像的接收类别与同一图像的真实类别进行比较，并在学习过程中逐渐调整其参数，以使算法预测的类别趋于与人指定的类别相对应。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
因此，可以总结出以下思想-与老师一起学习的算法是任何机器学习算法，从我们的角度出发，我们为算法提供了正确完成算法的方式。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
而且，继续进行并不重要-如果是分类任务，则应将此图像指定给哪个类；如果是分割任务，则应绘制对象的轮廓；如果算法是自动驾驶，则应绘制方向盘的方向，重要的是，对于每种特定情况，我们我们向算法明确指出正确答案在哪里，如何正确执行。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
这是理解有老师的学习算法与没有老师的学习算法有根本区别的关键。</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">第2章。没有老师的学习</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
弄清楚了什么-与老师一起教书，现在我们将了解它是什么-没有老师在教书。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
正如在上一章中所发现的，在与老师一起教学时，对于每种教学情况，我们都会从自己的角度让算法了解正确的答案，然后从相反的角度进行理解-在没有老师的情况下，对于每种具体情况，我们都不会对算法给出这样的答案将会。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
但是，随之而来的问题是，如果我们不给算法一个明确的指示，说明正确地做什么，那么算法将学到什么？如何训练算法而又不知道在哪里调整其内部参数以做正确的事情并最终解决我们想要的问题。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
让我们考虑一下这个话题。但是对我们来说重要的是，该算法可以整体解决问题，而在解决此问题的过程中它将如何发挥作用以及解决该问题的方式与我们无关，我们将其放弃给算法本身，我们只希望最终结果。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
因此，无论算法是否很好地解决了我们的问题，我们都会让该算法理解最终结果。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
因此，总结以上所有内容，我们得出的结论是，我们称这种学习算法为无老师，没有明确指示该算法如何执行的方法，但是仅对解决问题过程中其所有动作进行了总体评估。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/qi/ci/nn/qicinnqzow8o5b7lacir_mrs4aa.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
在球拍试图捕捉从上方掉落的立方体的游戏示例中，我们没有告诉算法在什么特定时间点将球拍移动到哪里来控制球拍。</font><font style="vertical-align: inherit;">我们将仅告诉算法其动作的结果-他是否用球拍抓住了立方体。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/-f/dp/df/-fdpdf5nbjpawt1mutegmxzw9iu.gif"><br>
<br>
<img src="https://habrastorage.org/webt/ve/jk/wz/vejkwz-fezjvbjcjydzmx7q4hk8.gif"><br>
<br>
<img src="https://habrastorage.org/webt/ws/gv/nb/wsgvnb34jwzwk--6iehpg3y3lqw.gif"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
这是没有老师学习的本质。</font><font style="vertical-align: inherit;">该算法本身必须学会根据对所有动作总和的最终评估来决定在每种情况下该做什么。</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">第3章代理，环境和奖励</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
在弄清楚没有老师的培训是什么之后，我们将深入研究可以学习如何解决问题的算法，而无需掌握正确解决问题的技巧。</font></font><br>
 <br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
现在是时候向我们介绍将来将要使用的术语了。</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
我们将称为算法的代理，该代理可以分析环境状态并在其中执行某些操作。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
环境是我们的代理程序所在的虚拟世界，通过其行为可以更改其状态。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
奖励-从环境到代理程序的反馈，作为对其行为的响应。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/23/dp/gw/23dpgwboqw80sv_1uj5_ubifms8.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
我们的代理人所处的环境可能会非常复杂，代理人甚至可能不知道其结构如何制定决策和执行操作。</font><font style="vertical-align: inherit;">对于代理而言，只有从环境中获得的奖励形式的反馈才是重要的。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
如果我们更详细地考虑代理与环境之间的交互过程，则可以用以下方案表示：</font></font><br>
<br>
<img src="https://habrastorage.org/webt/ml/f5/eq/mlf5eqvivcvcwodu66mfyddfpdo.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
步骤t中的环境状态</font><font style="vertical-align: inherit;">
at- </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
步骤t </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">rt </font><font style="vertical-align: inherit;">中的代理</font><font style="vertical-align: inherit;">-步骤t中的奖励</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
在每个时间点t，我们的代理观察介质的状态-St，执行-在其收到来自介质的奖励-rt时，介质进入状态St + 1，我们的代理观察到，执行状态-在+ 1，对于从媒介-rt + 1和这样的状态t获得奖励，我们可以有一个无限集-n。</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">第4章。无老师学习任务的参数化</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
为了培训代理人，我们需要以某种方式参数化没有老师的学习任务，换句话说，就是要了解我们要优化的功能。</font><font style="vertical-align: inherit;">在强化学习中，以下将我们称为没有老师的培训，它具有三个这样的基本功能：</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
1）p（a | s）-策略功能</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
动作最优性的概率函数为a，取决于环境状态-s。</font><font style="vertical-align: inherit;">它向我们展示了在介质s的状态下a的作用如何是最优的。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
2）v（s）-值函数</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
状态</font><font style="vertical-align: inherit;">值函数</font><font style="vertical-align: inherit;">为s。</font><font style="vertical-align: inherit;">它向我们展示了国家s在奖励方面通常对我们有价值多少</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
3）Q（s，a）-Q函数</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Q是最佳策略函数。它使我们能够根据这种最优策略在状态-s中选择针对该状态的最优动作- </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
首先，我们考虑功能-策略功能，这是最简单直观的理解强化学习功能。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
因为我们要通过神经网络解决强化学习的问题。然后，示意性地，我们可以如下通过神经网络对策略函数进行参数化。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/oa/fv/1x/oafv1xdki7zeejvxvy-tsgd5oeu.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
我们将状态s提供给神经网络的输入，并构造神经网络的输出，以使神经网络的输出层为SoftMax层，输出数量等于环境中代理可能采取的行动数量。</font><font style="vertical-align: inherit;">因此，通过输出将状态s传递到神经网络的各个层，我们获得了状态s中行为的概率分布。</font><font style="vertical-align: inherit;">实际上，我们需要什么才能开始训练我们的神经网络并通过错误的反向传播算法迭代地改善策略功能（现在基本上是我们的神经网络）。</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">第5章。通过神经网络训练改善政策功能</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
为了训练神经网络，我们使用梯度下降法。由于神经网络的最后一层是SoftMax层，因此它的损失函数为：</font></font><br>
<br>
<img src="https://habrastorage.org/webt/29/y9/u2/29y9u2wizl5w2yf5yeyw08nk1ic.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
其中：</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
1-真实标签* log（预测的标签）</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
2-所有示例的总和</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
但是，如果我们还没有正确的状态代理行为标签，我们该如何训练神经网络S0-Sj？而且我们不需要它们，而不是正确的标签，我们将通过执行神经网络为他预测的动作，使用代理商从媒介那里获得的奖励。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/46/jp/or/46jporp-h9pj-bujcmytwhnfmze.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
我们拥有这样做的一切权利，因为对于交叉熵损失，yj是正确类别的真实标签，并且等于1；对于策略功能损失，rj是环境为其代理执行的行动所获得的奖励。</font><font style="vertical-align: inherit;">也就是说，当训练神经网络时，rj充当误差反向传播中的梯度的权重。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
收到了积极的奖励-这意味着您需要增加将梯度定向到的神经网络的权重。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/2e/ge/bq/2egebqgdxjutx0tzxvwawunwffq.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
如果奖励为负，则根据误差指向的梯度方向，我们减少神经网络中的相应权重。</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">第6章。建立训练数据集</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
为了训练我们的Agent（使用经典机器学习的神经网络）通过错误的反向传播方法，我们需要组装一个数据集。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
从问题的陈述中可以清楚地看出，在神经网络的输入处，我们要提交介质S的状态-带有下降的立方体和捕获它们的球拍的井的图像。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/4h/pv/hr/4hpvhry6ju1kcckfjj4vxu_7eps.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
和Y-我们将根据状态S进行收集，这将是神经网络的预期动作-以及环境为该动作在代理程序上产生的奖励-r</font></font><br>
<br>
<img src="https://habrastorage.org/webt/an/gh/wo/anghwor5xj9mqdbgnnjrjgvzbta.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
但是毕竟，星期三，在比赛过程中，经纪人无法为每一个动作提供奖励，例如，在我们的案例中，经纪人只有在球拍落下时才获得正收益。如果球拍没有抓住方块而落到底部，那么星期三将向经纪人收取负奖励。在其余时间中，无论代理商如何移动球拍，直到魔方击中球拍或跌至谷底为止，星期三都会向代理商收取等于零的奖励。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/7f/rh/5u/7frh5uda0syl7dbcutr8ez8fcq8.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
从对我们游戏过程的描述中可以看出，代理商中的正面或负面奖励是极为罕见的，但基本上，无论其行为如何，奖励都是零。大多数情况下，如何培训代码代理，他不会收到环境对其行为的响应。从我们的主体是神经网络的角度来看，来自环境的回报为零，那么在大多数情况下，数据集中错误通过神经网络的反向传播的梯度将为零，神经网络的权重将无处改变，这意味着我们的主体将不会学习任何东西。</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
如何在将要训练代理的大多数数据集中以零奖励解决问题？</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
有两种方法可以摆脱这种情况：</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
第一种方法是分配代理在多维数据集掉落期间采取的所有行动，并获得与情节+1或-1相同的最终奖励，具体取决于代理是否捕获了多维数据集。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
因此，如果代理正确抓住了魔方，我们将考虑其所有行为，并在训练过程中巩固代理的这种行为，并给予他们积极的奖励。如果特工没有赶上魔方，那么我们将对该情节中特工的所有行动给予负面奖励，并将训练他避免日后采取此类行动。</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
第二个-最终奖，按折扣顺序递减，适用于本集中的所有特工动作。换句话说，代理人的动作越接近最终结果，则该动作的奖励就越接近+1或-1。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
通过为动作从情节结束处移出以降序的方式引入这种打折奖励，我们让特工了解到，他采取的最后动作对游戏情节的结局比开始时采取的动作更为重要。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
通常，折价报酬是通过公式计算得出的-情节的最终报酬乘以该情节中所有代理动作的步数乘以1的幂的折现系数（对于多维数据集掉落的时间）。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/4r/cf/w6/4rcfw67kmlniyfhczs_p2v2kb-c.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
伽玛-折扣系数（奖励减少）。它的取值范围通常是0到1。通常，色域的取值范围是0.95。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
在决定了要在DataSet中收集哪些数据，运行环境模拟器并连续几次玩具有多个情节的游戏之后，请收集有关以下内容的数据：</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">环境状况 </font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 代理采取的行动 </font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">代理收到的奖励。</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
为了便于理解，让我们将一个立方体跌落到井中-游戏的一个情节，我们还假设游戏本身将包含几个情节。</font><font style="vertical-align: inherit;">这意味着在一场比赛中，我们将依次将几个骰子丢入孔中，而球拍将尝试抓住它们。</font><font style="vertical-align: inherit;">对于每个抓到的骰子，特工将获得+1分，对于每个跌到谷底且球拍没有抓到他的骰子，特工将获得-1分。</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">第7章代理的内部结构和环境</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
环境-由于我们具有要在其中存在代理的环境，因此它基本上由一个井的矩阵组成，每个时钟以一线的速度将多维数据集降落到一个井的内部，并且代理也朝一个方向移动一个单元。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
我们将编写一个环境模拟器，该模拟器可以在任意时刻从任意列的顶行滴下一个立方体，知道如何从代理那里接收将球拍沿一个方向移动一个单元的命令，然后它检查下落的立方体是否被球拍抓住或掉落了井底。依赖于此，模拟器将其行为所获得的报酬返回给代理。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
主体的主要元素是我们的神经网络，该神经网络能够返回给定环境状态下所有动作的概率，因为环境在输入时对它是忠诚的。</font><font style="vertical-align: inherit;">代理从神经网络接收到的动作的概率中，选择最佳，将其发送到环境，以来自环境的奖励的形式接收来自环境的反馈。</font><font style="vertical-align: inherit;">而且，代理必须具有内部算法，基于该算法，他将能够学习以最大化从环境中获得的报酬。</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">第8章代理培训</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
为了训练Agent，我们需要根据模拟器中的数据和Agent采取的行动来累积统计信息。</font><font style="vertical-align: inherit;">我们将收集统计数据以进行三重值的训练-环境状态，代理商的行为以及对该行为的回报。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/yo/qq/od/yoqqodgrfzatgri5dljps6uqnvw.png"><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">有关如何收集统计信息的一些代码</font></font></h4><br>
<img src="https://habrastorage.org/webt/ph/hr/6x/phhr6xb35udm3hyc3mr9rwv5yno.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
我们将这三个值分别放在一个特殊的内存缓冲区中，我们在模拟游戏并累积其统计数据的同时一直存储它们。</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">用于组织内存缓冲区的代码：</font></font></h4><br>
<img src="https://habrastorage.org/webt/u2/55/ty/u255ty2pzaoeagitk_osyxzxirg.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
在星期三进行了一系列代理商的游戏并累积了统计数据之后，我们可以进行代理商的培训。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
为此，我们从具有累积统计信息的内存缓冲区中以三值形式获取一批数据，将其解压缩并将其转换为Pytorch张量。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/jo/se/6t/jose6tkxmmtzsttgrnc-q0xkrf0.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 将Pytorch张量形式的环境状态批次应用于代理人神经网络的输入，我们获得匹配中每个环境状态下每个代理人移动的概率分布，对数这些概率，将代理人每次移动获得的报酬乘以这些概率的对数，然后取乘积的平均值将此表示为负面：</font></font><br>
<br>
<img src="https://habrastorage.org/webt/hx/om/3i/hxom3ikgq9n13pb6brfzsfsbmkw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
第一</font></font><br>
<br>
<img src="https://habrastorage.org/webt/4j/gh/8l/4jgh8layvkcwg0dhrylxuwul2ri.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
阶段第二阶段</font></font><br>
<br>
<img src="https://habrastorage.org/webt/pn/ac/pj/pnacpjuzovqe4-zql7lxlarerly.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
获得损失函数的值后，我们通过神经网络进行反向遍历以获得梯度，并使该步骤成为优化器来调整权重。</font><font style="vertical-align: inherit;">在此，我们代理的培训周期正在下载中。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
由于优化器改变了神经网络的权重后，我们收集的统计数据不再相关，因为权重发生变化的神经网络将为完全相同的环境条件下的操作提供完全不同的概率，并且代理商培训将再次出错。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
因此，我们会清除内存缓冲区，再次输掉一定数量的游戏来收集统计信息，然后重新开始座席训练过程。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
这是在没有老师的情况下使用“策略梯度”方法进行学习的学习循环。</font></font><br>
<br>
<ol>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">统计累积</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">代理商培训</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">重置统计信息</font></font></li>
</ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 我们重复学习过程很多次，直到我们的代理学会从系统接收适合我们的奖励为止。</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">第9章使用Agent和环境进行实验</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
让我们开始一系列实验来训练我们的特工。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
对于实验，我们选择以下环境参数：</font></font><br>
<br>
<img src="https://habrastorage.org/webt/xi/ym/2_/xiym2_kitmamvf52hhuff6tkpqs.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
对于Agent，我们选择-我们构建一个神经网络-它是卷积的（在处理图像时），它将有9个输出（在输出上为1右，2左，3上，4向下） 5右上，6左上，7右下，8左下，9不执行）和SoftMax以获取每个动作的概率。</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">神经网络架构</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
第一Conv2d层32神经元图像大小1 * 32 * 16 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
MaxPool2d层图像大小32 * 16 * 8 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
第二Conv2d层32神经元图像大小32 * 16 * 8 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
MaxPool2d层图像大小32 * 8 * 4展</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
平-将图像拉直为1024 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
线性1024个神经元的层</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
辍学（0.25）层</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
512个神经元的</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
线性层256个神经元的</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
线性层9个神经元和SoftMax的线性层</font></font><br>
<br>
<img src="https://habrastorage.org/webt/2d/tz/ne/2dtzneqr2lsxdkz-yvu9xonh0uw.png"><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pytorch神经网络创建代码</font></font></h4><br>
<img src="https://habrastorage.org/webt/wq/mx/yw/wqmxyw67mbjtmnpnbj86zry8y4a.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
我们将在环境中一个接一个地启动三个Agent训练周期，上面讨论了其参数：</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">实验1-代理商学习了13,600个游戏周期来解决问题</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
座席的初始状态座席的</font></font><br>
<br>
<img src="https://habrastorage.org/webt/ws/gv/nb/wsgvnb34jwzwk--6iehpg3y3lqw.gif"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
培训计划</font><font style="vertical-align: inherit;">座席</font><font style="vertical-align: inherit;">的受训</font></font><br>
<br>
<img src="https://habrastorage.org/webt/jt/kd/3q/jtkd3qmdl7xixi-bwpkdp-tk0uy.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
状态。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/4a/v5/9l/4av59lvev4oget2gotgg-xsfy2w.gif"><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">实验2-代理商学会了在8250个游戏周期中解决问题的方法</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
座席的初始状态座席的</font></font><br>
<br>
<img src="https://habrastorage.org/webt/ve/jk/wz/vejkwz-fezjvbjcjydzmx7q4hk8.gif"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
培训计划</font><font style="vertical-align: inherit;">座席</font><font style="vertical-align: inherit;">的受训</font></font><br>
<br>
<img src="https://habrastorage.org/webt/9e/zc/1x/9ezc1x4gfv7bilgb8sx-r3iemgy.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
状态。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/l9/r7/bn/l9r7bnstfbaqszje4npotjrze4o.gif"><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">实验3-代理商学会了在19800个游戏周期中解决问题的方法</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
座席的初始状态座席的</font></font><br>
<br>
<img src="https://habrastorage.org/webt/hz/wm/ss/hzwmssjogsxwzv0wrik-1nb8loc.gif"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
培训计划</font><font style="vertical-align: inherit;">座席</font><font style="vertical-align: inherit;">的受训</font></font><br>
<br>
<img src="https://habrastorage.org/webt/sh/xv/zd/shxvzdpumy7fnoctkb6bqnhie2o.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
状态。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/2h/ga/yh/2hgayhvtgypaq1fvu-i54d4l4ok.gif"><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">第10章结论</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
从图表中可以看出，对Agent的培训当然很慢。</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
代理很长时间以来一直在为自己的行为寻找至少一些合理的策略，以开始获得积极的回报。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
这时，在排定的第一阶段，游戏的奖励逐渐增长，然后探员突然为他的举动找到了一个不错的选择，并且他从游戏中获得的奖励急剧增加并上升，然后接近最大奖励，探员再次走当他改善自己已经学会的举动策略时，效率会缓慢提高，但努力像其他贪婪算法一样，完全获得回报。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
我还想指出，对于使用“策略梯度”方法训练Agent的计算非常需要，因为该算法主要工作的时间是收集有关特工行动的统计数据，而不是其训练的统计数据。收集了整个阵列的移动统计信息之后，我们仅使用一个数据批次来训练Agent，而将所有其他数据视为已经不适合训练而丢弃。我们再次收集新数据。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
您仍然可以使用此算法和环境进行大量实验-通过更改孔的深度和宽度，增加或减少游戏过程中落下的骰子数量，使这些骰子具有不同的颜色。观察这种影响将对Agent的有效性和培训速度产生什么影响。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
此外，神经网络的参数是实验的一个广阔领域，实质上，我们正在训练我们的Agent，您可以更改层，卷积核，启用和调整正则化。</font><font style="vertical-align: inherit;">是的，您还可以尝试提高培训特工的效率。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
因此，通过使用Policy Gradient方法在没有老师的情况下进行教学的实践实验，我们确信没有老师的教学是有地方的，而且确实有效。</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
代理商经过独立培训，可以在游戏中获得最大的回报。</font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用适用于Google Colab笔记本电脑的代码链接到GitHub</font></font></a></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN506358/index.html">他们收到检察官锁定案件时的3种法院行为选择</a></li>
<li><a href="../zh-CN506362/index.html">我们如何在网上制作舞会</a></li>
<li><a href="../zh-CN506370/index.html">比较SQL Server中检查约束和外键的性能</a></li>
<li><a href="../zh-CN506372/index.html">在人工智能中发现了复制的本能</a></li>
<li><a href="../zh-CN506380/index.html">免费或家用软件。标准或免费学费</a></li>
<li><a href="../zh-CN506386/index.html">如何辍学并将孩子转移到偏远地区</a></li>
<li><a href="../zh-CN506392/index.html">Roskomnadzor关于2020年处理个人数据的通知</a></li>
<li><a href="../zh-CN506394/index.html">Anycubic Mega X 3D打印机：价格适中的出色打印机</a></li>
<li><a href="../zh-CN506396/index.html">伊隆·马斯克（Elon Musk）：“激光雷达是在浪费时间。所有依靠激光雷达的人都注定了”</a></li>
<li><a href="../zh-CN506398/index.html">乔尔·斯波斯基（Joel Spolsky）：“并非仅凭可用性”</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>