<!doctype html>
<html class="no-js" lang="es">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèΩ‚Äçüöí üöÑ ü•à "Lo siento, reconoc√≠ ..." o reconoce frambuesas y controladores utilizando la API de detecci√≥n de objetos de Tensorflow üë©üèº‚Äçüç≥ ‚ùî üëÄ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="A fines del a√±o pasado, escrib√≠ un art√≠culo sobre c√≥mo me intrigaba la capacidad de reconocer objetos en im√°genes usando redes neuronales. En ese art√≠...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>"Lo siento, reconoc√≠ ..." o reconoce frambuesas y controladores utilizando la API de detecci√≥n de objetos de Tensorflow</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/494804/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A fines del a√±o pasado, escrib√≠ </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">un art√≠culo</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> sobre c√≥mo me intrigaba la capacidad de reconocer objetos en im√°genes usando redes neuronales. En ese art√≠culo, usando PyTorch, categorizamos las frambuesas o un controlador tipo arduino en video. Y a pesar del hecho de que me gustaba PyTorch, recurr√≠ a √©l porque no pod√≠a tratar con TensorFlow de inmediato. Pero promet√≠ que volver√© al tema del reconocimiento de objetos en el video. Parece que ha llegado el momento de cumplir la promesa. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En este art√≠culo, probaremos en nuestra m√°quina local para volver a entrenar el modelo terminado en Tensorflow 1.13 y la API de detecci√≥n de objetos en nuestro propio conjunto de im√°genes, y luego lo usaremos para reconocer bayas y controladores en la transmisi√≥n de video de una c√°mara web usando OpenCV.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
¬øQuieres mejorar tu habilidad de reconocimiento de bayas para el verano? </font><font style="vertical-align: inherit;">Entonces eres bienvenido bajo cat.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/fu/do/rd/fudordve5xz-8gwdnbvlnkkjusm.png"><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Contenido: </font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Parte I: introducci√≥n </font></font></a><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Parte II: entrenar el modelo en TenosrFlow </font></font></a><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Parte III: aplicar el modelo en OpenCV </font></font></a><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Parte IV: conclusi√≥n</font></font></a><br>
<a name="I"></a><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Parte I: Introducci√≥n</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Quienes han le√≠do el art√≠culo anterior sobre PyTorch ya saben que soy un aficionado en cuestiones de redes neuronales. Por lo tanto, no percibas este art√≠culo como la verdad √∫ltima. Pero de todos modos, espero poder ayudar a alguien a lidiar con los conceptos b√°sicos del reconocimiento de video utilizando la API de detecci√≥n de objetos de Tensorflow. </font></font><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Esta vez no intent√© hacer un tutorial, por lo que el art√≠culo ser√° m√°s corto de lo habitual.</font></font></i><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Para empezar, el </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">tutorial oficial</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> sobre el uso de la API de detecci√≥n de objetos en una m√°quina local, por decirlo suavemente, no es exhaustivo. Como novato, era completamente inadecuado y ten√≠a que centrarme en los art√≠culos del blog.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para ser sincero, me gustar√≠a probar TensorFlow 2.0, pero en la mayor√≠a de las publicaciones, en el momento de escribir este art√≠culo, los problemas de migraci√≥n no se resolvieron por completo. </font><font style="vertical-align: inherit;">Por lo tanto, al final, me decid√≠ por TF 1.13.2.</font></font><br>
<a name="II"></a><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Parte II: ense√±ando un modelo en TensorFlow </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Aprend√≠ instrucciones para ense√±ar el modelo </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">de este art√≠culo</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , o m√°s bien de su primera mitad, hasta que se aplic√≥ JavaScript </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(si no habla ingl√©s, puede ver un art√≠culo sobre el mismo tema </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">en Habr√©</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> )</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Es cierto que en mi caso hay varias diferencias:</font></font><br>
<br>
<ol>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Us√© Linux porque Anaconda para Linux ya ha construido protobuf y pycocoapi, as√≠ que no tuve que construirlos yo mismo.</font></font></li>
<li>   TensorFlow 1.13.2,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow">Object Detection API 1.13</a> ,       TensorFlow 1.13.2.   master        TF 1.15,         1.13.</li>
<li>      numpy ‚Äî 1.17.5,  1.18    .</li>
<li>  faster_rcnn_inception_v2_coco    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow">ssd_mobilenet_v2_coco</a>,    ,     .</li>
</ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 Por las dudas, dir√© que no utilic√© un acelerador de gr√°ficos. La capacitaci√≥n se llev√≥ a cabo solo en las capacidades del procesador. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Se puede descargar un conjunto de im√°genes, un archivo de configuraci√≥n, un gr√°fico guardado, as√≠ como un script para reconocer im√°genes usando OpenCV, como siempre, desde </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GitHub</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Han pasado 23 horas de entrenamiento modelo, todo el t√© en la casa ya se ha bebido, "¬øQu√©? ¬øD√≥nde? ¬øCuando?" inspeccionado y ahora mi paciencia finalmente lleg√≥ a su fin. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dejamos de entrenar y guardamos el modelo. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Instale OpenCV en el mismo entorno de "Anaconda" con el siguiente comando:</font></font><br>
<br>
<pre><code class="plaintext hljs">conda install -c conda-forge opencv</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Finalmente instal√© la versi√≥n 4.2 </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Adem√°s, las instrucciones </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">de este art√≠culo</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ya no las necesitaremos. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Despu√©s de guardar el modelo, comet√≠ un error que no era obvio para m√≠, a saber, inmediatamente intent√© sustituir el archivo graph.pbtxt utilizado anteriormente en la carpeta de entrenamiento / en la funci√≥n:</font></font><br>
<br>
<pre><code class="python hljs">cv2.dnn.readNetFromTensorflow()</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Desafortunadamente, esto no funciona de esta manera y tendremos que hacer una manipulaci√≥n m√°s para obtener graph.pbtxt para OpenCV. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Lo m√°s probable es que el hecho de que ahora aconseje no sea una buena manera, pero para m√≠ funciona. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Descargue </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">tf_text_graph_ssd.py</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , y tambi√©n </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">tf_text_graph_common.py p√≥ngalos</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> en la carpeta donde est√° nuestro gr√°fico guardado (tengo esta carpeta inference_graph). </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Luego vaya a la consola en esta carpeta y ejecute desde ella un comando de aproximadamente los siguientes contenidos:</font></font><br>
<br>
<pre><code class="plaintext hljs">python tf_text_graph_ssd.py --input frozen_inference_graph.pb --config pipeline.config --output graph.pbtxt</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Y eso es todo lo que queda para subir nuestro modelo a OpenCV.</font></font><br>
<br>
<a name="III"></a><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Parte III: aplique el modelo en OpenCV </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Como en el art√≠culo sobre PyTorch sobre el trabajo con OpenCV, tom√© como base el c√≥digo del programa de </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">esta publicaci√≥n</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Hice peque√±os cambios para simplificarlo un poco m√°s, pero como no entiendo completamente el c√≥digo, no lo comentar√©. </font><font style="vertical-align: inherit;">Funciona y agradable. </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Est√° claro que el c√≥digo podr√≠a haber sido mejor, pero todav√≠a no tengo tiempo para sentarme para los tutoriales de OpenCV</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<div class="spoiler" role="button" tabindex="0">
                        <b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">C√≥digo OpenCV</font></font></b>
                        <div class="spoiler_text"><pre><code class="python hljs">
<span class="hljs-comment"># USAGE</span>
<span class="hljs-comment"># based on this code https://proglib.io/p/real-time-object-detection/</span>
<span class="hljs-comment"># import the necessary packages</span>
<span class="hljs-keyword">from</span> imutils.video <span class="hljs-keyword">import</span> VideoStream
<span class="hljs-keyword">from</span> imutils.video <span class="hljs-keyword">import</span> FPS
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> imutils
<span class="hljs-keyword">import</span> time
<span class="hljs-keyword">import</span> cv2<font></font>
<font></font>
prototxt=<span class="hljs-string">"graph.pbtxt"</span>
model=<span class="hljs-string">"frozen_inference_graph.pb"</span>
min_confidence = <span class="hljs-number">0.5</span><font></font>
<font></font>
<span class="hljs-comment"># initialize the list of class labels MobileNet SSD was trained to</span>
<span class="hljs-comment"># detect, then generate a set of bounding box colors for each class</span>
CLASSES = [<span class="hljs-string">"background"</span>, <span class="hljs-string">"duino"</span>,<span class="hljs-string">"raspb"</span>]<font></font>
COLORS = [(<span class="hljs-number">40</span>,<span class="hljs-number">50</span>,<span class="hljs-number">60</span>),((<span class="hljs-number">140</span>,<span class="hljs-number">55</span>,<span class="hljs-number">130</span>)),(<span class="hljs-number">240</span>,<span class="hljs-number">150</span>,<span class="hljs-number">25</span>)]<font></font>
<font></font>
<span class="hljs-comment"># load our serialized model from disk</span>
print(<span class="hljs-string">"[INFO] loading model..."</span>)<font></font>
<font></font>
net =cv2.dnn.readNetFromTensorflow(model,prototxt)<font></font>
<font></font>
<span class="hljs-comment"># initialize the video stream, allow the cammera sensor to warmup,</span>
<span class="hljs-comment"># and initialize the FPS counter</span>
print(<span class="hljs-string">"[INFO] starting video stream..."</span>)<font></font>
vs = VideoStream(src=<span class="hljs-number">0</span>).start()<font></font>
time.sleep(<span class="hljs-number">0.5</span>)<font></font>
fps = FPS().start()<font></font>
<font></font>
<span class="hljs-comment"># loop over the frames from the video stream</span>
<span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:
	<span class="hljs-comment"># grab the frame from the threaded video stream and resize it</span>
	<span class="hljs-comment"># to have a maximum width of 400 pixels</span><font></font>
	frame = vs.read()<font></font>
	frame = imutils.resize(frame, width=<span class="hljs-number">300</span>)<font></font>
<font></font>
	<span class="hljs-comment"># grab the frame dimensions and convert it to a blob</span>
	(h, w) = frame.shape[:<span class="hljs-number">2</span>]<font></font>
	blob = cv2.dnn.blobFromImage(frame, size=(<span class="hljs-number">300</span>, <span class="hljs-number">300</span>), swapRB=<span class="hljs-literal">True</span>)<font></font>
<font></font>
	<span class="hljs-comment"># pass the blob through the network and obtain the detections and</span>
	<span class="hljs-comment"># predictions</span><font></font>
	net.setInput(blob)<font></font>
	detections = net.forward()<font></font>
<font></font>
	<span class="hljs-comment"># loop over the detections</span>
	<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> np.arange(<span class="hljs-number">0</span>, detections.shape[<span class="hljs-number">2</span>]):
		<span class="hljs-comment"># extract the confidence (i.e., probability) associated with</span>
		<span class="hljs-comment"># the prediction</span>
		<span class="hljs-keyword">print</span> (detections)<font></font>
		confidence = detections[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, i, <span class="hljs-number">2</span>]<font></font>
<font></font>
		<span class="hljs-keyword">if</span> confidence &gt; min_confidence:
			<span class="hljs-comment"># extract the index of the class label from the</span>
			<span class="hljs-comment"># `detections`, then compute the (x, y)-coordinates of</span>
			<span class="hljs-comment"># the bounding box for the object</span>
			idx = int(detections[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, i, <span class="hljs-number">1</span>])<font></font>
			box = detections[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, i, <span class="hljs-number">3</span>:<span class="hljs-number">7</span>] * np.array([w, h, w, h])<font></font>
			(startX, startY, endX, endY) = box.astype(<span class="hljs-string">"int"</span>)<font></font>
<font></font>
			<span class="hljs-comment"># draw the prediction on the frame</span>
			label = <span class="hljs-string">"{}: {:.2f}%"</span>.format(CLASSES[idx],<font></font>
				confidence * <span class="hljs-number">100</span>)<font></font>
			cv2.rectangle(frame, (startX, startY), (endX, endY),<font></font>
				COLORS[idx], <span class="hljs-number">2</span>)<font></font>
			y = startY - <span class="hljs-number">15</span> <span class="hljs-keyword">if</span> startY - <span class="hljs-number">15</span> &gt; <span class="hljs-number">15</span> <span class="hljs-keyword">else</span> startY + <span class="hljs-number">15</span>
			cv2.putText(frame, label, (startX, y+<span class="hljs-number">3</span>),<font></font>
				cv2.FONT_HERSHEY_SIMPLEX, <span class="hljs-number">0.5</span>, COLORS[idx], <span class="hljs-number">1</span>)<font></font>
<font></font>
	<span class="hljs-comment"># show the output frame</span>
	cv2.imshow(<span class="hljs-string">"Frame"</span>, frame)<font></font>
	key = cv2.waitKey(<span class="hljs-number">1</span>) &amp; <span class="hljs-number">0xFF</span><font></font>
<font></font>
	<span class="hljs-comment"># if the `q` key was pressed, break from the loop</span>
	<span class="hljs-keyword">if</span> key == ord(<span class="hljs-string">"q"</span>):
		<span class="hljs-keyword">break</span><font></font>
<font></font>
	<span class="hljs-comment"># update the FPS counter</span><font></font>
	fps.update()<font></font>
<font></font>
<span class="hljs-comment"># stop the timer and display FPS information</span><font></font>
fps.stop()<font></font>
print(<span class="hljs-string">"[INFO] elapsed time: {:.2f}"</span>.format(fps.elapsed()))<font></font>
print(<span class="hljs-string">"[INFO] approx. FPS: {:.2f}"</span>.format(fps.fps()))<font></font>
<font></font>
<span class="hljs-comment"># do a bit of cleanup</span><font></font>
cv2.destroyAllWindows()<font></font>
vs.stop()<font></font>
</code></pre><br>
</div>
                    </div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Entonces, todo est√° listo. Lanzamos el modelo, apuntamos la lente a mi viejo CraftDuino y disfrutamos del resultado: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/bw/hj/yd/bwhjyd9pddoeop9yaz7fxbqozzo.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
a primera vista, no est√° nada mal, pero es solo a primera vista. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Parece que en 23 horas, el modelo fue reentrenado, por lo tanto, produce errores graves al definir objetos. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Aqu√≠ hay una demostraci√≥n visual: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/1w/3y/gf/1w3ygfo-ufytpuyct1kaarpsgls.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
como puede ver, no solo un cuchillo, sino incluso un fondo negro, este modelo lo define como un controlador tipo arduino. Quiz√°s esto se deba a que en los datos de entrenamiento hab√≠a im√°genes oscuras con el Arduino y sus an√°logos, en las que el modelo logr√≥ toparse en 23 horas. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Como resultado, tuve que cargar mi computadora durante otras 8 horas y entrenar un nuevo modelo. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Las cosas est√°n mucho mejor con ella. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Aqu√≠ hay un ejemplo con CraftDuino:</font></font><br>
<br>
<img src="https://habrastorage.org/webt/_c/8m/62/_c8m62y2q6as-l8sun5ah5ivppk.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Las frambuesas vivas no est√°n a la mano. </font><font style="vertical-align: inherit;">Tuve que imprimir fotos. </font><font style="vertical-align: inherit;">Desde la pantalla del tel√©fono o monitor, tambi√©n puede reconocer, pero desde el papel fue m√°s conveniente. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/63/_k/ou/63_koujmchte7jor0ulqzxcvgcs.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Veamos c√≥mo el modelo reconoce el Arduino nano, que a su debido tiempo</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" class="user_link"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Drzugrik</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">para m√≠, soldar√© a mi mega dispositivo con sensores: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ub/33/61/ub3361ozwkiwvl2sosx6yldvsou.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
como puede ver, reconoce bastante bien, pero con un √°ngulo muy pobre y con una iluminaci√≥n c√°lida, puede reconocer algunos fragmentos como las frambuesas. Pero, de hecho, un marco con un error fue dif√≠cil de atrapar en la lente. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ahora veamos c√≥mo clasifica esos objetos en los que no fue entrenada. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
De nuevo, un ejemplo con un cuchillo y un fondo negro: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/io/ja/6a/ioja6aexferclondu4228nsr06y.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
esta vez todo funciona como deber√≠a. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ofreceremos nuestro modelo para reconocer el peque√±o controlador Canny 3, sobre el que escrib√≠ en un </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">art√≠culo anterior</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br>
<img src="https://habrastorage.org/webt/xp/ay/14/xpay14o7clhp1y1twu4vyltiay4.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dado que nuestro modelo no conoce nada m√°s que frambuesas y controladores similares a arduino, podemos decir que el modelo reconoci√≥ el controlador Canny con bastante √©xito.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Es cierto que, como en el caso del Arduino nano, mucho depende del √°ngulo y la iluminaci√≥n. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Con la luz c√°lida de una l√°mpara incandescente y con un √°ngulo fallido, el controlador puede no solo no ser reconocido, sino incluso definido como frambuesa. </font><font style="vertical-align: inherit;">Es cierto que, como en el caso anterior, estos √°ngulos todav√≠a ten√≠an que intentar atrapar la lente. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/01/ut/h_/01uth_-raiwnzasg7ypn-aoxezs.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bueno, el √∫ltimo caso es una especie de reverencia para el art√≠culo sobre la </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">clasificaci√≥n de im√°genes en PyTorch</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Como la √∫ltima vez, la computadora de placa √∫nica Raspberry Pi 2 y su logotipo son compatibles en un solo cuadro. </font><font style="vertical-align: inherit;">A diferencia del art√≠culo anterior, en el que resolvimos el problema de clasificaci√≥n y elegimos un objeto m√°s probable para la imagen, en este caso se reconoce tanto el logotipo como la propia Frambuesa.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/vx/fv/us/vxfvusfgitn6vk1pe6o4rvoen9i.png"><br>
<br>
<a name="IV"></a><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Parte IV: Conclusi√≥n </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En conclusi√≥n, quiero decir que, a pesar de la inexperiencia de este peque√±o ejemplo de trabajo con la API de detecci√≥n de objetos de Tensorflow, me tom√≥ dos d√≠as libres y parte del lunes, no me arrepiento de nada. Cuando al menos un poco de comprensi√≥n sobre c√≥mo usarlo todo se vuelve incre√≠blemente curioso. En el proceso de aprendizaje, comienza a considerar el modelo como uno vivo, sigue sus √©xitos y fracasos. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Por lo tanto, recomiendo a todos los que no est√©n familiarizados con esto alg√∫n d√≠a que traten de reconocer algo propio.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Adem√°s, como ha aumentado en el proceso, ni siquiera necesita comprar una c√°mara web real. El hecho es que durante la preparaci√≥n del art√≠culo, logr√© romper mi c√°mara web (romp√≠ el mecanismo de enfoque) y ya pens√© que tendr√≠a que abandonar todo. Pero result√≥ que con la ayuda de Droidcam puede usar un tel√©fono inteligente en lugar de una c√°mara web (no cuente para publicidad). Adem√°s, la calidad de disparo result√≥ ser mucho mejor que la de una c√°mara rota, y esto influy√≥ mucho en la calidad del reconocimiento de objetos en la imagen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Por cierto, ya que Anaconda tiene una construcci√≥n normal de </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">picocotoles</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Encontr√© solo para Linux, y era demasiado flojo para cambiar entre sistemas operativos, prepar√© este art√≠culo completo solo con software de c√≥digo abierto. </font><font style="vertical-align: inherit;">Hab√≠a an√°logos de Word y Photoshop e incluso un controlador para la impresora. </font><font style="vertical-align: inherit;">La primera vez en mi vida esto sucedi√≥. </font><font style="vertical-align: inherit;">Result√≥ que las versiones modernas del sistema operativo Linux y los programas de aplicaci√≥n pueden ser muy convenientes, incluso para una persona que usa el sistema operativo Microsoft por m√°s de 25 a√±os. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
PD: si alguien sabe c√≥mo ejecutar correctamente la API de detecci√≥n de objetos </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
para Tensorflow versi√≥n 2 y superior, cancele la suscripci√≥n en PM o en un comentario. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
¬°Que tengas un buen d√≠a y buena salud!</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es488468/index.html">Introduciendo FastAPI</a></li>
<li><a href="../es488470/index.html">Kim Dotcom: Atrapado, la persona m√°s buscada en l√≠nea. Parte 4</a></li>
<li><a href="../es488472/index.html">Lectura de fin de semana: 10 materiales sobre dispositivos de audio, desde radios de autom√≥viles sovi√©ticos hasta enchufes con cancelaci√≥n de ruido</a></li>
<li><a href="../es488474/index.html">Sobre el color, el sonido y la "exploraci√≥n de multitudes" como un tipo separado de belleza</a></li>
<li><a href="../es494800/index.html">Ingenier√≠a inversa del protocolo chino de transceptor IR USB</a></li>
<li><a href="../es494806/index.html">Cyber ‚Äã‚Äãapunta a 2019 como tendencias 2020: los hackers han cambiado su enfoque</a></li>
<li><a href="../es494808/index.html">Analista de producto: qu√© hace, cu√°nto gana, qu√© beneficios aporta el negocio</a></li>
<li><a href="../es494810/index.html">Introducci√≥n a 3D: conceptos b√°sicos de Three.js</a></li>
<li><a href="../es494814/index.html">¬øEs √∫til Slurm?</a></li>
<li><a href="../es494818/index.html">C√≥mo elegir un terminal comercial para trabajar en el intercambio</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>