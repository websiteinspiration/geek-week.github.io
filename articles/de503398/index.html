<!doctype html>
<html class="no-js" lang="de">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🐊 😲 📓 Wie man schnell eine thematische Modellierung eines Forums erstellt oder was Menschen mit Zöliakie stört 😽 🛢️ ☮️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In diesem Artikel werde ich ein Beispiel dafür erzählen und zeigen, wie eine Person mit minimaler Data Science-Erfahrung Daten aus dem Forum sammeln u...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Wie man schnell eine thematische Modellierung eines Forums erstellt oder was Menschen mit Zöliakie stört</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/503398/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In diesem Artikel werde ich ein Beispiel dafür erzählen und zeigen, wie eine Person mit minimaler Data Science-Erfahrung Daten aus dem Forum sammeln und Beiträge mithilfe des LDA-Modells thematisch modellieren konnte, und schmerzhafte Themen für Menschen mit Zöliakie-Intoleranz aufdecken. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Letztes Jahr musste ich meine Kenntnisse im Bereich des maschinellen Lernens dringend verbessern. Ich bin Produktmanager für Data Science, Maschinelles Lernen und KI oder auf andere Weise Technischer Produktmanager KI / ML. Geschäftsfähigkeiten und die Fähigkeit, Produkte zu entwickeln, wie dies normalerweise bei Projekten der Fall ist, die sich an Benutzer richten, die nicht im technischen Bereich tätig sind, reichen nicht aus. Sie müssen die grundlegenden technischen Konzepte der ML-Branche verstehen und bei Bedarf selbst ein Beispiel schreiben können, um das Produkt zu demonstrieren.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Seit ungefähr 5 Jahren entwickle ich Front-End-Projekte, entwickle komplexe Webanwendungen für JS und React, aber ich habe mich nie mit maschinellem Lernen, Laptops und Algorithmen befasst. Als ich die Nachricht von </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Otus sah,</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> dass sie </font><font style="vertical-align: inherit;">ohne zu zögern </font><font style="vertical-align: inherit;">einen fünfmonatigen experimentellen </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Kurs über maschinelles Lernen eröffneten</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , entschied ich mich daher, mich einem Probetest zu unterziehen, und nahm an dem Kurs teil.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Fünf Monate lang gab es jede Woche zweistündige Vorträge und Hausaufgaben für sie. Dort lernte ich die Grundlagen von ML kennen: verschiedene Regressionsalgorithmen, Klassifikationen, Modellensembles, Gradientenverstärkung und sogar leicht betroffene Cloud-Technologien. Wenn Sie sich jede Vorlesung genau anhören, gibt es im Prinzip genügend Beispiele und Erklärungen für Hausaufgaben. Trotzdem musste ich mich manchmal, wie bei jedem anderen Codierungsprojekt, der Dokumentation zuwenden. Angesichts meiner Vollzeitbeschäftigung war es sehr praktisch zu studieren, da ich die Aufzeichnung einer Online-Vorlesung jederzeit überarbeiten konnte.</font></font><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Am Ende der Schulung dieses Kurses mussten alle das Abschlussprojekt absolvieren. </font><font style="vertical-align: inherit;">Die Idee für das Projekt entstand ganz spontan. Zu diesem Zeitpunkt begann ich bei Stanford mit der Ausbildung zum Unternehmer, wo ich in das Team kam, das an dem Projekt für Menschen mit Zöliakie-Intoleranz arbeitete. </font><font style="vertical-align: inherit;">Während der Marktforschung war ich interessiert zu wissen, über welche Sorgen, worüber sie sprechen, worüber sich Leute mit dieser Funktion beschweren. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Im Verlauf der Studie fand ich auf </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;">celiac.com</font></a><font style="vertical-align: inherit;"> ein Forum</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">mit einer großen Menge an Material über Zöliakie. </font><font style="vertical-align: inherit;">Es war offensichtlich, dass manuelles Scrollen und Lesen von mehr als 100.000 Posts unpraktisch war. </font><font style="vertical-align: inherit;">So kam mir die Idee, das Wissen, das ich in diesem Kurs erhalten habe, anzuwenden: alle Fragen und Kommentare aus dem Forum zu einem bestimmten Thema zu sammeln und eine thematische Modellierung mit den häufigsten Wörtern in jedem von ihnen zu erstellen.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Schritt 1. Datenerfassung aus dem Forum</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das Forum besteht aus vielen Themen unterschiedlicher Größe. </font><font style="vertical-align: inherit;">Insgesamt hat dieses Forum ungefähr 115.000 Themen und ungefähr eine Million Beiträge mit Kommentaren dazu. </font><font style="vertical-align: inherit;">Ich interessierte mich für das spezifische Unterthema </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">„Umgang mit Zöliakie“</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , was wörtlich „mit Zöliakie fertig werden“ bedeutet, wenn es auf Russisch mehr bedeutet, „weiterhin mit der Diagnose einer Zöliakie zu leben und irgendwie mit Schwierigkeiten fertig zu werden“. </font><font style="vertical-align: inherit;">Dieses Unterthema enthält ca. 175.000 Kommentare. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das Herunterladen der Daten erfolgte in zwei Schritten. </font><font style="vertical-align: inherit;">Zunächst musste ich alle Seiten unter dem Thema durchgehen und alle Links zu allen Posts sammeln, damit ich im nächsten Schritt bereits einen Kommentar sammeln konnte.</font></font><br>
<br>
<pre><code class="python hljs">url_coping = <span class="hljs-string">'https://www.celiac.com/forums/forum/5-coping-with-celiac-disease/'</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Da sich das Forum als ziemlich alt herausstellte, hatte ich großes Glück und die Website hatte keine Sicherheitsprobleme. </font><i><font style="vertical-align: inherit;">Um</font></i><font style="vertical-align: inherit;"> die Daten zu sammeln, reichte es aus, die </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">User-Agent-</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Kombination </font><font style="vertical-align: inherit;">aus der Bibliothek </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">fake_useragent</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Beautiful Soup zu</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> verwenden </font><font style="vertical-align: inherit;">, um mit HTML-Markup zu arbeiten und die Anzahl der Seiten zu kennen:</font></font><br>
<br>
<pre><code class="python hljs">
<span class="hljs-comment"># Get total number of pages</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_pages_count</span>(<span class="hljs-params">url</span>):</span>
    response = requests.get(url, headers={<span class="hljs-string">'User-Agent'</span>: UserAgent().chrome})<font></font>
    soup = BeautifulSoup(response.content, <span class="hljs-string">'html.parser'</span>)<font></font>
    last_page_section = soup.find(<span class="hljs-string">'li'</span>, attrs = {<span class="hljs-string">'class'</span>:<span class="hljs-string">'ipsPagination_last'</span>})
    <span class="hljs-keyword">if</span> (last_page_section):<font></font>
        count_link = last_page_section.find(<span class="hljs-string">'a'</span>)
        <span class="hljs-keyword">return</span> int(count_link[<span class="hljs-string">'data-page'</span>])
    <span class="hljs-keyword">else</span>: 
        <span class="hljs-keyword">return</span> <span class="hljs-number">1</span><font></font>
<font></font>
coping_pages_count = get_pages_count(url_coping)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Laden Sie dann das HTML-DOM jeder Seite herunter, um mithilfe der </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">BeautifulSoup</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Python-Bibliothek einfach und problemlos Daten daraus abzurufen </font><font style="vertical-align: inherit;">.</font></font><br>
<br>
<pre><code class="python hljs">
<span class="hljs-comment"># collect pages</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">retrieve_pages</span>(<span class="hljs-params">pages_count, url</span>):</span><font></font>
    pages = []<font></font>
    <span class="hljs-keyword">for</span> page <span class="hljs-keyword">in</span> range(pages_count):<font></font>
        response = requests.get(<span class="hljs-string">'{}page/{}'</span>.format(url, page), headers={<span class="hljs-string">'User-Agent'</span>: UserAgent().chrome})<font></font>
        soup = BeautifulSoup(response.content, <span class="hljs-string">'html.parser'</span>)<font></font>
        pages.append(soup)<font></font>
    <span class="hljs-keyword">return</span> pages<font></font>
<font></font>
coping_pages = retrieve_pages(coping_pages_count, url_coping)<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Um die Daten herunterzuladen, musste ich die für die Analyse erforderlichen Felder ermitteln: Suchen Sie die Werte dieser Felder im DOM und speichern Sie sie im Wörterbuch. </font><font style="vertical-align: inherit;">Ich selbst kam aus dem Front-End-Bereich, daher war es für mich trivial, mit Haushalten und Gegenständen zu arbeiten.</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">collect_post_info</span>(<span class="hljs-params">pages</span>):</span><font></font>
    posts = []<font></font>
    <span class="hljs-keyword">for</span> page <span class="hljs-keyword">in</span> pages:<font></font>
        posts_list_soup = page.find(<span class="hljs-string">'ol'</span>, attrs = {<span class="hljs-string">'class'</span>: <span class="hljs-string">'ipsDataList'</span>}).findAll(<span class="hljs-string">'li'</span>, attrs = {<span class="hljs-string">'class'</span>: <span class="hljs-string">'ipsDataItem'</span>})
        <span class="hljs-keyword">for</span> post_soup <span class="hljs-keyword">in</span> posts_list_soup:<font></font>
            post = {}<font></font>
            post[<span class="hljs-string">'id'</span>] = uuid.uuid4()
            <span class="hljs-comment"># collecting titles and urls</span>
            title_section = post_soup.find(<span class="hljs-string">'span'</span>, attrs = {<span class="hljs-string">'class'</span>:<span class="hljs-string">'ipsType_break ipsContained'</span>})
            <span class="hljs-keyword">if</span> (title_section):<font></font>
                title_section_a = title_section.find(<span class="hljs-string">'a'</span>)<font></font>
                post[<span class="hljs-string">'title'</span>] = title_section_a[<span class="hljs-string">'title'</span>]<font></font>
                post[<span class="hljs-string">'url'</span>] = title_section_a[<span class="hljs-string">'data-ipshover-target'</span>]
            <span class="hljs-comment"># collecting author &amp; last action</span>
            author_section = post_soup.find(<span class="hljs-string">'div'</span>, attrs = {<span class="hljs-string">'class'</span>:<span class="hljs-string">'ipsDataItem_meta'</span>})
            <span class="hljs-keyword">if</span> (author_section):<font></font>
                author_section_a = post_soup.find(<span class="hljs-string">'a'</span>)<font></font>
                author_section_time = post_soup.find(<span class="hljs-string">'time'</span>)<font></font>
                post[<span class="hljs-string">'author'</span>] = author_section_a[<span class="hljs-string">'data-ipshover-target'</span>]<font></font>
                post[<span class="hljs-string">'last_action'</span>] = author_section_time[<span class="hljs-string">'datetime'</span>]
            <span class="hljs-comment"># collecting stats</span>
            stats_section = post_soup.find(<span class="hljs-string">'ul'</span>, attrs = {<span class="hljs-string">'class'</span>:<span class="hljs-string">'ipsDataItem_stats'</span>})
            <span class="hljs-keyword">if</span> (stats_section):<font></font>
                stats_section_replies = post_soup.find(<span class="hljs-string">'span'</span>, attrs = {<span class="hljs-string">'class'</span>:<span class="hljs-string">'ipsDataItem_stats_number'</span>})
                <span class="hljs-keyword">if</span> (stats_section_replies):<font></font>
                    post[<span class="hljs-string">'replies'</span>] = stats_section_replies.getText()<font></font>
                stats_section_views = post_soup.find(<span class="hljs-string">'li'</span>, attrs = {<span class="hljs-string">'class'</span>:<span class="hljs-string">'ipsType_light'</span>})
                <span class="hljs-keyword">if</span> (stats_section_views):<font></font>
                    post[<span class="hljs-string">'views'</span>] = stats_section_views.find(<span class="hljs-string">'span'</span>, attrs = {<span class="hljs-string">'class'</span>:<span class="hljs-string">'ipsDataItem_stats_number'</span>}).getText()<font></font>
            posts.append(post)<font></font>
    <span class="hljs-keyword">return</span> posts
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Insgesamt habe ich ungefähr 15.450 Beiträge zu diesem Thema gesammelt.</font></font><br>
<br>
<pre><code class="python hljs">coping_posts_info = collect_post_info(coping_pages)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Jetzt konnten sie in den DataFrame übertragen werden, so dass sie dort wunderschön lagen, und gleichzeitig in einer CSV-Datei gespeichert werden, sodass Sie nicht erneut warten mussten, wenn die Daten von der Site gesammelt wurden, wenn das Notebook versehentlich kaputt ging oder ich versehentlich eine Variable neu definiert habe, bei der. </font></font><br>
<br>
<pre><code class="python hljs">df_coping = pd.DataFrame(coping_posts_info, <font></font>
               columns =[<span class="hljs-string">'title'</span>, <span class="hljs-string">'url'</span>, <span class="hljs-string">'author'</span>, <span class="hljs-string">'last_action'</span>, <span class="hljs-string">'replies'</span>, <span class="hljs-string">'views'</span>]) <font></font>
<font></font>
<span class="hljs-comment"># format data</span>
df_coping[<span class="hljs-string">'replies'</span>] = df_coping[<span class="hljs-string">'replies'</span>].astype(int)<font></font>
df_coping[<span class="hljs-string">'views'</span>] = df_coping[<span class="hljs-string">'views'</span>].apply(<span class="hljs-keyword">lambda</span> x: int(x.replace(<span class="hljs-string">','</span>,<span class="hljs-string">''</span>)))<font></font>
df_coping.to_csv(<span class="hljs-string">'celiac_forum_coping.csv'</span>, sep=<span class="hljs-string">','</span>)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nachdem ich eine Sammlung von Beiträgen gesammelt hatte, sammelte ich die Kommentare selbst.</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">collect_postpage_details</span>(<span class="hljs-params">pages, df</span>):</span><font></font>
    comments = []<font></font>
    <span class="hljs-keyword">for</span> i, page <span class="hljs-keyword">in</span> enumerate(pages):<font></font>
        articles = page.findAll(<span class="hljs-string">'article'</span>)
        <span class="hljs-keyword">for</span> k, article <span class="hljs-keyword">in</span> enumerate(articles):<font></font>
            comment = {<font></font>
                <span class="hljs-string">'url'</span>: df[<span class="hljs-string">'url'</span>][i]<font></font>
            }<font></font>
            <span class="hljs-keyword">if</span>(k == <span class="hljs-number">0</span>):<font></font>
                comment[<span class="hljs-string">'question'</span>] = <span class="hljs-number">1</span>
            <span class="hljs-keyword">else</span>:<font></font>
                comment[<span class="hljs-string">'question'</span>] = <span class="hljs-number">0</span>
            <span class="hljs-comment"># collecting comments</span>
            comment_section = article.find(<span class="hljs-string">'div'</span>, attrs = {<span class="hljs-string">'class'</span>:<span class="hljs-string">'ipsComment_content'</span>})
            <span class="hljs-keyword">if</span> (comment_section):<font></font>
                comment_section_p = comment_section.find(<span class="hljs-string">'p'</span>)
                <span class="hljs-keyword">if</span>(comment_section_p):<font></font>
                    comment[<span class="hljs-string">'comment'</span>] = comment_section_p.getText()<font></font>
            comment[<span class="hljs-string">'date'</span>] = comment_section.find(<span class="hljs-string">'time'</span>)[<span class="hljs-string">'datetime'</span>]<font></font>
            author_section = article.find(<span class="hljs-string">'strong'</span>)
            <span class="hljs-keyword">if</span> (author_section):<font></font>
                author_section_url = author_section.find(<span class="hljs-string">'a'</span>)
                <span class="hljs-keyword">if</span> (author_section_url):<font></font>
                    comment[<span class="hljs-string">'author'</span>] = author_section_url[<span class="hljs-string">'data-ipshover-target'</span>]<font></font>
            comments.append(comment)<font></font>
    <span class="hljs-keyword">return</span> comments<font></font>
<font></font>
coping_data = collect_postpage_details(coping_comments_pages, df_coping)<font></font>
df_coping_comments.to_csv(<span class="hljs-string">'celiac_forum_coping_comments_1.csv'</span>, sep=<span class="hljs-string">','</span>)<font></font>
<font></font>
</code></pre><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SCHRITT 2 Datenanalyse und thematische Modellierung</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Im vorherigen Schritt haben wir Daten aus dem Forum gesammelt und die endgültigen Daten in Form von 153777 Zeilen mit Fragen und Kommentaren erhalten. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Aber nur die gesammelten Daten sind nicht interessant. Das erste, was ich tun wollte, war eine sehr einfache Analyse: Ich habe Statistiken für die 30 meistgesehenen und 30 am meisten kommentierten Themen abgeleitet. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/9o/ai/dj/9oaidjzovi7va3alxg7vdwkts84.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die meistgesehenen Beiträge stimmten nicht mit den am meisten kommentierten überein. Die Titel kommentierter Beiträge sind bereits auf den ersten Blick erkennbar. Ihre Namen sind emotionaler: </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Ich hasse, ich hasse, ich hasse"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> oder " </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Arrogante Kommentare"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> oder </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Wow, ich bin in Schwierigkeiten</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><i><font style="vertical-align: inherit;">"</font></i><font style="vertical-align: inherit;"> Und die meistgesehenen haben ein Fragenformat: </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Kann ich Soja essen?", "Warum kann ich Wasser nicht richtig aufnehmen?"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">und andere. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wir haben eine einfache Textanalyse durchgeführt. </font><font style="vertical-align: inherit;">Um direkt zu einer komplexeren Analyse zu gelangen, müssen Sie die Daten selbst vorbereiten, bevor Sie sie zur Eingabe nach Themen an die Eingabe des LDA-Modells senden. </font><font style="vertical-align: inherit;">Entfernen Sie dazu Kommentare mit weniger als 30 Wörtern, um Spam und bedeutungslose kurze Kommentare herauszufiltern. </font><font style="vertical-align: inherit;">Wir bringen sie in Kleinbuchstaben.</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-comment"># Let's get rid of text &lt; 30 words</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">filter_text_words</span>(<span class="hljs-params">text, min_words = <span class="hljs-number">30</span></span>):</span><font></font>
    text = str(text)<font></font>
    <span class="hljs-keyword">return</span> len(text.split()) &gt; <span class="hljs-number">30</span>
filtered_comments = filtered_comments[filtered_comments[<span class="hljs-string">'comment'</span>].apply(filter_text_words)]<font></font>
comments_only = filtered_comments[<span class="hljs-string">'comment'</span>]<font></font>
comments_only= comments_only.apply(<span class="hljs-keyword">lambda</span> x: x.lower())<font></font>
comments_only.head()</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Löschen Sie unnötige Stoppwörter, um unsere Textauswahl zu löschen</font></font><br>
<br>
<pre><code class="python hljs">stop_words = stopwords.words(<span class="hljs-string">'english'</span>)
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">remove_stop_words</span>(<span class="hljs-params">tokens</span>):</span><font></font>
    new_tokens = []<font></font>
    <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> tokens:<font></font>
        token = []<font></font>
        <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> t:
            <span class="hljs-keyword">if</span> word <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> stop_words:<font></font>
                token.append(word)<font></font>
        new_tokens.append(token)<font></font>
    <span class="hljs-keyword">return</span> new_tokens<font></font>
<font></font>
tokens = remove_stop_words(data_words)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wir fügen auch Bigramme hinzu und bilden eine Tüte mit Wörtern, um stabile Phrasen hervorzuheben, z. B. </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">glutenfrei, support_group</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> und andere Phrasen, die, wenn sie gruppiert sind, eine bestimmte Bedeutung haben.</font></font><br>
<br>
<pre><code class="python hljs">
bigram = gensim.models.Phrases(tokens, min_count=<span class="hljs-number">5</span>, threshold=<span class="hljs-number">100</span>)<font></font>
bigram_mod = gensim.models.phrases.Phraser(bigram)<font></font>
bigram_mod.save(<span class="hljs-string">'bigram_mod.pkl'</span>)<font></font>
bag_of_words = [bigram_mod[w] <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> tokens]
<span class="hljs-keyword">with</span> open(<span class="hljs-string">'bigrams.pkl'</span>, <span class="hljs-string">'wb'</span>) <span class="hljs-keyword">as</span> f:<font></font>
    pickle.dump(bag_of_words, f)<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Jetzt sind wir endlich bereit, das LDA-Modell selbst direkt zu trainieren.</font></font><br>
<br>
<pre><code class="python hljs"><font></font>
id2word = corpora.Dictionary(bag_of_words)<font></font>
id2word.save(<span class="hljs-string">'id2word.pkl'</span>)<font></font>
id2word.filter_extremes(no_below=<span class="hljs-number">3</span>, no_above=<span class="hljs-number">0.4</span>, keep_n=<span class="hljs-number">3</span>*<span class="hljs-number">10</span>**<span class="hljs-number">6</span>)<font></font>
corpus = [id2word.doc2bow(text) <span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> bag_of_words]<font></font>
<font></font>
lda_model = gensim.models.ldamodel.LdaModel(<font></font>
    corpus, <font></font>
    id2word=id2word, <font></font>
    eval_every=<span class="hljs-number">20</span>,<font></font>
    random_state=<span class="hljs-number">42</span>,<font></font>
    num_topics=<span class="hljs-number">30</span>, <font></font>
    passes=<span class="hljs-number">5</span><font></font>
    )<font></font>
lda_model.save(<span class="hljs-string">'lda_default_2.pkl'</span>)<font></font>
topics = lda_model.show_topics(num_topics=<span class="hljs-number">30</span>, num_words=<span class="hljs-number">100</span>, formatted=<span class="hljs-literal">False</span>)
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Am Ende des Trainings erhalten wir letztendlich das Ergebnis der gebildeten Themen. Was ich am Ende dieses Beitrags angehängt habe.</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> range(lda_model.num_topics):<font></font>
    plt.figure(figsize=(<span class="hljs-number">15</span>, <span class="hljs-number">10</span>))<font></font>
    plt.imshow(WordCloud(background_color=<span class="hljs-string">"white"</span>, max_words=<span class="hljs-number">100</span>, width=<span class="hljs-number">900</span>, height=<span class="hljs-number">900</span>, collocations=<span class="hljs-literal">False</span>)<font></font>
               .fit_words(dict(topics[t][<span class="hljs-number">1</span>])))<font></font>
    plt.axis(<span class="hljs-string">"off"</span>)<font></font>
    plt.title(<span class="hljs-string">"Topic #"</span> + themes_headers[t])<font></font>
    plt.show()<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wie sich vielleicht bemerkbar macht, haben sich die Themen inhaltlich als ziemlich unterschiedlich herausgestellt. </font><font style="vertical-align: inherit;">Ihnen zufolge wird klar, wovon Menschen mit Zöliakie-Intoleranz sprechen. </font><font style="vertical-align: inherit;">Grundsätzlich geht es um Essen, Restaurants, kontaminiertes Essen mit Gluten, schreckliche Schmerzen, Behandlung, Arztbesuche, Familienangehörige, Missverständnisse und andere Dinge, denen sich Menschen im Zusammenhang mit ihrem Problem jeden Tag stellen müssen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das ist alles. </font><font style="vertical-align: inherit;">Vielen Dank für Ihre Aufmerksamkeit. </font><font style="vertical-align: inherit;">Ich hoffe, Sie finden dieses Material interessant und nützlich. </font><font style="vertical-align: inherit;">Und doch, da ich kein DS-Entwickler bin, urteile nicht streng. </font><font style="vertical-align: inherit;">Wenn es etwas hinzuzufügen oder zu verbessern gibt, freue ich mich immer über konstruktive Kritik, schreibe. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
30 Themen anzeigen</font></font><br>
<br>
<div class="spoiler" role="button" tabindex="0">
                        <b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Achtung, viele Bilder</font></font></b>
                        <div class="spoiler_text"><img src="https://habrastorage.org/webt/cn/dy/tb/cndytbwrtz9ujkkh6xicy0mjyds.png"><br>
<br>
<img src="https://habrastorage.org/webt/no/n2/iq/non2iqgux8nvb5hnitr8n7yra_w.png"><br>
<br>
<img src="https://habrastorage.org/webt/dx/dh/nl/dxdhnlddgexrb_noeq8psjb7nps.png"><br>
<br>
<img src="https://habrastorage.org/webt/x1/_f/q6/x1_fq6omll0iigewzz0ba8tjvys.png"><br>
<br>
<img src="https://habrastorage.org/webt/7v/qy/fh/7vqyfh-uwk_bhypzdgisxxabzjs.png"><br>
<br>
<img src="https://habrastorage.org/webt/v7/1z/fn/v71zfn2kb0xj7rpsthrplgznzzw.png"><br>
<br>
<img src="https://habrastorage.org/webt/ab/tt/t7/abttt7c8aqydfc28gxyq9ai7a4q.png"><br>
<br>
<img src="https://habrastorage.org/webt/oz/hc/m7/ozhcm72ldjjenp5onkjydxgpvly.png"><br>
<br>
<img src="https://habrastorage.org/webt/fe/ex/lw/feexlw8tcrcwni5wmy8k8rv8k3e.png"><br>
<br>
<img src="https://habrastorage.org/webt/w0/hu/5j/w0hu5jix2zrewo2l9jnbkddd3tk.png"><br>
<br>
<img src="https://habrastorage.org/webt/zf/ye/kw/zfyekw6s6qfxuqwy-qxhv_dehrq.png"><br>
<br>
<img src="https://habrastorage.org/webt/l0/9s/vw/l09svwry19fhz1y-1-pooeo_vew.png"><br>
<br>
<img src="https://habrastorage.org/webt/pm/mt/bk/pmmtbkkybu50vhgttl-0kz4tcf4.png"><br>
<br>
<img src="https://habrastorage.org/webt/1h/hu/vr/1hhuvrmmfjxwfzh3fbhf9dbut38.png"><br>
<br>
<img src="https://habrastorage.org/webt/bw/is/ad/bwisadbn9a000lt6xp927szic2u.png"><br>
<br>
<img src="https://habrastorage.org/webt/iu/bf/4q/iubf4qt_juq9uip17rmngbr7wxe.png"><br>
<br>
<img src="https://habrastorage.org/webt/jk/of/sa/jkofsalh2hev8zx6jjlom0pnnxy.png"><br>
<br>
<img src="https://habrastorage.org/webt/js/bs/ls/jsbslsv_4ly4rwe7wir6xvcs6t4.png"><br>
<br>
<img src="https://habrastorage.org/webt/_e/ly/wr/_elywrkbtgk-4fvlnuzfr6zqq4o.png"><br>
<br>
<img src="https://habrastorage.org/webt/4j/x8/pa/4jx8paomlrca7t0syfunmtmlxk4.png"><br>
<br>
<img src="https://habrastorage.org/webt/y2/he/s1/y2hes1fvuepisygriea98m_yavw.png"><br>
<br>
<img src="https://habrastorage.org/webt/9k/xs/sr/9kxssr9rxlyeobjw12fwju0-xkq.png"><br>
<br>
<img src="https://habrastorage.org/webt/i-/sl/qd/i-slqdug6x9dkwybnfnxmdolho8.png"><br>
<br>
<img src="https://habrastorage.org/webt/nq/pk/x5/nqpkx5q6j8e_6mkpfak0ytkkvfc.png"><br>
<br>
<img src="https://habrastorage.org/webt/jv/3b/pa/jv3bpafludpki_2a-4pgajhreh0.png"><br>
<br>
<img src="https://habrastorage.org/webt/sw/-e/pn/sw-epnxhrwa4t7i6uksmggczs-8.png"><br>
<br>
<img src="https://habrastorage.org/webt/-y/qj/0t/-yqj0t-jkax-s09bivkgx8a3mqa.png"><br>
<br>
<img src="https://habrastorage.org/webt/ta/rp/4w/tarp4wr8bcui0zszwuzl7l9h8zo.png"><br>
<br>
<img src="https://habrastorage.org/webt/eo/xl/m2/eoxlm2i2z9weffxhgm-zzgszd3q.png"><br>
<br>
<hr><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">    «Machine Learning»  OTUS</a>.<br>
<br>
<hr><br>
</div>
                    </div></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de503382/index.html">Gehen Sie auf einem sauberen Feld spazieren oder sammeln Sie MAC-Adressen von Wi-Fi-Geräten in der Nähe</a></li>
<li><a href="../de503386/index.html">Agile Coach gesunde Person</a></li>
<li><a href="../de503388/index.html">Digitalisierung der Panik: DIT von Moskau gegen Moskauer - ein runder Tisch am 23. Mai</a></li>
<li><a href="../de503390/index.html">Warum setzt Intel auf die Chipentwicklung für das Genie von Jim Keller?</a></li>
<li><a href="../de503394/index.html">Erfahrung in Aktieninvestitionen</a></li>
<li><a href="../de503402/index.html">Wie man sich an jeden persönlich erinnert oder wie man effektiv nach Gesichtern in einer großen Datenbank sucht</a></li>
<li><a href="../de503404/index.html">Semantische digitale Systeme</a></li>
<li><a href="../de503406/index.html">Semantik und Aktivität</a></li>
<li><a href="../de503408/index.html">Blazor Client Side Online Store: Teil 7 - Auf Version 3.2.0 aktualisiert und Bildanzeige hinzugefügt</a></li>
<li><a href="../de503410/index.html">Polygone Eine andere Welt: Sega Genesis</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>