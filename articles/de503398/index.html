<!doctype html>
<html class="no-js" lang="de">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üêä üò≤ üìì Wie man schnell eine thematische Modellierung eines Forums erstellt oder was Menschen mit Z√∂liakie st√∂rt üòΩ üõ¢Ô∏è ‚òÆÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In diesem Artikel werde ich ein Beispiel daf√ºr erz√§hlen und zeigen, wie eine Person mit minimaler Data Science-Erfahrung Daten aus dem Forum sammeln u...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Wie man schnell eine thematische Modellierung eines Forums erstellt oder was Menschen mit Z√∂liakie st√∂rt</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/503398/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In diesem Artikel werde ich ein Beispiel daf√ºr erz√§hlen und zeigen, wie eine Person mit minimaler Data Science-Erfahrung Daten aus dem Forum sammeln und Beitr√§ge mithilfe des LDA-Modells thematisch modellieren konnte, und schmerzhafte Themen f√ºr Menschen mit Z√∂liakie-Intoleranz aufdecken. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Letztes Jahr musste ich meine Kenntnisse im Bereich des maschinellen Lernens dringend verbessern. Ich bin Produktmanager f√ºr Data Science, Maschinelles Lernen und KI oder auf andere Weise Technischer Produktmanager KI / ML. Gesch√§ftsf√§higkeiten und die F√§higkeit, Produkte zu entwickeln, wie dies normalerweise bei Projekten der Fall ist, die sich an Benutzer richten, die nicht im technischen Bereich t√§tig sind, reichen nicht aus. Sie m√ºssen die grundlegenden technischen Konzepte der ML-Branche verstehen und bei Bedarf selbst ein Beispiel schreiben k√∂nnen, um das Produkt zu demonstrieren.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Seit ungef√§hr 5 Jahren entwickle ich Front-End-Projekte, entwickle komplexe Webanwendungen f√ºr JS und React, aber ich habe mich nie mit maschinellem Lernen, Laptops und Algorithmen befasst. Als ich die Nachricht von </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Otus sah,</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> dass sie </font><font style="vertical-align: inherit;">ohne zu z√∂gern </font><font style="vertical-align: inherit;">einen f√ºnfmonatigen experimentellen </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Kurs √ºber maschinelles Lernen er√∂ffneten</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , entschied ich mich daher, mich einem Probetest zu unterziehen, und nahm an dem Kurs teil.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
F√ºnf Monate lang gab es jede Woche zweist√ºndige Vortr√§ge und Hausaufgaben f√ºr sie. Dort lernte ich die Grundlagen von ML kennen: verschiedene Regressionsalgorithmen, Klassifikationen, Modellensembles, Gradientenverst√§rkung und sogar leicht betroffene Cloud-Technologien. Wenn Sie sich jede Vorlesung genau anh√∂ren, gibt es im Prinzip gen√ºgend Beispiele und Erkl√§rungen f√ºr Hausaufgaben. Trotzdem musste ich mich manchmal, wie bei jedem anderen Codierungsprojekt, der Dokumentation zuwenden. Angesichts meiner Vollzeitbesch√§ftigung war es sehr praktisch zu studieren, da ich die Aufzeichnung einer Online-Vorlesung jederzeit √ºberarbeiten konnte.</font></font><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Am Ende der Schulung dieses Kurses mussten alle das Abschlussprojekt absolvieren. </font><font style="vertical-align: inherit;">Die Idee f√ºr das Projekt entstand ganz spontan. Zu diesem Zeitpunkt begann ich bei Stanford mit der Ausbildung zum Unternehmer, wo ich in das Team kam, das an dem Projekt f√ºr Menschen mit Z√∂liakie-Intoleranz arbeitete. </font><font style="vertical-align: inherit;">W√§hrend der Marktforschung war ich interessiert zu wissen, √ºber welche Sorgen, wor√ºber sie sprechen, wor√ºber sich Leute mit dieser Funktion beschweren. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Im Verlauf der Studie fand ich auf </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;">celiac.com</font></a><font style="vertical-align: inherit;"> ein Forum</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">mit einer gro√üen Menge an Material √ºber Z√∂liakie. </font><font style="vertical-align: inherit;">Es war offensichtlich, dass manuelles Scrollen und Lesen von mehr als 100.000 Posts unpraktisch war. </font><font style="vertical-align: inherit;">So kam mir die Idee, das Wissen, das ich in diesem Kurs erhalten habe, anzuwenden: alle Fragen und Kommentare aus dem Forum zu einem bestimmten Thema zu sammeln und eine thematische Modellierung mit den h√§ufigsten W√∂rtern in jedem von ihnen zu erstellen.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Schritt 1. Datenerfassung aus dem Forum</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das Forum besteht aus vielen Themen unterschiedlicher Gr√∂√üe. </font><font style="vertical-align: inherit;">Insgesamt hat dieses Forum ungef√§hr 115.000 Themen und ungef√§hr eine Million Beitr√§ge mit Kommentaren dazu. </font><font style="vertical-align: inherit;">Ich interessierte mich f√ºr das spezifische Unterthema </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ÄûUmgang mit Z√∂liakie‚Äú</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , was w√∂rtlich ‚Äûmit Z√∂liakie fertig werden‚Äú bedeutet, wenn es auf Russisch mehr bedeutet, ‚Äûweiterhin mit der Diagnose einer Z√∂liakie zu leben und irgendwie mit Schwierigkeiten fertig zu werden‚Äú. </font><font style="vertical-align: inherit;">Dieses Unterthema enth√§lt ca. 175.000 Kommentare. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das Herunterladen der Daten erfolgte in zwei Schritten. </font><font style="vertical-align: inherit;">Zun√§chst musste ich alle Seiten unter dem Thema durchgehen und alle Links zu allen Posts sammeln, damit ich im n√§chsten Schritt bereits einen Kommentar sammeln konnte.</font></font><br>
<br>
<pre><code class="python hljs">url_coping = <span class="hljs-string">'https://www.celiac.com/forums/forum/5-coping-with-celiac-disease/'</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Da sich das Forum als ziemlich alt herausstellte, hatte ich gro√ües Gl√ºck und die Website hatte keine Sicherheitsprobleme. </font><i><font style="vertical-align: inherit;">Um</font></i><font style="vertical-align: inherit;"> die Daten zu sammeln, reichte es aus, die </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">User-Agent-</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Kombination </font><font style="vertical-align: inherit;">aus der Bibliothek </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">fake_useragent</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Beautiful Soup zu</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> verwenden </font><font style="vertical-align: inherit;">, um mit HTML-Markup zu arbeiten und die Anzahl der Seiten zu kennen:</font></font><br>
<br>
<pre><code class="python hljs">
<span class="hljs-comment"># Get total number of pages</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_pages_count</span>(<span class="hljs-params">url</span>):</span>
    response = requests.get(url, headers={<span class="hljs-string">'User-Agent'</span>: UserAgent().chrome})<font></font>
    soup = BeautifulSoup(response.content, <span class="hljs-string">'html.parser'</span>)<font></font>
    last_page_section = soup.find(<span class="hljs-string">'li'</span>, attrs = {<span class="hljs-string">'class'</span>:<span class="hljs-string">'ipsPagination_last'</span>})
    <span class="hljs-keyword">if</span> (last_page_section):<font></font>
        count_link = last_page_section.find(<span class="hljs-string">'a'</span>)
        <span class="hljs-keyword">return</span> int(count_link[<span class="hljs-string">'data-page'</span>])
    <span class="hljs-keyword">else</span>: 
        <span class="hljs-keyword">return</span> <span class="hljs-number">1</span><font></font>
<font></font>
coping_pages_count = get_pages_count(url_coping)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Laden Sie dann das HTML-DOM jeder Seite herunter, um mithilfe der </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">BeautifulSoup</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Python-Bibliothek einfach und problemlos Daten daraus abzurufen </font><font style="vertical-align: inherit;">.</font></font><br>
<br>
<pre><code class="python hljs">
<span class="hljs-comment"># collect pages</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">retrieve_pages</span>(<span class="hljs-params">pages_count, url</span>):</span><font></font>
    pages = []<font></font>
    <span class="hljs-keyword">for</span> page <span class="hljs-keyword">in</span> range(pages_count):<font></font>
        response = requests.get(<span class="hljs-string">'{}page/{}'</span>.format(url, page), headers={<span class="hljs-string">'User-Agent'</span>: UserAgent().chrome})<font></font>
        soup = BeautifulSoup(response.content, <span class="hljs-string">'html.parser'</span>)<font></font>
        pages.append(soup)<font></font>
    <span class="hljs-keyword">return</span> pages<font></font>
<font></font>
coping_pages = retrieve_pages(coping_pages_count, url_coping)<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Um die Daten herunterzuladen, musste ich die f√ºr die Analyse erforderlichen Felder ermitteln: Suchen Sie die Werte dieser Felder im DOM und speichern Sie sie im W√∂rterbuch. </font><font style="vertical-align: inherit;">Ich selbst kam aus dem Front-End-Bereich, daher war es f√ºr mich trivial, mit Haushalten und Gegenst√§nden zu arbeiten.</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">collect_post_info</span>(<span class="hljs-params">pages</span>):</span><font></font>
    posts = []<font></font>
    <span class="hljs-keyword">for</span> page <span class="hljs-keyword">in</span> pages:<font></font>
        posts_list_soup = page.find(<span class="hljs-string">'ol'</span>, attrs = {<span class="hljs-string">'class'</span>: <span class="hljs-string">'ipsDataList'</span>}).findAll(<span class="hljs-string">'li'</span>, attrs = {<span class="hljs-string">'class'</span>: <span class="hljs-string">'ipsDataItem'</span>})
        <span class="hljs-keyword">for</span> post_soup <span class="hljs-keyword">in</span> posts_list_soup:<font></font>
            post = {}<font></font>
            post[<span class="hljs-string">'id'</span>] = uuid.uuid4()
            <span class="hljs-comment"># collecting titles and urls</span>
            title_section = post_soup.find(<span class="hljs-string">'span'</span>, attrs = {<span class="hljs-string">'class'</span>:<span class="hljs-string">'ipsType_break ipsContained'</span>})
            <span class="hljs-keyword">if</span> (title_section):<font></font>
                title_section_a = title_section.find(<span class="hljs-string">'a'</span>)<font></font>
                post[<span class="hljs-string">'title'</span>] = title_section_a[<span class="hljs-string">'title'</span>]<font></font>
                post[<span class="hljs-string">'url'</span>] = title_section_a[<span class="hljs-string">'data-ipshover-target'</span>]
            <span class="hljs-comment"># collecting author &amp; last action</span>
            author_section = post_soup.find(<span class="hljs-string">'div'</span>, attrs = {<span class="hljs-string">'class'</span>:<span class="hljs-string">'ipsDataItem_meta'</span>})
            <span class="hljs-keyword">if</span> (author_section):<font></font>
                author_section_a = post_soup.find(<span class="hljs-string">'a'</span>)<font></font>
                author_section_time = post_soup.find(<span class="hljs-string">'time'</span>)<font></font>
                post[<span class="hljs-string">'author'</span>] = author_section_a[<span class="hljs-string">'data-ipshover-target'</span>]<font></font>
                post[<span class="hljs-string">'last_action'</span>] = author_section_time[<span class="hljs-string">'datetime'</span>]
            <span class="hljs-comment"># collecting stats</span>
            stats_section = post_soup.find(<span class="hljs-string">'ul'</span>, attrs = {<span class="hljs-string">'class'</span>:<span class="hljs-string">'ipsDataItem_stats'</span>})
            <span class="hljs-keyword">if</span> (stats_section):<font></font>
                stats_section_replies = post_soup.find(<span class="hljs-string">'span'</span>, attrs = {<span class="hljs-string">'class'</span>:<span class="hljs-string">'ipsDataItem_stats_number'</span>})
                <span class="hljs-keyword">if</span> (stats_section_replies):<font></font>
                    post[<span class="hljs-string">'replies'</span>] = stats_section_replies.getText()<font></font>
                stats_section_views = post_soup.find(<span class="hljs-string">'li'</span>, attrs = {<span class="hljs-string">'class'</span>:<span class="hljs-string">'ipsType_light'</span>})
                <span class="hljs-keyword">if</span> (stats_section_views):<font></font>
                    post[<span class="hljs-string">'views'</span>] = stats_section_views.find(<span class="hljs-string">'span'</span>, attrs = {<span class="hljs-string">'class'</span>:<span class="hljs-string">'ipsDataItem_stats_number'</span>}).getText()<font></font>
            posts.append(post)<font></font>
    <span class="hljs-keyword">return</span> posts
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Insgesamt habe ich ungef√§hr 15.450 Beitr√§ge zu diesem Thema gesammelt.</font></font><br>
<br>
<pre><code class="python hljs">coping_posts_info = collect_post_info(coping_pages)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Jetzt konnten sie in den DataFrame √ºbertragen werden, so dass sie dort wundersch√∂n lagen, und gleichzeitig in einer CSV-Datei gespeichert werden, sodass Sie nicht erneut warten mussten, wenn die Daten von der Site gesammelt wurden, wenn das Notebook versehentlich kaputt ging oder ich versehentlich eine Variable neu definiert habe, bei der. </font></font><br>
<br>
<pre><code class="python hljs">df_coping = pd.DataFrame(coping_posts_info, <font></font>
               columns =[<span class="hljs-string">'title'</span>, <span class="hljs-string">'url'</span>, <span class="hljs-string">'author'</span>, <span class="hljs-string">'last_action'</span>, <span class="hljs-string">'replies'</span>, <span class="hljs-string">'views'</span>]) <font></font>
<font></font>
<span class="hljs-comment"># format data</span>
df_coping[<span class="hljs-string">'replies'</span>] = df_coping[<span class="hljs-string">'replies'</span>].astype(int)<font></font>
df_coping[<span class="hljs-string">'views'</span>] = df_coping[<span class="hljs-string">'views'</span>].apply(<span class="hljs-keyword">lambda</span> x: int(x.replace(<span class="hljs-string">','</span>,<span class="hljs-string">''</span>)))<font></font>
df_coping.to_csv(<span class="hljs-string">'celiac_forum_coping.csv'</span>, sep=<span class="hljs-string">','</span>)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nachdem ich eine Sammlung von Beitr√§gen gesammelt hatte, sammelte ich die Kommentare selbst.</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">collect_postpage_details</span>(<span class="hljs-params">pages, df</span>):</span><font></font>
    comments = []<font></font>
    <span class="hljs-keyword">for</span> i, page <span class="hljs-keyword">in</span> enumerate(pages):<font></font>
        articles = page.findAll(<span class="hljs-string">'article'</span>)
        <span class="hljs-keyword">for</span> k, article <span class="hljs-keyword">in</span> enumerate(articles):<font></font>
            comment = {<font></font>
                <span class="hljs-string">'url'</span>: df[<span class="hljs-string">'url'</span>][i]<font></font>
            }<font></font>
            <span class="hljs-keyword">if</span>(k == <span class="hljs-number">0</span>):<font></font>
                comment[<span class="hljs-string">'question'</span>] = <span class="hljs-number">1</span>
            <span class="hljs-keyword">else</span>:<font></font>
                comment[<span class="hljs-string">'question'</span>] = <span class="hljs-number">0</span>
            <span class="hljs-comment"># collecting comments</span>
            comment_section = article.find(<span class="hljs-string">'div'</span>, attrs = {<span class="hljs-string">'class'</span>:<span class="hljs-string">'ipsComment_content'</span>})
            <span class="hljs-keyword">if</span> (comment_section):<font></font>
                comment_section_p = comment_section.find(<span class="hljs-string">'p'</span>)
                <span class="hljs-keyword">if</span>(comment_section_p):<font></font>
                    comment[<span class="hljs-string">'comment'</span>] = comment_section_p.getText()<font></font>
            comment[<span class="hljs-string">'date'</span>] = comment_section.find(<span class="hljs-string">'time'</span>)[<span class="hljs-string">'datetime'</span>]<font></font>
            author_section = article.find(<span class="hljs-string">'strong'</span>)
            <span class="hljs-keyword">if</span> (author_section):<font></font>
                author_section_url = author_section.find(<span class="hljs-string">'a'</span>)
                <span class="hljs-keyword">if</span> (author_section_url):<font></font>
                    comment[<span class="hljs-string">'author'</span>] = author_section_url[<span class="hljs-string">'data-ipshover-target'</span>]<font></font>
            comments.append(comment)<font></font>
    <span class="hljs-keyword">return</span> comments<font></font>
<font></font>
coping_data = collect_postpage_details(coping_comments_pages, df_coping)<font></font>
df_coping_comments.to_csv(<span class="hljs-string">'celiac_forum_coping_comments_1.csv'</span>, sep=<span class="hljs-string">','</span>)<font></font>
<font></font>
</code></pre><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SCHRITT 2 Datenanalyse und thematische Modellierung</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Im vorherigen Schritt haben wir Daten aus dem Forum gesammelt und die endg√ºltigen Daten in Form von 153777 Zeilen mit Fragen und Kommentaren erhalten. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Aber nur die gesammelten Daten sind nicht interessant. Das erste, was ich tun wollte, war eine sehr einfache Analyse: Ich habe Statistiken f√ºr die 30 meistgesehenen und 30 am meisten kommentierten Themen abgeleitet. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/9o/ai/dj/9oaidjzovi7va3alxg7vdwkts84.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die meistgesehenen Beitr√§ge stimmten nicht mit den am meisten kommentierten √ºberein. Die Titel kommentierter Beitr√§ge sind bereits auf den ersten Blick erkennbar. Ihre Namen sind emotionaler: </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Ich hasse, ich hasse, ich hasse"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> oder " </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Arrogante Kommentare"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> oder </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Wow, ich bin in Schwierigkeiten</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><i><font style="vertical-align: inherit;">"</font></i><font style="vertical-align: inherit;"> Und die meistgesehenen haben ein Fragenformat: </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Kann ich Soja essen?", "Warum kann ich Wasser nicht richtig aufnehmen?"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">und andere. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wir haben eine einfache Textanalyse durchgef√ºhrt. </font><font style="vertical-align: inherit;">Um direkt zu einer komplexeren Analyse zu gelangen, m√ºssen Sie die Daten selbst vorbereiten, bevor Sie sie zur Eingabe nach Themen an die Eingabe des LDA-Modells senden. </font><font style="vertical-align: inherit;">Entfernen Sie dazu Kommentare mit weniger als 30 W√∂rtern, um Spam und bedeutungslose kurze Kommentare herauszufiltern. </font><font style="vertical-align: inherit;">Wir bringen sie in Kleinbuchstaben.</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-comment"># Let's get rid of text &lt; 30 words</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">filter_text_words</span>(<span class="hljs-params">text, min_words = <span class="hljs-number">30</span></span>):</span><font></font>
    text = str(text)<font></font>
    <span class="hljs-keyword">return</span> len(text.split()) &gt; <span class="hljs-number">30</span>
filtered_comments = filtered_comments[filtered_comments[<span class="hljs-string">'comment'</span>].apply(filter_text_words)]<font></font>
comments_only = filtered_comments[<span class="hljs-string">'comment'</span>]<font></font>
comments_only= comments_only.apply(<span class="hljs-keyword">lambda</span> x: x.lower())<font></font>
comments_only.head()</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
L√∂schen Sie unn√∂tige Stoppw√∂rter, um unsere Textauswahl zu l√∂schen</font></font><br>
<br>
<pre><code class="python hljs">stop_words = stopwords.words(<span class="hljs-string">'english'</span>)
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">remove_stop_words</span>(<span class="hljs-params">tokens</span>):</span><font></font>
    new_tokens = []<font></font>
    <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> tokens:<font></font>
        token = []<font></font>
        <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> t:
            <span class="hljs-keyword">if</span> word <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> stop_words:<font></font>
                token.append(word)<font></font>
        new_tokens.append(token)<font></font>
    <span class="hljs-keyword">return</span> new_tokens<font></font>
<font></font>
tokens = remove_stop_words(data_words)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wir f√ºgen auch Bigramme hinzu und bilden eine T√ºte mit W√∂rtern, um stabile Phrasen hervorzuheben, z. B. </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">glutenfrei, support_group</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> und andere Phrasen, die, wenn sie gruppiert sind, eine bestimmte Bedeutung haben.</font></font><br>
<br>
<pre><code class="python hljs">
bigram = gensim.models.Phrases(tokens, min_count=<span class="hljs-number">5</span>, threshold=<span class="hljs-number">100</span>)<font></font>
bigram_mod = gensim.models.phrases.Phraser(bigram)<font></font>
bigram_mod.save(<span class="hljs-string">'bigram_mod.pkl'</span>)<font></font>
bag_of_words = [bigram_mod[w] <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> tokens]
<span class="hljs-keyword">with</span> open(<span class="hljs-string">'bigrams.pkl'</span>, <span class="hljs-string">'wb'</span>) <span class="hljs-keyword">as</span> f:<font></font>
    pickle.dump(bag_of_words, f)<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Jetzt sind wir endlich bereit, das LDA-Modell selbst direkt zu trainieren.</font></font><br>
<br>
<pre><code class="python hljs"><font></font>
id2word = corpora.Dictionary(bag_of_words)<font></font>
id2word.save(<span class="hljs-string">'id2word.pkl'</span>)<font></font>
id2word.filter_extremes(no_below=<span class="hljs-number">3</span>, no_above=<span class="hljs-number">0.4</span>, keep_n=<span class="hljs-number">3</span>*<span class="hljs-number">10</span>**<span class="hljs-number">6</span>)<font></font>
corpus = [id2word.doc2bow(text) <span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> bag_of_words]<font></font>
<font></font>
lda_model = gensim.models.ldamodel.LdaModel(<font></font>
    corpus, <font></font>
    id2word=id2word, <font></font>
    eval_every=<span class="hljs-number">20</span>,<font></font>
    random_state=<span class="hljs-number">42</span>,<font></font>
    num_topics=<span class="hljs-number">30</span>, <font></font>
    passes=<span class="hljs-number">5</span><font></font>
    )<font></font>
lda_model.save(<span class="hljs-string">'lda_default_2.pkl'</span>)<font></font>
topics = lda_model.show_topics(num_topics=<span class="hljs-number">30</span>, num_words=<span class="hljs-number">100</span>, formatted=<span class="hljs-literal">False</span>)
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Am Ende des Trainings erhalten wir letztendlich das Ergebnis der gebildeten Themen. Was ich am Ende dieses Beitrags angeh√§ngt habe.</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> range(lda_model.num_topics):<font></font>
    plt.figure(figsize=(<span class="hljs-number">15</span>, <span class="hljs-number">10</span>))<font></font>
    plt.imshow(WordCloud(background_color=<span class="hljs-string">"white"</span>, max_words=<span class="hljs-number">100</span>, width=<span class="hljs-number">900</span>, height=<span class="hljs-number">900</span>, collocations=<span class="hljs-literal">False</span>)<font></font>
               .fit_words(dict(topics[t][<span class="hljs-number">1</span>])))<font></font>
    plt.axis(<span class="hljs-string">"off"</span>)<font></font>
    plt.title(<span class="hljs-string">"Topic #"</span> + themes_headers[t])<font></font>
    plt.show()<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wie sich vielleicht bemerkbar macht, haben sich die Themen inhaltlich als ziemlich unterschiedlich herausgestellt. </font><font style="vertical-align: inherit;">Ihnen zufolge wird klar, wovon Menschen mit Z√∂liakie-Intoleranz sprechen. </font><font style="vertical-align: inherit;">Grunds√§tzlich geht es um Essen, Restaurants, kontaminiertes Essen mit Gluten, schreckliche Schmerzen, Behandlung, Arztbesuche, Familienangeh√∂rige, Missverst√§ndnisse und andere Dinge, denen sich Menschen im Zusammenhang mit ihrem Problem jeden Tag stellen m√ºssen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das ist alles. </font><font style="vertical-align: inherit;">Vielen Dank f√ºr Ihre Aufmerksamkeit. </font><font style="vertical-align: inherit;">Ich hoffe, Sie finden dieses Material interessant und n√ºtzlich. </font><font style="vertical-align: inherit;">Und doch, da ich kein DS-Entwickler bin, urteile nicht streng. </font><font style="vertical-align: inherit;">Wenn es etwas hinzuzuf√ºgen oder zu verbessern gibt, freue ich mich immer √ºber konstruktive Kritik, schreibe. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
30 Themen anzeigen</font></font><br>
<br>
<div class="spoiler" role="button" tabindex="0">
                        <b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Achtung, viele Bilder</font></font></b>
                        <div class="spoiler_text"><img src="https://habrastorage.org/webt/cn/dy/tb/cndytbwrtz9ujkkh6xicy0mjyds.png"><br>
<br>
<img src="https://habrastorage.org/webt/no/n2/iq/non2iqgux8nvb5hnitr8n7yra_w.png"><br>
<br>
<img src="https://habrastorage.org/webt/dx/dh/nl/dxdhnlddgexrb_noeq8psjb7nps.png"><br>
<br>
<img src="https://habrastorage.org/webt/x1/_f/q6/x1_fq6omll0iigewzz0ba8tjvys.png"><br>
<br>
<img src="https://habrastorage.org/webt/7v/qy/fh/7vqyfh-uwk_bhypzdgisxxabzjs.png"><br>
<br>
<img src="https://habrastorage.org/webt/v7/1z/fn/v71zfn2kb0xj7rpsthrplgznzzw.png"><br>
<br>
<img src="https://habrastorage.org/webt/ab/tt/t7/abttt7c8aqydfc28gxyq9ai7a4q.png"><br>
<br>
<img src="https://habrastorage.org/webt/oz/hc/m7/ozhcm72ldjjenp5onkjydxgpvly.png"><br>
<br>
<img src="https://habrastorage.org/webt/fe/ex/lw/feexlw8tcrcwni5wmy8k8rv8k3e.png"><br>
<br>
<img src="https://habrastorage.org/webt/w0/hu/5j/w0hu5jix2zrewo2l9jnbkddd3tk.png"><br>
<br>
<img src="https://habrastorage.org/webt/zf/ye/kw/zfyekw6s6qfxuqwy-qxhv_dehrq.png"><br>
<br>
<img src="https://habrastorage.org/webt/l0/9s/vw/l09svwry19fhz1y-1-pooeo_vew.png"><br>
<br>
<img src="https://habrastorage.org/webt/pm/mt/bk/pmmtbkkybu50vhgttl-0kz4tcf4.png"><br>
<br>
<img src="https://habrastorage.org/webt/1h/hu/vr/1hhuvrmmfjxwfzh3fbhf9dbut38.png"><br>
<br>
<img src="https://habrastorage.org/webt/bw/is/ad/bwisadbn9a000lt6xp927szic2u.png"><br>
<br>
<img src="https://habrastorage.org/webt/iu/bf/4q/iubf4qt_juq9uip17rmngbr7wxe.png"><br>
<br>
<img src="https://habrastorage.org/webt/jk/of/sa/jkofsalh2hev8zx6jjlom0pnnxy.png"><br>
<br>
<img src="https://habrastorage.org/webt/js/bs/ls/jsbslsv_4ly4rwe7wir6xvcs6t4.png"><br>
<br>
<img src="https://habrastorage.org/webt/_e/ly/wr/_elywrkbtgk-4fvlnuzfr6zqq4o.png"><br>
<br>
<img src="https://habrastorage.org/webt/4j/x8/pa/4jx8paomlrca7t0syfunmtmlxk4.png"><br>
<br>
<img src="https://habrastorage.org/webt/y2/he/s1/y2hes1fvuepisygriea98m_yavw.png"><br>
<br>
<img src="https://habrastorage.org/webt/9k/xs/sr/9kxssr9rxlyeobjw12fwju0-xkq.png"><br>
<br>
<img src="https://habrastorage.org/webt/i-/sl/qd/i-slqdug6x9dkwybnfnxmdolho8.png"><br>
<br>
<img src="https://habrastorage.org/webt/nq/pk/x5/nqpkx5q6j8e_6mkpfak0ytkkvfc.png"><br>
<br>
<img src="https://habrastorage.org/webt/jv/3b/pa/jv3bpafludpki_2a-4pgajhreh0.png"><br>
<br>
<img src="https://habrastorage.org/webt/sw/-e/pn/sw-epnxhrwa4t7i6uksmggczs-8.png"><br>
<br>
<img src="https://habrastorage.org/webt/-y/qj/0t/-yqj0t-jkax-s09bivkgx8a3mqa.png"><br>
<br>
<img src="https://habrastorage.org/webt/ta/rp/4w/tarp4wr8bcui0zszwuzl7l9h8zo.png"><br>
<br>
<img src="https://habrastorage.org/webt/eo/xl/m2/eoxlm2i2z9weffxhgm-zzgszd3q.png"><br>
<br>
<hr><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">    ¬´Machine Learning¬ª  OTUS</a>.<br>
<br>
<hr><br>
</div>
                    </div></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de503382/index.html">Gehen Sie auf einem sauberen Feld spazieren oder sammeln Sie MAC-Adressen von Wi-Fi-Ger√§ten in der N√§he</a></li>
<li><a href="../de503386/index.html">Agile Coach gesunde Person</a></li>
<li><a href="../de503388/index.html">Digitalisierung der Panik: DIT von Moskau gegen Moskauer - ein runder Tisch am 23. Mai</a></li>
<li><a href="../de503390/index.html">Warum setzt Intel auf die Chipentwicklung f√ºr das Genie von Jim Keller?</a></li>
<li><a href="../de503394/index.html">Erfahrung in Aktieninvestitionen</a></li>
<li><a href="../de503402/index.html">Wie man sich an jeden pers√∂nlich erinnert oder wie man effektiv nach Gesichtern in einer gro√üen Datenbank sucht</a></li>
<li><a href="../de503404/index.html">Semantische digitale Systeme</a></li>
<li><a href="../de503406/index.html">Semantik und Aktivit√§t</a></li>
<li><a href="../de503408/index.html">Blazor Client Side Online Store: Teil 7 - Auf Version 3.2.0 aktualisiert und Bildanzeige hinzugef√ºgt</a></li>
<li><a href="../de503410/index.html">Polygone Eine andere Welt: Sega Genesis</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>