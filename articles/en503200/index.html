<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üßöüèø üôèüèΩ üßÄ YOLOv4 - the most accurate real-time neural network on the Microsoft COCO dataset üíà ü§ûüèº ü•Å</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Darknet YOLOv4 is faster / more accurate than Google TensorFlow EfficientDet and FaceBook Pytorch / Detectron RetinaNet / MaskRCNN. 
 
 The same artic...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>YOLOv4 - the most accurate real-time neural network on the Microsoft COCO dataset</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/503200/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Darknet YOLOv4 is faster / more accurate than Google TensorFlow EfficientDet and FaceBook Pytorch / Detectron RetinaNet / MaskRCNN. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The same article on medium</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">medium </font></font></a><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Code</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/AlexeyAB/darknet </font></font></a><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Article</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">arxiv.org/abs/2004.10934</font></font></a><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/3h/nc/sr/3hncsroz9wt8u3ycqskubgu1xk8.gif"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We will show some nuances of comparing and using neural networks to detect objects. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Our goal was to develop an object detection algorithm for use in real products, and not just move science forward. </font><font style="vertical-align: inherit;">The accuracy of the YOLOv4 neural network (608x608) is 43.5% AP / 65.7% AP50 Microsoft-COCO-testdev. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">62 FPS</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - YOLOv4 (608x608 batch = 1) on Tesla V100 - by using Darknet-framework </font></font><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">400 FPS</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - YOLOv4 (416x416 batch = 4) on RTX 2080 Ti - by using TensorRT + tkDNN </font></font><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">32 FPS</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - YOLOv4 (416x416 batch = 1) on Jetson AGX Xavier - by using TensorRT + tkDNN</font></font><br>
<br>
<img src="https://habrastorage.org/webt/p_/ep/cl/p_epcl_aaw_trgeltekatagtqkg.jpeg"> <br>
<a name="habracut"></a><br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/1_SiUOYUoOI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">First, some useful links.</font></font></h3><br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">You can read a detailed description of the features used in YOLOv4 in this article: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">medium.com/@jonathan_hui/yolov4-c9901eaa8e61</font></font></a></li>
<li>  YOLOv4: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://lutzroeder.github.io/netron/%3Furl%3D" rel="nofollow">lutzroeder.github.io/netron/?url=https%3A%2F%2Fraw.githubusercontent.com%2FAlexeyAB%2Fdarknet%2Fmaster%2Fcfg%2Fyolov4.cfg</a></li>
<li>     YOLOv4  GPU   Google-cloud  Jupyter Notebook ‚Äì      ,   - ¬´Open in Playground¬ª,         [ ] ‚Äì    ,  ,  ,    5    : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow">colab.research.google.com/drive/12QusaaRj_lUwCGDvQNfICpa7kA7_a2dE</a>      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow">www.youtube.com/watch?v=mKAEGSxwOAY</a></li>
<li>  Darknet   : <br>
 ‚Äî <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow">  Darknet YOLOv4</a><br>
 ‚Äî <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow">  Darknet YOLOv4</a><br>
 </li>
</ul><br>
<h3>   </h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Our YOLOv4 neural network and our own Darknet DL framework (C / C ++ / CUDA) are better in FPS speed and AP50: 95 and AP50 accuracy on Microsoft COCO datasets than DL frameworks and neural networks: Google TensorFlow EfficientDet, FaceBook Detectron RetinaNet / MaskRCNN, PyTorch Yolov3-ASFF, and many others ... YOLOv4 achieves accuracy of 43.5% AP / 65.7% AP50 on the Microsoft COCO test at a speed of 62 FPS TitanV or 34 FPS RTX 2070. Unlike other modern detectors, YOLOv4 can train anyone with whoever has the nVidia gaming graphics card with 8-16 GB VRAM. Now, not only large companies can train a neural network on hundreds of GPU / TPUs to use large mini-batch sizes to achieve higher accuracy, so we are returning control of artificial intelligence to ordinary users, because for YOLOv4 a large mini-lot is not required,can be limited to a size of 2 - 8.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
YOLOV4 is optimal for using real-time, because the network lies </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">on the Pareto optimality curve</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> in the AP (accuracy) / FPS (speed) graph. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/2k/77/as/2k77aszzprngk0qmtistcehkz8c.png"><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Graphs of accuracy (AP) and speed (FPS) of many neural networks for detecting objects measured on GPUs TitanV / TeslaV100, TitanXP / TeslaP100, TitanX / TeslaM40 for the two main indicators of accuracy AP50: 95 and AP50</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
For a fair comparison, we take data from articles and compare only on the GPU with the same architecture. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Most practical tasks have the minimum necessary requirements for detectors - these are the minimum acceptable accuracy and speed. Usually the minimum allowable speed of 30 FPS (frames per second) and higher for real-time systems. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As can be seen from the graphs, in Real-time systems with FPS 30 or more:</font></font><br>
<br>
<ul>
<li> YOLOv4-608   RTX 2070  <b>450$</b> (34 FPS)   <b>43.5% AP / 65.7% AP50</b></li>
<li> EfficientDet-D2   TitanV  <b>2250$</b> (42 FPS)   <b>43.0% AP / 62.3% AP50</b></li>
<li> EfficientDet-D0   RTX 2070  <b>450$</b> (34 FPS)   <b>33.8% AP / 52.2% AP50</b></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Those. YOLOv4 requires </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">5 times cheaper equipment</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> and more accurately than EfficientDet-D2 (Google-TensorFlow). You can use EfficientDet-D0 (Google-TensorFlow) then the cost of equipment will be the same, but the accuracy will be 10% AP lower. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In addition, some industrial systems have limitations on heat dissipation or on the use of a passive cooling system - in this case you can not use TitanV even with money. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
When using YOLOv4 (416x416) on an RTX 2080 Ti GPU using TensorRT + tkDNN, we achieve a speed of 2x times faster, and when using batch = 4 it is 3x-4x times faster - for an honest comparison, we do not present these results in an article on arxiv. org:</font></font><br>
<img src="https://habrastorage.org/webt/ci/j7/uq/cij7uqas0ypsjcpsfkhvdxuyxzs.png"><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
YOLOv4 neural network (416x416) FP16 (Tensor Cores) batch = </font><font style="vertical-align: inherit;">1 reaches at 32 FPS calculator nVidia Jetson AGX Xavier using libraries + tkDNN TensorRT: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/ceccocats/tkDNN</font></font></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
slightly slower speed gives OpenCV-dnn library compiled with CUDA: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">docs .opencv.org / master / da / d9d / tutorial_dnn_yolo.html</font></font></a><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Sometimes the speed (FPS) of some neural networks in articles is indicated when using a high batch size or when testing with specialized software (TensorRT), which optimizes the network and shows an increased FPS value. Comparison of some networks on TRT with other networks without TRT is not fair. Using a high batch size increases FPS, but also increases latency (rather than decreasing it) compared to batch = 1. If the network with batch = 1 shows 40 FPS, and with batch = 32 it shows 60 FPS, then the delay will be 25ms for batch = 1, and ~ 500ms for batch = 32, because only ~ 2 packets (32 images each) will be processed per second, which is why using batch = 32 is not acceptable in many industrial systems. Therefore, we compared the results on the graphs only with batch = 1 and without using TensorRT.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Any process can be controlled either by people or by computers. When a computer system acts with a big delay due to low speed and makes too many mistakes, then it cannot be entrusted with complete control of actions, in this case the person controls the process, and the computer system only gives hints - this is a recommendation system - the person works, and the system only tells where mistakes were made. When the system works quickly and with high accuracy, such a system can control the process independently, and a person only looks after it. Therefore, accuracy and system speed are always important. If it seems to you that 120 FPS for YOLOv4 416x416 is too much for your task, and it is better to take the algorithm more slowly and more accurately, then most likely in real tasks you will use something weaker than the Tesla V100 250 Watt,for example, RTX 2060 / Jetson-Xavier 30-80 Watt, in this case you will get 30 FPS on YOLOv4 416x416, and other neural networks at 1-15 FPS or will not start at all.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Features of training various neural networks</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
You have to train EfficientDet with mini-batch = 128 size on several Tesla V100 32GB GPUs, while YOLOv4 was trained on just one Tesla V100 32GB GPU with mini-batch = 8 = batch / subdivisions, and can be trained on a regular gaming graphics card 8-16GB GPU-VRAM. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The next nuance is the difficulty of training a neural network to detect its own objects. </font><font style="vertical-align: inherit;">No matter how much time you train other networks on the same 1080 Ti GPU, you will not get the stated accuracy shown in the graph above. </font><font style="vertical-align: inherit;">Most networks (EfficientDet, ASFF, ...) need to be trained on 4 - 128 GPUs (with a large mini-batch size using syncBN) and it is necessary to train each time anew for each network resolution, without meeting both conditions it is impossible to achieve the AP or AP50 accuracy declared by them.</font></font><br>
 <br>
<img src="https://habrastorage.org/webt/p4/sx/p3/p4sxp3ewxd9owskis23n6dyrv58.jpeg"><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">You can see the dependence of the detection accuracy of objects on the size of the minibatch in other detectors, i.e. </font><font style="vertical-align: inherit;">using 128 video cards instead of 8 video cards and the learning speed is 10 times higher and the final accuracy is 1.5 AP higher - MegDet: A Large Mini-Batch Object Detector </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">arxiv.org/abs/1711.07240</font></font></a></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Yolo ASFF: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">arxiv.org/abs/1911.09516</font></font></a> <br>
<blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Following [43], we introduce a bag of tricks in the training process, such as the mixup algorithm [12], the cosine [26] learning rate schedule, and the synchronized batch normalization technique [30].</font></font></blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
EfficientDet: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">arxiv.org/abs/1911.09070</font></font></a> <br>
<blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Synchronized batch normalization is added after every convolution with batch norm decay 0.99 and epsilon 1e-3. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Each model is trained 300 epochs with batch total size 128 on 32 TPUv3 cores.</font></font></blockquote><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow">cloud.google.com/tpu/docs/types-zones#europe</a><br>
<blockquote>v3-32 TPU type (v3) ‚Äì 32 TPU v3 cores ‚Äì 512 GiB Total TPU memory</blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
You must use 512 GB TPU / GPU-RAM to train the EfficientDet model with Synchronized batch normalization at batch = 128, while mini-batch = 8 and only 32 GB GPU-RAM were used to train YOLOv4. Despite this, YOLOv4 is faster / more accurate than public networks, although it is trained only 1 time with a resolution of 512x512 per 1 GPU (Tesla V100 32GB / 16GB). At the same time, using the smaller mini-batch size and GPU-VRAM does not lead to such a dramatic loss of accuracy as in other neural networks: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/xi/ol/rs/xiolrsvx4vzpjvahb6kvambdvgq.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Source: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">arxiv.org/abs/2004.10934</font></font></a></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
So you can train artificial intelligence locally on your PC, instead of downloading Dataset to the cloud - this guarantees the protection of your personal data and makes artificial intelligence training available to everyone.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
It is enough to train our network once with a network resolution 512x512, and then it can be used with different network resolutions in the range: [416x416 - 512x512 - 608x608]. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Most other models need to be trained each time separately for each network resolution, because of this, training takes many times longer.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Features of measuring accuracy of object detection algorithms</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
You can always find an image on which one algorithm will work poorly, and another algorithm will work well, and vice versa. Therefore, to test detection algorithms, a large set of ~ 20,000 images and 80 different types of objects is used - MSCOCO test-dev dataset. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
So that the algorithm does not try to just remember the hash of each image and the coordinates on it (overfitting), the accuracy of object detection is always checked on images and labels that the algorithm did not see during training - this ensures that the algorithm can detect objects on images / videos that it never saw.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
So that no one could make a mistake in calculating accuracy, in the public domain there are only test-dev test images on which you detect, and send the results to the CodaLab evaluation server, on which the program itself compares your results with test annotations that are not accessible to anyone . </font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">MSCOCO dataset consists of 3 parts</font></font></a><br>
<br>
<ol>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tutorial: 120,000 images and a json file with the coordinates of each object</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Validation set: 5,000 images and a json file with the coordinates of each object</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Test suite: 41,000 jpg-images without the coordinates of objects (some of these images are used to determine accuracy in tasks: Object Detection, Instance Segmentation, Keypoints, ...)</font></font></li>
</ol><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Features of the development of YOLOv4</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
When developing YOLOv4, I had to develop both the YOLOv4 neural network and the Darknet framework on C / C ++ / CUDA myself. </font><font style="vertical-align: inherit;">Because </font><font style="vertical-align: inherit;">in Darknet there is no automatic differentiation and automatic execution of the chain-rule, then all the gradients have to be implemented manually. </font><font style="vertical-align: inherit;">On the other hand, we can depart from strict adherence to the chain-rule, change backpropagation and try very non-trivial things to increase learning stability and accuracy.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Additional findings when creating neural networks</font></font></h3><br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Not always the best network for classifying objects will be the best as a backbone for the network used to detect objects</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Using weights trained with features that have increased accuracy in classification can adversely affect detector accuracy (on some networks)</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Not all features stated in various studies improve network accuracy.</font></font></li>
<li>                .</li>
<li>      BFLOPS  ,   BFLOPS    </li>
<li>                  ,     receptive field     ,       stride=2 / conv3x3,    weights (filters)      . </li>
</ul><br>
<h3>   YOLOv4</h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Object detection using trained YOLOv4 models is built into the OpenCV-dnn library </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/opencv/opencv/issues/17148</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> so that you can use YOLOv4 directly from OpenCV without using the Darknet framework. </font><font style="vertical-align: inherit;">The OpenCV library supports the implementation of neural networks on the CPU, GPU (nVidia GPU), VPU (Intel Myriad X). </font><font style="vertical-align: inherit;">More details: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">docs.opencv.org/master/da/d9d/tutorial_dnn_yolo.html </font></font></a><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">OpenCV</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (dnn) framework:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">C ++ example: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/opencv/opencv/blob/master/samples/dnn/object_detection.cpp</font></font></a></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Python example: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/opencv/opencv/blob/master/samples/dnn/object_detection.py</font></font></a></li>
</ul><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Darknet</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> framework:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Instructions for using YOLOv4 to detect objects: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/AlexeyAB/darknet#how-to-use-on-the-command-line</font></font></a></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Instructions for training a neural network to detect its own objects: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/AlexeyAB/darknet#how-to-train-to-detect-your-custom-objects</font></font></a></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Instructions for training the CSPDarknet53 classifier on the ILSVRC2012 dataset (ImageNet): </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/AlexeyAB/darknet/wiki/Train-Classifier-on-ImageNet-</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (ILSVRC2012)</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Instructions for training YOLOv4 on the MS COCO dataset: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/AlexeyAB/darknet/wiki/Train-Detector-on-MS-COCO-</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (trainvalno5k-2014) -dataset</font></font></li>
</ul><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">tkDNN + TensorRT</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - Maximum speed of object detection using YOLOv4: TensorRT + tkDNN </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/ceccocats/tkDNN</font></font></a><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">400 FPS - YOLOv4 (416x416 batch = 4) on RTX 2080 Ti</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">32 FPS - YOLOv4 (416x416 batch = 1) on Jetson AGX Xavier</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Use of YOLOv4 can be expanded to detect 3D-Rotated-Bboxes or key points / facial landmarks, for example: </font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/ouyanghuiyu/darknet_face_with_landmark</font></font></a><br>
<br>
<img src="https://habrastorage.org/webt/z7/vs/dv/z7vsdvhcpfbrgmdv1byhbpzd1cu.jpeg"></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../en503182/index.html">"I am the first blind developer in my company." Part 1</a></li>
<li><a href="../en503184/index.html">We invite you to the online meeting Zabbix</a></li>
<li><a href="../en503192/index.html">oVirt in 2 hours. Part 4. Basic operations</a></li>
<li><a href="../en503194/index.html">ISA does not forgive mistakes</a></li>
<li><a href="../en503196/index.html">450 free courses from the Ivy League</a></li>
<li><a href="../en503204/index.html">How to flash Xiaomi Redmi 4 Prime / Pro / Premium on Android 10</a></li>
<li><a href="../en503208/index.html">When is the best time to invest?</a></li>
<li><a href="../en503210/index.html">Can phishing sites be eradicated?</a></li>
<li><a href="../en503212/index.html">30 life hacks to complete the online course</a></li>
<li><a href="../en503214/index.html">Load optimization on a Highload project with ElasticSearch</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>