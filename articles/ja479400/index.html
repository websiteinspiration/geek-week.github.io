<!doctype html>
<html class="no-js" lang="ja">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🦂 🔨 🚊 ニューラルネットワークからジャーナリストを作成する方法、または「単語なしでハブレのテキストを削減する秘密」 ♟️ 👋🏿 👩🏿‍🌾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="驚かないでください。この投稿への2番目の見出しは、ニューラルネットワーク、つまり要約アルゴリズムを生成しました。そして、サマライゼーションとは何ですか？
 
 これは、自然言語処理（NLP）の重要かつ古典的な課題の 1つです。これは、テキストを入力として受け取り、その要約バージョンを出力するアルゴリ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>ニューラルネットワークからジャーナリストを作成する方法、または「単語なしでハブレのテキストを削減する秘密」</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/abbyy/blog/479400/"><img src="https://habrastorage.org/webt/iq/f_/fg/iqf_fgkfyqb1rbtwvba0p4gbsiq.png" align="left"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">驚かないでください。この投稿への2番目の見出しは、ニューラルネットワーク、つまり要約アルゴリズムを生成しました。そして、サマライゼーションとは何ですか？</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
これは</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、自然言語処理（NLP）</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">の重要かつ古典的な</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">課題の</font></a><font style="vertical-align: inherit;"> 1つです</font><font style="vertical-align: inherit;">。これは、テキストを入力として受け取り、その要約バージョンを出力するアルゴリズムを作成することから成ります。さらに、正しい構造（言語の規範に対応）がその中に保持され、テキストの主な考え方が正しく送信されます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
このようなアルゴリズムは業界で広く使用されています。たとえば、これらは検索エンジンに役立ちます。テキスト削減を使用すると、サイトまたはドキュメントの主要なアイデアが検索クエリと相関しているかどうかを簡単に理解できます。これらは、メディアデータの大規模なストリームで関連情報を検索し、情報のゴミを除外するために使用されます。テキスト削減は、金融研究、法的契約の分析、科学論文への注釈付けなどに役立ちます。ちなみに、サマライゼーションアルゴリズムはこの投稿のすべての小見出しを生成しました。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
驚いたことに、Habréにはサマライゼーションに関する記事がほとんどなかったので、この方向での研究と結果を共有することにしました。今年は、カンファレンス「</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">Dialogue</font></a><font style="vertical-align: inherit;">」のコンペトラックに参加しました</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ニューラルネットワークを使用して、ニュースアイテムや詩の見出しジェネレーターを試しました。</font><font style="vertical-align: inherit;">この投稿では、最初に要約の理論的な部分について簡単に説明し、次に見出しの生成の例を示します。テキストを短縮するときにモデルがどのような困難を抱えているか、およびこれらのモデルを改善して見出しを改善する方法について説明します。</font></font><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
以下は、ニュース項目とその元の参照見出しの例です。</font><font style="vertical-align: inherit;">これから説明するモデルは、この例でヘッダーを生成するようにトレーニングします。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/8n/yy/gc/8nyygcx_ymy38dwrqpbqdiylvsu.png" alt="画像"><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">テキストseq2seqアーキテクチャをカットする秘訣 </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
テキスト削減方法には次の2つのタイプがあります。 </font></font><br>
<br>
<ol>
<li><b></b>.                 .       ,     .</li>
<li><b></b>.       ,     .          ,    (    ) —      .</li>
</ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
2番目のアプローチは、アルゴリズムが言語の依存関係を考慮に入れ、言い換えて一般化することを意味します。彼はまた、実際のエラーを防ぐために、現実の世界についてある程度の知識を持ちたいと考えています。長い間、これは困難な作業であると考えられ、研究者は高品質のソリューション、つまり主要なアイデアを維持しながら文法的に正しいテキストを得ることができませんでした。そのため、以前は、テキスト全体を選択して結果に転送することでソースと同じレベルのリテラシーを維持できるため、ほとんどのアルゴリズムは抽出アプローチに基づいていました。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
しかし、これはニューラルネットワークのブームとそのNLPへの差し迫った浸透の前でした。 2014年に、</font><b><font style="vertical-align: inherit;">注意メカニズムを備えたseq2seq</font></b><font style="vertical-align: inherit;">アーキテクチャが導入されました</font></font><b><font style="vertical-align: inherit;"></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、いくつかのテキストシーケンスを読み取り、他のテキストシーケンスを生成することができます（これは、モデルが出力を学習した内容に依存します）（</font><font style="vertical-align: inherit;">Sutskever et al。による</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">記事</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">）。</font><font style="vertical-align: inherit;">2016年に、このようなアーキテクチャはsammarizationの問題、それによって抽象的なアプローチを実現し、（有能な人が書くことができるものに匹敵する結果を得るの溶液に直接適用された</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">物品</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Nallapatiらから、2016; </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">物品</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。ラッシュら2015年から、 ）</font><font style="vertical-align: inherit;">このアーキテクチャはどのように機能しますか？</font></font><br>
<br>
<img src="https://habrastorage.org/webt/px/4x/wi/px4xwin577dd65z-lahsx7fvexs.png" alt="画像"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Seq2Seqは2つの部分で構成されています。</font></font><br>
<br>
<ol>
<li><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">エンコーダー</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">（エンコーダー）-双方向RNN。これは、入力シーケンスを読み取るために使用されます。つまり、コンテキストをより適切に考慮するために、入力要素を左から右および右から左に同​​時に順次処理します。</font></font></li>
<li><b> </b>(Decoder) –  RNN,       .</li>
</ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
まず、入力シーケンスは埋め込みシーケンスに変換されます（つまり、埋め込みは単語をベクトルとして簡潔に表現したものです）。埋め込みは、エンコーダの再帰的なネットワークを通過します。したがって、各単語について、エンコーダーの非表示の状態（</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">図では赤い四角形で示されています</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">）が取得され、トークン自体とそのコンテキストに関する情報が含まれているため、単語間の言語接続を考慮することができます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
入力を処理した後、エンコーダーは最後の非表示状態（テキスト全体に関する圧縮情報を含む）をデコーダーに転送します。デコーダーは特別なトークンを受け取り</font></font><img src="https://habrastorage.org/webt/qn/ud/38/qnud38u15vpvzie4zkne-doze7k.png" alt="画像"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、出力シーケンスの最初の単語を作成します（</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">図では「ドイツ」です）</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">）次に、以前の出力を周期的に取得して自分にフィードし、次の出力要素を再度表示します（</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">「ドイツ」が「ビート」に移行した後、「ビート」が次の単語に移行した後など</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">）。これは、特別なトークンが発行されるまで繰り返され</font></font><img src="https://habrastorage.org/webt/vw/kv/4g/vwkv4gs-ul7vlvnkjwxag8njicw.png" alt="画像"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ます。これは世代の終わりを意味します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
次の要素を出力するために、デコーダーはエンコーダーと同様に、入力トークンを埋め込みに変換し、再帰的ネットワークのステップを実行し、デコーダーの次の非表示状態を受け取ります（</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">図の黄色の四角形</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">）。次に、完全に接続された層を使用して、事前にコンパイルされたモデル辞書からすべての単語の確率分布が取得されます。最も可能性の高い単語はモデルによって推定されます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
追加</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">アテンションメカニズム</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">は、デコーダーが入力情報をより有効に利用するのに役立ちます。生成の各ステップのメカニズムは、いわゆる</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">注意の分布を</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">決定します</font><font style="vertical-align: inherit;">（図の</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">青い四角形は、元のシーケンスの要素に対応する重みのセットであり、重みの合計は1、すべての重み&gt; = 0</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">）、それから、エンコーダーのすべての非表示状態の重み付き合計を受け取り、それによって形成しますコンテキストベクトル（</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">図は、青いストロークの赤い長方形を示しています</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">）このベクトルは、潜在状態を計算する段階でデコーダ入力ワードを埋め込むことと、次のワードを決定する段階で潜在状態自体と連結します。そのため、出力の各ステップで、モデルは現時点で最も重要なエンコーダーの状態を判別できます。つまり、どの入力単語を最も考慮に入れるかを決定します（たとえば、図では、「ビート」という単語を表示します。注意メカニズムは、「勝利」トークンと「勝利」トークンに大きな重みを付け、残りをゼロに近づけます）。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ヘッダーの生成もサマライゼーションのタスクの1つであるため、可能な限り最小の出力（1〜12ワード）</font><b><font style="vertical-align: inherit;">で、アテンションメカニズムでseq2seq</font></b><font style="vertical-align: inherit;">を使用することに</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">しました</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">そして私たちの場合のために。そのようなシステムは、ニュースなどの見出しのあるテキストでトレーニングします。さらに、トレーニング段階では、自分の出力ではなく実際の見出し（教師の強制）の単語をデコーダに送信することをお勧めします。これにより、自分とモデルの生活が楽になります。エラー関数として、標準のクロスエントロピー損失関数を使用して、出力ワードと実際のヘッダーのワードの確率分布がどれだけ近いかを示します。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/3v/co/b9/3vcob9esvdhgsydjcnjfpzv2wbi.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
トレーニング済みモデルを使用する場合、貪欲アルゴリズムを使用するよりも、レイ検索を使用して、より可能性の高い単語のシーケンスを見つけます。</font><font style="vertical-align: inherit;">これを行うには、生成の各ステップで、最も可能性の高い単語を導き出しませんが、同時に、最も可能性の高い単語のシーケンスのBeam_sizeを調べます。</font><font style="vertical-align: inherit;">それらが終了すると（それぞれがで終了する</font></font><img src="https://habrastorage.org/webt/vw/kv/4g/vwkv4gs-ul7vlvnkjwxag8njicw.png" alt="画像"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">）、最も可能性の高いシーケンスが導出されます。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/t2/dt/ni/t2dtnicefjxn0hwd0elaeduekbk.png" alt="画像"><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/yx/ln/q-/yxlnq-7z2-lwuyylmz83pferopm.png"></div><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/ey/81/9u/ey819uvi_jhx4wbwchr-wsixvgs.png"></div><div style="text-align:center;"><img src="https://habrastorage.org/webt/q1/tn/zu/q1tnzuopjtdslrltsmn8euup-kq.png"></div><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">モデルの進化</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
seq2seqのモデルの問題の1つは、辞書にない単語を引用できないことです。</font><font style="vertical-align: inherit;">たとえば、モデルは上記の記事から「オバマケア」を推定することはできません。</font><font style="vertical-align: inherit;">同じことが当てはまります：</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">珍しい姓と名前 </font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">新しい用語 </font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">他の言語の言葉、 </font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ハイフンでつながれた異なる単語のペア（「共和党上院議員」として） </font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">その他のデザイン。 </font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
もちろん、辞書を拡張することもできますが、これにより、トレーニングされるパラメーターの数が大幅に増えます。さらに、ジェネレーターが定性的な方法でそれらを使用することを学習できるように、これらのまれな単語が見つかった多数のドキュメントを提供する必要があります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
この問題に対するもう1つのより洗練されたソリューションは、2017年の記事「</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ポイントに到達する：ポインタージェネレーターネットワークによる要約</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">」（Abigail See et al。）で</font><font style="vertical-align: inherit;">提示されました</font><font style="vertical-align: inherit;">。彼女は私たちのモデルに新しいメカニズムを追加しました- </font><font style="vertical-align: inherit;">ポインター</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">メカニズム</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。ソーステキストから単語を選択し、生成されたシーケンスに直接挿入できます。テキストにOOV（</font><i><font style="vertical-align: inherit;">語彙外-辞書にない単語）が</font></i><font style="vertical-align: inherit;">含まれている場合</font></font><i><font style="vertical-align: inherit;"></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">）、モデルが必要だと判断した場合は、OOVを分離して出力に挿入できます。このようなシステムは</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">「</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ポインタージェネレーター」（ポインタージェネレーターまたはpg）と</font><font style="vertical-align: inherit;">呼ばれ、</font><font style="vertical-align: inherit;">サマライズへの2つのアプローチを組み合わせたものです。彼女自身は、彼女がどのステップで抽象化されるべきか、そしてどのステップで-抽出するかを決めることができます。彼女がそれをどのように行うか、私たちは今それを理解します。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/f-/9y/xw/f-9yxwborbgpjpalzwd5e_f74xi.png" alt="画像"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
通常のseq2seqモデルとの主な違いは、p </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">gen</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">が</font><font style="vertical-align: inherit;">計算される追加のアクション、つまり</font><font style="vertical-align: inherit;">生成確率です。これは、デコーダーの隠し状態とコンテキストベクトルを使用して行われます。追加アクションの意味は単純です。 p </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">genが</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 1に</font><font style="vertical-align: inherit;">近いほど</font><font style="vertical-align: inherit;">、モデルが抽象生成を使用して辞書から単語を発行する可能性が高くなります。より近いp</font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">gen</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">を0にすると、ジェネレーターがテキストから単語を抽出する可能性が高くなります。これは、以前に取得した注意の分布によって導かれます。単語の結果の最終的な確率分布は、生成された単語の確率分布（OOVがない場合）にp </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">gen</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">を</font><font style="vertical-align: inherit;">掛け</font><font style="vertical-align: inherit;">、注意分布（たとえばOOVは図では「2-0」）を掛けたものに（1-pを掛けたものです。</font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">gen</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">）。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/iv/8f/8q/iv8f8qvol1j3bbyx79vt5zo7-oq.png" alt="画像"><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/if/ew/s3/ifews3gqgojf5mclbxd0lpkz2q8.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ポインティングメカニズムに加えて、記事で</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">はカバレッジメカニズムを</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">紹介</font><b><font style="vertical-align: inherit;">し</font></b><font style="vertical-align: inherit;">ます。これは、単語の繰り返しを避けるのに役立ちます。</font><font style="vertical-align: inherit;">私も試してみましたが、見出しの品質に大きな改善は見られませんでした。本当に必要なわけではありません。</font><font style="vertical-align: inherit;">ほとんどの場合、これはタスクの詳細に起因します。少数の単語を出力する必要があるため、ジェネレーターはそれ自体を繰り返す時間がありません。</font><font style="vertical-align: inherit;">しかし、注釈などの他のサマライゼーションのタスクには便利です。</font><font style="vertical-align: inherit;">興味があれば、元の</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">記事で</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">それについて読むことができます</font><font style="vertical-align: inherit;">。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/4q/bx/op/4qbxopqt862cphikmbezyaoadlk.png" alt="画像"><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/--/3m/mz/--3mmzahhdk8kelwoqyh9rdjg8e.png"></div><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">多種多様なロシア語 </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
出力ヘッダーの品質を向上させる別の方法は、入力シーケンスを適切に前処理することです。</font><font style="vertical-align: inherit;">大文字の明らかな処理に加えて、ソーステキストの単語をスタイルと活用形のペア（つまり、ファンデーションとエンディング）に変換することも試みました。</font><font style="vertical-align: inherit;">分割するには、Porter Stemmerを使用します。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/jy/j8/-y/jyj8-yonzpmvpyoye6mbygrlcjg.png"></div><div style="text-align:center;"><img src="https://habrastorage.org/webt/_k/__/vy/_k__vyhouqm_ctxpsp8ukcselwe.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
他のトークンと区別するために、すべての語形変化の先頭に「+」記号を付けます。</font><font style="vertical-align: inherit;">私たちは、各トピックと活用形を別々の単語と見なし、単語と同じ方法でそれらから学びます。</font><font style="vertical-align: inherit;">つまり、それらから埋め込みを取得し、簡単に単語に変換できるシーケンス（基礎と終了に分解される）を導出します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
このような変換は、ロシア語のような形態学的に豊富な言語を扱うときに非常に役立ちます。</font><font style="vertical-align: inherit;">さまざまなロシア語の単語形式で巨大な辞書を作成する代わりに、これらの単語の多数の語幹（単語形式の数よりも数倍小さい）と非常に小さい語尾のセット（450語の活用形がたくさんあります）に制限できます。</font><font style="vertical-align: inherit;">したがって、モデルがこの「富」を処理しやすくすると同時に、アーキテクチャの複雑さやパラメーターの数を増やすことはありません。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/a4/es/s5/a4ess5qr3vaxprksv7avun3obl8.png" alt="画像"><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/3d/rk/6h/3drk6hntzcow2i3soi-heul7ckq.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
また、補題+文法変換を使用してみました。</font><font style="vertical-align: inherit;">つまり、処理前の各単語から、pymorphyパッケージを使用してその最初の形式と文法上の意味を取得できます（たとえば、「was」</font></font><img src="https://habrastorage.org/webt/95/wb/-a/95wb-aopfalycfvrshqfynxnwd4.png" alt="画像"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">「To be」と「VERB | impf | past | sing | femn」）。したがって、1組の並列シーケンス（一方は初期形式、もう一方は文法値）を取得しました。シーケンスのタイプごとに、埋め込みをコンパイルし、それを連結して、前述のパイプラインに送信しました。その中で、デコーダーは単語ではなく、見出し語と文法を与えることを学びました。しかし、そのようなシステムは、トピックに関するpgと比較して目に見える改善をもたらしませんでした。おそらく、文法的な値を処理するための非常に単純なアーキテクチャであり、出力の文法カテゴリごとに個別の分類子を作成する価値がありました。しかし、私はそのような、またはより複雑なモデルで実験しませんでした。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ポインタージェネレーターの元のアーキテクチャに別の追加を試しましたが、前処理には適用されません。</font><font style="vertical-align: inherit;">これは、エンコーダーとデコーダーのリカレントネットワークのレイヤー数（最大3）の増加です。</font><font style="vertical-align: inherit;">最後の層の非表示状態には、単一層RNNの非表示状態よりもはるかに長い入力サブシーケンスに関する情報が含まれる可能性があるため、再帰ネットワークの深さを増やすと、出力の品質が向上します。</font><font style="vertical-align: inherit;">これは、入力シーケンスの要素間の複雑な拡張されたセマンティック接続を考慮するのに役立ちます。</font><font style="vertical-align: inherit;">確かに、これはモデルパラメータの数を大幅に増やし、学習を複雑にします。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/cf/7g/ej/cf7gejf-hxd5pbscvqgnvuuviqy.png" alt="画像"><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/4g/ww/zy/4gwwzytsrt506_xllgy5xcis6vk.png"></div><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ヘッダージェネレーターの実験</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ヘッドラインジェネレータに関する私のすべての実験は、2つのタイプに分けることができます。ニュース記事と詩の実験です。</font><font style="vertical-align: inherit;">それらについて順に説明します。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ニュース実験</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ニュースを扱うときは、seq2seq、pg、語幹と語尾変化のあるpgなどのモデルを使用しました。私はグラムで機能するモデルも検討しましたが、それらについて伝えたいことはすべて、すでに上記で説明しました。このセクションで説明されているすべてのpgはコーティングメカニズムを使用していることをすぐに言っておく必要がありますが、結果への影響は疑わしいです（それがないと、それほど悪くはなかったためです）。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
対話会議で見出し生成トラックを実施するために、Rossiya Segodnya通信社から提供されたRIA Novostiデータセットについてトレーニングを行いました。データセットには、2010年1月から2014年12月までに発行された1,003,869のニュース記事が含まれています。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
調査されたすべてのモデルは、同じ埋め込み（128）、語彙（100k）、潜在状態（256）を使用し、同じ数の時代に合わせてトレーニングされました。</font><font style="vertical-align: inherit;">したがって、アーキテクチャまたは前処理の質的な変更のみが結果に影響を与える可能性があります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
前処理されたテキストを処理するように適合されたモデルは、単語を処理するモデルよりも優れた結果を提供します。</font><font style="vertical-align: inherit;">トピックと活用形に関する情報を使用する3層pgが最適です。</font><font style="vertical-align: inherit;">任意のpgを使用すると、seq2seqと比較して期待されるヘッダーの品質の向上も表示されます。これは、ヘッダーを生成するときのポインターの優先的な使用を示唆しています。</font><font style="vertical-align: inherit;">すべてのモデルの操作の例を次に示します。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/6w/4u/0r/6w4u0ruudngxtt2szek31w0achq.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
生成されたヘッダーを見ると、次の問題と調査中のモデルを区別できます。</font></font><br>
<br>
<ol>
<li>     .           (   );</li>
<li> ,      ,   ,   ,   ,     (   );</li>
<li>      ,         .</li>
</ol><br>
<img src="https://habrastorage.org/webt/9c/-b/ot/9c-bote3bwomvfu_3bwaixhqnks.png" alt="画像"><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/e-/3c/px/e-3cpxuknykthyu-xvf-cp-rdsi.png"></div><br>
<img src="https://habrastorage.org/webt/lf/ry/ep/lfryepksl4kmcqxttb14ikgx4wg.png" alt="画像"><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/b3/4m/fw/b34mfwpsffud2kaa93tccjvyja0.png"></div><br>
<h3>  </h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
テーマを含む3層のpgは、生成されたヘッダーの不正確さが最も少ないため、これは、詩の実験のために選択したモデルです。私は、サイト「stihi.ru」からの600万のロシアの詩で構成される事件について彼女に教えました。彼らは愛（詩の約半分はこのトピックに専念しています）、市民（約四分の一）、都市と風景の詩が含まれています。執筆期間：2014年1月-2019年5月。詩の見出しの例を挙げます。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/yv/mo/da/yvmodarkck7cgra-ymxigffwhik.png"></div><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/3u/mb/fn/3umbfnpzo1hwhqy0_glxyhxynu4.png"></div><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/tp/tk/5e/tptk5eoq9wa1xrnj9w0gewjeayk.png"></div><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
モデルはほとんどが抽出であることがわかりました。ほとんどすべてのヘッダーは単一行であり、多くの場合最初または最後のスタンザから抽出されます。例外的なケースでは、モデルは詩にない単語を生成する可能性があります。これは、ケース内の非常に多くのテキストが実際に行の1つを名前として持つためです。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
結論として、語幹に取り組み、単層のデコーダーとエンコーダーを使用するインデックスジェネレーターは、</font><font style="vertical-align: inherit;">コンピューター言語学「Dialogue」に関する科学的対話会議でニュース記事の見出しを生成するため</font><font style="vertical-align: inherit;">の</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">競争トラック</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">で</font><font style="vertical-align: inherit;">2位</font><font style="vertical-align: inherit;">でした。この会議の主な主催者はABBYYです。同社は、自然言語処理のほぼすべての現代的な領域の研究に従事しています。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
最後に、少しインタラクティブな方法を紹介します。コメントでニュースを送信し、ニューラルネットワークが生成するヘッダーを確認します。</font></font><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Matby、ABBYYのNLP Groupの開発者</font></font></i></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../ja479384/index.html">MegaFonでビッグデータはどのように機能し、そこに到達する方法を教えてください。</a></li>
<li><a href="../ja479388/index.html">国家データセンター、ミハリッチの建設の特徴</a></li>
<li><a href="../ja479392/index.html">Pinebook Pro：Chromebookは廃止</a></li>
<li><a href="../ja479394/index.html">15のソリューションからヘルプデスクを検索した方法...が見つかりませんでした</a></li>
<li><a href="../ja479398/index.html">線形回帰方程式を行列形式にします</a></li>
<li><a href="../ja479402/index.html">海外でフリーランサーサービスに正式に支払い、税金を0％支払い、支払いシステムにフィードしない方法</a></li>
<li><a href="../ja479404/index.html">サンタクロースのスタッフ</a></li>
<li><a href="../ja479406/index.html">Kotlin言語でのAndroidの16の開発のヒント。パート1</a></li>
<li><a href="../ja479414/index.html">目標を見つける方法。チャンスの役割</a></li>
<li><a href="../ja479416/index.html">どこに向かっているのかを確認します（周辺視野vs認知負荷）</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>