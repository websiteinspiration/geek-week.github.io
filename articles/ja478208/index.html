<!doctype html>
<html class="no-js" lang="ja">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🖖🏾 ⬅️ 😺 「あなたは千からそれを知るでしょう...」またはPyTorchを使用してリアルタイムでウェブカメラからの画像を分類します 🦋 ✋🏻 🛷</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="ここでそれは人生で起こります。あなたは自分のいたずらに座り、誰にも触れないで、プリムスを修理し、次にこのプリムスから、テレビから、そして実際にすべての鉄から、「ニューラルネットワーク、ディープラーニング、人工知能、デジタルエコノミー...」と聞きます。
 
 私は男です。つまり、好奇心旺盛で貪欲な生...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>「あなたは千からそれを知るでしょう...」またはPyTorchを使用してリアルタイムでウェブカメラからの画像を分類します</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/478208/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ここでそれは人生で起こります。あなたは自分のいたずらに座り、誰にも触れないで、プリムスを修理し、次にこのプリムスから、テレビから、そして実際にすべての鉄から、「ニューラルネットワーク、ディープラーニング、人工知能、デジタルエコノミー...」と聞きます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
私は男です</font><font style="vertical-align: inherit;">。</font><font style="vertical-align: inherit;">つまり、好奇心旺盛で</font></font><strike><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">貪欲な</font></font></strike><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">生き物</font><font style="vertical-align: inherit;">です。繰り返しになりますが、私は抵抗することができず、実際にニューラルネットワークとは何か、また何と一緒に食べるのかを知ることにしました。</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
「自分自身を学びたいなら、他の人に教え始めたい」ということわざがそうであるように、これについて私は引用を注ぐのをやめて、仕事に取り掛かります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
この記事では、私はあなたの問題を解決しようとします。それは、結局のところ、私の心だけでなく興奮します。</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
数学とプログラミングの分野で十分な基本的な知識がなければ、OpenCVとPython機械学習ライブラリPyTorchを使用して、ウェブカメラからの画像をリアルタイムで分類しようとします。</font><font style="vertical-align: inherit;">その過程で、ニューラルネットワークの使用において初心者に役立つ可能性があるいくつかのポイントについて学びます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
私たちの分類子がArduino互換コントローラをラズベリーから区別できるかどうか知りたいですか？</font><font style="vertical-align: inherit;">その後、猫の下で大歓迎です。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/hw/bt/jk/hwbtjkrckjokl0xkhtwrojlfhf8.png"><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
内容：</font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">パートI：</font></font></a><font style="vertical-align: inherit;"><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">はじめに</font></a><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">パート</font></a></font><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">II：ニューラルネットワークを使用した画像の認識</font></font></a><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">パートIII：PyTorchを使用する準備</font></font></a><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">パートIV：Pythonコードの記述</font></font></a><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">パートV：労働の成果</font></font></a><br>
<a name="I"></a><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">パートI：はじめに</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
正直なところ、</font><font style="vertical-align: inherit;">機械学習</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Courseraの</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">専門</font><font style="vertical-align: inherit;">講座を</font><font style="vertical-align: inherit;">終えたほぼ直後</font><font style="vertical-align: inherit;">に、初心者の目で機械学習に関する一連の記事の資料を準備したいと思いました。</font></font><br>
<br>
<div class="spoiler"><b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">その他のサイクル記事</font></font></b><div class="spoiler_text">1.  :<br>
<br>
<ul>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=">« Data   !» — (    Data Science  Cognitive Class)</a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=">«    »       (Data Science from Scratch)</a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=">«  !»       DataScience  kaggle</a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=">«,  !»  «     »,    Data Science</a></li>
</ul><br>
2.   <br>
<br>
<ul>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="> “ Learning”     Data Science    </a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="> «  !»    (Data science)  C#   Accord.NET Framework</a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="> «   , !»      </a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="> «4    »         </a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="> «  »     C#   Accord.NET     </a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="> «   …»      C#   Ml.NET (DataScience)</a></li>
</ul><br>
</div></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
しかし、力を集めて書くことは、今だけになってしまいました。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
最初に警告する必要があります。よく考えて、この素晴らしい世界に没頭する準備はできていますか？</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
この記事を準備するのに、丸4日休み、週に2度の夜を要しました。また、事前の準備なしに問題に対処するための多くの失敗した試みもあります。したがって、この場合は、よく知られているグラフィックユーモアスクをMax Planckで再編集するのが適切です。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/hq/ei/ax/hqeiaxyedxlqlxi2bxqo7s-z9c0.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
また、この記事のコード例を理解するには、Python言語の基本的な知識と、機械学習に使用されるその一般的なライブラリ（NumPyなど）が依然として必要であるとも言えます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
この記事では、理論的な計算やニューラルネットワークの詳細な説明、およびそれらの研究の原理については説明しません。</font><font style="vertical-align: inherit;">したがって、私はどのように私が苦しめられたかを見て、同じ過ちを犯さないように努め、結局私たちは何かを認識することさえできます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
あなたはまだこのテキストを読んでいますか？</font><font style="vertical-align: inherit;">素晴らしい、今私はあなたの精神とスタミナの力に自信を持っています。</font></font><br>
<a name="II"></a><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> パートII：ニューラルネットワークを使用した画像認識 </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ニューラルネットワークに関連するすべての開発には、多くの賢く才能のある人々が関わっているという事実のおかげで、車の番号の認識からテキストの翻訳まで、問題を解決するための無料のツールや例がたくさんあります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
信頼性の高いウェブカメラを使用して、人やモニターがいることを認識したいと考えています。これは問題ではありません。実装されている</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">例を</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">見つけ、</font><font style="vertical-align: inherit;">100行未満のコードをコピーして、</font></font><strike><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">「ほらほら」</font></font></strike> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">「ほらほら」</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">と</font><font style="vertical-align: inherit;">答えただけ</font><font style="vertical-align: inherit;">です。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/c9/xn/4_/c9xn4_d8nkxel2wcf6dh3qlw-fm.png" title="ビデオのフレーム画像https://youtu.be/u9ogVfao4Os"></div><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
これらすべてが、単純さの欺瞞的な錯覚を生み出す可能性があります。</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
あなたがあなた自身の何かをしたい場合、困難が生じます。</font><font style="vertical-align: inherit;">結局のところ、まだ自転車を組み立てていない場合は、少なくともクールなホーンと懐中電灯をそれにねじ込みます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
したがって、上記の例では、関数を満たしています。</font></font><br>
<br>
<pre><code class="python hljs"> net = cv2.dnn.readNetFromCaffe(args_prototxt, args_model)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
これは、訓練を受けていない人でも、ニューラルネットワークに「カフェ」と呼ばれるものが使用されていることを明確に示唆しています。</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">結局のところ</font></a></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
、</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Caffe</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">はディープ機械学習フレームワークです。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
無知で嫌いだったとは言いがたいです。おそらく</font></font><strike><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">退屈な壁紙の</font></font></strike><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">プロジェクトサイト</font><font style="vertical-align: inherit;">、</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">TensorFlow</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">は単なる噂だったかもしれませ</font><b><font style="vertical-align: inherit;">ん</font></b><font style="vertical-align: inherit;">が</font><font style="vertical-align: inherit;">、Caffeから始めないことを固く決めました。</font></font><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">何も考えないで、私はカフェを批判したくありません。これは良いフレームワークだと思いますが、第一印象だけではうまくいきませんでした。私はきっといつか後で彼に戻るでしょう</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
それでは、TensorFlowを見てください。ロシア語版ウィキペディアを信じている場合：</font></font><br>
<br>
<blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">TensorFlowは、Googleが開発した機械学習用のオープンソフトウェアライブラリで、画像を自動的に検出して分類し、人間の知覚の品質を実現するために、ニューラルネットワークの構築とトレーニングの問題を解決します。</font></font><br>
</blockquote><br><font style="vertical-align: inherit;"></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">データセットの</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
画像を分類するための既製モデルのトレーニングを含め、出くわしたすべてのチュートリアルにぶつかることになりました</font><font style="vertical-align: inherit;">。</font></font><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ほんの少しの喜びの後、私はOpenCVメソッドを喜んで見つけました：</font></font><br>
 <br>
<pre><code class="python hljs">cv.dnn.readNetFromTensorflow(model[, config]	) </code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
そして、次の質問をする人々の列を補充するために、インターネットに喜んで走ります。</font></font><br>
<blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">「.h5のモデルを.pbファイルに変換する</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
方法</font><font style="vertical-align: inherit;">」</font><font style="vertical-align: inherit;">「フリーズ（フリーズ）_ .pbファイルを取得する</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
方法</font><font style="vertical-align: inherit;">」</font><font style="vertical-align: inherit;">「.pbファイルから.pbファイルを作成する方法」</font></font></blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
長い間、スタックオーバーフローでこれらやその他のアマチュア的な質問をする人々との一体感を感じていませんでした。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
さらに、私が遭遇したすべての例では、何らかのキャッチがあり</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、OpenCVの</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">例</font><font style="vertical-align: inherit;">でも起動時にエラーが発生し</font><font style="vertical-align: inherit;">て何かが機能しませんでした</font><font style="vertical-align: inherit;">。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
その結果、最初の1週間は、詳細を掘り下げることなく、基本を勉強せずに、TensorFlowとOpenCVから少なくとも何かを達成するために費やされました。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
しかし、ある日の午前4時の後で、プログラムはウェブカメラの公平な目で私を見て、私</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">は飛行機</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">である</font><b><font style="vertical-align: inherit;">可能性が22％高い</font></b><font style="vertical-align: inherit;">と言っ</font><b><font style="vertical-align: inherit;">たので、</font></b><font style="vertical-align: inherit;">場合。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Caffeと同様に、私がTensorFlowに対して何かを持っているとは思わないでください。</font><font style="vertical-align: inherit;">これは巨大なコミュニティーを持つ非常にクールなライブラリーであり、私は嵐でそれを受け入れることができず、失望し、さらに解決策を探しに行きました。</font><font style="vertical-align: inherit;">しかし、実際には、私は間違いなくもっと準備ができている彼女に戻り、それについての短い記事を書きます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
その間、私たちは心の失敗によって興奮し悲しんでいることを追跡し続けます。</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">ニューラルネットワーク</font></a><font style="vertical-align: inherit;">を操作</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">するための</font></a></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
OpenCV </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">メソッド</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">をもう一度見てみましょう</font><font style="vertical-align: inherit;">。</font></font><br>
<br>
<pre><code class="python hljs">cv.dnn.readNetFromTorch(model[, isBinary[, evaluate]])</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
うわー！</font><font style="vertical-align: inherit;">別の機械学習ライブラリに慣れる時がきたようです。</font></font><br>
<a name="III"></a> <br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">パートIII：PyTorchを使用する準備</font></font></h2><br><font style="vertical-align: inherit;"></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PyTorchが</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
言ったように</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">、</font></a><font style="vertical-align: inherit;">これはFacebookの人々が</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">関与していた</font></a><font style="vertical-align: inherit;">かなり人気のある別の機械学習フレームワークです。主観的には、PyTorchウェブサイトにはトレーニングの例が少なく、フレームワークのコミュニティはTensorFlowのコミュニティよりもやや小さいように見えましたが、これから何をやったのか、本来意図したものとほぼ同じだと思います。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
したがって、上記の第2週で書いたように、既成のモデルを構築し、OpenCVを使用してWebカメラからの画像を認識することを期待して、何をしているのか理解せずに、他の人の例をブラインドにコピーするだけでどこにも行かないということがわかりました。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
そのため、スクリプトを並行して記述し、ニューラルネットワークについて学ぶことにしました。</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
この問題で非常に役立つのは、Rashid Tarikの著書「私たちはニューラルネットワークを作成します」で、これはHabréhabr.com </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">/</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ru </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">/</font></a><font style="vertical-align: inherit;"> post </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">/440190で</font></a><font style="vertical-align: inherit;">すでに言及されてい</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">ます</font></a><font style="vertical-align: inherit;">。この本は、ニューラルネットワークの基本概念について非常にアクセスしやすい言語で書かれていますが、もちろん、PyTorchの使用方法については説明しません。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
したがって、</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PyTorchを使用したディープラーニングに関する</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">チュートリアル</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">を60分で</font></a><font style="vertical-align: inherit;">実行することは不必要ではありません</font><font style="vertical-align: inherit;">。実際にはそれほど時間はかかりませんが、作業の基本原則について少なくとも部分的には理解できます。 Google Colabとローカルの両方でコードを実行できます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ですから、マシン上でコードを書く方法の問題にスムーズに取り組みました。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
公式ウェブサイトでは、さまざまな</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">インストールオプション</font></a><font style="vertical-align: inherit;">が提供され</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ます</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">オペレーティングシステム、目的のpythonバージョンとインストール方法によって異なります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Python機械学習を愛する初心者には</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、Anacondaディストリビューション</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">を使用し</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">て</font></a><font style="vertical-align: inherit;"> PyTorchをインストールすることをお勧めし</font><font style="vertical-align: inherit;">ます。これは、WindowsとLinuxの両方でうまく機能します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
次に、実験用にAnacondaで別の環境を作成し、OpenCVを含むすべての必要なパッケージをゆっくりとインストールし始めます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
AnacondaにPyTorchをインストールするには、グラフィカルインターフェイスまたはコンソール（Anaconda Prompt for Windows）を使用します。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/v3/gh/x9/v3ghx9b1bkgdywmp5jwg0rdodas.png"></div><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
画像認識には、トーチビジョンライブラリが必要です。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
コンソールに入力したテキストで「cudatoolkit = 10.1」を既に表示していて、CUDAとは何か、どのバージョンをインストールする必要があるか、インストールする必要があるかどうかについても質問された可能性があります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
私が理解しているように、CUDAではプロセッサからNvidiaのビデオカードに計算を転送できます。レビューから判断すると、モデルの計算プロセスが時々スピードアップします。しかし、私と同じように古代のビデオカードの幸せな所有者である場合、軟膏のフライには軟膏があります。インストール後、CUDA PyTorchを使用しようとすると、最初にビデオドライバーを更新するように求められ、その後、ビデオカードが古すぎてそうではないというすべての期待を破るように求められる場合があります。サポートされています。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
たとえば、私のGTX 760はCUDAバージョン3.5でサポートされているようです。このバージョンまたはそのバージョンのCUDAでサポートされているビデオカードのリストは見つかりませんでしたが、かなり古いビデオカードのすべての所有者はバージョン9.2と10.1をためらわず、サポートなしですぐにバージョンをインストールすると思います。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
コードを書き始める前に、文字通り最後の仕上げがあります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
モデルが心に近いオブジェクトを認識できるようにしたいので、独自の画像セットを組み立てる必要があります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
この場合、torchvision.datasets.ImageFolderクラスが役立ちます。これをtorch.utils.data.DataLoaderクラスと組み合わせて使用​​すると、画像フォルダーの構造化されたセットから独自のデータセットを作成できます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
画像セットの構造は次のようになります。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/ey/0g/s5/ey0gs5j6nybindivxrs8oah0hgg.png"></div><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
つまり、「someduino」および「raspberry」というクラスの認識されたクラスを含むフォルダーが配置されているTestおよびTrainフォルダーがあります。トレーニングサンプルでは、​​クラスごとに128枚の画像がテストサンプル-28で提示されます。優れたニューラルネットワークは、何百万枚または少なくとも数十万枚の画像をゼロから学習する必要があると言わなければなりませんが、実際のタスクを自分で設定することはせず、その事実に対処しようとします。なんとか集めました。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ちなみに、データセットの画像を収集することについての2つの言葉。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
データセットをネットワークに配置し、著作権侵害の悪夢がない場合は、これを可能にする権利の画像を使用することをお勧めします。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
たとえば、下の図に示すように、検索フィルターで適切なアイテムを選択することにより、Google画像での検索を使用してそれらを見つけることができます。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/bf/ld/gu/bfldgupswxyj_iahgwribviru3c.png"></div><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
まあ、画像の一部は常に独立して行うことができます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Arduinoとラズベリーpiの画像を比較すると思った記事のタイトル画像から、おそらく恥ずかしいと思いますが、クラス間の大きな違いについては、Arduino（およびそのクローン）の異なるバージョンの画像をラズベリーと比較します。</font></font><br>
<br>
<a name="IV"></a><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> パートIV：Pythonコードの記述 </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
だから私たちは最も興味深いものに行きました。</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
いつものように、画像を含むこの記事のすべての資料は、無料でアクセスできるように</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GitHub</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">に投稿しました</font><font style="vertical-align: inherit;">。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
まず、モデルをトレーニングするJupyterノートブックから始めます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ノートブックは2つの論理部分に分かれています。1つは単純なモデルの作成とゼロからの学習専用で、もう1つは既製のトレーニング済みモデルに転移学習を適用します。</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
最初のケースでは、この</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">チュートリアルに</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">焦点を当て</font><font style="vertical-align: inherit;">ます。</font></font><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
よくわからないことを壊すのが怖かったので、チュートリアルの内容はほとんど何もしていません。そのため、原則として、上記の例を検討して、同様の結果を得ることができます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
開始するには、必要なライブラリをインポートします（それらがインストールされていることを確認してください）。</font></font><br>
<br>
<pre><code class="python hljs">
<span class="hljs-keyword">from</span> __future__ <span class="hljs-keyword">import</span> print_function, division
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn
<span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim
<span class="hljs-keyword">from</span> torch.optim <span class="hljs-keyword">import</span> lr_scheduler
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> torchvision
<span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> datasets, models
<span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> transforms
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> time
<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> copy
<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn
<span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F
<span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim
<span class="hljs-keyword">from</span> torch.autograd <span class="hljs-keyword">import</span> Variable
<span class="hljs-keyword">import</span> torch.onnx
<span class="hljs-keyword">import</span> torchvision<font></font>
%matplotlib inline<font></font>
plt.ion()   <span class="hljs-comment"># interactive moden</span>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
次に、データセットをロードするアドレスを決定します</font></font><br>
<br>
<pre><code class="python hljs">
<span class="hljs-comment">#get address such as C:\\(folder with you notebook)</span><font></font>
dir = os.path.abspath(os.curdir)<font></font>
<span class="hljs-comment"># i suppose what your image folders placed in datasets directory </span>
data_dir=os.path.join(dir, <span class="hljs-string">"datasets\\"</span>)
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
以下は、画像を変換するためのコードです。必要に応じて、モデルに供給する前にデータを削減、トリミング、暗く、または明るくします。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
今回のケースでは、画像を32x32ピクセルに縮小して正規化します（ここでは計算を行いません）。</font></font><br>
<br>
<pre><code class="python hljs">
<span class="hljs-comment"># Data scaled and normalization for training and testing</span><font></font>
data_transforms = {<font></font>
    <span class="hljs-string">'train'</span>: transforms.Compose([<font></font>
        transforms.RandomResizedCrop(<span class="hljs-number">32</span>),<font></font>
        transforms.ToTensor(),<font></font>
        transforms.Normalize([<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>], [<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>])<font></font>
    ]),<font></font>
    <span class="hljs-string">'test'</span>: transforms.Compose([<font></font>
        transforms.RandomResizedCrop(<span class="hljs-number">32</span>),<font></font>
        transforms.ToTensor(),<font></font>
        transforms.Normalize([<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>], [<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>])<font></font>
    ]),<font></font>
}<font></font>
 </code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
次に、さらにトレーニングするために、画像をデータの配列（テンソル）に変換する関数を記述します。</font><font style="vertical-align: inherit;">この関数は、フォルダーへのパスだけでなく変換スキームも入力パラメーターとして使用されるため、2回使用するため、例で示したものとは少し異なります。</font></font><br>
<br>
<pre><code class="python hljs">
<span class="hljs-comment">#Create function to get your(my) images dataset and resize it to size for model</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_dataset</span>(<span class="hljs-params">data_dir, data_transforms </span>):</span>
    <span class="hljs-comment"># create train and test datasets</span><font></font>
    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),<font></font>
                                              data_transforms[x])<font></font>
                      <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> [<span class="hljs-string">'train'</span>, <span class="hljs-string">'test'</span>]}<font></font>
    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=<span class="hljs-number">4</span>,<font></font>
                                                 shuffle=<span class="hljs-literal">True</span>, num_workers=<span class="hljs-number">4</span>)
                  <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> [<span class="hljs-string">'train'</span>, <span class="hljs-string">'test'</span>]}<font></font>
    dataset_sizes = {x: len(image_datasets[x]) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> [<span class="hljs-string">'train'</span>, <span class="hljs-string">'test'</span>]}
    <span class="hljs-comment">#get classes from train dataset folders name</span>
    classes = image_datasets[<span class="hljs-string">'train'</span>].classes<font></font>
<font></font>
    <span class="hljs-keyword">return</span> dataloaders[<span class="hljs-string">"train"</span>], dataloaders[<span class="hljs-string">'test'</span>], classes, dataset_sizes
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
出力では、関数はニューラルネットワークモデルに必要な形式で2つのデータセットを返します。また、ボーナスとして、フォルダー名から取得した画像クラスに関する情報、トレーニングとトレーニングサンプルのサイズも返します。</font></font><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
データセットを作成するときは、trainとtestのフォルダー構造が同じ（クラスの数が同じ）であることと、.jpg画像を使用していることを確認してください。無実の画像を装って、ゴミが入り込んで処理中にエラーが発生する場合があります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
新しく作成した関数を使用します。</font></font><br>
<br>
<pre><code class="python hljs"><font></font>
trainloader, testloader, classes, dataset_sizes=get_dataset(data_dir,data_transforms)<font></font>
print(<span class="hljs-string">'Classes: '</span>,  classes)<font></font>
print(<span class="hljs-string">'The datasest have: '</span>,  dataset_sizes ,<span class="hljs-string">" images"</span>)
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
予想通り、</font></font><strike><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">プロレタリアとブルジョアジーの</font></font></a></strike><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">「ラズベリー」と「サムドゥイーノ」の</font><font style="vertical-align: inherit;">2つのクラスしかありません</font><font style="vertical-align: inherit;">。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
トレーニングサンプルには256枚の画像、コントロールサンプルには56枚が含ま</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
れています。トレーニングサンプルでのデータの様子を見てみましょう。</font></font><br>
<br>
<pre><code class="python hljs">
<span class="hljs-comment"># create function for print unnormalized images</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">imshow</span>(<span class="hljs-params">img</span>):</span>
    img = img / <span class="hljs-number">2</span>+<span class="hljs-number">0.5</span>      <span class="hljs-comment"># unnormalize</span><font></font>
    npimg = img.numpy()<font></font>
    plt.imshow(np.transpose(npimg, (<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>)))<font></font>
    plt.show()<font></font>
<span class="hljs-comment"># get some random training images</span><font></font>
dataiter = iter(trainloader)<font></font>
images, labels = next(dataiter)<font></font>
<span class="hljs-comment">#images, labels = dataiter.next()</span>
<span class="hljs-comment"># show images</span><font></font>
imshow(torchvision.utils.make_grid(images))<font></font>
<span class="hljs-comment"># print labels</span>
print(<span class="hljs-string">' '</span>.join(<span class="hljs-string">'%5s'</span> % classes[labels[j]] <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">4</span>)))
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
目で見て、32x32の画像内のオブジェクトを区別できる場合、ニューラルネットワークで識別できるはずです。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/_4/ws/aa/_4wsaad5dvbfq4hwgzhx-pss0wo.png"></div><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
さらに、モデル自体。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ここでは、入力と出力の次元が異なるレイヤーと、変換が実行されるための関数を作成します。</font><font style="vertical-align: inherit;">残念ながら、私の頭の中のこの段階は完全には収まりませんでしたので、それを使用するだけです。主なことは、それが機能することです。</font></font><br>
<br>
<pre><code class="python hljs">
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Net</span>(<span class="hljs-params">nn.Module</span>):</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><font></font>
        super(Net, self).__init__()<font></font>
        self.conv1 = nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <span class="hljs-number">5</span>)<font></font>
        self.pool = nn.MaxPool2d(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<font></font>
        self.conv2 = nn.Conv2d(<span class="hljs-number">6</span>, <span class="hljs-number">16</span>, <span class="hljs-number">5</span>)<font></font>
        self.fc1 = nn.Linear(<span class="hljs-number">16</span> * <span class="hljs-number">5</span> * <span class="hljs-number">5</span>, <span class="hljs-number">120</span>)<font></font>
        self.fc2 = nn.Linear(<span class="hljs-number">120</span>, <span class="hljs-number">84</span>)<font></font>
        self.fc3 = nn.Linear(<span class="hljs-number">84</span>, <span class="hljs-number">2</span>)
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><font></font>
        x = self.pool(F.relu(self.conv1(x)))<font></font>
        x = self.pool(F.relu(self.conv2(x)))<font></font>
        x = x.view(<span class="hljs-number">-1</span>, <span class="hljs-number">16</span> * <span class="hljs-number">5</span> * <span class="hljs-number">5</span>)<font></font>
        x = F.relu(self.fc1(x))<font></font>
        x = F.relu(self.fc2(x))<font></font>
        x = self.fc3(x)<font></font>
        <span class="hljs-keyword">return</span> x<font></font>
net = Net()<font></font>
print(net)<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
このコードの最後で、実際にニューラルネットワークを作成しました。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
さらにクラスが必要な場合は、以下の行で2を3に置き換えてみてください。</font></font><br>
<br>
<pre><code class="python hljs"> self.fc3 = nn.Linear(<span class="hljs-number">84</span>, <span class="hljs-number">2</span>) </code></pre><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
モデルに何かを教えることは残っています。</font></font><br>
<br>
<pre><code class="python hljs"><font></font>
criterion = nn.CrossEntropyLoss()<font></font>
optimizer = optim.SGD(net.parameters(), lr=<span class="hljs-number">0.001</span>, momentum=<span class="hljs-number">0.9</span>)<font></font>
device = torch.device(<span class="hljs-string">"cpu"</span>)
<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(<span class="hljs-number">11</span>):  <span class="hljs-comment"># loop over the dataset multiple times</span>
    running_loss = <span class="hljs-number">0.0</span>
    <span class="hljs-keyword">for</span> i, data <span class="hljs-keyword">in</span> enumerate(trainloader, <span class="hljs-number">0</span>):
        <span class="hljs-comment"># get the inputs; data is a list of [inputs, labels]</span><font></font>
        inputs, labels = data<font></font>
        <span class="hljs-comment"># zero the parameter gradients</span><font></font>
        optimizer.zero_grad()<font></font>
        <span class="hljs-comment"># forward + backward + optimize</span><font></font>
        outputs = net(inputs)<font></font>
        loss = criterion(outputs, labels)<font></font>
        loss.backward()<font></font>
        optimizer.step()<font></font>
        <span class="hljs-comment"># print statistics</span><font></font>
        running_loss += loss.item()<font></font>
        <span class="hljs-keyword">if</span> i % <span class="hljs-number">15</span> == <span class="hljs-number">14</span>:    <span class="hljs-comment"># print every 15 mini-batches</span>
            print(<span class="hljs-string">'[%d, %5d] loss: %.3f'</span> %<font></font>
                  (epoch + <span class="hljs-number">1</span>, i + <span class="hljs-number">1</span>, running_loss / <span class="hljs-number">15</span>))<font></font>
            running_loss = <span class="hljs-number">0.0</span>
print(<span class="hljs-string">'Finished Training'</span>)
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
上記のコードスニペットでは、最初に最適化と完了の基準を定義しました。</font><font style="vertical-align: inherit;">そして、彼らは私たちのモデルを11時代にわたって教え始めました。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
時代が多いほど、モデルはよりよく学習するはずです（予測誤差は少なくなります）が、より多くの時間が必要になります。</font><font style="vertical-align: inherit;">データセットのランダムな混合により、モデル（ほとんどの場合）は毎回異なるエラーを生成しますが、11番目の時代までには、依然としてゼロに近づく傾向があります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
このサイクルでは、わかりやすくするために、トレーニングの進捗状況に関する情報が定期的に表示されます。</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
これは、モデルがコンピューター上で生成するものです。</font><font style="vertical-align: inherit;">テキストはスポイラーの下に隠れるために長く押し込まれます。</font></font><br>
<br>
<div class="spoiler"><b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">異なる時代のモデルのエラー</font></font></b><div class="spoiler_text"><code>[1, 15] loss: 0.597<br>
[1, 30] loss: 0.588<br>
[1, 45] loss: 0.539<br>
[1, 60] loss: 0.550<br>
[2, 15] loss: 0.515<br>
[2, 30] loss: 0.424<br>
[2, 45] loss: 0.434<br>
[2, 60] loss: 0.391<br>
[3, 15] loss: 0.392<br>
[3, 30] loss: 0.392<br>
[3, 45] loss: 0.282<br>
[3, 60] loss: 0.211<br>
[4, 15] loss: 0.292<br>
[4, 30] loss: 0.247<br>
[4, 45] loss: 0.197<br>
[4, 60] loss: 0.343<br>
[5, 15] loss: 0.400<br>
[5, 30] loss: 0.206<br>
[5, 45] loss: 0.254<br>
[5, 60] loss: 0.299<br>
[6, 15] loss: 0.258<br>
[6, 30] loss: 0.231<br>
[6, 45] loss: 0.241<br>
[6, 60] loss: 0.332<br>
[7, 15] loss: 0.243<br>
[7, 30] loss: 0.324<br>
[7, 45] loss: 0.211<br>
[7, 60] loss: 0.271<br>
[8, 15] loss: 0.207<br>
[8, 30] loss: 0.200<br>
[8, 45] loss: 0.201<br>
[8, 60] loss: 0.392<br>
[9, 15] loss: 0.255<br>
[9, 30] loss: 0.207<br>
[9, 45] loss: 0.367<br>
[9, 60] loss: 0.296<br>
[10, 15] loss: 0.180<br>
[10, 30] loss: 0.230<br>
[10, 45] loss: 0.345<br>
[10, 60] loss: 0.232<br>
[11, 15] loss: 0.239<br>
[11, 30] loss: 0.239<br>
[11, 45] loss: 0.218<br>
[11, 60] loss: 0.288<br>
Finished Training</code><br>
</div></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
それでは、モデルが画像を分類できるかどうかをコントロールサンプルで確認します。 </font></font><br>
<br>
<pre><code class="python hljs">
correct = <span class="hljs-number">0</span>
total = <span class="hljs-number">0</span>
<span class="hljs-keyword">with</span> torch.no_grad():
    <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> testloader:<font></font>
        images, labels = data<font></font>
        outputs = net(images)<font></font>
        _, predicted = torch.max(outputs.data, <span class="hljs-number">1</span>)<font></font>
        <font></font>
        <span class="hljs-keyword">for</span> printdata <span class="hljs-keyword">in</span> list(zip(predicted,labels,outputs)):<font></font>
            printclass =[classes[int(printdata[<span class="hljs-number">0</span>])],classes[int(printdata[<span class="hljs-number">1</span>])]]<font></font>
            print(<span class="hljs-string">'Predict class - {0}, real class - {1}, probability ({2},{3}) - {4}'</span>.format( printclass[<span class="hljs-number">0</span>],printclass[<span class="hljs-number">1</span>],<font></font>
                                                                              classes[<span class="hljs-number">0</span>], classes [<span class="hljs-number">1</span>],printdata[<span class="hljs-number">2</span>]))<font></font>
<font></font>
        total += labels.size(<span class="hljs-number">0</span>)<font></font>
        correct += (predicted == labels).sum().item()<font></font>
        imshow(torchvision.utils.make_grid(images))<font></font>
        <span class="hljs-comment">#print('GroundTruth: ', ' '.join('%5s' % classes[predicted[j]] for j in range(4)))</span>
print(<span class="hljs-string">'Accuracy of the network on the'</span>, dataset_sizes[<span class="hljs-string">'test'</span>], <span class="hljs-string">'test images: %d %%'</span> % (
    <span class="hljs-number">100</span> * correct / total))
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
私たちはコードを実行し、はい、それはかなり可能であることを確認します。</font><font style="vertical-align: inherit;">しかし、2〜3時代だけを費やした場合、結果は悲惨なものになります。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/-p/k1/dr/-pk1drd-xwqp3yuicwsyndpnphe.png"></div><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
当然のことながら、28枚すべてのコントロール画像の結果が表示されますが、それらは画像に収まらないという明らかな理由があります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
次はオプションのコードです。</font><font style="vertical-align: inherit;">モデルを保存およびロードする方法を示し、コントロールイメージの分析を再表示して、品質が低下していないことを確認します。</font></font><br>
<br>
<pre><code class="python hljs">
<span class="hljs-comment">#(Optional) </span>
<span class="hljs-comment">#Save and load model</span>
PATH =os.path.join(dir, <span class="hljs-string">"my_model.pth"</span>)<font></font>
torch.save(net.state_dict(), PATH)<font></font>
net = Net()<font></font>
net.load_state_dict(torch.load(PATH))<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
画像を出力するコードは、上記のものと同様です。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
さて、画像を認識するように最初のモデルをトレーニングしたのはこれだけです。保存されたバージョンを関数に小規模に挿入する場合は、依然としてそうです。</font></font><br>
<br>
<pre><code class="python hljs">cv.dnn.readNetFromTorch(<span class="hljs-string">"my_model.pth"</span>)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
しかし、ああ、恐怖！</font><font style="vertical-align: inherit;">PyTorch辞書とTourchが保存するモデルファイルは同じものではないため、このコードはエラーになりますが、ここではTorchモデルが想定されています。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/dk/2x/ol/dk2xolwavgp_fzyokddsbjdhxrq.png"></div><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
パニックにならない。</font><font style="vertical-align: inherit;">もう一度、OpenCVが使用できるモデルを調べ、</font></font><br>
<br>
<pre><code class="python hljs">cv.dnn.readTensorFromONNX(path)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ここに私たちの素晴らしい妥協点があります。</font><font style="vertical-align: inherit;">モデルを.onnxに変換するだけです。</font></font><br>
<br>
<pre><code class="python hljs">
<span class="hljs-comment"># Export model to onnx format</span>
PATH =os.path.join(dir, <span class="hljs-string">"my_model.onnx"</span>)
<span class="hljs-comment">#(1, 3, 32, 32) –   , 3   3232 ,</span>
dummy_input = Variable(torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>)) <font></font>
torch.onnx.export(net, dummy_input, PATH)<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
これからは、すべてがうまくいくと思いますが、ウェブカメラでの作業に移る前に。</font><font style="vertical-align: inherit;">より良いモデルを取り、それをデータセットでトレーニングしてみましょう。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ノートブックのこの半分は、この</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">チュートリアルに</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">基づいており</font><font style="vertical-align: inherit;">、resnet18モデルをトレーニングします。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
このモデルでは、ソースデータの少し異なる準備が必要です。</font><font style="vertical-align: inherit;">原則として、モデルの説明で変換パラメーターについて読むか、単に他の人の例を借りることができます。</font></font><br>
<br>
<pre><code class="python hljs">
<span class="hljs-comment">#Data scaled and normalization for training and testing for resnet18</span><font></font>
data_transforms = {<font></font>
    <span class="hljs-string">'train'</span>: transforms.Compose([<font></font>
        transforms.RandomResizedCrop(<span class="hljs-number">224</span>),<font></font>
        transforms.RandomHorizontalFlip(),<font></font>
        transforms.ToTensor(),<font></font>
        transforms.Normalize([<span class="hljs-number">0.485</span>, <span class="hljs-number">0.456</span>, <span class="hljs-number">0.406</span>], [<span class="hljs-number">0.229</span>, <span class="hljs-number">0.224</span>, <span class="hljs-number">0.225</span>])<font></font>
    ]),<font></font>
    <span class="hljs-string">'test'</span>: transforms.Compose([<font></font>
        transforms.Resize(<span class="hljs-number">256</span>),<font></font>
        transforms.CenterCrop(<span class="hljs-number">224</span>),<font></font>
        transforms.ToTensor(),<font></font>
        transforms.Normalize([<span class="hljs-number">0.485</span>, <span class="hljs-number">0.456</span>, <span class="hljs-number">0.406</span>], [<span class="hljs-number">0.229</span>, <span class="hljs-number">0.224</span>, <span class="hljs-number">0.225</span>])<font></font>
    ]),<font></font>
}<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
次は、実際には同様のコードで、データセットを準備して画像を表示するため、コメントせずにそのままにしておきます。</font></font><br>
<br>
<pre><code class="python hljs">
<span class="hljs-comment"># get train and test data</span><font></font>
trainloader, testloader, classes, dataset_sizes=get_dataset(data_dir, data_transforms)<font></font>
print(<span class="hljs-string">'Classes: '</span>,  classes)<font></font>
print(<span class="hljs-string">'The datasest have: '</span>,  dataset_sizes ,<span class="hljs-string">" images"</span>)
<span class="hljs-comment"># Create new image show function for new transofration </span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">imshow_resNet18</span>(<span class="hljs-params">inp, title=None</span>):</span>
    <span class="hljs-string">"""Imshow for Tensor."""</span>
    inp = inp.numpy().transpose((<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>))<font></font>
    mean = np.array([<span class="hljs-number">0.485</span>, <span class="hljs-number">0.456</span>, <span class="hljs-number">0.406</span>])<font></font>
    std = np.array([<span class="hljs-number">0.229</span>, <span class="hljs-number">0.224</span>, <span class="hljs-number">0.225</span>])<font></font>
    inp = std * inp + mean<font></font>
    inp = np.clip(inp, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<font></font>
    plt.imshow(inp)<font></font>
    <span class="hljs-keyword">if</span> title <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<font></font>
        plt.title(title)<font></font>
    plt.pause(<span class="hljs-number">0.001</span>)  <span class="hljs-comment"># pause a bit so that plots are updated</span>
<span class="hljs-comment"># get some random training images</span><font></font>
dataiter = iter(trainloader)<font></font>
images, labels = next(dataiter)<font></font>
<span class="hljs-comment">#images, labels = dataiter.next()</span>
<span class="hljs-comment"># show images</span><font></font>
imshow_resNet18(torchvision.utils.make_grid(images))<font></font>
<span class="hljs-comment"># print labels</span>
print(<span class="hljs-string">' '</span>.join(<span class="hljs-string">'%5s'</span> % classes[labels[j]] <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">4</span>)))
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
そして、これがモデルをトレーニングするための関数です。</font><font style="vertical-align: inherit;">半分は分からなかったので、とりあえずそのまま使います。</font></font><br>
<br>
<pre><code class="python hljs">
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train_model</span>(<span class="hljs-params">model, criterion, optimizer, scheduler, num_epochs=<span class="hljs-number">25</span></span>):</span><font></font>
    since = time.time()<font></font>
<font></font>
    best_model_wts = copy.deepcopy(model.state_dict())<font></font>
    best_acc = <span class="hljs-number">0.0</span><font></font>
<font></font>
    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(num_epochs):<font></font>
        print(<span class="hljs-string">'Epoch {}/{}'</span>.format(epoch, num_epochs - <span class="hljs-number">1</span>))<font></font>
        print(<span class="hljs-string">'-'</span> * <span class="hljs-number">10</span>)<font></font>
<font></font>
        <span class="hljs-comment"># Each epoch has a training and validation phase</span>
        <span class="hljs-keyword">for</span> phase <span class="hljs-keyword">in</span> [<span class="hljs-string">'train'</span>, <span class="hljs-string">'test'</span>]:
            <span class="hljs-keyword">if</span> phase == <span class="hljs-string">'train'</span>:<font></font>
                model.train()  <span class="hljs-comment"># Set model to training mode</span>
            <span class="hljs-keyword">else</span>:<font></font>
                model.eval()   <span class="hljs-comment"># Set model to evaluate mode</span>
            running_loss = <span class="hljs-number">0.0</span>
            running_corrects = <span class="hljs-number">0</span>
            <span class="hljs-comment"># Iterate over data.</span>
            <span class="hljs-keyword">for</span> inputs, labels <span class="hljs-keyword">in</span> dataloaders[phase]:<font></font>
                inputs = inputs.to(device)<font></font>
                labels = labels.to(device)<font></font>
                <span class="hljs-comment"># zero the parameter gradients</span><font></font>
                optimizer.zero_grad()<font></font>
                <span class="hljs-comment"># forward</span>
                <span class="hljs-comment"># track history if only in train</span>
                <span class="hljs-keyword">with</span> torch.set_grad_enabled(phase == <span class="hljs-string">'train'</span>):<font></font>
                    outputs = model(inputs)<font></font>
                    _, preds = torch.max(outputs, <span class="hljs-number">1</span>)<font></font>
                    loss = criterion(outputs, labels)<font></font>
                    <span class="hljs-comment"># backward + optimize only if in training phase</span>
                    <span class="hljs-keyword">if</span> phase == <span class="hljs-string">'train'</span>:<font></font>
                        loss.backward()<font></font>
                        optimizer.step()<font></font>
                <span class="hljs-comment"># statistics</span>
                running_loss += loss.item() * inputs.size(<span class="hljs-number">0</span>)<font></font>
                running_corrects += torch.sum(preds == labels.data)<font></font>
            <span class="hljs-keyword">if</span> phase == <span class="hljs-string">'train'</span>:<font></font>
                scheduler.step()<font></font>
            epoch_loss = running_loss / dataset_sizes[phase]<font></font>
            epoch_acc = running_corrects.double() / dataset_sizes[phase]<font></font>
            print(<span class="hljs-string">'{} Loss: {:.4f} Acc: {:.4f}'</span>.format(<font></font>
                phase, epoch_loss, epoch_acc))<font></font>
            <span class="hljs-comment"># deep copy the model</span>
            <span class="hljs-keyword">if</span> phase == <span class="hljs-string">'test'</span> <span class="hljs-keyword">and</span> epoch_acc &gt; best_acc:<font></font>
                best_acc = epoch_acc<font></font>
                best_model_wts = copy.deepcopy(model.state_dict())<font></font>
        print()<font></font>
    time_elapsed = time.time() - since<font></font>
    print(<span class="hljs-string">'Training complete in {:.0f}m {:.0f}s'</span>.format(<font></font>
        time_elapsed // <span class="hljs-number">60</span>, time_elapsed % <span class="hljs-number">60</span>))<font></font>
    print(<span class="hljs-string">'Best val Acc: {:4f}'</span>.format(best_acc))
    <span class="hljs-comment"># load best model weights</span><font></font>
    model.load_state_dict(best_model_wts)<font></font>
    <span class="hljs-keyword">return</span> model
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
このコードでは、コメントするのも難しいです。</font></font><br>
<br>
<pre><code class="python hljs">
<span class="hljs-comment"># Let's prepare the parameters for training the model</span>
dataloaders = {<span class="hljs-string">'train'</span>: trainloader, <span class="hljs-string">'test'</span>: testloader}<font></font>
model_ft = models.resnet18(pretrained=<span class="hljs-literal">True</span>)<font></font>
num_ftrs = model_ft.fc.in_features<font></font>
<span class="hljs-comment"># Here the size of each output sample is set to 2.</span>
<span class="hljs-comment"># Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).</span>
model_ft.fc = nn.Linear(num_ftrs, <span class="hljs-number">2</span>)<font></font>
model_ft = model_ft.to(device)<font></font>
criterion = nn.CrossEntropyLoss()<font></font>
<span class="hljs-comment"># Observe that all parameters are being optimized</span>
optimizer_ft = optim.SGD(model_ft.parameters(), lr=<span class="hljs-number">0.001</span>, momentum=<span class="hljs-number">0.9</span>)
<span class="hljs-comment"># Decay LR by a factor of 0.1 every 7 epochs</span>
exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=<span class="hljs-number">7</span>, gamma=<span class="hljs-number">0.1</span>)
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
しかし、それでも私は何かについてコメントします。</font></font><br>
<br>
<pre><code class="python hljs">dataloaders = {<span class="hljs-string">'train'</span>: trainloader, <span class="hljs-string">'test'</span>: testloader}</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
この辞書は、チュートリアルのモデルの学習機能で何にも触れないようにすると同時に、データセットを準備するために以前に作成した関数を保存するために必要でした。</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
そして、2番目の重要な点は、以前の場合と同様に、さらにクラスが必要な場合は、2つを置き換えます。</font></font><br>
<br>
<pre><code class="python hljs">model_ft.fc = nn.Linear(num_ftrs, <span class="hljs-number">2</span>)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
うまくいくと思います。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
モデルをトレーニングします。</font></font><br>
<br>
<pre><code class="python hljs">
<span class="hljs-comment">#Train the model</span><font></font>
model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,<font></font>
                       num_epochs=<span class="hljs-number">5</span>)
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
11時代の最初のモデルよりも悪くないエラーを取得するには、時代を減らしてください。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/f7/bc/ui/f7bcuiru_qqi3lnstqgnhdgkobw.png"></div><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
確かに、各時代には何倍も時間がかかります。</font><font style="vertical-align: inherit;">もちろん、より複雑なタスクについては、より多くの時代が必要になります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
元のチュートリアルの認識可能な画像の検証を、最初のモデルのテストサンプルの認識結果を視覚化するために使用したわずかに変更されたコードに置き換えました。</font></font><br>
<br>
<pre><code class="python hljs">
<span class="hljs-comment"># Visualization results of analysis test data</span>
correct = <span class="hljs-number">0</span>
total = <span class="hljs-number">0</span>
<span class="hljs-keyword">with</span> torch.no_grad():
    <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> testloader:<font></font>
        images, labels = data<font></font>
        outputs = model_ft(images)<font></font>
        _, predicted = torch.max(outputs.data, <span class="hljs-number">1</span>)<font></font>
      <font></font>
        <span class="hljs-keyword">for</span> printdata <span class="hljs-keyword">in</span> list(zip(predicted,labels,outputs)):<font></font>
            printclass =[classes[int(printdata[<span class="hljs-number">0</span>])],classes[int(printdata[<span class="hljs-number">1</span>])]]<font></font>
            print(<span class="hljs-string">'Predict class - {0}, real class - {1}, probability ({2},{3}) - {4}'</span>.format( printclass[<span class="hljs-number">0</span>],printclass[<span class="hljs-number">1</span>],<font></font>
                                                                              classes[<span class="hljs-number">0</span>], classes [<span class="hljs-number">1</span>],printdata[<span class="hljs-number">2</span>]))<font></font>
<font></font>
        total += labels.size(<span class="hljs-number">0</span>)<font></font>
        correct += (predicted == labels).sum().item()<font></font>
        imshow_resNet18(torchvision.utils.make_grid(images))<font></font>
        <span class="hljs-comment">#print('GroundTruth: ', ' '.join('%5s' % classes[predicted[j]] for j in range(4)))</span>
print(<span class="hljs-string">'Accuracy of the network on the'</span>, dataset_sizes[<span class="hljs-string">'test'</span>], <span class="hljs-string">'test images: %d %%'</span> % (
    <span class="hljs-number">100</span> * correct / total))
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
認識-完璧。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/lx/d6/uz/lxd6uziwug1btiwkll_zm0ewoji.png"></div><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
保存する必要があります。</font></font><br>
<br>
<pre><code class="python hljs">
<span class="hljs-comment"># Export model to onnx format</span>
PATH =os.path.join(dir, <span class="hljs-string">"my_resnet18.onnx"</span>)<font></font>
dummy_input = Variable(torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">224</span>, <span class="hljs-number">224</span>))<font></font>
torch.onnx.export(model_ft, dummy_input, PATH)<font></font>
</code></pre><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> パートV：労働の成果 </font></font></h2><br>
<a name="V"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
残っているのは、OpenCVに保存されたモデルを「フィード」することだけです。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
私は</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">この例</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">に導か</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">れました</font></a><font style="vertical-align: inherit;">が、原則として自分で簡単に書くことができます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
まず、ライブラリをインポートします。</font></font><br>
<br>
<pre><code class="python hljs">
<span class="hljs-comment"># import the necessary packages</span>
<span class="hljs-keyword">from</span> imutils.video <span class="hljs-keyword">import</span> VideoStream
<span class="hljs-keyword">from</span> imutils.video <span class="hljs-keyword">import</span> FPS
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> imutils
<span class="hljs-keyword">import</span> time
<span class="hljs-keyword">import</span> cv2
<span class="hljs-keyword">import</span> os
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
その後-準備段階。</font></font><br>
<br>
<pre><code class="python hljs">
path=os.path.join(os.path.abspath(os.curdir) , <span class="hljs-string">'my_model.onnx'</span>)<font></font>
args_confidence = <span class="hljs-number">0.2</span>
<span class="hljs-comment"># initialize the list of class labels </span>
CLASSES = [<span class="hljs-string">'raspberry'</span>, <span class="hljs-string">'someduino'</span>]
<span class="hljs-comment"># load our serialized model from disk</span>
print(<span class="hljs-string">"[INFO] loading model..."</span>)<font></font>
net = cv2.dnn.readNetFromONNX (path)<font></font>
<span class="hljs-comment"># initialize the video stream, allow the c</span>
<span class="hljs-comment">#cammera sensor to warmup,</span>
<span class="hljs-comment"># and initialize the FPS counter</span>
print(<span class="hljs-string">"[INFO] starting video stream..."</span>)<font></font>
vs = VideoStream(src=<span class="hljs-number">0</span>).start()<font></font>
time.sleep(<span class="hljs-number">2.0</span>)<font></font>
fps = FPS().start()<font></font>
frame = vs.read()<font></font>
frame = imutils.resize(frame, width=<span class="hljs-number">400</span>)
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
原則として、複雑なことは何もありません。</font><font style="vertical-align: inherit;">モデルの場所を指定し、手動で分類用のラベルを割り当て、モデルをロードし、Webカメラからの画像が表示されるウィンドウを初期化します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
次は、認識が行われるメインループです。</font></font><br>
<br>
<pre><code class="python hljs">
<span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:
	<span class="hljs-comment"># grab the frame from the threaded video stream and resize it</span>
	<span class="hljs-comment"># to have a maximum width of 400 pixels</span><font></font>
	frame = vs.read()<font></font>
	frame = imutils.resize(frame, width=<span class="hljs-number">400</span>)<font></font>
<font></font>
	<span class="hljs-comment"># grab the frame dimensions and convert it to a blob</span>
	(h, w) = frame.shape[:<span class="hljs-number">2</span>]<font></font>
	blob = cv2.dnn.blobFromImage(cv2.resize(frame, (<span class="hljs-number">32</span>, <span class="hljs-number">32</span>)),scalefactor=<span class="hljs-number">1.0</span>/<span class="hljs-number">32</span>
                              , size=(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>), mean= (<span class="hljs-number">128</span>,<span class="hljs-number">128</span>,<span class="hljs-number">128</span>), swapRB=<span class="hljs-literal">True</span>)<font></font>
	cv2.imshow(<span class="hljs-string">"Cropped image"</span>, cv2.resize(frame, (<span class="hljs-number">32</span>, <span class="hljs-number">32</span>)))<font></font>
<font></font>
	<span class="hljs-comment"># pass the blob through the network and obtain the detections and</span>
	<span class="hljs-comment"># predictions</span><font></font>
	net.setInput(blob)<font></font>
	detections = net.forward()<font></font>
	print(list(zip(CLASSES,detections[<span class="hljs-number">0</span>])))
	<span class="hljs-comment"># loop over the detections</span><font></font>
<font></font>
		<span class="hljs-comment"># extract the confidence (i.e., probability) associated with</span>
		<span class="hljs-comment"># the prediction</span>
	confidence = abs(detections[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]-detections[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>])<font></font>
	print(<span class="hljs-string">"confidence = "</span>, confidence)
		<span class="hljs-comment"># filter out weak detections by ensuring the `confidence` is</span>
		<span class="hljs-comment"># greater than the minimum confidence</span>
	<span class="hljs-keyword">if</span> (confidence &gt; args_confidence) :<font></font>
		<font></font>
		class_mark=np.argmax(detections)<font></font>
		cv2.putText(frame, CLASSES[class_mark], (<span class="hljs-number">30</span>,<span class="hljs-number">30</span>),cv2.FONT_HERSHEY_SIMPLEX, <span class="hljs-number">0.6</span>, (<span class="hljs-number">242</span>, <span class="hljs-number">230</span>, <span class="hljs-number">220</span>), <span class="hljs-number">2</span>)<font></font>
<font></font>
	<span class="hljs-comment"># show the output frame</span>
	cv2.imshow(<span class="hljs-string">"Web camera view"</span>, frame)<font></font>
	key = cv2.waitKey(<span class="hljs-number">1</span>) &amp; <span class="hljs-number">0xFF</span><font></font>
<font></font>
	<span class="hljs-comment"># if the `q` key was pressed, break from the loop</span>
	<span class="hljs-keyword">if</span> key == ord(<span class="hljs-string">"q"</span>):
		<span class="hljs-keyword">break</span><font></font>
<font></font>
	<span class="hljs-comment"># update the FPS counter</span><font></font>
	fps.update()<font></font>
<font></font>
<span class="hljs-comment"># stop the timer and display FPS information</span><font></font>
fps.stop()<font></font>
<span class="hljs-comment"># do a bit of cleanup</span><font></font>
cv2.destroyAllWindows()<font></font>
vs.stop()<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ここで説明する価値があると思います。 </font></font><br>
<br>
<pre><code class="python hljs">
blob = cv2.dnn.blobFromImage(cv2.resize(frame, (<span class="hljs-number">32</span>, <span class="hljs-number">32</span>)),scalefactor=<span class="hljs-number">1.0</span>/<span class="hljs-number">32</span>
                              , size=(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>), mean= (<span class="hljs-number">128</span>,<span class="hljs-number">128</span>,<span class="hljs-number">128</span>), swapRB=<span class="hljs-literal">True</span>)
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
これは、Jupyterノートブックで行ったのと同様に、画像をデータ配列に変換したものです。画像を32ピクセルに減らし、RGBチャネルの平均を設定します（0.5、0.5、0.5だったことを思い出してください）。任意のスケールを設定できます。これは、モデルの予測の数値の数にのみ影響します。これは、detections = net.forward（）を使用して取得します。</font></font><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">検出</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">はベクトルであり、最初の数値はオブジェクトが最初のクラス（ラズベリー）に起因する可能性であり、2番目の数値はそれぞれ2番目のクラスに帰属する確率です。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
それで、何が得られるか見てみましょう。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
このコードを実行するために、Anacondaに含まれているSpiderを使用しました。 OpenCV 4thバージョンもインストールしました。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Arduinoから始めましょう。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/ne/nv/rk/nenvrkbtucs24w_anwaxpnohppg.png"></div><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
トレーニングセットにはさまざまなタイプのArduinoのようなデバイスがあったため、モデルはCraftDuino v 1.0も認識します。</font><font style="vertical-align: inherit;">そしてArduino Nanoは私の友達</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" class="user_link"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Drzugrik</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">他のコンポーネントと共にボードにはんだ付けされます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
私はライブラズベリーを持っていないので、チラシの写真を認識しています（スマートフォンの画面からも可能です）。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/zk/3r/ee/zk3reekem8wnjhtl0jpofaxzvbs.png"></div><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
それでは、最初のモデルが対処しました。2番目のモデルがどのように対処するかを見てみましょう。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
2番目のモデルのコードは、画像変換パラメーターを除いてほとんど同じです。</font><font style="vertical-align: inherit;">したがって、スポイラーの下に隠します。</font></font><br>
<br>
<div class="spoiler"><b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">モデルResnet18のコード</font></font></b><div class="spoiler_text"><pre><code class="python hljs">
<span class="hljs-comment">#based on https://proglib.io/p/real-time-object-detection/</span>
<span class="hljs-comment"># import the necessary packages</span>
<span class="hljs-keyword">from</span> imutils.video <span class="hljs-keyword">import</span> VideoStream
<span class="hljs-keyword">from</span> imutils.video <span class="hljs-keyword">import</span> FPS
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> imutils
<span class="hljs-keyword">import</span> time
<span class="hljs-keyword">import</span> cv2
<span class="hljs-keyword">import</span> os<font></font>
path=os.path.join(os.path.abspath(os.curdir) , <span class="hljs-string">'my_resnet18.onnx'</span>)<font></font>
args_confidence = <span class="hljs-number">0.2</span><font></font>
<font></font>
<span class="hljs-comment"># initialize the list of class labels </span>
CLASSES = [<span class="hljs-string">'raspberry'</span>, <span class="hljs-string">'someduino'</span>]<font></font>
<font></font>
<span class="hljs-comment"># load our serialized model from disk</span>
print(<span class="hljs-string">"[INFO] loading model..."</span>)<font></font>
net = cv2.dnn.readNetFromONNX (path)<font></font>
<font></font>
<span class="hljs-comment"># initialize the video stream, allow the c</span>
<span class="hljs-comment">#cammera sensor to warmup,</span>
<span class="hljs-comment"># and initialize the FPS counter</span>
print(<span class="hljs-string">"[INFO] starting video stream..."</span>)<font></font>
vs = VideoStream(src=<span class="hljs-number">0</span>).start()<font></font>
time.sleep(<span class="hljs-number">2.0</span>)<font></font>
fps = FPS().start()<font></font>
<font></font>
frame = vs.read()<font></font>
frame = imutils.resize(frame, width=<span class="hljs-number">400</span>)<font></font>
<font></font>
<span class="hljs-comment"># loop over the frames from the video stream</span>
<span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:
	<span class="hljs-comment"># grab the frame from the threaded video stream and resize it</span>
	<span class="hljs-comment"># to have a maximum width of 400 pixels</span><font></font>
	frame = vs.read()<font></font>
	frame = imutils.resize(frame, width=<span class="hljs-number">400</span>)<font></font>
<font></font>
	<span class="hljs-comment"># grab the frame dimensions and convert it to a blob</span>
	(h, w) = frame.shape[:<span class="hljs-number">2</span>]<font></font>
	blob = cv2.dnn.blobFromImage(cv2.resize(frame, (<span class="hljs-number">224</span>, <span class="hljs-number">224</span>)),scalefactor=<span class="hljs-number">1.0</span>/<span class="hljs-number">224</span>
                              , size=(<span class="hljs-number">224</span>, <span class="hljs-number">224</span>), mean= (<span class="hljs-number">104</span>, <span class="hljs-number">117</span>, <span class="hljs-number">123</span>), swapRB=<span class="hljs-literal">True</span>)<font></font>
	cv2.imshow(<span class="hljs-string">"Cropped image"</span>, cv2.resize(frame, (<span class="hljs-number">224</span>, <span class="hljs-number">224</span>)))<font></font>
<font></font>
	<span class="hljs-comment"># pass the blob through the network and obtain the detections and</span>
	<span class="hljs-comment"># predictions</span><font></font>
	net.setInput(blob)<font></font>
	detections = net.forward()<font></font>
	print(list(zip(CLASSES,detections[<span class="hljs-number">0</span>])))
	<span class="hljs-comment"># loop over the detections</span><font></font>
<font></font>
		<span class="hljs-comment"># extract the confidence (i.e., probability) associated with</span>
		<span class="hljs-comment"># the prediction</span>
	confidence = abs(detections[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]-detections[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>])<font></font>
	print(confidence)<font></font>
		<span class="hljs-comment"># filter out weak detections by ensuring the `confidence` is</span>
		<span class="hljs-comment"># greater than the minimum confidence</span>
	<span class="hljs-keyword">if</span> (confidence &gt; args_confidence) :<font></font>
		<font></font>
		class_mark=np.argmax(detections)<font></font>
		cv2.putText(frame, CLASSES[class_mark], (<span class="hljs-number">30</span>,<span class="hljs-number">30</span>),cv2.FONT_HERSHEY_SIMPLEX, <span class="hljs-number">0.6</span>, (<span class="hljs-number">242</span>, <span class="hljs-number">230</span>, <span class="hljs-number">220</span>), <span class="hljs-number">2</span>)<font></font>
<font></font>
	<span class="hljs-comment"># show the output frame</span>
	cv2.imshow(<span class="hljs-string">"Web camera view"</span>, frame)<font></font>
	key = cv2.waitKey(<span class="hljs-number">1</span>) &amp; <span class="hljs-number">0xFF</span><font></font>
<font></font>
	<span class="hljs-comment"># if the `q` key was pressed, break from the loop</span>
	<span class="hljs-keyword">if</span> key == ord(<span class="hljs-string">"q"</span>):
		<span class="hljs-keyword">break</span><font></font>
<font></font>
	<span class="hljs-comment"># update the FPS counter</span><font></font>
	fps.update()<font></font>
<font></font>
<span class="hljs-comment"># stop the timer and display FPS information</span><font></font>
fps.stop()<font></font>
<span class="hljs-comment"># do a bit of cleanup</span><font></font>
cv2.destroyAllWindows()<font></font>
vs.stop()<font></font>
</code></pre><br>
</div></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
このモデルは少なくとも同じように認識されると私は言っています。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
したがって、より複雑なタスクを準備します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
つまり、Raspberry piを認識します。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/s-/dl/o0/s-dlo0z9v7y3vtvxuf3p466yhwa.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
まあ、すべてはそれほど単純ではありませんが、それはかなり予想されますが、「ラズベリー」という名前にもかかわらず、「ラズベリーはバラと呼んでも、それをバラと呼んでいる」ため、Arduinoに似ています。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
最終的に。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
このソリューションには大きな欠点が1つあります。認識のために間違ったモデルを選択しました。したがって、カメラがボイドに目を向けても、オブジェクトをラズベリーまたはコントローラーとして分類しようとします。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/av/bt/r-/avbtr-nflcvxkt9h5obzbhhia40.png"></div><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
色付きのフレームで画像を認識するためには、分類のためのモデルだけでなく、画像内のオブジェクトを認識するためのモデルを使用する必要があります。</font><font style="vertical-align: inherit;">しかし、これは完全に別の話です。これについては、少し後で説明します。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/4o/em/ga/4oemgacrdecwhu0cmzflgik0gro.png"></div></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../ja478196/index.html">OpenStreetMap No. 487の世界からのニュース（11/12/2019-11/18/2019）</a></li>
<li><a href="../ja478198/index.html">Raspberry Pi Apocalypse Survival Computer</a></li>
<li><a href="../ja478200/index.html">一般的なスクラム製品の所有者の間違い</a></li>
<li><a href="../ja478202/index.html">英国のデジタルタレントビザ：個人的な経験</a></li>
<li><a href="../ja478204/index.html">3つの立方体の合計が非常に難しい数学の問題である理由</a></li>
<li><a href="../ja478210/index.html">仕事と開発の楽しみ（メンタル）＃1。私が6月だったときにこれを教えられたことに感謝しました</a></li>
<li><a href="../ja478216/index.html">モバイルデベロッパー向けの興味深い資料の要約＃324（11月24日-12月1日）</a></li>
<li><a href="../ja478218/index.html">ヨーロッパは独自のクラウドインフラストラクチャを開発する-これが良いアイデアであると誰もが信じているわけではない</a></li>
<li><a href="../ja478220/index.html">エントロピー管理</a></li>
<li><a href="../ja478224/index.html">量子エンタングルメントの本当にわかりやすい説明（EPRパラドックス）</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>