<!doctype html>
<html class="no-js" lang="es">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üç´ ü§öüèæ üßú Aprendizaje autom√°tico de Unity: ense√±ar a los agentes de MO a saltar por encima de las paredes ‚úäüèø ü§µüèª ‚Ü™Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ha habido grandes avances en el aprendizaje por refuerzo (RL) en los √∫ltimos a√±os, desde el primer uso exitoso en el entrenamiento de p√≠xeles en bruto...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Aprendizaje autom√°tico de Unity: ense√±ar a los agentes de MO a saltar por encima de las paredes</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/pixonic/blog/492548/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ha habido grandes avances en el aprendizaje por refuerzo (RL) en los √∫ltimos a√±os, desde el primer uso exitoso en el entrenamiento de p√≠xeles en bruto hasta el entrenamiento de roboristas de IA abierta, y se necesitan entornos cada vez m√°s sofisticados para un mayor progreso, lo que ayuda La unidad viene. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La herramienta Unity ML-Agents es un nuevo complemento en el motor del juego Unity, que le permite utilizar Unity como un constructor de entorno para entrenar agentes de MO. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Desde jugar f√∫tbol hasta caminar, saltar desde las paredes y aprender a perseguir la IA con un palo, Unity ML-Agents Toolkit ofrece una amplia gama de condiciones de entrenamiento para los agentes. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En este art√≠culo, veremos c√≥mo funcionan los agentes de Unity MO y luego le ense√±aremos a uno de estos agentes a saltar por encima de las paredes.</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/9c6/5ad/bdb/9c65adbdbb38b939262c10e5ac728dde.gif" alt="imagen"><br>
<br>
<a name="habracut"></a><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">¬øQu√© es Unity ML-Agents?</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Unity ML-Agents es un nuevo complemento para el motor del juego Unity, que le permite crear o utilizar entornos listos para entrenar a nuestros agentes. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El complemento consta de tres componentes: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/zo/mx/m_/zomxm_3g9svsej0iarjpw1vbnta.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
el primero: un </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">entorno de aprendizaje</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ( </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">el entorno de aprendizaje</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ), que contiene escenas de Unity y elementos ambientales. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El segundo es la </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">API de Python</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , en la que se encuentran los algoritmos RL (como PPO - Optimizaci√≥n de pol√≠tica proximal y SAC - Actor-cr√≠tico suave). </font><font style="vertical-align: inherit;">Usamos esta API para lanzar capacitaci√≥n, pruebas, etc. Est√° conectada al entorno de aprendizaje a trav√©s del tercer componente: </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">un comunicador externo</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En qu√© consiste el ambiente de aprendizaje</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El componente de capacitaci√≥n consta de varios elementos: </font></font><br>
 <br>
<img src="https://habrastorage.org/webt/hy/1u/pt/hy1upt9en2esnt-amk-iipklmn4.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
el primer agente es el actor de escena. </font><font style="vertical-align: inherit;">Es a √©l a quien entrenaremos optimizando un componente llamado "Cerebro", en el que se registran las acciones que se deben realizar en cada uno de los estados posibles. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El tercer elemento, la Academia, gestiona los agentes y sus procesos de toma de decisiones y procesa las solicitudes de la API de Python. </font><font style="vertical-align: inherit;">Para comprender mejor su papel, recordemos el proceso de RL. </font><font style="vertical-align: inherit;">Se puede representar como un ciclo que funciona de la siguiente manera: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/5y/ge/ef/5ygeefzasafou6bkwggdnaq7ows.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
supongamos que un agente necesita aprender a jugar un juego de plataformas. </font><font style="vertical-align: inherit;">El proceso de RL en este caso se ver√° as√≠:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El agente recibe el estado </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">S </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0</font></font></sub></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> del entorno: este ser√° el primer fotograma de nuestro juego.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Seg√∫n el estado </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">S </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0, el</font></font></sub></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> agente realiza la acci√≥n </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0</font></font></sub></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y se desplaza hacia la derecha.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El medio ambiente entra en un nuevo estado </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">S </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El agente recibe </font><i><sub><font style="vertical-align: inherit;">una</font></sub></i><font style="vertical-align: inherit;"> recompensa </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">R </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> por no estar muerto ( </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Recompensa positiva</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> +1).</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Este ciclo RL forma una secuencia de estado, acci√≥n y recompensa. </font><font style="vertical-align: inherit;">El objetivo del agente es maximizar la recompensa total esperada. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/qm/mr/ml/qmmrmlkz7urj8cywqpsg1ejajve.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Por lo tanto, Academy env√≠a instrucciones a los agentes y proporciona sincronizaci√≥n en su ejecuci√≥n, a saber:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Colecci√≥n de observaciones;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La elecci√≥n de la acci√≥n de acuerdo con las instrucciones establecidas;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ejecuci√≥n de acciones;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Restablezca si se ha alcanzado el n√∫mero de pasos o si se ha alcanzado el objetivo.</font></font></li>
</ul><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ense√±amos al agente a saltar a trav√©s de las paredes.</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ahora que sabemos c√≥mo funcionan los agentes de Unity, entrenaremos a uno para saltar a trav√©s de las paredes. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Los modelos ya entrenados tambi√©n se pueden descargar en </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GitHub</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Entorno de aprendizaje de salto de pared</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El objetivo de este entorno es ense√±arle al agente a llegar al mosaico verde. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Considere tres casos: </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
1. No hay paredes, y nuestro agente solo necesita llegar al mosaico. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/17a/8fd/da8/17a8fdda8bbe767f64a34d8d05fd9be9.png" alt="imagen"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
2. El agente necesita aprender a saltar para alcanzar el mosaico verde. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/d2c/149/0d0/d2c1490d0b90c4fdefb8eb4922f446b2.png" alt="imagen"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
3. El caso m√°s dif√≠cil: el muro es demasiado alto para que el agente salte, por lo que primero debe saltar sobre el bloque blanco. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/458/175/9b6/4581759b653a175597054fcf60a0050e.png" alt="imagen"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Le ense√±aremos al agente dos escenarios de comportamiento dependiendo de la altura del muro:</font></font><br>
<br>
<ul>
<li><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SmallWallJump</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> en casos sin paredes o a baja altura de pared;</font></font></li>
<li><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">BigWallJump</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> en el caso de paredes altas.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As√≠ es como se ver√° el sistema de recompensas: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/rt/lo/hm/rtlohmnturcucyikrnhfsadztgi.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
en nuestras observaciones, no estamos usando un marco regular, sino 14 reykast, cada uno de los cuales puede detectar 4 posibles objetos. </font><font style="vertical-align: inherit;">En este caso, reykast puede ser percibido como rayos l√°ser que pueden determinar si pasan a trav√©s de un objeto. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Tambi√©n utilizaremos la posici√≥n de agente global en nuestro programa. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/a06/0fc/fa2/a060fcfa26e80154ef988d76dc9b8f05.png" alt="imagen"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Hay cuatro opciones posibles en nuestro espacio: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ni/g6/zs/nig6zskfzterisy_hskg3k-rsnk.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
el objetivo es lograr un mosaico verde </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">con una recompensa promedio de 0.8</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">¬°Entonces empecemos!</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En primer lugar, abra el proyecto </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">UnitySDK</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Entre los ejemplos que necesita para encontrar y abrir la escena </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">WallJump</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Como puede ver, hay muchos agentes en el escenario, cada uno de los cuales se toma del mismo prefabricado, y todos tienen el mismo "cerebro". </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/bad/195/820/bad195820da3e5b2c74309e9f44e9935.png" alt="imagen"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Como en el caso del cl√°sico Deep Reinforcement Learning, despu√©s de lanzar varias instancias del juego (por ejemplo, 128 entornos paralelos), ahora solo copiamos y pegamos los agentes para tener estados m√°s diferentes. Y dado que queremos entrenar a nuestro agente desde cero, antes que nada necesitamos eliminar el "cerebro" del agente. Para hacer esto, vaya a la carpeta prefabricados y abra Prefab.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A continuaci√≥n, en la jerarqu√≠a Prefabricada, debe seleccionar el agente e ir a la configuraci√≥n. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En los par√°metros de comportamiento, debe eliminar el modelo. Si tenemos varias GPU a nuestra disposici√≥n, puede usar el dispositivo de inferencia de la CPU como GPU. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/d0d/bdb/6ec/d0dbdb6ec05655b6fecdf675d629d126.png" alt="imagen"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En el componente Agente de salto de pared, debe eliminar los cerebros para un caso sin paredes, as√≠ como para paredes bajas y altas. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/618/e81/c8c/618e81c8c3c0cb15ae5fbca791c33c8c.png" alt="imagen"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Despu√©s de eso, puede comenzar a entrenar a su agente desde cero. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para nuestro primer entrenamiento, simplemente cambiamos el n√∫mero total de pasos de entrenamiento para dos escenarios de comportamiento: SmallWallJump y BigWallJump. Entonces podemos lograr el objetivo en solo 300 mil pasos. Para hacer esto, en </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">config / trainer config.yaml,</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> cambie max_steps a 3e5 para los casos SmallWallJump y BigWallJump.</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/b80/bbb/1b2/b80bbb1b21f0f9e445fa8d2583c39d52.png" alt="imagen"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para capacitar a nuestro agente, utilizaremos </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PPO</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (Optimizaci√≥n de pol√≠tica proximal). El algoritmo incluye la acumulaci√≥n de experiencia en la interacci√≥n con el entorno y su uso para actualizar las pol√≠ticas de toma de decisiones. Despu√©s de actualizarlo, los eventos anteriores se descartan y la recopilaci√≥n de datos posterior ya se lleva a cabo seg√∫n los t√©rminos de la pol√≠tica actualizada. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Entonces, primero, usando la API de Python, necesitamos llamar a un comunicador externo para que instruya a la Academia a lanzar agentes. Para hacer esto, abra el terminal donde se encuentra ml-agents-master y escr√≠balo: </font></font><br>
<br>
<code>mlagents-learn config/trainer_config.yaml ‚Äî run-id=‚ÄùWallJump_FirstTrain‚Äù ‚Äî train</code><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
este comando le pedir√° que inicie la escena de Unity. Para hacer esto, presione ‚ñ∫ en la parte superior del editor. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/212/874/4fb/2128744fb143fec6308d95938e0383f7.png" alt="imagen"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Puede ver la capacitaci√≥n de sus agentes en Tensorboard con el siguiente comando:</font></font><br>
<br>
<code>tensorboard ‚Äî logdir=summaries</code><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Cuando finaliza la capacitaci√≥n, debe mover los archivos de modelo guardados en </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ml-agents-master / models a UnitySDK / Assets / ML-Agents / examples / WallJump / TFModels</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Luego, abra nuevamente el editor de Unity y seleccione la escena </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">WallJump</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , donde </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">abriremos el</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> objeto </font><i><font style="vertical-align: inherit;">WallJumpArea</font></i><font style="vertical-align: inherit;"> terminado </font><font style="vertical-align: inherit;">. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Despu√©s de eso, seleccione el agente y en sus par√°metros de comportamiento arrastre el archivo </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SmallWallJump.nn</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> al </font><i><font style="vertical-align: inherit;">Marcador de posici√≥n</font></i><font style="vertical-align: inherit;"> del modelo. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/d0d/bdb/6ec/d0dbdb6ec05655b6fecdf675d629d126.png" alt="imagen"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Tambi√©n mover:</font></font><br>
<br>
<ol>
<li><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SmallWallJump.nn</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> en No Wall Brain Placeholder.</font></font></li>
<li><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SmallWallJump.nn</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> en Small Wall Brain Placeholder.</font></font></li>
<li><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">BigWallJump.nn</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> en No Wall Brain Placeholder.</font></font></li>
</ol><br>
<img src="https://habrastorage.org/getpro/habr/post_images/618/e81/c8c/618e81c8c3c0cb15ae5fbca791c33c8c.png" alt="imagen"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Despu√©s de eso, presiona el bot√≥n ‚ñ∫ en la parte superior del editor y listo. </font><font style="vertical-align: inherit;">El algoritmo de configuraci√≥n de entrenamiento del agente ahora est√° completo.</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/9c6/5ad/bdb/9c65adbdbb38b939262c10e5ac728dde.gif" alt="imagen"><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tiempo de experimento</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La mejor manera de aprender es intentar constantemente traer algo nuevo. </font><font style="vertical-align: inherit;">Ahora que ya hemos logrado buenos resultados, intentaremos poner algunas hip√≥tesis y probarlas.</font></font><br>
<br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Reduciendo el coeficiente de descuento a 0.95</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Entonces sabemos que:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cuanto mayor sea la gamma, menor ser√° el descuento. </font><font style="vertical-align: inherit;">Es decir, el agente est√° m√°s preocupado por las recompensas a largo plazo.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Por otro lado, cuanto m√°s peque√±a es la gamma, mayor es el descuento. </font><font style="vertical-align: inherit;">En este caso, la prioridad del agente es la compensaci√≥n a corto plazo.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La idea de este experimento es que si aumentamos el descuento al disminuir la gama de 0,99 a 0,95, la recompensa a corto plazo ser√° una prioridad para el agente, lo que puede ayudarlo a acercarse r√°pidamente a la pol√≠tica de comportamiento √≥ptima. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/u_/cm/hv/u_cmhvpc0dk-npolzz49loxofiq.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Curiosamente, en el caso de un salto a trav√©s de una pared baja, el agente se esforzar√° por obtener el mismo resultado. </font><font style="vertical-align: inherit;">Esto puede explicarse por el hecho de que este caso es bastante simple: el agente solo necesita moverse al mosaico verde y, si es necesario, saltar si hay una pared al frente. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/yc/ic/tt/ycicttb0mkwxmwn5c1ghgm2iiiu.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Por otro lado, en el caso de Big Wall Jump, esto funciona peor, porque nuestro agente se preocupa m√°s por la recompensa a corto plazo y, por lo tanto, no entiende que necesita subir al bloque blanco para saltar sobre el muro.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mayor complejidad de la red neuronal</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Finalmente, planteamos la hip√≥tesis de si nuestro agente se volver√° m√°s inteligente si aumentamos la complejidad de la red neuronal. </font><font style="vertical-align: inherit;">Para hacer esto, aumente el tama√±o del nivel oculto de 256 a 512. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Y encontramos que en este caso el nuevo agente funciona peor que nuestro primer agente. </font><font style="vertical-align: inherit;">Esto significa que no tiene sentido aumentar la complejidad de nuestra red, porque de lo contrario el tiempo de aprendizaje tambi√©n aumentar√°. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/zo/g7/qm/zog7qm1uemu7l7taqc5uuwlqauw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Entonces, entrenamos al agente para saltar sobre las paredes, y eso es todo por hoy. </font><font style="vertical-align: inherit;">Recuerde que para comparar los resultados, los modelos entrenados se pueden descargar </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">aqu√≠</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/af6/f00/ce3/af6f00ce3784c60469522296276d2651.gif" alt="imagen"></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es492530/index.html">MVVM basados ‚Äã‚Äãen combinaci√≥n en aplicaciones UIKit y SwiftUI para desarrolladores de UIKit</a></li>
<li><a href="../es492534/index.html">Entonces, ¬øhay huracanes reales en Mosc√∫ o no? Analizamos el caso del 13 de marzo de 2020 en persecuci√≥n</a></li>
<li><a href="../es492538/index.html">Wrike TechClub: Infraestructura de entrega: procesos y herramientas (DevOps + QAA). Papeles en ingles</a></li>
<li><a href="../es492540/index.html">El juego "¬°Espera un momento!" en arduino</a></li>
<li><a href="../es492546/index.html">Comprobaci√≥n de la vulnerabilidad de cualquier sitio que use Nikto</a></li>
<li><a href="../es492552/index.html">C√≥mo vivir y trabajar en cuarentena en Barcelona</a></li>
<li><a href="../es492558/index.html">Hola, esto es COVID19: ¬øEl coronavirus vive en la superficie de un tel√©fono inteligente?</a></li>
<li><a href="../es492560/index.html">Tabla hash simple para GPU</a></li>
<li><a href="../es492562/index.html">Tres √∫tiles webinars de Apache Ignite en su programa de cuarentena</a></li>
<li><a href="../es492566/index.html">An√°lisis de la combinaci√≥n de un algoritmo de b√∫squeda de clic codicioso con enumeraci√≥n parcial de v√©rtices de gr√°ficos</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>