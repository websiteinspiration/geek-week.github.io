<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ… ğŸ”§ ğŸ’‚ Transformer in pictures ğŸ¤›ğŸ¼ ğŸ†’ ğŸƒ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In a previous article, we examined the attention mechanism, an extremely common method in modern deep learning models that can improve the performance...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Transformer in pictures</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/486358/"><p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In a </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">previous article,</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> we examined the attention mechanism, an extremely common method in modern deep learning models that can improve the performance indicators of neural machine translation applications. In this article, we will look at Transformer, a model that uses the attention mechanism to increase learning speed. Moreover, for a number of tasks, Transformers outperform Googleâ€™s neural machine translation model. However, the biggest advantage of Transformers is their high efficiency in parallelization conditions. Even Google Cloud recommends using Transformer as a model when working on </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cloud TPU</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Let's try to figure out what the model consists of and what functions it performs.</font></font></p><br>
<p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The Transformer model was first proposed in the article </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Attention is All You Need</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">An implementation on TensorFlow is available as part of the </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tensor2Tensor</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> package </font><font style="vertical-align: inherit;">, in addition, a group of NLP researchers from Harvard created a </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">guide annotation of the article with an implementation on PyTorch</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">In this same guide, we will try to most simply and consistently outline the main ideas and concepts, which, we hope, will help people who do not have a deep knowledge of the subject area to understand this model.</font></font></p><a name="habracut"></a><br>
<h1 id="vysokourovnevnyy-obzor"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">High Level Review</font></font></h1><br>
<p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Let's look at the model as a kind of black box. </font><font style="vertical-align: inherit;">In machine translation applications, it accepts a sentence in one language as input and displays a sentence in another.</font></font></p><br>
<p><img src="https://habrastorage.org/webt/vo/iv/w7/voivw7zwvgnsmmjzbxkn8ypdo2s.png" alt="the_transformer_3"></p><br>
<p>     ,  ,       ,      .</p><br>
<p><img src="https://habrastorage.org/webt/mj/3a/qq/mj3aqqrifxbnb8f5gv_r5jbdf7g.png" alt="The_transformer_encoders_decoders"></p><br>
<p>  â€“   ;      6 ,     (  6   ,       ).   â€“   ,     .</p><br>
<p><img src="https://habrastorage.org/webt/-y/7z/tl/-y7ztlhyq5_817b89qhjjikwopk.png" alt="The_transformer_encoder_decoder_stack"></p><br>
<p>    ,     .      :</p><br>
<p><img src="https://habrastorage.org/webt/2_/bq/if/2_bqifdpdhluoadxjo_ncvpgdcm.png" alt="Transformer_encoder"></p><br>
<p> ,   ,       (self-attention),              .       .</p><br>
<p>          (feed-forward neural network).           .</p><br>
<p>     ,      ,          (   ,      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="> seq2seq</a>).</p><br>
<p><img src="https://habrastorage.org/webt/34/f7/lm/34f7lmnxvdcchib66hhxbdx8vzg.png" alt="Transformer_decoder"></p><br>
<h1 id="na-scenu-vyhodyat-tenzory">   </h1><br>
<p>,      ,    /,        ,       .</p><br>
<p>     NLP-,    ,     ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="> </a> (word embeddings).</p><br>
<p><img src="https://habrastorage.org/webt/po/lw/jl/polwjl7ap50biqcjrwegkczazy4.png" alt="embeddings"></p><br>
<p><em>      512.         .</em></p><br>
<p>      .   ,    ,  :      512 (       ,   â€“    ).      ,    , ,  ,        .</p><br>
<p>        ,          .</p><br>
<p><img src="https://habrastorage.org/webt/a8/o9/z-/a8o9z-cte22jxysrcoqqfmbgyqu.png" alt="encoder_with_tensors"></p><br>
<p>      :         . ,          ,        ,            .</p><br>
<p>  ,           .</p><br>
<h1 id="teper-kodiruem"> !</h1><br>
<p>   ,        ,         â€“     , , ,      .</p><br>
<p><img src="https://habrastorage.org/webt/s7/qm/ln/s7qmln-nbaxuocg1rswbf6fjang.png" alt="encoder_with_tensors_2"></p><br>
<p><em>         .       ,       .</em></p><br>
<h1 id="vysokourovnevyy-vzglyad-na-mehanizm-vnutrennego-vnimaniya">     </h1><br>
<p> ,   Â« Â»    -,     .            ,   Â«Attention is All You NeedÂ».  ,   .</p><br>
<p>   â€“   ,    :</p><br>
<blockquote>â€The animal didn't cross the street because it was too tiredâ€</blockquote><p>   Â«itÂ»   ?   (street)    (animal)?         .</p><br>
<p>    Â«itÂ»,     ,  Â«itÂ»   Â«animalÂ».</p><br>
<p>        (    ),             ,     .</p><br>
<p>       (RNN), ,      RNN     /,    ,    .    â€“  ,     ,   Â«Â»       .</p><br>
<p><img src="https://habrastorage.org/webt/pz/li/tj/pzlitjsvdvi0ehs5yuw0jmqfoje.png" alt="transformer_self-attention_visualization"></p><br>
<p><em>   Â«itÂ»   #5 (   ),      Â«The animalÂ»        Â«itÂ».</em></p><br>
<p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=">  Tensor2Tensor</a>,         ,    .</p><br>
<h1 id="mehanizm-vnutrennego-vnimaniya-v-detalyah">    </h1><br>
<p>  ,       ,     ,         .</p><br>
<p><strong> </strong>     â€“         (   â€“   ):   (Query vector),   (Key vector)    (Value vector).          ,       .</p><br>
<p>,       ,   .    64,       /     512.     ,                 (multi-head attention)  .</p><br>
<p><img src="https://habrastorage.org/webt/d1/o4/fd/d1o4fdyczdrehbvczb2m2d3whcs.png" alt="transformer_self_attention_vectors"></p><br>
<p><em> x1    WQ  q1,  Â«Â»,    .      Â«Â», Â«Â»  Â«Â»      .</em></p><br>
<p>    Â«Â», Â«Â»  Â«Â»?</p><br>
<p> ,         .  ,   ,     ,           .</p><br>
<p><strong> </strong>    â€“   (score). ,           â€“ Â«ThinkingÂ».             .  ,               .</p><br>
<p>            .  ,          #1,      q1  k1,  â€”   q1  k2.</p><br>
<p><img src="https://habrastorage.org/webt/c_/wx/af/c_wxaft2ynanknfd_osg0842_us.png" alt="transformer_self_attention_score"></p><br>
<p><strong>   </strong> â€“     8 (    ,    â€“ 64;          ,      ),        (softmax).     ,         1.</p><br>
<p><img src="https://habrastorage.org/webt/em/kp/4s/emkp4swx5jqhbqljq7tpm4ne-t0.png" alt="self-attention_softmax"></p><br>
<p> - (softmax score) ,            . ,        -,       ,   .</p><br>
<p><strong> </strong> â€“      - (  ).   :      ,    ,        (    , , 0.001).</p><br>
<p><strong> </strong> â€“    .             (  ).</p><br>
<p><img src="https://habrastorage.org/webt/tp/wo/s0/tpwos0dakwx0ebolkn3hzem3wsi.png" alt="self-attention-output"></p><br>
<p>     .     ,         .   , ,          . ,       ,    .</p><br>
<h1 id="matrichnye-vychisleniya-vnutrennego-vnimaniya">   </h1><br>
<p><strong> </strong> â€“   ,   .         X      ,    (WQ, WK, WV).</p><br>
<p><img src="https://habrastorage.org/webt/i_/ei/5g/i_ei5g0b1yt95qxkqve1jxdumrq.png" alt="self-attention-matrix-calculation"></p><br>
<p><em>         .         (512,  4   )   q/k/v (64,  3 ).</em></p><br>
<p><strong></strong>,      ,     2-6         .</p><br>
<p><img src="https://habrastorage.org/webt/ms/wn/cx/mswncxi-t4jjxh9mwybx9e7qmug.png" alt="self-attention-matrix-calculation-2"></p><br>
<p><em>     .</em></p><br>
<h1 id="mnogogolovaya-gidra"> </h1><br>
<p>         ,    (multi-head attention).           :</p><br>
<ol>
<li>      . ,   , z1      ,        .       Â«The animal didnâ€™t cross the street because it was too tiredÂ»,   ,     Â«itÂ».</li>
<li>    Â« Â» (representation subspaces).    ,         ,     // (  8 Â«Â» ,        8    /).       .           (    /)    . </li>
</ol><br>
<p><img src="https://habrastorage.org/webt/c-/jh/8n/c-jh8nfwmfjoiciycph9wsywqf0.png" alt="transformer_attention_heads_qkv"></p><br>
<p><em>   ,    WQ/WK/WV     Â«Â»,      Q/K/V .    ,    WQ/WK/WV    Q/K/V .</em></p><br>
<p>     ,    , 8     ,    8  Z .</p><br>
<p><img src="https://habrastorage.org/webt/uc/0b/hi/uc0bhij9fzgajpjhem0sia2mna8.png" alt="transformer_attention_heads_z"></p><br>
<p>    .      ,     8  â€“     (   ),        Z .</p><br>
<p>  ?          WO.</p><br>
<p><img src="https://habrastorage.org/webt/s1/d7/md/s1d7md5obt_ps-nzg-adgfhr2ks.png" alt="transformer_attention_heads_weight_matrix_o"></p><br>
<p>  ,  ,       . ,    .        ,       .</p><br>
<p><img src="https://habrastorage.org/webt/he/j3/f1/hej3f1fkxked2nrlj_xnz9k3vmy.png" alt="transformer_multi-headed_self-attention-recap"></p><br>
<p>,    Â«Â» ,      ,  ,     Â«Â»    Â«itÂ»   :</p><br>
<p><img src="https://habrastorage.org/webt/t9/ax/jv/t9axjvckxd8xqxtpcygunzpf8ty.png" alt="transformer_self-attention_visualization_2"></p><br>
<p><em>      Â«itÂ»,  Â«Â»      Â«the animalÂ»,      â€”  Â«tiredÂ».  ,     Â«itÂ»      Â«animalÂ»  Â«tiredÂ».</em></p><br>
<p>    Â«Â»    , ,    .</p><br>
<p><img src="https://habrastorage.org/webt/w1/z1/po/w1z1pou6afrqqohpzswfk6kdnos.png" alt="transformer_self-attention_visualization_3"></p><br>
<h1 id="predstavlenie-poryadka-v-posledovatelnosti-s-pomoschyu-pozicionnogo-kodirovaniya">       </h1><br>
<p>        â€”       .</p><br>
<p>           .     ,                 .     ,                  Q/K/V       .</p><br>
<p><img src="https://habrastorage.org/webt/ee/bv/n7/eebvn7g_nwsete4fzv4jcuguz4c.png" alt="transformer_positional_encoding_vectors"></p><br>
<p><em> ,     ,     ,     .</em></p><br>
<p>  ,     4,       :</p><br>
<p><img src="https://habrastorage.org/webt/ts/av/pt/tsavptjlxb3tm5s-nlhmiqekspk.png" alt="transformer_positional_encoding_example"></p><br>
<p>    ?</p><br>
<p>        : ,    ,          ,   â€”      ..    512   -1  1.      ,     .</p><br>
<p><img src="https://habrastorage.org/webt/v9/dv/oh/v9dvohtljexbjop_vrykyyqdzbk.png" alt="transformer_positional_encoding_large_example"></p><br>
<p><em>     20  ()    512 (). ,    :      ( ),   â€“  ( ).          .</em></p><br>
<p>       ( 3.5).          <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=">get_timing_signal_1d()</a>.        , ,        (,       ,   ,    ).</p><br>
<h1 id="ostatki-sladki"> </h1><br>
<p>    ,    ,    ,   ,    ( , )        ,       (layer-normalization step).</p><br>
<p><img src="https://habrastorage.org/webt/gp/ji/3r/gpji3ragynjcuq7y6hkhqp1ws5m.png" alt="transformer_resideual_layer_norm"></p><br>
<p>     ,    ,   :</p><br>
<p><img src="https://habrastorage.org/webt/h3/hm/94/h3hm94cg9y9acyf5odv4uxt1e5q.png" alt="transformer_resideual_layer_norm_2"></p><br>
<p>       .        ,      :</p><br>
<p><img src="https://habrastorage.org/webt/dm/wa/pi/dmwapi3jsz1arewhc4xg3_hgevo.png" alt="transformer_resideual_layer_norm_3"></p><br>
<h1 id="chto-po-chasti-dekodera">   </h1><br>
<p>,        ,    ,    .   ,       .</p><br>
<p>    .          K  V.       Â«-Â»  ,          :</p><br>
<p><img src="https://habrastorage.org/webt/l6/ry/nl/l6rynlkppwpzjaenv35ipi8vcpq.gif" alt="transformer_decoding_1"></p><br>
<p><em>      .         (   â€“    ).</em></p><br>
<p>      , ,       .           ,       ,    .    ,      ,        ,      .</p><br>
<p><img src="https://habrastorage.org/webt/nm/yu/oq/nmyuoqb-9wqu9bssgxkvmhv9gmc.gif" alt="transformer_decoding_2"></p><br>
<p>           .</p><br>
<p>             .          (   â€“inf)       .</p><br>
<p>Â«-Â»      ,  ,       ,   ,          .</p><br>
<h1 id="finishnaya-pryamaya"> </h1><br>
<p>         .       ?           .</p><br>
<p>  â€“     ,   ,   ,    ,    (logits vector).</p><br>
<p>    10     (Â« Â»  ),      .  ,       10 000    â€“       .           .</p><br>
<p>       ( ,    1).               .</p><br>
<p><img src="https://habrastorage.org/webt/hs/i1/go/hsi1go4q7rri9w6vydeuiyyqkqi.png" alt="transformer_decoder_output_softmax"></p><br>
<p><em>   ,        ,      .</em></p><br>
<h1 id="itog-obucheniya"> </h1><br>
<p>,     ,   ,      ,     .</p><br>
<p>           ,   .  ..       ,        .</p><br>
<p>  ,       6  (Â«aÂ», Â«amÂ», Â«iÂ», Â«thanksÂ», Â«studentÂ»  Â«&lt;eos&gt;Â» (Â« Â»).</p><br>
<p><img src="https://habrastorage.org/webt/6w/qz/6w/6wqz6wpimd6gstmtqr2scouikfs.png" alt="vocabulary"></p><br>
<p><em>           .</em></p><br>
<p>     ,              (,   one-hot-). ,     Â«amÂ»,   :</p><br>
<p><img src="https://habrastorage.org/webt/an/sf/tw/ansftwzhb-z4txzgf5zuexvyql4.png" alt="one-hot-vocabulary-example"></p><br>
<p><em>: one-hot-   .</em></p><br>
<p>     (loss function)   â€“ ,           ,   ,  .</p><br>
<h1 id="funkciya-poter"> </h1><br>
<p>,             .      â€“  Â«merciÂ»  Â«thanksÂ».</p><br>
<p>   ,     ,    ,    Â«thanksÂ».  ..    ,      .</p><br>
<p><img src="https://habrastorage.org/webt/ev/87/xq/ev87xqtv05wyamo1jcrqlrhnxho.png" alt="transformer_logits_output_and_label"></p><br>
<p><em>   ()   ,           /.       ,    ,       ,     .</em></p><br>
<p>     ?      .    , . <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="> </a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="> -</a>.</p><br>
<p> ,     .             . ,    Â«je suis Ã©tudiantÂ» â€“    Â«I am a studentÂ».  ,   ,        ,   :</p><br>
<ul>
<li>            (6    ,   â€“  3000  10000);</li>
<li>       ,   Â«iÂ»;</li>
<li>       ,   Â«amÂ»;</li>
<li> ..         ,       .</li>
</ul><br>
<p><img src="https://habrastorage.org/webt/vj/ge/o0/vjgeo0a-6nwsevrrkttu79epj3g.png" alt="output_target_probability_distributions"></p><br>
<p>           ,        :</p><br>
<p><img src="https://habrastorage.org/webt/nh/ry/zd/nhryzdfhby64om8iz-vp2l9jxvu.png" alt="output_trained_model_probability_distributions"></p><br>
<p><em>,         . ,       ,        (.: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="> </a>).  ,        ,    ,          â€“      ,    .</em></p><br>
<p>,   ,        ,   ,              .    ,    (greedy decoding).   â€“ , ,  2   (  , Â«IÂ»  Â«aÂ»)  ,   ,   :    ,        Â«IÂ»,   , ,     Â«aÂ».  ,        ,    .      #2  #3  ..    Â« Â» (beam search).      (beam_size)    (..         #1  #2),  - (top_beams)    (    ).         .</p><br>
<h1 id="vyvod"></h1><br>
<p>,          .       ,   :</p><br>
<ul>
<li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=">Attention is All You Need</a>,    (<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=">Transformer: A Novel Neural Network Architecture for Language Understanding</a>)  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=">Tensor2Tensor announcement</a>;</li>
<li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=">Åukasz Kaiserâ€™s talk</a>,      ;</li>
<li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=">Jupyter-   Tensor2Tensor</a>;</li>
<li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="> Tensor2Tensor</a>.</li>
</ul><br>
<p>   :</p><br>
<ul>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=">Depthwise Separable Convolutions for Neural Machine Translation</a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=">One Model To Learn Them All</a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=">Discrete Autoencoders for Sequence Models</a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Generating Wikipedia by Summarizing Long Sequences</font></font></a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Image transformer</font></font></a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Training Tips for the Transformer Model</font></font></a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Self-Attention with Relative Position Representations</font></font></a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fast Decoding in Sequence Models using Discrete Latent Variables</font></font></a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Adafactor: Adaptive Learning Rates with Sublinear Memory Cost</font></font></a></li>
</ul><br>
<h1 id="avtory"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Authors</font></font></h1><br>
<ul>
<li><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Original</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> by </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Jay Alammar</font></font></a></li>
<li><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Translation</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ekaterina Smirnova</font></font></a></li>
<li><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Editing and layout</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Shkarin Sergey</font></font></a></li>
</ul></div>
      
    </div><p class="reference-to-source js-reference-to-source">Source: https://habr.com/ru/post/undefined/</p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../en486346/index.html">Just about NModbus (RTU)</a></li>
<li><a href="../en486348/index.html">CRUD operations with Entities in DialogFlow (C #)</a></li>
<li><a href="../en486350/index.html">Cashback in a robotics club, or how my students earn on their skills</a></li>
<li><a href="../en486354/index.html">Georgy Potapov: â€œI am a professional OpenStreetMap data consumerâ€</a></li>
<li><a href="../en486356/index.html">The Ember Times - Issue 132</a></li>
<li><a href="../en486360/index.html">How to migrate from mocha to jest in 15 easy steps - and why</a></li>
<li><a href="../en486362/index.html">How I did not become a product manager after training at Product Univercity Moreynis and Chernyak</a></li>
<li><a href="../en486368/index.html">~ SMAK ~ - programmable controllers for smart homesteads using the asyncio library on MicroPython</a></li>
<li><a href="../en486372/index.html">Czech programmers wrote a site worth 16 million euros for free? Truth?</a></li>
<li><a href="../en486376/index.html">How level designers use architecture theory to create game levels</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>