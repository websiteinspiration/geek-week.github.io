<!doctype html>
<html class="no-js" lang="zh-CN">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🐯 🌋 🕙 自然语言处理。2019年业绩和2020年趋势 🆗 🐤 🏴</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="大家好。经过一段时间的延迟，我决定发布此文章。每年我都会总结自然语言处理领域发生的情况。今年也没有例外。
 
 BERT，BERT无处不在
 让我们按顺序开始。如果您过去一年半没有去偏僻的西伯利亚针叶林或果阿度假，那么您一定听过BERT这个词。在过去的时间里，这种模型出现在2018年底，已经如此受欢...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>自然语言处理。2019年业绩和2020年趋势</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/huawei/blog/487730/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">大家好。</font><font style="vertical-align: inherit;">经过一段时间的延迟，我决定发布此文章。</font><font style="vertical-align: inherit;">每年我都会总结自然语言处理领域发生的情况。</font><font style="vertical-align: inherit;">今年也没有例外。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">BERT，BERT无处不在</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
让我们按顺序开始。</font><font style="vertical-align: inherit;">如果您过去一年半没有去偏僻的西伯利亚针叶林或果阿度假，那么您一定听过BERT这个词。</font><font style="vertical-align: inherit;">在过去的时间里，这种模型出现在2018年底，已经如此受欢迎，以至于这样的画面将是正确的：</font></font><br>
<br>
<img src="https://habrastorage.org/webt/cu/vm/_i/cuvm_irxzrscw8rctmtyoqywxss.png"><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
BERT确实吸引了NLP可以填充的所有内容。</font><font style="vertical-align: inherit;">它们开始用于分类，识别命名实体，甚至用于机器翻译。</font><font style="vertical-align: inherit;">简而言之，您不能绕过它们，而您仍然必须告诉它是什么。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/5j/mo/sq/5jmosqk9vhjts6ai88v8hcrdhci.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
该图显示了该场合的英雄（左）与两个声音相同的模型的比较。</font><font style="vertical-align: inherit;">右边是BERT的直接前身</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">-ELMo</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">模型</font><font style="vertical-align: inherit;">。</font></font><br>
<br>
<div class="spoiler"><b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">抒情离题。</font></font></b><div class="spoiler_text"><img src="https://habrastorage.org/getpro/habr/post_images/8a1/bb1/e07/8a1bb1e076e3b3b1b2637343e28359d4.jpg" alt="image"><br>
         « »:           ,        ,   Elmo,  Bert —   ;    ,   ,   , —    .         .  ,    ,   .<br>
</div></div><br><font style="vertical-align: inherit;"></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Allen AI</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
ELMo模型</font><font style="vertical-align: inherit;">是过去几年该地区整个发展的一种继任者-即双向递归神经网络，以及一些新的启动技巧。</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">OpenAI</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">同事已</font><font style="vertical-align: inherit;">决定可以做的更好的事情。为此，您只需要将Google一年前提出的</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Transformer</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">架构</font><font style="vertical-align: inherit;">应用于此任务即可。我相信在过去的2.5年中，每个人都已经熟悉了这种体系结构，因此我将不对其进行详细介绍。对于那些希望获得圣餐的人，我参考</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">了2017年以来的评论</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
他们（OpenAI员工）将其称为</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GPT-2</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">模型</font><font style="vertical-align: inherit;">。然后，在这个模型上，他们</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">做得很好</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。但是，让我们留心他们的良知，然后回到我们的羊群中去，那就是模特儿。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ELMo最重要的窍门之一是在一个未分配的大型案例上进行预培训。结果非常好，Google的同事决定我们可以做得更好。除了应用Transformer架构（已经存在于GPT-2中）之外，BERT代表着Transformers的双向编码器表示，即基于Transformer架构的双向编码器的矢量表示，还包含一些其他重要内容。具体而言，最重要的是在大型案例上进行训练的方法。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/lb/hw/yw/lbhwywgm70j3shvnrtzrnx6clyy.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
图片显示了一种标记未分配数据的方法。一次专门显示了两种布局方法。首先，采用一系列标记（单词），例如一个句子，并以此顺序屏蔽一个任意标记（[MASK]）。并且学习过程中的模型应该猜测伪装了哪种代币。第二种方式-两个句子顺序地或从文本中的任意位置取。并且模型必须猜测这些句子是否是连续的（[CLS]和[SEP]）。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
这种培训的想法非常有效。来自Facebook发誓的朋友的答案是</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">RoBERTa</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">模型</font><font style="vertical-align: inherit;">，有关该模型的文章称为“可持续优化的BERT培训”。更进一步。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
由于无聊的事实，我不会列出所有改进基于Transfomer架构的大型语言模型的训练的方法。</font><font style="vertical-align: inherit;">也许我只提到香港</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ERNIE</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">同事的工作</font><font style="vertical-align: inherit;">。</font><font style="vertical-align: inherit;">在工作中，同事们通过使用知识图来丰富培训。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
在继续之前，这里有一些有用的链接：关于</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">BERT</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">的文章</font><font style="vertical-align: inherit;">。</font><font style="vertical-align: inherit;">以及一</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">组</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">训练有素的俄语俄语BERT和ELMo模型。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">小模型</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
但是关于BERT足够了。还有几个更重要的趋势。首先，这是减小模型尺寸的趋势。相同的BERT对资源的要求非常高，许多人开始考虑如何保持（或不会真正失去）质量，减少模型运行所需的资源。 Google同事提出了一个小BERT，我不是在开玩笑</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">-ALBERT：一个小BERT</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。您会看到，在执行大多数任务时，小型BERT甚至超过了它的老兄，而参数却少了一个数量级。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/y5/su/h3/y5suh3uzlmgy16l8stcoahmio4w.png"> <br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
我的香港同事再次对同一个酒吧提出了另一种方法。他们想出了一个小小的BERT- </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">TinyBERT</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。 （如果在这一点上您认为名字开始重复，那么我倾向于同意您的观点。）</font></font><br>
<br>
<img src="https://habrastorage.org/webt/7_/bb/el/7_bbelco1m2dewd3gj6cjqq2upw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
上述两个模型之间的根本区别在于，如果ALBERT使用棘手的技巧来减少原始BERT模型，例如，通过矩阵分解来共享参数并减少内部矢量表示的维数，则TinyBERT会使用根本不同的方法，即知识的提炼，即存在一个小模型，可以在学习过程中跟随姐姐学习。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">小案件</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
近年来（自1990年左右Internet出现以来），可用建筑物数量有所增加。然后是能够处理如此大的外壳的算法（这就是我们所说的“深度学习革命”，这是自2013年以来的一年）。结果，人们通常开始意识到，为了在某些任务中获得良好的质量，需要大量的标记数据-在我们的案例中是文本语料库。例如，当今学习机器翻译任务的典型案例以成对的句子来衡量。长期以来，很明显，对于许多任务来说，在合理的时间内和合理的金额下组装这种情况是不可能的。长期以来，不清楚如何处理。但是去年（您会想到谁？）BERT出现了。该模型能够对大量未分配的文本进行预训练，并且完成的模型在很小的情况下易于适应任务。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/7_/bb/el/7_bbelco1m2dewd3gj6cjqq2upw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
该表中列出的所有任务都有一支训练小组，规模达</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">数千个</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">单位。</font><font style="vertical-align: inherit;">也就是说，少两个到三个数量级。</font><font style="vertical-align: inherit;">这也是BERT（及其后代和亲戚）如此受欢迎的另一个原因。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">新趋势</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
好吧，最后，我看到了一些新趋势。首先，这是对文本态度的根本改变。如果在大多数任务中都是以前的时间，则文本仅被视为输入材料，而输出则是有用的东西，例如类标签。现在，社区有机会记住文本主要是一种交流手段，也就是说，您可以与模型“交谈” —提出问题并以人类可读的文本形式获得答案。这就是Google </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">T5</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">的新文章所说的</font><font style="vertical-align: inherit;">（名称可以翻译为“五次变形金刚”）。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/ba/vz/mj/bavzmjwryypmza-ywo18njxfbjy.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
另一个重要趋势是该地区正在重新学习以处理长文本。从20世纪70年代开始，社区就可以使用任意长度的文本-采用相同的TF-IDF。但是这些型号都有自己的质量限制。但是新的深度学习模型无法处理长文本（同一BERT的输入文本长度限制为512个令牌）。但是最近，至少出现了两本书，从不同角度探讨了长文本的问题。 Ruslan Salakhutdinov组的第一个作品叫Transformer-XL。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/ci/op/gj/ciopgjs1htbc2gmucz7dwkiwqtk.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
在这项工作中，这个想法得以复兴，使循环网络变得如此流行-即使您没有及时向后倾斜（BPTT），您也可以保存前一个状态并用它来构建下一个状态。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
第二个</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">这项工作</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">适用于勒让德多项式，并在其帮助下可以使用递归神经网络处理成千上万个令牌的序列。</font></font><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
在此，我想结束对已经发生的变化和新趋势的回顾。</font><font style="vertical-align: inherit;">让我们看看今年会发生什么，我敢肯定会有很多有趣的事情。</font><font style="vertical-align: inherit;">关于数据树上同一主题的演讲视频：</font></font><br>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/cdlAUcaOCDY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
附注：我们很快将发布一些更有趣的公告，请不要切换！</font></font></div>
      
    </div><p class="reference-to-source js-reference-to-source">Source: https://habr.com/ru/post/undefined/</p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN487706/index.html">使用Consul示例的分布式系统中的服务发现。亚历山大·西加切夫（Alexander Sigachev）</a></li>
<li><a href="../zh-CN487716/index.html">完美的SAST。解析器</a></li>
<li><a href="../zh-CN487720/index.html">关于竞争性的白痴症（以反应性编程为例）</a></li>
<li><a href="../zh-CN487724/index.html">BlazingPizza：Blazor应用程序从头到尾。第2部分。添加组件</a></li>
<li><a href="../zh-CN487728/index.html">@Pythonetc汇编，2020年1月</a></li>
<li><a href="../zh-CN487734/index.html">加速实体框架核心</a></li>
<li><a href="../zh-CN487738/index.html">SCADA中的模式动画</a></li>
<li><a href="../zh-CN487740/index.html">组装便携式磁力计</a></li>
<li><a href="../zh-CN487742/index.html">套利交易（Bellman-Ford算法）</a></li>
<li><a href="../zh-CN487744/index.html">FARO推出FOCUS S 70激光3D扫描仪</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>