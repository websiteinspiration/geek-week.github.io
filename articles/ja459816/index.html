<!doctype html>
<html class="no-js" lang="ja">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👩‍❤️‍👩 🧚 👨🏽‍🤝‍👨🏼 ニューラルネットワークとディープラーニング、第3章、パート2：正規化が再トレーニングの削減に役立つ理由 ♂️ ↙️ 👨🏿‍🏫</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="コンテンツ

- 第1章：ニューラルネットワークを使用して手書きの数字を認識する
- 第2章：逆伝播アルゴリズムのしくみ
- 第3章： パート1：ニューラルネットワークのトレーニング方法の改善 パート2：なぜ正規化は再トレーニングを減らすのに役立つのですか？ パート3：ニューラルネットワークのハイパ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>ニューラルネットワークとディープラーニング、第3章、パート2：正規化が再トレーニングの削減に役立つ理由</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/459816/"><div class="spoiler"><b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">コンテンツ</font></font></b><div class="spoiler_text"><ul>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">第1章：ニューラルネットワークを使用して手書きの数字を認識する</font></font></a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">第2章：逆伝播アルゴリズムのしくみ</font></font></a></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">第3章：</font></font><ul>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">パート1：ニューラルネットワークのトレーニング方法の改善</font></font></a><br>
</li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">パート2：なぜ正規化は再トレーニングを減らすのに役立つのですか？</font></font></a><br>
</li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">パート3：ニューラルネットワークのハイパーパラメーターの選択方法</font></font></a><br>
</li>
</ul></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="> 4:   ,      </a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="> 5:      ?</a></li>
<li> 6:<ul>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=">.1:  </a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=">.2:     </a></li>
</ul></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=">:       ?</a></li>
</ul></div></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
経験的に、正則化は再トレーニングを減らすのに役立つことがわかりました。これは刺激的ですが、残念ながら、正則化が役立つ理由は明らかではありません。通常、人々はそれを何らかの方法で説明します。ある意味では、重みが小さいほど複雑さが少なくなり、データの説明がより簡単で効率的になるため、推奨されます。ただし、これは短すぎる説明であり、その一部は疑わしい、または不思議に思えるかもしれません。この話を展開して、批判的な目で見てみましょう。これを行うには、モデルを作成する単純なデータセットがあるとします。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/0f/h2/4p/0fh24p1sl8wmgoov1ewqmqbv900.png"></div><a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
意味の観点から、ここでは現実世界の現象を研究し、xとyは実際のデータを表します。</font><font style="vertical-align: inherit;">私たちの目標は、xの関数としてyを予測できるモデルを構築することです。</font><font style="vertical-align: inherit;">ニューラルネットワークを使用してそのようなモデルを作成することもできますが、もっと簡単な方法を提案します。yをxの多項式としてシミュレートしてみます。</font><font style="vertical-align: inherit;">多項式を使用すると説明が特に明確になるため、ニューラルネットワークの代わりにこれを行います。</font><font style="vertical-align: inherit;">多項式のケースを処理したらすぐに、国会に移ります。</font><font style="vertical-align: inherit;">上のグラフには10個のポイントがあります。つまり</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、一意の</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 9次</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">多項式</font></a><font style="vertical-align: inherit;"> y = a </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> x </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">9</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> + a </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> x </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">8</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> + ... + a </font><sub><font style="vertical-align: inherit;">9</font></sub><font style="vertical-align: inherit;">を</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">見つける</font></a><font style="vertical-align: inherit;">ことができ</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">ます</font></a><font style="vertical-align: inherit;">。</font></font><sub><font style="vertical-align: inherit;"></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">データに完全に適合します。</font><font style="vertical-align: inherit;">そして、これがこの多項式のグラフです。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/o6/lj/6j/o6lj6j82dn6rly8xnv-fmzvvwn0.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
完璧なヒット。</font><font style="vertical-align: inherit;">しかし、線形モデルy = 2xを使用して適切な近似を得ることができます。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/-3/qk/sq/-3qksq4rmlcq54obwxcxy4_dvtm.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
どっちがいいですか？</font><font style="vertical-align: inherit;">どちらが本当である可能性が高いですか？</font><font style="vertical-align: inherit;">現実世界の同じ現象の他の例にどちらがより一般化されますか？</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
難しい質問。</font><font style="vertical-align: inherit;">そして、根本的な現実の現象についての追加情報がなければ、それらに対する正確な答えを得ることができません。</font><font style="vertical-align: inherit;">ただし、2つの可能性を見てみましょう。（1）9次多項式のモデルは実際に現実の現象を正確に記述しているため、完全に一般化されます。</font><font style="vertical-align: inherit;">（2）正しいモデルはy = 2xですが、測定誤差に関連する追加のノイズがあるため、モデルは完全に適合しません。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
先験的に、2つの可能性のどちらが正しいか（または3分の1が存在しない）とは言えません。論理的には、それらのいずれもが真であることが判明する可能性があります。そして、それらの違いは重要です。はい、利用可能なデータに基づいて、モデル間のわずかな違いがあると言うことができます。しかし、グラフに示されているどの値よりもはるかに大きい、xのある大きな値に対応するyの値を予測したいとします。これを行おうとすると、2次モデルの予測の間に大きな差が生じます。これは、9次多項式ではx </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">9項</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">が支配的で</font><font style="vertical-align: inherit;">あり、線形モデルは線形のままである</font><font style="vertical-align: inherit;">ため</font><font style="vertical-align: inherit;">です。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
何が起こっているかについての1つの視点は、可能であれば、科学ではより単純な説明を使用すべきであると述べることです。多くの参照ポイントを説明する単純なモデルを見つけたら、「Eureka！」と叫びたいだけです。結局のところ、単純な説明が純粋に偶然に現れることはほとんどありません。モデルは現象に関連するいくつかの真実を生成する必要があると考えています。この場合、モデルy = 2x +ノイズはy = a </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> x </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">9</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> + a </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> x </font><sup><font style="vertical-align: inherit;">8</font></sup><font style="vertical-align: inherit;">よりもはるかに単純に見えます</font></font><sup><font style="vertical-align: inherit;"></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+ ...単純さが偶然に生じたのは驚くべきことなので、y = 2x +ノイズはいくつかの根本的な真実を表していると思われます。この観点から、9次モデルは単に局所的なノイズの影響を研究します。 9次モデルはこれらの特定の参照点に対して完全に機能しますが、他の点に一般化することはできません。その結果、ノイズのある線形モデルはより優れた予測能力を持ちます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
この視点がニューラルネットワークにとって何を意味するか見てみましょう。私たちのネットワークでは、正規化されたネットワークで通常そうであるように、主に低い重みがあると仮定します。重みが小さいため、ランダムな入力が何度か変更されても、ネットワークの動作はあまり変わりません。正規化されたネットワークの結果として、データに存在するローカルノイズの影響を知ることは困難です。これは、個々の証拠がネットワーク全体の出力に大きな影響を与えないようにしたいという要望に似ています。代わりに、正規化されたネットワークは、トレーニングデータでよく見られる証拠に対応するようにトレーニングされます。逆に、重みが大きいネットワークは、入力データの小さな変化に応答して、その動作を非常に強く変更できます。したがって、不規則なネットワークでは、大きな重みを使用して複雑なモデルをトレーニングできます。トレーニングデータに多くのノイズ情報が含まれています。つまり、正規化されたネットワークの制限により、トレーニングデータによく見られるパターンに基づいて比較的単純なモデルを作成でき、トレーニングデータのノイズによって引き起こされる偏差に耐性があります。これにより、私たちのネットワークが現象自体を研究し、得られた知識をより一般化することが期待されます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
とはいえ、より簡単な説明を優先するという考えは、緊張するでしょう。時々人々はこの考えを「オッカムのかみそり」と呼び、まるでそれが一般的な科学的原理のステータスを持っているかのように熱心にそれを適用します。しかし、これはもちろん、一般的な科学原理ではありません。複雑な説明よりも単純な説明を優先するアプリオリな理由はありません。より複雑な説明が正しい場合もあります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
より複雑な説明が正しいことが判明した2つの例について説明します。 1940年代に、物理学者マルセルシェーンは新しい粒子の発見を発表しました。彼が働いた会社であるGeneral Electricは興奮し、このイベントの出版物を広く配布しました。しかし、物理学者のハンス・ベーテは懐疑的でした。ベーテはシェーンを訪問し、シェーンの新しい粒子の痕跡でプレートを研究しました。シェーンはプレートごとにベータ版を示しましたが、ベテはそれらのそれぞれでこのデータを拒否する必要性を示す問題を発見しました。最後に、シェーンはベータに適合したレコードを示しました。ベテは、それはおそらく単なる統計的偏差だったと言いました。シェーン：「はい、しかしそれが統計によるものである可能性は、たとえあなた自身の公式であっても、5分の1です。」ベテ：「しかし、私はすでに5つのレコードを見てきました。」最後に、シェーンは言った：「しかし、あなたは私の記録のそれぞれ、他の理論によってすべての良い画像を説明しました、そして私はすべての記録を一度に説明する1つの仮説があります、それから私たちは新しい粒子について話しているということになります。」ベテは答えた。「私の説明とあなたの説明の唯一の違いは、あなたの説明が間違っていて、私の説明が正しいということです。あなたの単一の説明は正しくなく、私の説明はすべて正しいです。」その後、自然がベテと一致し、シェーンの粒子が蒸発したことがわかりました。その性質はベーテと一致し、シェーンの粒子は蒸発した。その性質はベーテと一致し、シェーンの粒子は蒸発した。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
2番目の例では、1859年に天文学者アーバインジャンジョセフルヴェリエが、水星の軌道の形状がニュートンの普遍的な重力の理論に対応していないことを発見しました。この理論からのわずかな逸脱があり、それから問題を解決するためのいくつかのオプションが提案されました、それは結局ニュートンの理論が全体として正しいという事実に結ばれ、わずかな変更のみを必要とします。そして1916年に、アインシュタインはこの偏差が彼の一般相対性理論を使用して十分に説明できることを示しました。ニュートン重力とは根本的に異なり、はるかに複雑な数学に基づいています。この追加の複雑性にもかかわらず、アインシュタインの説明が正しいこと、およびニュートン重力が</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">修正された形式であっ</font></a><font style="vertical-align: inherit;">て</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">も</font></a><font style="vertical-align: inherit;">正しくないことが今日一般に受け入れられています</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。これは特に、アインシュタインの理論がニュートンの理論が困難であった他の多くの現象を説明していることを今日知っているために起こります。さらに、さらに驚くべきことに、アインシュタインの理論は、ニュートン重力がまったく予測しなかったいくつかの現象を正確に予測します。しかし、これらの印象的な品質は、過去には明らかではありませんでした。単なる単純さに基づいて判断すると、ニュートン理論のいくつかの修正された形式がより魅力的に見えたでしょう。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
これらの物語から3つの道徳を引き出すことができます。まず、2つの説明のどちらが「より簡単」になるかを判断するのが難しい場合があります。第二に、たとえそのような決定を下したとしても、簡潔さは非常に注意深く導かれるべきです！第三に、モデルの真のテストは単純さではなく、新しい動作条件での新しい現象をどれだけうまく予測できるかです。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
これらすべてを考慮し、注意を払って、私たちは経験的な事実を受け入れます-正規化されたNSは通常、不規則なNSよりも一般化されています。したがって、本の後半では、しばしば正則化を使用します。言及されたストーリーは、なぜ正規化がネットワークの一般化に役立つのかについて完全に説得力のある理論的説明を誰も開発していない理由を説明するためにのみ必要です。研究者は、正規化へのさまざまなアプローチを試み、それらを比較し、何が最も効果的であるかを調べ、さまざまなアプローチがより良くまたはよりよく機能する理由を理解しようと試みる作品を引き続き公開しています。正則化は</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">クラウドの</font></a><font style="vertical-align: inherit;">ように扱うことができます</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。それが非常に頻繁に役立つ場合、私たちは何が起こっているかについて完全に満足できる体系的な理解を持っていません-不完全なヒューリスティックで実用的なルールだけです。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ここに、科学の核心に潜むより深い一連の問題があります。これは一般化の問題です。正則化は、ネットワークがデータをより一般化するのに役立つ計算の魔法の杖を与えることができますが、それは、汎化がどのように機能するか、そしてそれに対する最善のアプローチは何かについての基本的な理解を提供しません。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
これらの</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">問題は帰納の</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">問題に戻り</font><font style="vertical-align: inherit;">、スコットランドの哲学者</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">デビッド・ヒューム</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">が著書「</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">人間の認知に関する研究</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">」（1748）</font><font style="vertical-align: inherit;">でそのよく知られた解釈を行った</font><font style="vertical-align: inherit;">。 「</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">食事なしの定理</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">」は</font><font style="vertical-align: inherit;">帰納の問題に捧げられている</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">デビッド・ウォルパートとウィリアム・マクレディ</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">（1977）。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
そして、これは特に煩わしいことです。なぜなら、日常生活の中で、人々は驚くほどデータを一般化できるからです。子供に象の画像をいくつか見せれば、彼はすぐに他の象を認識することを学びます。もちろん、彼は時々、例えばサイと象を混同するなど、間違いをする可能性がありますが、一般的にこのプロセスは驚くほど正確に機能します。これで、人間の脳である膨大な量の無料パラメーターを備えたシステムができました。そして、1つ以上のトレーニング画像が表示された後、システムはそれらを他の画像に一般化する方法を学習します。私たちの脳は、ある意味で、正則化が驚くほど上手です！しかし、これをどうやって行うのでしょうか？現時点では、これは不明です。将来的には、人工ニューラルネットワークでより強力な正則化技術、最終的には国会がデータを一般化できるようにする技術を開発すると思います。さらに小さなデータセットに基づいています。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
実際、私たちのネットワークはすでに、アプリオリに予想されるよりもはるかに一般化しています。</font><font style="vertical-align: inherit;">100個の隠れたニューロンを持つネットワークには、80,000近くのパラメーターがあります。</font><font style="vertical-align: inherit;">トレーニングデータには50,000画像しかありません。</font><font style="vertical-align: inherit;">これは、50,000の参照点を超えて80,000次数の多項式を引き出そうとするようなものです。</font><font style="vertical-align: inherit;">すべての兆候によって、私たちのネットワークはひどく再訓練する必要があります。</font><font style="vertical-align: inherit;">それでも、これまで見てきたように、このようなネットワークは実際にはかなり一般化されています。</font><font style="vertical-align: inherit;">なぜそれが起こるのですか？</font><font style="vertical-align: inherit;">これは完全に明確ではありません。</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">仮説</font></a><font style="vertical-align: inherit;">が</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">立て</font></a><font style="vertical-align: inherit;">られた</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">「多層ネットワークにおける勾配降下法による学習のダイナミクスは、自己規制の対象です」。</font><font style="vertical-align: inherit;">これは異常な成功ですが、なぜこれが起こるのか理解していないため、かなり憂慮すべき事実でもあります。</font><font style="vertical-align: inherit;">その間、私たちは実用的なアプローチをとり、可能な限り正則化を使用します。</font><font style="vertical-align: inherit;">これは私たちの国会にとって有益です。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
L2の正則化は変位を制限しないという前に説明しなかったことに戻って、このセクションを終了しましょう。当然、バイアスを正則化するように正則化手順を変更するのは簡単です。しかし、経験的には、これは結果を目立った方法で変更しないことが多いため、ある程度、変位を正則化するかどうかは、合意の問題です。ただし、大きな変位によってニューロンが大きな重みのような入力に敏感にならないことは注目に値します。したがって、ネットワークがトレーニングデータのノイズを学習できるようにするための大きなオフセットについて心配する必要はありません。同時に、大きな変位を可能にすることで、ネットワークの動作をより柔軟にします-特に、大きな変位は、希望するニューロンの飽和を促進します。このため、通常は正則化にオフセットを含めません。</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">その他の正則化手法</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
L2以外にも多くの正則化手法があります。</font><font style="vertical-align: inherit;">実際、非常に多くの技法がすでに開発されており、すべての欲望をもって、すべてを簡単に説明することはできませんでした。</font><font style="vertical-align: inherit;">このセクションでは、再トレーニングを削減する他の3つのアプローチについて簡単に説明します。L1を正則化し、</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ドロップアウトし</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、</font><font style="vertical-align: inherit;">トレーニングセットを人為的に増やします。</font><font style="vertical-align: inherit;">前のトピックほど深くは学習しません。</font><font style="vertical-align: inherit;">代わりに、それらを知るだけでなく、既存のさまざまな正則化手法を高く評価しています。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">正則化L1</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
このアプローチでは、重みの絶対値の合計を追加することにより、不規則なコスト関数を変更します：</font></font><br>
<br>
<p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display"><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mtable displaystyle=&quot;true&quot;><mlabeledtr><mtd id=&quot;mjx-eqn-95&quot;><mtext>(95)</mtext></mtd><mtd><mi>C</mi><mo>=</mo><msub><mi>C</mi><mn>0</mn></msub><mo>+</mo><mfrac><mi>&amp;#x03BB;</mi><mi>n</mi></mfrac><munder><mo>&amp;#x2211;</mo><mi>w</mi></munder><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow></mtd></mlabeledtr></mtable></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="97.25ex" height="6.259ex" viewBox="0 -1616.6 41871.4 2695" role="img" focusable="false" style="vertical-align: -2.505ex; max-width: 778px;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g transform="translate(40091,0)"><g id="mjx-eqn-95" transform="translate(0,143)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-28"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-39" x="389" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-35" x="890" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-29" x="1390" y="0"></use></g></g><g transform="translate(16860,0)"><g transform="translate(-14,0)"><g transform="translate(0,143)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMATHI-43" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-3D" x="1038" y="0"></use><g transform="translate(2094,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMATHI-43" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-30" x="1011" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-2B" x="3486" y="0"></use><g transform="translate(4264,0)"><g transform="translate(342,0)"><rect stroke="none" width="720" height="60" x="0" y="220"></rect><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMATHI-3BB" x="68" y="676"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMATHI-6E" x="60" y="-686"></use></g></g><g transform="translate(5614,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJSZ2-2211" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMATHI-77" x="663" y="-1487"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-7C" x="7225" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMATHI-77" x="7503" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-7C" x="8220" y="0"></use></g></g></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable displaystyle="true"><mlabeledtr><mtd id="mjx-eqn-95"><mtext>(95)</mtext></mtd><mtd><mi>C</mi><mo>=</mo><msub><mi>C</mi><mn>0</mn></msub><mo>+</mo><mfrac><mi>λ</mi><mi>n</mi></mfrac><munder><mo>∑</mo><mi>w</mi></munder><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mi>w</mi><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow></mtd></mlabeledtr></mtable></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-1"> C = C_0 + \frac{\lambda}{n} \sum_w |w| \tag{95} </script></p><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
直感的には、これはL2の正則化に似ています。これは、重みが大きい場合に細かくなり、ネットワークに重みが小さいことを優先させます。</font><font style="vertical-align: inherit;">もちろん、正則化項L1は正則化項L2と類似していないため、まったく同じ動作を期待するべきではありません。</font><font style="vertical-align: inherit;">正則化L1でトレーニングされたネットワークの動作が、正則化L2でトレーニングされたネットワークとどのように異なるかを理解してみましょう。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
これを行うには、コスト関数の偏微分を見てください。</font><font style="vertical-align: inherit;">微分すると（95）、以下が得られます。</font></font><br>
<br>
<p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display"><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mtable displaystyle=&quot;true&quot;><mlabeledtr><mtd id=&quot;mjx-eqn-96&quot;><mtext>(96)</mtext></mtd><mtd><mfrac><mrow><mi mathvariant=&quot;normal&quot;>&amp;#x2202;</mi><mi>C</mi></mrow><mrow><mi mathvariant=&quot;normal&quot;>&amp;#x2202;</mi><mi>w</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><mi mathvariant=&quot;normal&quot;>&amp;#x2202;</mi><msub><mi>C</mi><mn>0</mn></msub></mrow><mrow><mi mathvariant=&quot;normal&quot;>&amp;#x2202;</mi><mi>w</mi></mrow></mfrac><mo>+</mo><mfrac><mi>&amp;#x03BB;</mi><mi>n</mi></mfrac><mspace width=&quot;thinmathspace&quot; /><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;normal&quot;>s</mi><mi mathvariant=&quot;normal&quot;>g</mi><mi mathvariant=&quot;normal&quot;>n</mi></mrow><mo stretchy=&quot;false&quot;>(</mo><mi>w</mi><mo stretchy=&quot;false&quot;>)</mo></mtd></mlabeledtr></mtable></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="97.25ex" height="5.259ex" viewBox="0 -1401.3 41871.4 2264.4" role="img" focusable="false" style="vertical-align: -2.005ex; max-width: 778px;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g transform="translate(40091,0)"><g id="mjx-eqn-96" transform="translate(0,-90)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-28"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-39" x="389" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-36" x="890" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-29" x="1390" y="0"></use></g></g><g transform="translate(15901,0)"><g transform="translate(-14,0)"><g transform="translate(0,-90)"><g transform="translate(120,0)"><rect stroke="none" width="1448" height="60" x="0" y="220"></rect><g transform="translate(60,676)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-2202" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMATHI-43" x="567" y="0"></use></g><g transform="translate(82,-691)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-2202" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMATHI-77" x="567" y="0"></use></g></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-3D" x="1965" y="0"></use><g transform="translate(2744,0)"><g transform="translate(397,0)"><rect stroke="none" width="1856" height="60" x="0" y="220"></rect><g transform="translate(60,676)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-2202" x="0" y="0"></use><g transform="translate(567,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMATHI-43" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-30" x="1011" y="-213"></use></g></g><g transform="translate(286,-691)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-2202" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMATHI-77" x="567" y="0"></use></g></g></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-2B" x="5341" y="0"></use><g transform="translate(6119,0)"><g transform="translate(342,0)"><rect stroke="none" width="720" height="60" x="0" y="220"></rect><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMATHI-3BB" x="68" y="676"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMATHI-6E" x="60" y="-686"></use></g></g><g transform="translate(7469,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-73" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-67" x="394" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-6E" x="895" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-28" x="8920" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMATHI-77" x="9310" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-29" x="10026" y="0"></use></g></g></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable displaystyle="true"><mlabeledtr><mtd id="mjx-eqn-96"><mtext>(96)</mtext></mtd><mtd><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>C</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>w</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><mi mathvariant="normal">∂</mi><msub><mi>C</mi><mn>0</mn></msub></mrow><mrow><mi mathvariant="normal">∂</mi><mi>w</mi></mrow></mfrac><mo>+</mo><mfrac><mi>λ</mi><mi>n</mi></mfrac><mspace width="thinmathspace"></mspace><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="normal">s</mi><mi mathvariant="normal">g</mi><mi mathvariant="normal">n</mi></mrow><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo></mtd></mlabeledtr></mtable></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-2"> \frac{\partial C}{\partial w} = \frac{\partial C_0}{\partial w} + \frac{\lambda}{n} \, {\rm sgn}(w) \tag{96} </script></p><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ここで、sgn（w）はwの符号です。つまり、wが正の場合は+ 1、wが負の場合は-1です。</font><font style="vertical-align: inherit;">この式を使用して、正規化L1を使用して確率的勾配降下を実行するように、逆伝播をわずかに変更します。</font><font style="vertical-align: inherit;">L1正規化ネットワークの最終更新ルール：</font></font><br>
<br>
<p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display"><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mtable displaystyle=&quot;true&quot;><mlabeledtr><mtd id=&quot;mjx-eqn-97&quot;><mtext>(97)</mtext></mtd><mtd><mi>w</mi><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo><msup><mi>w</mi><mo>&amp;#x2032;</mo></msup><mo>=</mo><mi>w</mi><mo>&amp;#x2212;</mo><mfrac><mrow><mi>&amp;#x03B7;</mi><mi>&amp;#x03BB;</mi></mrow><mi>n</mi></mfrac><mstyle displaystyle=&quot;false&quot; scriptlevel=&quot;0&quot;><mtext>sgn</mtext></mstyle><mo stretchy=&quot;false&quot;>(</mo><mi>w</mi><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x2212;</mo><mi>&amp;#x03B7;</mi><mfrac><mrow><mi mathvariant=&quot;normal&quot;>&amp;#x2202;</mi><msub><mi>C</mi><mn>0</mn></msub></mrow><mrow><mi mathvariant=&quot;normal&quot;>&amp;#x2202;</mi><mi>w</mi></mrow></mfrac></mtd></mlabeledtr></mtable></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="97.25ex" height="5.259ex" viewBox="0 -1401.3 41871.4 2264.4" role="img" focusable="false" style="vertical-align: -2.005ex; max-width: 778px;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g transform="translate(40091,0)"><g id="mjx-eqn-97" transform="translate(0,-90)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-28"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-39" x="389" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-37" x="890" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-29" x="1390" y="0"></use></g></g><g transform="translate(13722,0)"><g transform="translate(-14,0)"><g transform="translate(0,-90)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMATHI-77" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-2192" x="994" y="0"></use><g transform="translate(2272,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMATHI-77" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-2032" x="1013" y="583"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-3D" x="3561" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMATHI-77" x="4617" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-2212" x="5556" y="0"></use><g transform="translate(6335,0)"><g transform="translate(342,0)"><rect stroke="none" width="1207" height="60" x="0" y="220"></rect><g transform="translate(60,691)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMATHI-3B7" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMATHI-3BB" x="503" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMATHI-6E" x="303" y="-686"></use></g></g><g transform="translate(8004,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-73"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-67" x="394" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-6E" x="895" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-28" x="9455" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMATHI-77" x="9845" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-29" x="10561" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-2212" x="11173" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMATHI-3B7" x="12174" y="0"></use><g transform="translate(12677,0)"><g transform="translate(120,0)"><rect stroke="none" width="1856" height="60" x="0" y="220"></rect><g transform="translate(60,676)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-2202" x="0" y="0"></use><g transform="translate(567,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMATHI-43" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-30" x="1011" y="-213"></use></g></g><g transform="translate(286,-691)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-2202" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMATHI-77" x="567" y="0"></use></g></g></g></g></g></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable displaystyle="true"><mlabeledtr><mtd id="mjx-eqn-97"><mtext>(97)</mtext></mtd><mtd><mi>w</mi><mo stretchy="false">→</mo><msup><mi>w</mi><mo>′</mo></msup><mo>=</mo><mi>w</mi><mo>−</mo><mfrac><mrow><mi>η</mi><mi>λ</mi></mrow><mi>n</mi></mfrac><mstyle displaystyle="false" scriptlevel="0"><mtext>sgn</mtext></mstyle><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo><mo>−</mo><mi>η</mi><mfrac><mrow><mi mathvariant="normal">∂</mi><msub><mi>C</mi><mn>0</mn></msub></mrow><mrow><mi mathvariant="normal">∂</mi><mi>w</mi></mrow></mfrac></mtd></mlabeledtr></mtable></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-3"> w \rightarrow w' = w-\frac{\eta \lambda}{n} \mbox{sgn}(w) - \eta \frac{\partial C_0}{\partial w} \tag{97} </script></p><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ここで、いつものように、∂C/∂wは、オプションでミニパケットの平均値を使用して推定できます。</font><font style="vertical-align: inherit;">これを正則化更新ルールL2（93）と比較してください。</font></font><br>
<br>
<p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display"><span class="MathJax_SVG" id="MathJax-Element-4-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mtable displaystyle=&quot;true&quot;><mlabeledtr><mtd id=&quot;mjx-eqn-98&quot;><mtext>(98)</mtext></mtd><mtd><mi>w</mi><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo><msup><mi>w</mi><mo>&amp;#x2032;</mo></msup><mo>=</mo><mi>w</mi><mrow><mo>(</mo><mrow><mn>1</mn><mo>&amp;#x2212;</mo><mfrac><mrow><mi>&amp;#x03B7;</mi><mi>&amp;#x03BB;</mi></mrow><mi>n</mi></mfrac></mrow><mo>)</mo></mrow><mo>&amp;#x2212;</mo><mi>&amp;#x03B7;</mi><mfrac><mrow><mi mathvariant=&quot;normal&quot;>&amp;#x2202;</mi><msub><mi>C</mi><mn>0</mn></msub></mrow><mrow><mi mathvariant=&quot;normal&quot;>&amp;#x2202;</mi><mi>w</mi></mrow></mfrac></mtd></mlabeledtr></mtable></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="97.25ex" height="5.884ex" viewBox="0 -1508.9 41871.4 2533.5" role="img" focusable="false" style="vertical-align: -2.38ex; max-width: 778px;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g transform="translate(40091,0)"><g id="mjx-eqn-98"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-28"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-39" x="389" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-38" x="890" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-29" x="1390" y="0"></use></g></g><g transform="translate(14125,0)"><g transform="translate(-14,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMATHI-77" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-2192" x="994" y="0"></use><g transform="translate(2272,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMATHI-77" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-2032" x="1013" y="583"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-3D" x="3561" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMATHI-77" x="4617" y="0"></use><g transform="translate(5501,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJSZ3-28"></use><g transform="translate(736,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-31" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-2212" x="722" y="0"></use><g transform="translate(1501,0)"><g transform="translate(342,0)"><rect stroke="none" width="1207" height="60" x="0" y="220"></rect><g transform="translate(60,691)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMATHI-3B7" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMATHI-3BB" x="503" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMATHI-6E" x="303" y="-686"></use></g></g></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJSZ3-29" x="3906" y="-1"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-2212" x="10366" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMATHI-3B7" x="11367" y="0"></use><g transform="translate(11870,0)"><g transform="translate(120,0)"><rect stroke="none" width="1856" height="60" x="0" y="220"></rect><g transform="translate(60,676)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-2202" x="0" y="0"></use><g transform="translate(567,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMATHI-43" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-30" x="1011" y="-213"></use></g></g><g transform="translate(286,-691)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-2202" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMATHI-77" x="567" y="0"></use></g></g></g></g></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable displaystyle="true"><mlabeledtr><mtd id="mjx-eqn-98"><mtext>(98)</mtext></mtd><mtd><mi>w</mi><mo stretchy="false">→</mo><msup><mi>w</mi><mo>′</mo></msup><mo>=</mo><mi>w</mi><mrow><mo>(</mo><mrow><mn>1</mn><mo>−</mo><mfrac><mrow><mi>η</mi><mi>λ</mi></mrow><mi>n</mi></mfrac></mrow><mo>)</mo></mrow><mo>−</mo><mi>η</mi><mfrac><mrow><mi mathvariant="normal">∂</mi><msub><mi>C</mi><mn>0</mn></msub></mrow><mrow><mi mathvariant="normal">∂</mi><mi>w</mi></mrow></mfrac></mtd></mlabeledtr></mtable></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-4"> w \rightarrow w' = w\left(1 - \frac{\eta \lambda}{n} \right) - \eta \frac{\partial C_0}{\partial w} \tag{98} </script></p><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
どちらの式でも、正則化の効果は重みを減らすことです。</font><font style="vertical-align: inherit;">これは、両方のタイプの正則化が大きな重みにペナルティを課すという直感的な概念と一致しています。</font><font style="vertical-align: inherit;">ただし、重量はさまざまな方法で削減されます。</font><font style="vertical-align: inherit;">L1の正則化では、重みは一定の値で減少し、0になる傾向があります。L2の正則化では、重みはwに比例する値で減少します。</font><font style="vertical-align: inherit;">したがって、ある重みが大きな値| w |を持つ場合、L1の正則化により、重みはL2ほどには減少しません。</font><font style="vertical-align: inherit;">逆の場合、| w | </font><font style="vertical-align: inherit;">L1の小さな正則化は、L2の正則化よりもはるかに軽量化します。</font><font style="vertical-align: inherit;">その結果、L1の正則化により、重要度の高い比較的少数のボンドにネットワークの重みが集中する傾向がありますが、他の重みはゼロになる傾向があります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
前のディスカッションで1つの問題を少し平滑化しました。偏微分∂C/∂wは、w = 0の場合は定義されません。</font><font style="vertical-align: inherit;">これは、関数| w | </font><font style="vertical-align: inherit;">w = 0のポイントに鋭い「ねじれ」があるため、そこで区別することはできません。</font><font style="vertical-align: inherit;">しかし、これは恐ろしいことではありません。</font><font style="vertical-align: inherit;">w = 0の場合、確率的勾配降下に通常の不規則な規則を適用します。</font><font style="vertical-align: inherit;">直感的には、何も問題はありません。正則化によって重みが削減されます。明らかに、0に等しい重みは削減できません。より正確には、sgn（0）=の条件で式（96）と（97）を使用します。 0。</font><font style="vertical-align: inherit;">これにより、正規化L1を使用した確率的勾配降下の便利でコンパクトなルールが得られます。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">例外[ドロップアウト]</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
例外は、完全に異なる正則化手法です。 L1とL2の正規化とは異なり、例外はコスト関数の変更を扱いません。代わりに、ネットワーク自体を変更しています。例外が機能する理由と結果を説明する前に、例外の操作の基本的な仕組みについて説明します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ネットワークをトレーニングしようとしていると仮定します。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ae5/671/838/ae5671838ebc48f82eb01c1a839b60a7.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
特に、トレーニング入力xと対応する必要な出力yがあるとします。通常、ネットワーク上でxを直接分散し、次に逆伝播して勾配の寄与を決定することにより、それをトレーニングします。例外がこのプロセスを変更します。まず、ネットワークの隠れたニューロンの半分をランダムに一時的に削除し、入力ニューロンと出力ニューロンを変更しません。その後、ほぼこのようなネットワークができます。除外されたニューロン、つまり一時的に削除されたニューロンは、図ではまだマークされていることに注意してください。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ec3/51c/b6a/ec351cb6a877c8f0688718e0d5980088.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
変更されたネットワークを介してxを直接配布し、変更されたネットワークを介して結果を逆配布します。例のミニパッケージでこれを行った後、対応する重みとオフセットを更新します。次に、このプロセスを繰り返し、最初に除外されたニューロンを復元してから、削除する非表示ニューロンの新しいランダムサブセットを選択し、別のミニパケットの勾配を評価して、ネットワークの重みとオフセットを更新します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
このプロセスを何度も繰り返すと、いくつかの重みと変位を学習したネットワークが得られます。当然、これらの重みとバイアスは、隠れたニューロンの半分が除外された条件下で学習されました。ネットワークを完全に起動すると、2倍の数のアクティブな非表示ニューロンがあります。これを補うために、隠れたニューロンからの重みを半分にします。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
除外手順は奇妙で恣意的に見えるかもしれません。なぜ彼女は正則化を手助けしなければならないのですか？何が起こっているのかを説明するために、しばらくは例外を忘れて、国会の訓練を標準的な方法で提示してほしい。特に、同じトレーニングデータを使用していくつかの異なるNSをトレーニングするとします。もちろん、最初はネットワークが異なる場合があり、トレーニングによって異なる結果が生じる場合があります。このような場合、どのような出力を受け入れるかを決定するために、ある種の平均化または投票スキームを適用できます。たとえば、5つのネットワークをトレーニングし、そのうちの3つが数値を「3」として分類した場合、これはおそらく真の3つです。そして、他の2つのネットワークはおそらく間違っています。このような平均化スキームは、多くの場合、再トレーニングを減らすための（高価ではありますが）便利な方法です。その理由はさまざまなネットワークがさまざまな方法で再トレーニングでき、平均化することでそのような再トレーニングを排除できます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
これはすべて例外とどのように関連していますか？発見的に、異なる中性子のセットを除外すると、まるで異なるNSを訓練しているかのようになります。したがって、除外手順は、非常に多数の異なるネットワークでの平均効果に似ています。異なるネットワークは異なる方法で再トレーニングするため、除外の平均的な効果が再トレーニングを減らすことが期待されます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
除外の利点に関する関連するヒューリスティックな説明</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">は、初期の作品の1つに記載されています</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">この手法を使用すると、次のようになります。「この手法は、ニューロンが特定の近傍の存在に依存できないため、ニューロンの複雑な共同適応を削減します。その結果、彼はニューロンの多くの異なるランダムなサブセットと一緒に働くのに役立つことができるより信頼できる機能を学ぶ必要があります。言い換えれば、私たちの国会を予測を行うモデルとして想像すると、例外は、証拠の個々の部分の損失に対するモデルの安定性を保証する方法になります。この意味で、この手法はL1とL2の正則化に似ています。これは、重みを減らし、ネットワーク内の個々の接続の損失に対するネットワークの耐性を高めます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
当然のことながら、除外の有用性の真の尺度は、ニューラルネットワークの効率を改善する上での大きな成功です。で</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">オリジナル作品</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">この方法が導入されたところ、それは多くの異なるタスクに適用されました。</font><font style="vertical-align: inherit;">私たちが調査したものと同様の単純な直接配信ネットワークを使用して、著者がMNISTからの番号の分類に例外を適用したという事実に特に興味があります。</font><font style="vertical-align: inherit;">それまでは、このようなアーキテクチャの最良の結果は98.4％の精度でした。</font><font style="vertical-align: inherit;">彼らは、除外と修正された形式の正則化L2の組み合わせを使用して、98.7％に改善しました。</font><font style="vertical-align: inherit;">同様に印象的な結果は、パターンや音声認識、自然言語処理など、他の多くのタスクでも得られました。</font><font style="vertical-align: inherit;">この例外は、再トレーニングの問題が頻繁に発生する大規模で深いネットワークのトレーニングに特に役立ちました。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">人為的に拡張されたトレーニングデータセット</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
先ほど、1,000のトレーニング画像を使用しただけで、MNIST分類精度が80％に低下したことを確認しました。そして、不思議ではありません-データが少ないと、私たちのネットワークは、人々が数字を書くためのより少ないオプションを満たします。効率の変化を確認するために、トレーニングセットの異なるボリュームを使用して、30の隠れたニューロンのネットワークをトレーニングしてみましょう。ミニパケットサイズ10、学習速度η= 0.5、正則化パラメーターλ= 5.0、およびクロスエントロピー付きのコスト関数を使用してトレーニングします。データの完全なセットを使用して30の時代のネットワークをトレーニングし、トレーニングデータの量の減少に比例して時代の数を増やします。トレーニングデータの異なるセットに対して同じ重み低減係数を保証するために、正則化パラメーターλ= 5を使用します完全なトレーニングセットでは0、データ量の減少に比例してそれを減らします。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7d6/a9f/6db/7d6a9f6db97493f1d716450344dd877f.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
分類データの精度がトレーニングデータの増加に伴って大幅に向上していることがわかります。この成長は、販売量のさらなる増加により継続する可能性があります。もちろん、上のグラフから判断して、飽和に近づいています。ただし、このグラフをトレーニングデータの量の対数依存に再実行するとします。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b7a/774/4e0/b7a7744e077b9522d251333f647318b2.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
結局、チャートはまだ上昇する傾向にあることがわかります。これは、50,000よりもはるかに大量のデータ（たとえば、数百万または数十億の手書きの例）を取得した場合、そのような小さなサイズでも、おそらくはるかに優れた作業ネットワークを取得できることを示唆しています。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
より多くのトレーニングデータを取得することは素晴らしいアイデアです。残念ながら、これは高価になる可能性があるため、実際には常に可能とは限りません。ただし、ほぼ同様に機能する別のアイデアがあります。データセットを人為的に増やします。たとえば、MNISTから5の画像を取得し、15度少し回転するとします。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/bcf/555/69f/bcf55569f0ecfda9357d6c1ce1f3e9fb.png"></div><br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/408/bcc/32a/408bcc32a8b3b03094eb4f2b7fdea833.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
これは明らかに同じ図です。しかし、ピクセルレベルでは、MNISTデータベースで利用可能な画像とは大きく異なります。この画像をトレーニングデータセットに追加すると、ネットワークが画像分類についてさらに学習しやすくなると想定するのは妥当です。さらに、画像を1つだけ追加する機能に限定されないことは明らかです。 MNISTからのすべてのトレーニング画像を少し回転させて、トレーニングデータの拡張セットを使用してネットワークの効率を上げることにより、トレーニングデータを拡張できます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
このアイデアは非常に強力で、広く使用されています。</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">科学的研究の</font></a><font style="vertical-align: inherit;">結果を見てみましょう</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">このアイデアのいくつかのバリエーションをMNISTに適用しました。彼らが検討したネットワークのアーキテクチャの1つは、使用するものと似ていました。クロスエントロピーのコスト関数を使用して、800個の隠れたニューロンを持つ直接分配ネットワークです。標準のMNISTトレーニングセットを使用してこのネットワークを起動することにより、98.4％の分類精度が得られました。しかし、それらは、上で説明した回転だけでなく、画像の転送と歪みも使用して、トレーニングデータを拡張しました。高度なデータでネットワークをトレーニングした結果、精度が98.9％に向上しました。彼らはまた、いわゆる手の筋肉の不規則な振動を排除するように設計された、特殊なタイプの画像の歪みである「弾性歪み」。弾性歪みを使用してデータを拡張すると、99.3％の精度が達成されました。基本的に、彼らはネットワークの経験を拡大し、実際の手書きに見られるさまざまな手書きのバリエーションを彼女に与えます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
このアイデアのバリエーションを使用すると、手書き認識だけでなく、多くの学習タスクのパフォーマンスを向上させることができます。一般的な原則は、実際に遭遇する変化を反映する操作をそれらに適用することにより、トレーニングデータを拡張することです。このようなバリエーションは簡単に思い付きます。音声認識システムを作成しているとします。周囲の雑音などの歪みがあっても、人は音声を認識できます。したがって、バックグラウンドノイズを追加してデータを拡張できます。また、加速された音声と遅い音声も認識できます。これは、トレーニングデータを拡張するもう1つの方法です。これらの手法は常に使用されるわけではありません。たとえば、ノイズを追加してトレーニングセットを拡張する代わりに、ノイズフィルターを適用して入力をクリーンアップする方が効率的です。それでも、トレーニングセットを拡張するという考えは覚えておく価値があります。それを使用する方法を探します。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">運動</font></font></h3><br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">上記で説明したように、MNISTからのトレーニングデータを拡張する1つの方法は、トレーニング画像の小さな回転を使用することです。</font><font style="vertical-align: inherit;">画像を任意の角度で回転させると、どのような問題が発生する可能性がありますか？</font></font></li>
</ul><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ビッグデータ余談と分類精度の比較の意味</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
NSの精度がトレーニングセットのサイズによってどのように変化するかを見てみましょう。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b7a/774/4e0/b7a7744e077b9522d251333f647318b2.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
NSを使用する代わりに、別の機械学習テクノロジーを使用して数値を分類するとします。たとえば、第1章で簡単に説明したサポートベクターマシン（SVM）の方法を使用してみましょう。その場合、SVMに慣れていなくても心配する必要はありません。その詳細を理解する必要はありません。 scikit-learnライブラリを通じてSVMを使用します。 SVMの効果がトレーニングセットのサイズによってどのように変化するかを次に示します。比較のため、国会の日程と結果を載せました。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9a6/21b/a21/9a621ba21ebb087e64fc7c68d3238ef5.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
おそらく最初に目を引くのは、NSがどのサイズのトレーニングセットでもSVMを上回ることです。事前定義されたscikit-learn設定を使用し、NSに非常に真剣に取り組んだので、これから広範囲の結論を引き出す価値はありませんが、これは良いことです。グラフから続く、それほど鮮明ではありませんが、より興味深い事実は、50,000の画像を使用してSVMをトレーニングすると、5000の画像でトレーニングされたNSよりもうまく機能することです（精度は94.48％）（ 93.24％）。つまり、トレーニングデータの量が増えると、MOアルゴリズムの違いが補われる場合があります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
もっと面白いことが起こるかもしれません。 2つのアルゴリズムMO、A、Bを使用して問題を解決しようとしていると想定します。場合によっては、あるセットのトレーニングデータでアルゴリズムAがアルゴリズムBよりも先になり、別のセットのトレーニングデータでアルゴリズムBがアルゴリズムAよりも先になることがあります。上ではこれは見られませんでした-グラフは交差します-しかし、</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">これは起こり</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ます。質問に対する正しい答え：「アルゴリズムAはアルゴリズムBより優れているか？」実際、これは、「どのトレーニングデータセットを使用していますか？」</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
開発中と科学論文を読むときの両方で、これらすべてを考慮する必要があります。多くの作品は、標準的な測定データセットでより良い結果を絞り出すための新しいトリックを見つけることに集中しています。 「私たちのスーパーデューパーテクノロジーは、標準の比較セットYをX％改善しました」-このような調査における標準的なアプリケーションフォーム。そのようなステートメントは実際には興味深い場合がありますが、特定のトレーニングセットのコンテキストでのみ適用できることを理解することは価値があります。最初に比較セットを作成した人々がより大きな研究助成金を受け取った別の物語を想像してみてください。彼らは追加のお金を使って追加のデータを収集することができます。より大きなデータセットでは、スーパーデューパーテクノロジーの「改善」がなくなる可能性があります。言い換えると、改善の本質は単なる偶然かもしれません。このことから、実用化の分野には次の道徳性を取り入れるべきです。アルゴリズムの改善とトレーニングデータの改善の両方が必要です。改善されたアルゴリズムを探すことには何の問題もありませんが、トレーニングデータの量や質を増やして勝つための簡単な方法を無視して、これに集中しないようにしてください。</font></font><br>
<br>
<h3></h3><br>
<ul>
<li> .              ?                 .        –  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=">   </a>,  ,   ,      .   ,             .        -   ?   ,      .</li>
</ul><br>
<h3></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
再トレーニングと正規化への没頭は完了しました。</font><font style="vertical-align: inherit;">もちろん、これらの問題に戻ります。</font><font style="vertical-align: inherit;">何度か述べたように、特にコンピューターがより強力になり、より大きなネットワークをトレーニングできるようになると、NSの分野では再トレーニングが大きな問題になります。</font><font style="vertical-align: inherit;">その結果、再トレーニングを減らすために効果的な正則化手法を開発することが急務であり、この分野は今日非常に活発です。</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">重みの初期化</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
NSを作成するときは、重みとオフセットの初期値を選択する必要があります。これまでは、第1章で簡単に説明したガイドラインに従ってそれらを選択しました。重みとオフセットは、数学的な期待値が0、標準偏差が1の独立したガウス分布に基づいて選択したことを思い出してください。それを修正し、初期の重みと変位を割り当てるためのより良い方法を見つけることが可能であるかどうかについて考え、おそらく、NSがより速く学習するのを助けます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
初期化プロセスは、正規化されたガウス分布と比較して大幅に改善できることがわかります。これを理解するために、たとえば1000のような多数の入力ニューロンを持つネットワークで作業するとします。そして、正規化されたガウス分布を使用して、最初の非表示層に接続された重みを初期化するとします。これまでのところ、入力ニューロンを非表示層の最初のニューロンに接続するスケールにのみ焦点を当て、残りのネットワークは無視します。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ac3/c9d/62f/ac3c9d62f301e85234b7cd4bbcbd0e0d.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
簡単にするために、入力ニューロンの半分がオンになっている、つまり値が1で半分がオフになっている、つまり値が0のネットワークを入力xでトレーニングしようとしていると想像してください。次の引数はより一般的なケースで機能しますが、簡単です。この特定の例で彼を理解します。</font><font style="vertical-align: inherit;">隠れニューロンの入力</font><font style="vertical-align: inherit;">の加重和z = ∑ </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> + bを</font><font style="vertical-align: inherit;">考え</font><font style="vertical-align: inherit;">ます。</font><font style="vertical-align: inherit;">合計の500メンバーは、対応するx </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">したがって、0は0です。したがって、zは501の正規化ガウス確率変数、500の重み、および1つの追加オフセットの合計です。</font><font style="vertical-align: inherit;">したがって、z値自体は、数学的な期待値が0、標準偏差が√501≈22.4のガウス分布です。</font><font style="vertical-align: inherit;">つまり、zには鋭いピークのないかなり広いガウス分布があります。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/an/fj/_3/anfj_3qej8-fcxrnig7beewu3qm.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
特に、このグラフは、| z |がかなり大きい、つまりz≫ 1またはz≫ -1である可能性が高いことを示しています。この場合、隠れニューロンの出力σ（z）は1または0に非常に近くなります。これは、隠れニューロンが飽和することを意味します。そして、これが発生すると、すでに知っているように、重みの小さな変化が隠れたニューロンの活性化に小さな変化をもたらします。これらの小さな変化は、ネットワーク内の残りの中性子には実質的に影響を与えず、コスト関数の対応する小さな変化がわかります。その結果、勾配降下アルゴリズムを使用すると、これらの重みのトレーニングが非常に遅くなります。これは、この章で既に説明したタスクに似ています。このタスクでは、誤った値で出力ニューロンが飽和し、学習が遅くなります。以前は、コスト関数を巧みに選択することでこの問題を解決していました。残念ながら、これは出力ニューロンの飽和には役立ちましたが、隠れたニューロンの飽和にはまったく役立ちません。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
次に、最初の非表示レイヤーの着信スケールについて話しました。当然、同じ引数が次の非表示層にも適用されます。後の非表示層の重みが正規化ガウス分布を使用して初期化される場合、それらのアクティブ化は多くの場合0または1に近く、トレーニングは非常に遅くなります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
重みとオフセットに最適な初期化オプションを選択して、そのような飽和が発生しないようにし、学習の遅延を回避する方法はありますか？入ってくる重みの数がn </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">で</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">あるニューロンがあると</font><sub><font style="vertical-align: inherit;">し</font></sub><font style="vertical-align: inherit;">ます。その後、我々は0の数学的期待値と1 /√Nの標準偏差とランダムなガウス分布とこれらの重みを初期化する必要が</font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">で</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。つまり、ガウシアンを圧縮し、ニューロンが飽和する可能性を減らします。次に、数学的な期待値が0で標準偏差が1のバイアスのガウス分布を選択します。これは、後で少し戻るためです。そのような選択をした後、再びz = ∑ </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+ bは、数学的な期待値が0のガウス分布の確率変数ですが、以前よりもはるかに顕著なピークがあります。</font><font style="vertical-align: inherit;">以前と同様に、500入力が0で500が1であるとします。次に、zが数学的な期待値0と標準偏差√（3/2）= 1.22 ...前のグラフと比較して縦軸のスケールを変更しなければならなかったので、このグラフははるかに鋭いピークを持っているので、下の図でも状況がいくぶん控えめに示されています。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/5e/ee/xk/5eeexk-kyxhmi3pdoslo9w_al44.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
そのようなニューロンは、はるかに低い確率で飽和するため、学習が遅くなる可能性は低くなります。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">運動</font></font></h3><br>
<ul>
<li>,     z = ∑<sub>j</sub>w<sub>j</sub>x<sub>j</sub> + b     √(3/2).    :           ;     .</li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
前述のとおり、数学的な期待値が0、標準偏差が1の独立したガウス分布に基づいて、以前と同様に変位を初期化します。これは正常です。これは、ニューロンの飽和の確率を大幅に増加させるわけではないためです。実際、飽和の問題を回避できれば、オフセットの初期化はそれほど重要ではありません。すべてのオフセットをゼロに初期化しようとする人もいますが、勾配降下法は適切なオフセットを学習できるという事実に依存しています。ただし、これが何かに影響を与える可能性は小さいため、以前と同じ初期化手順を引き続き使用します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
MNISTからの数値を分類するタスクを使用して、重みを初期化する新旧のアプローチの結果を比較してみましょう。</font><font style="vertical-align: inherit;">前と同じように、30個の隠れたニューロン、サイズ10のミニパケット、正則化パラメーターとラムダ= 5.0、およびクロスエントロピーを持つコスト関数を使用します。</font><font style="vertical-align: inherit;">学習速度がη= 0.5から0.1に徐々に低下します。これにより、結果がグラフ上で少し見やすくなります。</font><font style="vertical-align: inherit;">重みを初期化する古い方法を使用して学習できます。</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> mnist_loader
<span class="hljs-meta">&gt;&gt;&gt; </span>training_data, validation_data, test_data = \
<span class="hljs-meta">... </span>mnist_loader.load_data_wrapper()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> network2
<span class="hljs-meta">&gt;&gt;&gt; </span>net = network2.Network([<span class="hljs-number">784</span>, <span class="hljs-number">30</span>, <span class="hljs-number">10</span>], cost=network2.CrossEntropyCost)
<span class="hljs-meta">&gt;&gt;&gt; </span>net.large_weight_initializer()
<span class="hljs-meta">&gt;&gt;&gt; </span>net.SGD(training_data, <span class="hljs-number">30</span>, <span class="hljs-number">10</span>, <span class="hljs-number">0.1</span>, lmbda = <span class="hljs-number">5.0</span>,
<span class="hljs-meta">... </span>evaluation_data=validation_data, 
<span class="hljs-meta">... </span>monitor_evaluation_accuracy=<span class="hljs-literal">True</span>)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
また、重みを初期化するための新しいアプローチを使用して学習することもできます。</font><font style="vertical-align: inherit;">デフォルトではnetwork2は新しいアプローチを使用して重みを初期化するため、これはさらに簡単です。</font><font style="vertical-align: inherit;">これは、以前にnet.large_weight_initializer（）呼び出しを省略できることを意味します。</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-meta">&gt;&gt;&gt; </span>net = network2.Network([<span class="hljs-number">784</span>, <span class="hljs-number">30</span>, <span class="hljs-number">10</span>], cost=network2.CrossEntropyCost)
<span class="hljs-meta">&gt;&gt;&gt; </span>net.SGD(training_data, <span class="hljs-number">30</span>, <span class="hljs-number">10</span>, <span class="hljs-number">0.1</span>, lmbda = <span class="hljs-number">5.0</span>,
<span class="hljs-meta">... </span>evaluation_data=validation_data, 
<span class="hljs-meta">... </span>monitor_evaluation_accuracy=<span class="hljs-literal">True</span>)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
プロットします（プログラムweight_initialization.pyを使用）：</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/59d/640/4ab/59d6404ab3c5e01946106b26743ae130.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
どちらの場合でも、96％の分類精度が得られます。結果の精度はどちらの場合もほぼ同じです。しかし、新しい初期化手法はこのポイントにはるかに速く到達します。トレーニングの最後の時代の終わりに、重みを初期化するための古いアプローチは87％の精度に達し、新しいアプローチはすでに93％に近づいています。どうやら、重みを初期化するための新しいアプローチは、はるかに優れた位置から始まるため、優れた結果がはるかに速く得られます。 100個のニューロンを持つネットワークの結果を作成した場合も、同じ現象が観察されます。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ccd/72d/8ef/ccd72d8effddc7815bd526dcb1434acd.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
この場合、2つの曲線は発生しません。しかし、私の実験では、さらにいくつかの時代を追加すると、精度はほぼ一致し始めます。したがって、これらの実験に基づくと、重みの初期化を改善してもトレーニングの速度が上がるだけで、ネットワーク全体の効率が変わるわけではないと言えます。しかし、第4章では、長期的効率が大幅1 /√N介して重みの初期化の結果として改善されたのNSSの例が表示されます</font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">では</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。したがって、学習の速度だけでなく、結果として得られる効果も向上します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
1 /√n </font><sub><font style="vertical-align: inherit;">で</font></sub><font style="vertical-align: inherit;">重みを初期化</font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">する方法</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ニューラルネットワーク学習の改善に役立ちます。</font><font style="vertical-align: inherit;">重みを初期化する他の手法も提案されており、その多くはこの基本的な考え方に基づいています。</font><font style="vertical-align: inherit;">ここではそれらを考慮しません。1/√n </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">は</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">私たちの目的にうまく機能するから</font><font style="vertical-align: inherit;">です。</font><font style="vertical-align: inherit;">興味があれば</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ヨシュア・ベンジオの</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">2012年の作品の</font></a><font style="vertical-align: inherit;"> 14ページと15ページの議論を読むことをお勧めします</font><font style="vertical-align: inherit;">。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">仕事</font></font></h3><br>
<ul>
<li>      .   L2    ,      . ,       .   , , : (1)  λ    ,           ; (2)  ηλ ≪ n,      e<sup>−ηλ/m</sup>   ; (3)  λ    ,   ,       1/√n,  n –     . ,      ,       .</li>
</ul><br>
<br>
<h2>    : </h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
この章で説明するアイデアを実装してみましょう。新しいプログラムnetwork2.pyを開発します。これは、第1章で作成したnetwork.pyプログラムの改良版です。コードを長期間見ていなかった場合は、すぐに実行する価値があるかもしれません。これらはわずか74行のコードであり、簡単に理解できます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
network.pyの場合と同様に、network2.pyのスターはNetworkクラスであり、NSを表すために使用します。対応するネットワーク層のサイズのリストとコスト関数の選択でクラスインスタンスを初期化します。デフォルトでは、クロスエントロピーになります。</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Network</span>(<span class="hljs-params">object</span>):</span><font></font>
<font></font>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, sizes, cost=CrossEntropyCost</span>):</span><font></font>
        self.num_layers = len(sizes)<font></font>
        self.sizes = sizes<font></font>
        self.default_weight_initializer()<font></font>
        self.cost=cost</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
__init__メソッドの最初の数行はnetwork.pyと同じで、それ自体で理解できます。次の2行は新しく、彼らが何をしているかを詳細に理解する必要があります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
default_weight_initializerメソッドから始めましょう。彼は、重みを初期化するための新しい改良されたアプローチを使用しています。見てきたように、このアプローチでは、ニューロンに入る重みは、数学的な期待値が0で標準偏差が1である独立ガウス分布に基づいて初期化され、ニューロンへの着信リンク数の平方根で除算されます。また、このメソッドは、平均が0で標準偏差が1のガウス分布を使用してオフセットを初期化します。コードは次のとおりです。</font></font><br>
<br>
<pre><code class="python hljs">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">default_weight_initializer</span>(<span class="hljs-params">self</span>):</span>
        self.biases = [np.random.randn(y, <span class="hljs-number">1</span>) <span class="hljs-keyword">for</span> y <span class="hljs-keyword">in</span> self.sizes[<span class="hljs-number">1</span>:]]<font></font>
        self.weights = [np.random.randn(y, x)/np.sqrt(x) <font></font>
                        <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> zip(self.sizes[:<span class="hljs-number">-1</span>], self.sizes[<span class="hljs-number">1</span>:])]</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
それを理解するには、npが線形代数を扱うNumpyライブラリであることを覚えておく必要があります。プログラムの最初にインポートしました。また、ニューロンの最初の層の変位は初期化しないことに注意してください。最初のレイヤーはインバウンドなので、オフセットは使用されません。同じことがnetwork.pyでした。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
default_weight_initializerメソッドに加えて、large_weight_initializerメソッドを作成します。これは、第1章の古いアプローチを使用して重みとオフセットを初期化します。重みとオフセットは、数学的な期待値が0で標準偏差が1の独立したガウス分布に基づいて初期化されます。もちろん、このコードはdefault_weight_initializerと大差ありません。</font></font><br>
<br>
<pre><code class="python hljs">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">large_weight_initializer</span>(<span class="hljs-params">self</span>):</span>
        self.biases = [np.random.randn(y, <span class="hljs-number">1</span>) <span class="hljs-keyword">for</span> y <span class="hljs-keyword">in</span> self.sizes[<span class="hljs-number">1</span>:]]<font></font>
        self.weights = [np.random.randn(y, x) <font></font>
                        <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> zip(self.sizes[:<span class="hljs-number">-1</span>], self.sizes[<span class="hljs-number">1</span>:])]</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
この方法を含めたのは、主に、この章と第1章の結果を比較する方が便利なためです。実際に使用することをお勧めできるオプションは想像できません。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
__init__メソッドの2番目の目新しさは、コスト属性の初期化です。これがどのように機能するかを理解するために、クロスエントロピーコスト関数を表すために使用するクラスを見てみましょう（@staticmethodディレクティブは、このメソッドがオブジェクトから独立していることをインタープリターに通知するため、セルフパラメーターはfnメソッドとdeltaメソッドに渡されません）。</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">CrossEntropyCost</span>(<span class="hljs-params">object</span>):</span><font></font>
<font></font>
<span class="hljs-meta">    @staticmethod</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fn</span>(<span class="hljs-params">a, y</span>):</span>
        <span class="hljs-keyword">return</span> np.sum(np.nan_to_num(-y*np.log(a)-(<span class="hljs-number">1</span>-y)*np.log(<span class="hljs-number">1</span>-a)))<font></font>
<font></font>
<span class="hljs-meta">    @staticmethod</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">delta</span>(<span class="hljs-params">z, a, y</span>):</span>
        <span class="hljs-keyword">return</span> (a-y)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
それを理解しましょう。ここで最初にわかるのは、クロスエントロピーは数学的な観点からは関数ですが、Python関数ではなくPythonクラスとして実装することです。なぜこれを行うことにしたのですか？私たちのネットワークでは、価値は2つの異なる役割を果たします。明らか-これは、出力アクティブ化aが目的の出力yにどれだけよく対応するかの尺度です。この役割は、CrossEntropyCost.fnメソッドによって提供されます。 （ちなみに、CrossEntropyCost.fn内でnp.nan_to_numを呼び出すと、Numpyがゼロに近い数値の対数を正しく処理できるようになります）。ただし、コスト関数は2番目の方法でネットワークで使用されます。私たちは、バックプロパゲーションアルゴリズムを開始するとき、我々はネットワークδの出力誤差を検討する必要があること、第2章からリコール</font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Lを</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font><font style="vertical-align: inherit;">出力エラーの形式は、コスト関数によって異なります。コスト関数が異なると、出力エラーの形式も異なります。</font><font style="vertical-align: inherit;">クロスエントロピーの場合、式（66）からの出力エラーは次のようになります。</font></font><br>
<br>
<p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display"><span class="MathJax_SVG" id="MathJax-Element-5-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mtable displaystyle=&quot;true&quot;><mlabeledtr><mtd id=&quot;mjx-eqn-99&quot;><mtext>(99)</mtext></mtd><mtd><msup><mi>&amp;#x03B4;</mi><mi>L</mi></msup><mo>=</mo><msup><mi>a</mi><mi>L</mi></msup><mo>&amp;#x2212;</mo><mi>y</mi></mtd></mlabeledtr></mtable></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="97.25ex" height="3.134ex" viewBox="0 -916.9 41871.4 1349.5" role="img" focusable="false" style="vertical-align: -1.005ex; max-width: 778px;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g transform="translate(40091,0)"><g id="mjx-eqn-99" transform="translate(0,-73)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-28"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-39" x="389" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-39" x="890" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-29" x="1390" y="0"></use></g></g><g transform="translate(18508,0)"><g transform="translate(-14,0)"><g transform="translate(0,-73)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMATHI-3B4" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMATHI-4C" x="641" y="583"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-3D" x="1313" y="0"></use><g transform="translate(2369,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMATHI-61" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMATHI-4C" x="748" y="583"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMAIN-2212" x="3703" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhB6CXMGDnbe46HeQOg3M_EZXaojA#MJMATHI-79" x="4703" y="0"></use></g></g></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable displaystyle="true"><mlabeledtr><mtd id="mjx-eqn-99"><mtext>(99)</mtext></mtd><mtd><msup><mi>δ</mi><mi>L</mi></msup><mo>=</mo><msup><mi>a</mi><mi>L</mi></msup><mo>−</mo><mi>y</mi></mtd></mlabeledtr></mtable></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-5"> \delta^L = a^L-y \tag{99} </script></p><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
したがって、2番目のメソッドCrossEntropyCost.deltaを定義します。その目的は、出力エラーのカウント方法をネットワークに説明することです。次に、これら2つのメソッドを1つのクラスに結合します。1つのクラスには、ネットワークがコスト関数について知る必要があるすべてのものが含まれています。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
同様の理由で、network2.pyには2次コスト関数を表すクラスが含まれています。将来的には主にクロスエントロピーを使用するため、これを第1章の結果との比較に含めます。コードは以下のとおりです。 QuadraticCost.fnメソッドは、出力aと目的の出力yに関連付けられた2次コストの単純な計算です。 QuadraticCost.deltaによって返される値は、第2章で導出した2次値の出力誤差の式（30）に基づいています。</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">QuadraticCost</span>(<span class="hljs-params">object</span>):</span><font></font>
<font></font>
<span class="hljs-meta">    @staticmethod</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fn</span>(<span class="hljs-params">a, y</span>):</span>
        <span class="hljs-keyword">return</span> <span class="hljs-number">0.5</span>*np.linalg.norm(a-y)**<span class="hljs-number">2</span><font></font>
<font></font>
<span class="hljs-meta">    @staticmethod</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">delta</span>(<span class="hljs-params">z, a, y</span>):</span>
        <span class="hljs-keyword">return</span> (a-y) * sigmoid_prime(z)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
これで、network2.pyとnetwork2.pyの主な違いがわかりました。すべてが非常に簡単です。 L2の正則化の実装を含め、以下で説明する他の小さな変更があります。その前に、完全なnetwork2.pyコードを見てみましょう。詳細に学習する必要はありませんが、基本的な構造を理解すること、特にプログラムの各部分が何をしているかを理解するためにコメントを読むことは価値があります。もちろん、私はこの質問を好きなだけ掘り下げることを禁止していません！道に迷った場合は、プログラムの後にテキストを読み、もう一度コードに戻ってみてください。一般的に、これは次のとおりです。</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-string">"""network2.py
~~~~~~~~~~~~~~
  network.py,            .   –      , ,   .     ,    .   ,       .
"""</span><font></font>
<font></font>
<span class="hljs-comment">#### </span>
<span class="hljs-comment"># </span>
<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> random
<span class="hljs-keyword">import</span> sys<font></font>
<font></font>
<span class="hljs-comment">#  </span>
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<font></font>
<font></font>
<font></font>
<span class="hljs-comment">####   ,     </span><font></font>
<font></font>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">QuadraticCost</span>(<span class="hljs-params">object</span>):</span><font></font>
<font></font>
<span class="hljs-meta">    @staticmethod</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fn</span>(<span class="hljs-params">a, y</span>):</span>
        <span class="hljs-string">""" ,    ``a``    ``y``.

        """</span>
        <span class="hljs-keyword">return</span> <span class="hljs-number">0.5</span>*np.linalg.norm(a-y)**<span class="hljs-number">2</span><font></font>
<font></font>
<span class="hljs-meta">    @staticmethod</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">delta</span>(<span class="hljs-params">z, a, y</span>):</span>
        <span class="hljs-string">"""  delta   ."""</span>
        <span class="hljs-keyword">return</span> (a-y) * sigmoid_prime(z)<font></font>
<font></font>
<font></font>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">CrossEntropyCost</span>(<span class="hljs-params">object</span>):</span><font></font>
<font></font>
<span class="hljs-meta">    @staticmethod</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fn</span>(<span class="hljs-params">a, y</span>):</span>
        <span class="hljs-string">""" ,    ``a``    ``y``.                         np.nan_to_num    .  ,   ``a``  ``y``      1.0,   (1-y)*np.log(1-a)  nan. np.nan_to_num ,       (0.0).

        """</span>
        <span class="hljs-keyword">return</span> np.sum(np.nan_to_num(-y*np.log(a)-(<span class="hljs-number">1</span>-y)*np.log(<span class="hljs-number">1</span>-a)))<font></font>
<font></font>
<span class="hljs-meta">    @staticmethod</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">delta</span>(<span class="hljs-params">z, a, y</span>):</span>
        <span class="hljs-string">"""  delta   .  ``z``    ,          delta     .

        """</span>
        <span class="hljs-keyword">return</span> (a-y)<font></font>
<font></font>
<font></font>
<span class="hljs-comment">####   Network </span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Network</span>(<span class="hljs-params">object</span>):</span><font></font>
<font></font>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, sizes, cost=CrossEntropyCost</span>):</span>
<span class="hljs-string">"""  sizes      .  ,      Network      ,     ,     ,    ,  [2, 3, 1].       ,           ``self.default_weight_initializer`` (.  ).
"""</span><font></font>
        self.num_layers = len(sizes)<font></font>
        self.sizes = sizes<font></font>
        self.default_weight_initializer()<font></font>
        self.cost=cost<font></font>
<font></font>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">default_weight_initializer</span>(<span class="hljs-params">self</span>):</span>
        <span class="hljs-string">"""            0    1,       ,       .          0    1.
   ,         ,           .
        """</span>
        self.biases = [np.random.randn(y, <span class="hljs-number">1</span>) <span class="hljs-keyword">for</span> y <span class="hljs-keyword">in</span> self.sizes[<span class="hljs-number">1</span>:]]<font></font>
        self.weights = [np.random.randn(y, x)/np.sqrt(x)<font></font>
                        <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> zip(self.sizes[:<span class="hljs-number">-1</span>], self.sizes[<span class="hljs-number">1</span>:])]<font></font>
<font></font>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">large_weight_initializer</span>(<span class="hljs-params">self</span>):</span>
        <span class="hljs-string">"""          0    1.          0    1.
   ,         ,           .
        1,    .       .
        """</span>
        self.biases = [np.random.randn(y, <span class="hljs-number">1</span>) <span class="hljs-keyword">for</span> y <span class="hljs-keyword">in</span> self.sizes[<span class="hljs-number">1</span>:]]<font></font>
        self.weights = [np.random.randn(y, x)<font></font>
                        <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> zip(self.sizes[:<span class="hljs-number">-1</span>], self.sizes[<span class="hljs-number">1</span>:])]<font></font>
<font></font>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">feedforward</span>(<span class="hljs-params">self, a</span>):</span>
        <span class="hljs-string">"""  ,  ``a``  ."""</span>
        <span class="hljs-keyword">for</span> b, w <span class="hljs-keyword">in</span> zip(self.biases, self.weights):<font></font>
            a = sigmoid(np.dot(w, a)+b)<font></font>
        <span class="hljs-keyword">return</span> a<font></font>
<font></font>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">SGD</span>(<span class="hljs-params">self, training_data, epochs, mini_batch_size, eta,
            lmbda = <span class="hljs-number">0.0</span>,
            evaluation_data=None,
            monitor_evaluation_cost=False,
            monitor_evaluation_accuracy=False,
            monitor_training_cost=False,
            monitor_training_accuracy=False</span>):</span>
        <span class="hljs-string">"""     -    . ``training_data`` –   ``(x, y)``,       .       ,    ``lmbda``.    ``evaluation_data``,     ,   .         ,     ,   .      :   ,    ,   ,              .  ,      30 ,        30 ,        .     ,   .
        """</span>
        <span class="hljs-keyword">if</span> evaluation_data: n_data = len(evaluation_data)<font></font>
        n = len(training_data)<font></font>
        evaluation_cost, evaluation_accuracy = [], []<font></font>
        training_cost, training_accuracy = [], []<font></font>
        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> xrange(epochs):<font></font>
            random.shuffle(training_data)<font></font>
            mini_batches = [<font></font>
                training_data[k:k+mini_batch_size]<font></font>
                <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> xrange(<span class="hljs-number">0</span>, n, mini_batch_size)]
            <span class="hljs-keyword">for</span> mini_batch <span class="hljs-keyword">in</span> mini_batches:<font></font>
                self.update_mini_batch(<font></font>
                    mini_batch, eta, lmbda, len(training_data))<font></font>
            <span class="hljs-keyword">print</span> <span class="hljs-string">"Epoch %s training complete"</span> % j
            <span class="hljs-keyword">if</span> monitor_training_cost:<font></font>
                cost = self.total_cost(training_data, lmbda)<font></font>
                training_cost.append(cost)<font></font>
                <span class="hljs-keyword">print</span> <span class="hljs-string">"Cost on training data: {}"</span>.format(cost)
            <span class="hljs-keyword">if</span> monitor_training_accuracy:<font></font>
                accuracy = self.accuracy(training_data, convert=<span class="hljs-literal">True</span>)<font></font>
                training_accuracy.append(accuracy)<font></font>
                <span class="hljs-keyword">print</span> <span class="hljs-string">"Accuracy on training data: {} / {}"</span>.format(<font></font>
                    accuracy, n)<font></font>
            <span class="hljs-keyword">if</span> monitor_evaluation_cost:<font></font>
                cost = self.total_cost(evaluation_data, lmbda, convert=<span class="hljs-literal">True</span>)<font></font>
                evaluation_cost.append(cost)<font></font>
                <span class="hljs-keyword">print</span> <span class="hljs-string">"Cost on evaluation data: {}"</span>.format(cost)
            <span class="hljs-keyword">if</span> monitor_evaluation_accuracy:<font></font>
                accuracy = self.accuracy(evaluation_data)<font></font>
                evaluation_accuracy.append(accuracy)<font></font>
                <span class="hljs-keyword">print</span> <span class="hljs-string">"Accuracy on evaluation data: {} / {}"</span>.format(<font></font>
                    self.accuracy(evaluation_data), n_data)<font></font>
            <span class="hljs-keyword">print</span>
        <span class="hljs-keyword">return</span> evaluation_cost, evaluation_accuracy, \<font></font>
            training_cost, training_accuracy<font></font>
<font></font>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update_mini_batch</span>(<span class="hljs-params">self, mini_batch, eta, lmbda, n</span>):</span>
        <span class="hljs-string">"""    ,          -. ``mini_batch`` –    ``(x, y)``, ``eta`` –  , ``lmbda`` -  , ``n`` -     ."""</span>
        nabla_b = [np.zeros(b.shape) <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> self.biases]<font></font>
        nabla_w = [np.zeros(w.shape) <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> self.weights]
        <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> mini_batch:<font></font>
            delta_nabla_b, delta_nabla_w = self.backprop(x, y)<font></font>
            nabla_b = [nb+dnb <span class="hljs-keyword">for</span> nb, dnb <span class="hljs-keyword">in</span> zip(nabla_b, delta_nabla_b)]<font></font>
            nabla_w = [nw+dnw <span class="hljs-keyword">for</span> nw, dnw <span class="hljs-keyword">in</span> zip(nabla_w, delta_nabla_w)]<font></font>
        self.weights = [(<span class="hljs-number">1</span>-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw
                        <span class="hljs-keyword">for</span> w, nw <span class="hljs-keyword">in</span> zip(self.weights, nabla_w)]<font></font>
        self.biases = [b-(eta/len(mini_batch))*nb<font></font>
                       <span class="hljs-keyword">for</span> b, nb <span class="hljs-keyword">in</span> zip(self.biases, nabla_b)]<font></font>
<font></font>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backprop</span>(<span class="hljs-params">self, x, y</span>):</span>
        <span class="hljs-string">"""  ``(nabla_b, nabla_w)``,      C_x.  ``nabla_b``  ``nabla_w`` -    numpy,   ``self.biases`` and ``self.weights``."""</span>
        nabla_b = [np.zeros(b.shape) <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> self.biases]<font></font>
        nabla_w = [np.zeros(w.shape) <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> self.weights]
        <span class="hljs-comment">#  </span><font></font>
        activation = x<font></font>
        activations = [x] <span class="hljs-comment">#     </span>
        zs = [] <span class="hljs-comment">#     z-</span>
        <span class="hljs-keyword">for</span> b, w <span class="hljs-keyword">in</span> zip(self.biases, self.weights):<font></font>
            z = np.dot(w, activation)+b<font></font>
            zs.append(z)<font></font>
            activation = sigmoid(z)<font></font>
            activations.append(activation)<font></font>
        <span class="hljs-comment"># backward pass</span>
        delta = (self.cost).delta(zs[<span class="hljs-number">-1</span>], activations[<span class="hljs-number">-1</span>], y)<font></font>
        nabla_b[<span class="hljs-number">-1</span>] = delta<font></font>
        nabla_w[<span class="hljs-number">-1</span>] = np.dot(delta, activations[<span class="hljs-number">-2</span>].transpose())
<span class="hljs-string">"""
 l      ,      . l = 1    , l = 2 – ,   .    ,   python      .
"""</span>
        <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> xrange(<span class="hljs-number">2</span>, self.num_layers):<font></font>
            z = zs[-l]<font></font>
            sp = sigmoid_prime(z)<font></font>
            delta = np.dot(self.weights[-l+<span class="hljs-number">1</span>].transpose(), delta) * sp<font></font>
            nabla_b[-l] = delta<font></font>
            nabla_w[-l] = np.dot(delta, activations[-l<span class="hljs-number">-1</span>].transpose())
        <span class="hljs-keyword">return</span> (nabla_b, nabla_w)<font></font>
<font></font>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">accuracy</span>(<span class="hljs-params">self, data, convert=False</span>):</span>
        <span class="hljs-string">"""    ``data``,      .   –        .
 ``convert``  False,    –    ( )  True,   .    - ,  ``y`` -     .  ,       .           .          ?     –       ,      .   ,      .       mnist_loader.load_data_wrapper.

        """</span>
        <span class="hljs-keyword">if</span> convert:<font></font>
            results = [(np.argmax(self.feedforward(x)), np.argmax(y))<font></font>
                       <span class="hljs-keyword">for</span> (x, y) <span class="hljs-keyword">in</span> data]
        <span class="hljs-keyword">else</span>:<font></font>
            results = [(np.argmax(self.feedforward(x)), y)<font></font>
                        <span class="hljs-keyword">for</span> (x, y) <span class="hljs-keyword">in</span> data]
        <span class="hljs-keyword">return</span> sum(int(x == y) <span class="hljs-keyword">for</span> (x, y) <span class="hljs-keyword">in</span> results)<font></font>
<font></font>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">total_cost</span>(<span class="hljs-params">self, data, lmbda, convert=False</span>):</span>
        <span class="hljs-string">"""      ``data``.  ``convert``
          False,   –  (),   True,   – 
          . .    ,  
            ``accuracy``, .
        """</span>
        cost = <span class="hljs-number">0.0</span>
        <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> data:<font></font>
            a = self.feedforward(x)<font></font>
            <span class="hljs-keyword">if</span> convert: y = vectorized_result(y)<font></font>
            cost += self.cost.fn(a, y)/len(data)<font></font>
        cost += <span class="hljs-number">0.5</span>*(lmbda/len(data))*sum(<font></font>
            np.linalg.norm(w)**<span class="hljs-number">2</span> <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> self.weights)
        <span class="hljs-keyword">return</span> cost<font></font>
<font></font>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">save</span>(<span class="hljs-params">self, filename</span>):</span>
        <span class="hljs-string">"""    ``filename``."""</span>
        data = {<span class="hljs-string">"sizes"</span>: self.sizes,
                <span class="hljs-string">"weights"</span>: [w.tolist() <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> self.weights],
                <span class="hljs-string">"biases"</span>: [b.tolist() <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> self.biases],
                <span class="hljs-string">"cost"</span>: str(self.cost.__name__)}<font></font>
        f = open(filename, <span class="hljs-string">"w"</span>)<font></font>
        json.dump(data, f)<font></font>
        f.close()<font></font>
<font></font>
<span class="hljs-comment">####  Network</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">load</span>(<span class="hljs-params">filename</span>):</span>
    <span class="hljs-string">"""    ``filename``.     Network.
    """</span>
    f = open(filename, <span class="hljs-string">"r"</span>)<font></font>
    data = json.load(f)<font></font>
    f.close()<font></font>
    cost = getattr(sys.modules[__name__], data[<span class="hljs-string">"cost"</span>])<font></font>
    net = Network(data[<span class="hljs-string">"sizes"</span>], cost=cost)<font></font>
    net.weights = [np.array(w) <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> data[<span class="hljs-string">"weights"</span>]]<font></font>
    net.biases = [np.array(b) <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> data[<span class="hljs-string">"biases"</span>]]
    <span class="hljs-keyword">return</span> net<font></font>
<font></font>
<span class="hljs-comment">####  </span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">vectorized_result</span>(<span class="hljs-params">j</span>):</span>
    <span class="hljs-string">"""  10-    1.0   j     .      (0..9)     .

    """</span>
    e = np.zeros((<span class="hljs-number">10</span>, <span class="hljs-number">1</span>))<font></font>
    e[j] = <span class="hljs-number">1.0</span>
    <span class="hljs-keyword">return</span> e<font></font>
<font></font>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmoid</span>(<span class="hljs-params">z</span>):</span>
    <span class="hljs-string">"""."""</span>
    <span class="hljs-keyword">return</span> <span class="hljs-number">1.0</span>/(<span class="hljs-number">1.0</span>+np.exp(-z))<font></font>
<font></font>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmoid_prime</span>(<span class="hljs-params">z</span>):</span>
    <span class="hljs-string">""" ."""</span>
    <span class="hljs-keyword">return</span> sigmoid(z)*(<span class="hljs-number">1</span>-sigmoid(z))</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
より興味深い変更の中には、L2正則化が含まれています。これは大きな概念的な変更ですが、コード内で気付かないかもしれないほど簡単に実装できます。ほとんどの場合、これはlmbdaパラメータをさまざまなメソッド、特にNetwork.SGDに渡すだけです。すべての作業はプログラムの1行で実行され、Network.update_mini_batchメソッドの最後から4行目です。そこで、減量を含むように勾配降下法の更新ルールを変更します。変化はわずかですが、結果に深刻な影響を与えます！</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ちなみに、これはニューラルネットワークに新しい技術を実装するときによく起こります。私たちは正則化について話し合う何千もの言葉を費やしました。概念的には、これはかなり微妙で理解しにくいことです。ただし、簡単にプログラムに追加できます。予期せぬことに、コードを少し変更するだけで複雑な手法を実装できます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
コードのもう1つの小さいが重要な変更は、Network.SGD確率的勾配降下法にいくつかのオプションのフラグを追加することです。これらのフラグにより​​、Network.SGDに送信できるtraining_dataまたはEvaluation_dataのいずれかでコストと精度を追跡することが可能になります。この章の前半で、これらのフラグをよく使用しましたが、念のため、これらのフラグの使用例を示します。</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> mnist_loader
<span class="hljs-meta">&gt;&gt;&gt; </span>training_data, validation_data, test_data = \
<span class="hljs-meta">... </span>mnist_loader.load_data_wrapper()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> network2
<span class="hljs-meta">&gt;&gt;&gt; </span>net = network2.Network([<span class="hljs-number">784</span>, <span class="hljs-number">30</span>, <span class="hljs-number">10</span>], cost=network2.CrossEntropyCost)
<span class="hljs-meta">&gt;&gt;&gt; </span>net.SGD(training_data, <span class="hljs-number">30</span>, <span class="hljs-number">10</span>, <span class="hljs-number">0.5</span>,
<span class="hljs-meta">... </span>lmbda = <span class="hljs-number">5.0</span>,
<span class="hljs-meta">... </span>evaluation_data=validation_data,
<span class="hljs-meta">... </span>monitor_evaluation_accuracy=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>monitor_evaluation_cost=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>monitor_training_accuracy=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>monitor_training_cost=<span class="hljs-literal">True</span>)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
validation_dataを使用して評価データを設定します。ただし、test_dataおよびその他のデータセットのパフォーマンスを追跡できます。また、evaluation_dataとtraining_dataの両方のコストと精度を追跡する必要性を指定する4つのフラグがあります。これらのフラグはデフォルトでFalseに設定されていますが、ネットワークの有効性を追跡するためにここに含まれています。さらに、network2.pyのNetwork.SGDメソッドは、追跡結果を表す4要素のタプルを返します。次のように使用できます。</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-meta">&gt;&gt;&gt; </span>evaluation_cost, evaluation_accuracy, 
<span class="hljs-meta">... </span>training_cost, training_accuracy = net.SGD(training_data, <span class="hljs-number">30</span>, <span class="hljs-number">10</span>, <span class="hljs-number">0.5</span>,
<span class="hljs-meta">... </span>lmbda = <span class="hljs-number">5.0</span>,
<span class="hljs-meta">... </span>evaluation_data=validation_data,
<span class="hljs-meta">... </span>monitor_evaluation_accuracy=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>monitor_evaluation_cost=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>monitor_training_accuracy=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>monitor_training_cost=<span class="hljs-literal">True</span>)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
したがって、たとえば、evaluation_costは、各時代の終わりの推定データのコストを含む30項目のリストになります。このような情報は、ニューラルネットワークの動作を理解するのに非常に役立ちます。このような情報は、ネットワークの動作を理解するのに非常に役立ちます。たとえば、時間の経過に伴うネットワーク学習のグラフを描くために使用できます。このようにして、この章のすべてのグラフを作成しました。ただし、フラグの1つが設定されていない場合、対応するタプル要素は空のリストになります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
その他のコードの追加には、Networkオブジェクトをディスクに保存するNetwork.saveメソッド、およびそれをメモリにロードする機能が含まれます。保存と読み込みはJSONを介して行われ、PythonのピクルまたはcPickleモジュールでは行われません。これらは通常、ディスクに保存してPythonで読み込むために使用されます。 JSONを使用するには、pickleまたはcPickleに必要なコードよりも多くのコードが必要です。なぜJSONを選択したのかを理解するために、将来のある時点で、ネットワーククラスを変更してシグモイドニューロン以外のニューロンが存在するようにしたと想像してください。この変更を実装するには、おそらくNetwork .__ init__メソッドで定義された属性を変更します。また、保存にpickleを使用しただけでは、load関数は機能しません。明示的なシリアル化でJSONを使用すると、保証が簡単になります古いバージョンのネットワークオブジェクトをダウンロードできます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
コードには多くの小さな変更がありますが、これらはnetwork.pyの小さなバリエーションにすぎません。</font><font style="vertical-align: inherit;">最終結果は、74行のプログラムをより機能的な152行のプログラムに拡張したものです。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">仕事</font></font></h3><br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">正規化L1を導入して以下のコードを変更し、それを使用して、30個の隠れニューロンを持つネットワークによってMNIST桁を分類します。</font><font style="vertical-align: inherit;">正規化のないネットワークと比較して結果を改善できる正規化パラメーターを選択できますか？</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">network.pyのNetwork.cost_derivativeメソッドを見てください。</font><font style="vertical-align: inherit;">それは二次の値のために書かれました。</font><font style="vertical-align: inherit;">それをクロスエントロピー値に書き換える方法は？</font><font style="vertical-align: inherit;">そのようなバージョンのプログラムで発生する可能性のある問題を思いつくことができますか？</font><font style="vertical-align: inherit;">network2.pyでは、CrossEntropyCost.deltaにその機能を含めることにより、Network.cost_derivativeメソッドを完全に取り除きました。</font><font style="vertical-align: inherit;">これはあなたが見つけた問題をどのように解決しますか？</font></font></li>
</ul></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../ja459802/index.html">Habr Weekly＃9 /若者のバーンアウト、日本のインターフェース、Battle.netニューラルネットワーク、ゲーム、残酷さ</a></li>
<li><a href="../ja459804/index.html">WordPress + shMapperでクラウドソーシングヘルプカードを作成する</a></li>
<li><a href="../ja459806/index.html">猫ラプヌの扱い方</a></li>
<li><a href="../ja459810/index.html">マイクロサービスまたはモノリス：解決策を探す</a></li>
<li><a href="../ja459814/index.html">レンダリングエンジンとは または、ブラウザの表示モジュールのしくみ</a></li>
<li><a href="../ja459820/index.html">カードをスワイプするだけ：ニューヨークの地下鉄でのOS / 2の使用方法</a></li>
<li><a href="../ja459822/index.html">結果として、単純なニューラルネットワークの例は、何が何であるかを理解します</a></li>
<li><a href="../ja459828/index.html">毎週のニュース：ロシアのHyperloopチケット価格、Apolloコンピューターの主流マイニング、StarCraft IIのAIボット</a></li>
<li><a href="../ja459830/index.html">もちろん、彼らは力と機関銃からのラインを与えました。がんなど...医学の経験</a></li>
<li><a href="../ja459832/index.html">Visual Studioの9つのクールな拡張ルール</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>