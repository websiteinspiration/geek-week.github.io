<!doctype html>
<html class="no-js" lang="fr">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👼 🙍🏼 🎢 GPU Computing - Pourquoi, quand et comment. Plus quelques tests 🛀🏼 🚜 ☣️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Tout le monde sait depuis longtemps que sur les cartes vidéo, vous pouvez non seulement jouer à des jouets, mais aussi effectuer des choses qui ne son...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>GPU Computing - Pourquoi, quand et comment. Plus quelques tests</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/dbtc/blog/498374/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tout le monde sait depuis longtemps que sur les cartes vidéo, vous pouvez non seulement jouer à des jouets, mais aussi effectuer des choses qui ne sont pas liées aux jeux, par exemple, former un réseau de neurones, vous souvenir de la crypto-monnaie ou effectuer des calculs scientifiques. Comment cela s'est passé, vous pouvez le lire </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ici</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , mais je voulais aborder la question de </font></font><i><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">savoir pourquoi le GPU peut être intéressant pour le</font></font></strong></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> programmeur moyen (non lié à GameDev) </font></font><i><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">comment aborder le</font></font></strong></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> développement sur le GPU sans y consacrer beaucoup de temps, </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">décider</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> si regardez dans cette direction, et " </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">déterminez sur vos doigts" quel profit vous pouvez obtenir.</font></font></strong>&nbsp;<br>
<br>
<div style="text-align:center;"><img width="800" src="https://habrastorage.org/getpro/habr/post_images/3ee/2ac/893/3ee2ac8936a685e6993966cfa40f53fd.jpg"></div><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
L'article a été écrit sur la base de ma </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">présentation</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> en HighLoad ++. </font><font style="vertical-align: inherit;">Il aborde principalement les technologies proposées par NVIDIA. </font><font style="vertical-align: inherit;">Je n'ai aucun but de faire la publicité de produits, je les donne simplement à titre d'exemple, et à coup sûr, quelque chose de similaire peut être trouvé chez les fabricants concurrents.</font></font><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pourquoi compter sur le GPU?</font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Deux processeurs peuvent être comparés selon différents critères, les plus populaires étant probablement la fréquence et le nombre de cœurs, la taille des caches, etc., mais au final, nous nous intéressons au nombre d'opérations qu'un processeur peut effectuer par unité de temps, de quel type d'opération il s'agit, mais une question distincte Une métrique commune est le nombre d'opérations en virgule flottante par seconde - flops. Et lorsque nous voulons comparer le chaud au doux, et dans notre cas le GPU avec le CPU, cette métrique est très pratique. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Le graphique ci-dessous montre la croissance de ces mêmes flops au fil du temps pour les processeurs et les cartes vidéo.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5e2/048/3f5/5e20483f59e87b0a395b0fae0e6495c5.png"></div><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(Les données sont collectées à partir de sources ouvertes, il n'y a pas de données pour 2019-20 ans, car tout n'est pas si beau là-bas, mais les GPU gagnent toujours)</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Eh bien, c'est tentant, n'est-ce pas? </font><font style="vertical-align: inherit;">Nous déplaçons tous les calculs du CPU vers le GPU et obtenons huit fois les meilleures performances! </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Mais, bien sûr, tout n'est pas si simple. </font><font style="vertical-align: inherit;">Vous ne pouvez pas tout prendre et tout transférer sur le GPU, pourquoi, nous parlerons plus loin.</font></font><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Architecture GPU et sa comparaison avec le CPU</font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
J'apporte à beaucoup une image familière de l'architecture du CPU et des éléments de base:</font></font><br>
<br>
<div style="text-align:center;"><img width="400" src="https://habrastorage.org/getpro/habr/post_images/df0/8c2/4c3/df08c24c3fe92cd97356670729c318cd.png"></div><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">CPU Core</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Qu'est-ce qui est si spécial? </font><font style="vertical-align: inherit;">Un cœur et un tas de blocs auxiliaires. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Voyons maintenant l'architecture GPU:</font></font><br>
<br>
<div style="text-align:center;"><img width="400" src="https://habrastorage.org/getpro/habr/post_images/0fe/138/0cc/0fe1380ccbb321b289d16e39a499009a.png"></div><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GPU Core</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Une carte vidéo a beaucoup de cœurs de traitement, généralement plusieurs milliers, mais ils sont combinés en blocs; pour les cartes vidéo NVIDIA, généralement 32 chacune, et ont des éléments communs, y compris et registres. L'architecture du noyau GPU et des éléments logiques est beaucoup plus simple que sur le CPU, à savoir, il n'y a pas de prefetchers, de prédicteurs de brunch et bien plus encore. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Eh bien, ce sont les points clés de la différence dans l'architecture du CPU et du GPU, et, en fait, ils imposent des restrictions ou, inversement, ouvrent les possibilités de ce que nous pouvons lire efficacement sur le GPU.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Je n'ai pas mentionné un autre point important, généralement la carte vidéo et le processeur ne «fouinent» pas entre eux et n'écrivent pas de données sur la carte vidéo et ne lisent pas le résultat - ce sont des opérations distinctes et peuvent se révéler être un «goulot d'étranglement» dans votre système, un graphique du temps de pompage par rapport à la taille les données sont données plus loin dans l'article.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Limitations et fonctionnalités du GPU</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Quelles limitations cette architecture impose-t-elle aux algorithmes exécutables:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Si nous calculons sur un GPU, alors nous ne pouvons pas sélectionner un seul cœur, un bloc entier de cœurs sera alloué (32 pour NVIDIA).</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tous les cœurs exécutent les mêmes instructions, mais avec des données différentes (nous en parlerons plus tard), ces calculs sont appelés Single-Instruction-Multiple-Data ou SIMD (bien que NVIDIA introduise son raffinement).&nbsp;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En raison de l'ensemble relativement simple de blocs logiques et de registres généraux, le GPU n'aime vraiment pas la ramification, et en fait la logique complexe dans les algorithmes.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Quelles opportunités ouvre-t-il:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En fait, l'accélération de ces mêmes calculs SIMD. </font><font style="vertical-align: inherit;">L'exemple le plus simple est l'ajout élémentaire de matrices, et analysons-le.</font></font></li>
</ul><br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Réduction des algorithmes classiques à la représentation SIMD</font></font></h1><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Transformation</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nous avons deux tableaux, A et B, et nous voulons ajouter un élément du tableau B à chaque élément du tableau A. Ci-dessous est un exemple en C, bien que j'espère qu'il sera clair pour ceux qui ne parlent pas ce langage:</font></font><br>
<br>
<pre><code class="cpp hljs"><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">func</span><span class="hljs-params">(<span class="hljs-keyword">float</span> *A, <span class="hljs-keyword">float</span> *B, size)</span>
</span>{ 
   <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; size; i++) <font></font>
   { <font></font>
       A[i] += B[i]<font></font>
   } <font></font>
}<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bouclage classique des éléments dans une boucle et exécution linéaire. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Voyons maintenant à quoi ressemblera ce code pour le GPU:</font></font><br>
<br>
<pre><code class="cpp hljs"><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">func</span><span class="hljs-params">(<span class="hljs-keyword">float</span> *A, <span class="hljs-keyword">float</span> *B, size)</span> 
</span>{ 
   <span class="hljs-keyword">int</span> i = threadIdx.x; 
   <span class="hljs-keyword">if</span> (i &lt; size) <font></font>
      A[i] += B[i] <font></font>
}<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Et ici, c'est déjà intéressant, la variable threadIdx est apparue, que nous ne semblions déclarer nulle part. Oui, son système nous fournit. Imaginez que dans l'exemple précédent, le tableau se compose de trois éléments et que vous souhaitez l'exécuter dans trois threads parallèles. Pour ce faire, vous devez ajouter un autre paramètre - le numéro d'index ou de flux. C'est ce que la carte vidéo fait pour nous, bien qu'elle passe l'index en tant que variable statique et peut fonctionner avec plusieurs dimensions à la fois - x, y, z. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Autre nuance, si vous souhaitez démarrer un grand nombre de flux parallèles à la fois, les flux devront être divisés en blocs (une caractéristique architecturale des cartes vidéo). La taille de bloc maximale dépend de la carte vidéo, et l'indice de l'élément pour lequel nous effectuons les calculs devra être obtenu comme suit:</font></font><br>
<br>
<pre><code class="cpp hljs"><span class="hljs-keyword">int</span> i = blockIdx.x * blockDim.x + threadIdx.x; <span class="hljs-comment">// blockIdx –  , blockDim –  , threadIdx –    </span>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En conséquence, ce que nous avons: beaucoup de threads exécutés en parallèle qui exécutent le même code, mais avec des indices différents, et, par conséquent, des données, c'est-à-dire </font><font style="vertical-align: inherit;">le même SIMD. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
C'est l'exemple le plus simple, mais si vous souhaitez travailler avec le GPU, vous devez amener votre tâche sous la même forme. </font><font style="vertical-align: inherit;">Malheureusement, ce n'est pas toujours possible et dans certains cas peut faire l'objet d'une thèse de doctorat, mais néanmoins, les algorithmes classiques peuvent encore être amenés à cette forme.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Agrégation</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Voyons maintenant à quoi ressemblera l'agrégation castée en représentation SIMD:</font></font><br>
&nbsp;<br>
<div style="text-align:center;"><img width="400" src="https://habrastorage.org/getpro/habr/post_images/ecd/78a/bba/ecd78abbaff0c1be8799c1337f7652f8.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nous avons un tableau de n éléments. </font><font style="vertical-align: inherit;">À la première étape, nous démarrons n / 2 threads et chaque thread ajoute deux éléments, à savoir </font><font style="vertical-align: inherit;">en une seule itération, nous additionnons la moitié des éléments du tableau. </font><font style="vertical-align: inherit;">Et puis dans la boucle, nous répétons la même chose pour le tableau nouvellement créé, jusqu'à ce que nous agrégions les deux derniers éléments. </font><font style="vertical-align: inherit;">Comme vous pouvez le voir, plus la taille du tableau est petite, moins nous pouvons démarrer de threads parallèles, c'est-à-dire </font><font style="vertical-align: inherit;">sur un GPU, il est logique d'agréger des tableaux d'une taille suffisamment grande. </font><font style="vertical-align: inherit;">Un tel algorithme peut être utilisé pour calculer la somme des éléments (en passant, n'oubliez pas le débordement possible du type de données avec lequel vous travaillez), rechercher le maximum, le minimum ou simplement la recherche.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tri</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Mais le tri semble déjà beaucoup plus compliqué. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Les deux algorithmes de tri les plus populaires sur le GPU sont:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tri bitonique</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Radix-sort</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Mais radix-sort est toujours utilisé plus souvent, et une implémentation prête pour la production peut être trouvée dans certaines bibliothèques. </font><font style="vertical-align: inherit;">Je n'analyserai pas en détail le fonctionnement de ces algorithmes; ceux qui sont intéressés peuvent trouver une description de radix-sort sur </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">https://www.codeproject.com/Articles/543451/Parallel-Radix-Sort-on-the-GPU-using-Cplusplus- AMP</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> et </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">https://stackoverflow.com/a/26229897</font></font></a><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Mais l'idée est que même un algorithme non linéaire tel que le tri peut être réduit à une vue SIMD. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Et maintenant, avant de regarder les vrais chiffres qui peuvent être obtenus à partir du GPU, essayons de comprendre comment programmer pour ce miracle de la technologie?</font></font><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Où commencer</font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Les deux technologies les plus courantes pouvant être utilisées pour la programmation sous le GPU:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Opencl</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cuda</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
OpenCL est une norme prise en charge par la plupart des fabricants de cartes vidéo, notamment </font><font style="vertical-align: inherit;">et sur les appareils mobiles, le code écrit en OpenCL peut également être exécuté sur le CPU. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Vous pouvez utiliser OpenCL à partir de C / C ++, il existe des liants vers d'autres langages. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pour OpenCL, j'ai le plus apprécié le livre </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">OpenCL en action</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Il décrit également différents algorithmes sur le GPU, notamment </font><font style="vertical-align: inherit;">Tri bitonique et tri Radix. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
CUDA est la technologie et le SDK exclusifs de NVIDIA. </font><font style="vertical-align: inherit;">Vous pouvez écrire en C / C ++ ou utiliser des liaisons vers d'autres langages.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Comparer OpenCL et CUDA n'est pas correct, car </font><font style="vertical-align: inherit;">l'un est la norme, l'autre est l'ensemble du SDK. </font><font style="vertical-align: inherit;">Néanmoins, beaucoup de gens choisissent CUDA pour le développement de cartes vidéo, malgré le fait que la technologie est propriétaire, bien que gratuite et ne fonctionne que sur les cartes NVIDIA. </font><font style="vertical-align: inherit;">Il y a plusieurs raisons à cela:</font></font><br>
<br>
<ul>
<li>  API</li>
<li>    </li>
<li>,   GPU,      (host) </li>
<li> ,  ..  </li>
<li>   </li>
<li>  </li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Les particularités incluent le fait que CUDA est livré avec son propre compilateur, qui peut également compiler du code C / C ++ standard. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Le livre CUDA le plus complet que j'ai rencontré était </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Professional CUDA C Programming</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , bien qu'il soit déjà un peu dépassé, il aborde néanmoins de nombreuses nuances techniques de programmation pour les cartes NVIDIA. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Mais que faire si je ne veux pas passer quelques mois à lire ces livres, écrire mon propre programme pour une carte vidéo, tester et déboguer, puis découvrir que ce n'est pas pour moi?&nbsp; </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Comme je l'ai dit, il existe un grand nombre de bibliothèques qui cachent la complexité du développement sous le GPU: XGBoost, cuBLAS, TensorFlow, PyTorch et autres, nous considérerons la bibliothèque de </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">poussée</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, car il est moins spécialisé que les autres bibliothèques ci-dessus, mais en même temps il implémente des algorithmes de base, par exemple, le tri, la recherche, l'agrégation et, avec une forte probabilité, il peut être applicable à vos tâches. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Thrust est une bibliothèque C ++ qui vise à "remplacer" les algorithmes STL standard par des algorithmes basés sur GPU. </font><font style="vertical-align: inherit;">Par exemple, le tri d'un tableau de nombres à l'aide de cette bibliothèque sur une carte vidéo ressemblerait à ceci:</font></font><br>
<br>
<pre><code class="cpp hljs"><span class="hljs-function">thrust::host_vector&lt;DataType&gt; <span class="hljs-title">h_vec</span><span class="hljs-params">(size)</span></span>; <span class="hljs-comment">//    </span>
<span class="hljs-built_in">std</span>::generate(h_vec.begin(), h_vec.end(), rand); <span class="hljs-comment">//   </span>
thrust::device_vector&lt;DataType&gt; d_vec = h_vec; <span class="hljs-comment">//        &nbsp;</span>
thrust::sort(d_vec.begin(), d_vec.end()); <span class="hljs-comment">//    </span>
thrust::copy(d_vec.begin(), d_vec.end(), h_vec.begin()); <span class="hljs-comment">//   ,     </span>
</code></pre><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(n'oubliez pas que l'exemple doit être compilé par un compilateur de NVIDIA)</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Comme vous pouvez le voir, thrust :: sort est très similaire à un algorithme similaire de STL. Cette bibliothèque cache de nombreuses difficultés, en particulier le développement d'un sous-programme (plus précisément, le noyau), qui sera exécuté sur la carte vidéo, mais en même temps prive de flexibilité. Par exemple, si nous voulons trier plusieurs gigaoctets de données, il serait logique d'envoyer une donnée à la carte pour commencer le tri, et pendant le tri, envoyez plus de données à la carte. Cette approche est appelée masquage de latence et permet une utilisation plus efficace des ressources de mappage de serveur, mais, malheureusement, lorsque nous utilisons des bibliothèques de haut niveau, ces opportunités restent cachées. Mais pour le prototypage et la mesure des performances, ce sont les mêmes, en particulier avec la poussée, vous pouvez mesurer les frais généraux fournis par le transfert de données. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
J'ai écrit une petite </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">référence</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> en utilisant cette bibliothèque, qui exécute plusieurs algorithmes populaires avec différentes quantités de données sur le GPU, voyons quels sont les résultats.</font></font><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Résultats de l'algorithme GPU</font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pour tester le GPU, j'ai pris une instance dans AWS avec une carte vidéo Tesla k80, ce n'est pas la carte serveur la plus puissante à ce jour (la Tesla v100 la plus puissante), mais la plus abordable et embarquée:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">4992 CUDA Kernels</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">24 Go de mémoire</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">480 Gb / s - bande passante mémoire&nbsp;</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Et pour les tests sur le CPU, j'ai pris une instance avec un processeur Intel Xeon CPU E5-2686 v4 @ 2.30GHz</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Transformation</font></font></h2><br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/930/6e1/687/9306e1687be5ee95c29c8aac7b2ae337.png"></div><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Temps d'exécution de la transformation sur le GPU et le CPU en ms</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Comme vous pouvez le voir, la transformation habituelle des éléments du tableau est approximativement la même dans le temps, à la fois sur le GPU et sur le CPU. </font><font style="vertical-align: inherit;">Et pourquoi? </font><font style="vertical-align: inherit;">Parce que le surcoût pour l'envoi de données vers la carte et le dos absorbe toute l'augmentation des performances (nous parlerons du surcoût séparément), et il y a relativement peu de calculs sur la carte. </font><font style="vertical-align: inherit;">De plus, n'oubliez pas que les processeurs prennent également en charge les instructions SIMD et que les compilateurs dans des cas simples peuvent les utiliser efficacement.&nbsp;</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Voyons maintenant l'efficacité de l'agrégation sur le GPU.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Agrégation</font></font></h2><br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c92/d0e/cb9/c92d0ecb96c32866000e6948f5da61f9.png"></div><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Temps d'exécution d'agrégation sur GPU et CPU en ms</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Dans l'exemple d'agrégation, nous constatons déjà une augmentation significative des performances avec une augmentation du volume de données. </font><font style="vertical-align: inherit;">Il convient également de prêter attention au fait que nous pompons une grande quantité de données dans la mémoire de la carte, et qu'une seule valeur agrégée est reprise, c'est-à-dire </font><font style="vertical-align: inherit;">Les frais généraux pour le transfert de données de la carte vers la RAM sont minimes. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Passons à l'exemple le plus intéressant - le tri.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tri</font></font></h2><br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fde/504/8da/fde5048da5084d1f0902c9362b21d939.png"></div><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Temps de tri vers le GPU et le CPU en ms</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Malgré le fait que nous envoyons l'intégralité du tableau de données à la carte vidéo et vice versa, le tri vers le GPU 800 Mo de données est environ 25 fois plus rapide que sur le processeur.</font></font><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Frais généraux de transfert de données</font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Comme le montre l'exemple de transformation, il n'est pas toujours évident que le GPU sera efficace même dans les tâches qui sont bien parallèles. </font><font style="vertical-align: inherit;">La raison en est un surcoût pour le transfert de données de la RAM de l'ordinateur vers la mémoire de la carte vidéo (dans les consoles de jeu, en passant, la mémoire est partagée entre le CPU et le GPU, et il n'est pas nécessaire de transférer des données). </font><font style="vertical-align: inherit;">Une des caractéristiques d'une carte vidéo est la bande passante mémoire ou bande passante mémoire, qui détermine la bande passante théorique de la carte. </font><font style="vertical-align: inherit;">Pour Tesla k80, elle est de 480 Go / s, pour Tesla v100, elle est déjà de 900 Go / s. </font><font style="vertical-align: inherit;">De plus, la version PCI Express et l'implémentation de la façon dont vous transférerez les données sur la carte affecteront le débit, par exemple, cela peut être fait dans plusieurs flux parallèles.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Examinons les résultats pratiques obtenus pour la carte graphique Tesla k80 dans le cloud Amazon:</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/798/fb5/613/798fb56139f6158566232bc6283b24e7.png"></div><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Temps de transfert des données vers le GPU, tri et transfert des données vers la RAM en ms </font></font></i><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">HtoD - transfert des données vers la carte vidéo </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
GPU Exécution - tri sur la carte vidéo </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
DtoH - copie des données de la carte vidéo vers la RAM</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
La première chose à noter est que la lecture des données de la carte vidéo est plus rapide que écrivez-les là-bas. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La seconde - lorsque vous travaillez avec une carte vidéo, vous pouvez obtenir une latence de 350 microsecondes, et cela peut déjà être suffisant pour certaines applications à faible latence. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Le graphique ci-dessous montre une surcharge pour plus de données:</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d18/653/b96/d18653b96af325f35fade713bdaa8dae.png"></div><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Temps de transfert des données vers le GPU, tri et transfert des données vers la RAM en ms</font></font></i><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Utilisation du serveur</font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La question la plus courante est de savoir en quoi une carte vidéo de jeu diffère d'une carte serveur? </font><font style="vertical-align: inherit;">Selon les caractéristiques, ils sont très similaires, mais les prix diffèrent considérablement.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/203/01b/741/20301b7418ee616d9611f42d2b4a8f5d.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Les principales différences entre le serveur (NVIDIA) et la carte de jeu:</font></font><br>
<br>
<ul>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Garantie du fabricant</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (la carte de jeu n'est pas conçue pour une utilisation sur serveur)</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Problèmes de </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">virtualisation</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> possibles </font><font style="vertical-align: inherit;">pour une carte graphique grand public</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Disponibilité du mécanisme de correction d'erreur sur la carte serveur</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Le nombre de threads parallèles (pas les cœurs CUDA) ou la prise en charge d'Hyper-Q, qui vous permet de travailler avec la carte à partir de plusieurs threads sur le CPU, par exemple, télécharger des données sur la carte à partir d'un thread et démarrer les calculs à partir d'un autre</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ce sont peut-être les principales différences importantes que j'ai trouvées.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Multithreading</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Après avoir compris comment exécuter l'algorithme le plus simple sur la carte vidéo et quels résultats peuvent être attendus, la prochaine question logique est de savoir comment la carte vidéo se comportera lors du traitement de plusieurs demandes parallèles. En réponse, j'ai deux graphiques de calcul sur le GPU et un processeur à 4 et 32 ​​cœurs:</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0a1/1f6/022/0a11f6022198a582929f384be357fe43.png"></div><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Temps nécessaire pour effectuer des calculs mathématiques sur le GPU et le CPU avec des matrices de 1000 x 60 en ms</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
. Ce graphique effectue des calculs avec des matrices de 1000 x 60 éléments. </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Les calculs sont</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> lancés à </font><font style="vertical-align: inherit;">partir de plusieurs flux de programme, un flux séparé est créé pour le GPU pour chaque flux CPU (le très Hyper-Q est utilisé).&nbsp; </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Comme vous pouvez le voir, le processeur gère très bien cette charge, tandis que la latence pour une demande par GPU augmente considérablement avec une augmentation du nombre de demandes parallèles.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e09/22c/7fb/e0922c7fba0ef001cca97c7a99817c83.png"></div><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Le temps pour effectuer des calculs mathématiques sur le GPU et le CPU avec des matrices 10 000 x 60 en ms.</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Sur le deuxième graphique, les mêmes calculs, mais avec des matrices 10 fois plus longues, et le GPU se comporte beaucoup mieux sous une telle charge. Ces graphiques sont très indicatifs, et nous pouvons conclure: le comportement sous charge dépend de la nature de la charge elle-même. Un processeur peut également gérer les calculs matriciels assez efficacement, mais dans une certaine mesure. Pour une carte vidéo, il est caractéristique que pour une petite charge de calcul, les performances chutent de façon approximativement linéaire. Avec une augmentation de la charge et du nombre de threads parallèles, la carte vidéo s'adapte mieux.&nbsp;</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Il est difficile de supposer comment le GPU se comportera dans diverses situations, mais comme vous pouvez le voir, dans certaines conditions, une carte serveur peut traiter les demandes de plusieurs flux parallèles assez efficacement. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nous discuterons de quelques autres questions que vous pourriez avoir si vous décidez toujours d'utiliser le GPU dans vos projets.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Limite de ressources</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Comme nous l'avons déjà dit, les deux principales ressources d'une carte vidéo sont le calcul des cœurs et de la mémoire. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Par exemple, nous avons plusieurs processus ou conteneurs utilisant une carte vidéo, et nous aimerions pouvoir partager la carte vidéo entre eux. </font><font style="vertical-align: inherit;">Malheureusement, il n'y a pas d'API simple pour cela. </font><font style="vertical-align: inherit;">NVIDIA propose la technologie </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">vGPU</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , mais je n'ai pas trouvé la carte Tesla k80 dans la liste des cartes prises en charge, et d'après ce que je peux comprendre de la description, la technologie est plus axée sur les écrans virtuels que sur les calculs. </font><font style="vertical-align: inherit;">AMD propose peut-être quelque chose de plus approprié. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Par conséquent, si vous prévoyez d'utiliser le GPU dans vos projets, vous devez vous fier au fait que l'application utilisera exclusivement la carte vidéo, ou vous contrôlerez par programme la quantité de mémoire allouée et le nombre de cœurs utilisés pour les calculs.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Conteneurs et GPU</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Si vous avez déterminé la limite de ressources, alors la question logique suivante: que faire s'il y a plusieurs cartes vidéo sur le serveur? </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Encore une fois, vous pouvez décider au niveau de l'application quel GPU il utilisera. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Les conteneurs Docker sont un autre moyen plus pratique. </font><font style="vertical-align: inherit;">Vous pouvez utiliser des conteneurs réguliers, mais NVIDIA propose ses conteneurs </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">NGC</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , avec des versions optimisées de divers logiciels, bibliothèques et pilotes. </font><font style="vertical-align: inherit;">Pour un conteneur, vous pouvez limiter le nombre de GPU utilisés et leur visibilité sur le conteneur. </font><font style="vertical-align: inherit;">Les frais généraux liés à l'utilisation des conteneurs sont d'environ 3%.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Travailler en cluster</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Une autre question, que faire si vous souhaitez effectuer une tâche sur plusieurs GPU au sein du même serveur ou cluster? </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Si vous avez choisi une bibliothèque similaire à Thrust ou une solution de niveau inférieur, la tâche devra être résolue manuellement. </font><font style="vertical-align: inherit;">Les cadres de haut niveau, tels que pour l'apprentissage automatique ou les réseaux de neurones, prennent généralement en charge la possibilité d'utiliser plusieurs cartes prêtes à l'emploi. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
De plus, je voudrais noter que, par exemple, NVIDIA propose une interface pour l'échange direct de données entre les cartes - </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">NVLINK</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , qui est nettement plus rapide que PCI Express. </font><font style="vertical-align: inherit;">Et il existe une technologie pour l'accès direct à la mémoire de la carte à partir d'autres périphériques PCI Express - </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GPUDirect RDMA</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , incl. </font><font style="vertical-align: inherit;">et </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">réseau</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Recommandations</font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Si vous envisagez d'utiliser le GPU dans vos projets, le GPU vous convient probablement si:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Votre tâche peut être réduite à une vue SIMD</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Il est possible de charger la plupart des données sur la carte avant les calculs (cache)</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Le défi passe par l'informatique intensive</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Vous devez également poser des questions à l'avance:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Combien de requêtes parallèles seront&nbsp;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Quelle latence attendez-vous</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Avez-vous besoin d'une carte pour votre charge? Avez-vous besoin d'un serveur avec plusieurs cartes ou d'un cluster de serveurs GPU&nbsp;</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
C'est tout, j'espère que le matériel vous sera utile et vous aidera à prendre la bonne décision!</font></font><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Références</font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Benchmark et résultats sur github - </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">https://github.com/tishden/gpu_benchmark/tree/master/cuda</font></font></a><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
En plus du sujet, un enregistrement du rapport </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">«GPU Databases - Architecture, Performance and Prospects for Use»</font></font></a><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
NVIDIA NGC Containers Webinar Webinaires - </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">http : //bit.ly/2UmVIVt</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ou </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">http://bit.ly/2x4vJKF</font></font></a></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr498362/index.html">Kingston conserve son leadership dans les livraisons de SSD: comment faire?</a></li>
<li><a href="../fr498366/index.html">Quels algorithmes les développeurs Yandex implémentent-ils chaque jour</a></li>
<li><a href="../fr498368/index.html">L'histoire d'un interrupteur</a></li>
<li><a href="../fr498370/index.html">SAP UI5 et Windows de confirmation: encore une fois sur le contexte</a></li>
<li><a href="../fr498372/index.html">Tutoriel de simulateur de réseau ns-3. Chapitre 5</a></li>
<li><a href="../fr498378/index.html">Annonce de l'école du soir de Slurm par Agile</a></li>
<li><a href="../fr498380/index.html">La fenêtre d'Overton en action: comment une pandémie est utilisée pour limiter notre liberté</a></li>
<li><a href="../fr498390/index.html">IAR + Clion = amitié</a></li>
<li><a href="../fr498392/index.html">18 Les fonctionnalités de GitLab deviennent open source</a></li>
<li><a href="../fr498394/index.html">7 analogues gratuits de Screaming Frog et Netpeak Spider</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>