<!doctype html>
<html class="no-js" lang="zh-CN">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>☀️ 🤹🏻 👨🏿‍🏭 关于在Python中实现深度学习库 🦗 👨‍👨‍👧‍👦 🆙</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="从简单的神经网络到相当复杂的架构，深度学习技术在短时间内取得了长足的进步。为了支持这些技术的快速传播，已经开发了各种库和深度学习平台。这种库的主要目标之一是为开发人员提供简单的界面，以创建和训练神经网络模型。这样的库允许其用户更加关注要解决的任务，而不是模型实现的精妙之处。为此，您可能需要将基本机制...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>关于在Python中实现深度学习库</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ruvds/blog/486686/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">从简单的神经网络到相当复杂的架构，深度学习技术在短时间内取得了长足的进步。为了支持这些技术的快速传播，已经开发了各种库和深度学习平台。这种库的主要目标之一是为开发人员提供简单的界面，以创建和训练神经网络模型。这样的库允许其用户更加关注要解决的任务，而不是模型实现的精妙之处。为此，您可能需要将基本机制的实现隐藏在几个抽象级别的后面。而这反过来又使对深度学习库所基于的基本原理的理解更加复杂。</font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><img src="https://habrastorage.org/webt/bp/yi/sl/bpyislfb1o7e-qh7exvklh1oxuw.jpeg"></a><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
这篇文章（我们正在翻译的译本）旨在分析深度学习库的低级构建块设备的功能。</font><font style="vertical-align: inherit;">首先，我们简要讨论深度学习的本质。</font><font style="vertical-align: inherit;">这将使我们了解相应软件的功能要求。</font><font style="vertical-align: inherit;">然后，我们看一下使用NumPy在Python中开发一个简单但有效的深度学习库。</font><font style="vertical-align: inherit;">该库能够为简单的神经网络模型提供端到端的培训。</font><font style="vertical-align: inherit;">在此过程中，我们将讨论深度学习框架的各个组成部分。</font><font style="vertical-align: inherit;">我们将考虑的库很小，不到100行代码。</font><font style="vertical-align: inherit;">这意味着将很容易弄清楚。</font><font style="vertical-align: inherit;">我们将处理的完整项目代码可在</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">此处</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">找到</font><font style="vertical-align: inherit;">。</font></font><br>
<a name="habracut"></a><br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">一般信息</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
通常，深度学习库（例如TensorFlow和PyTorch）由下图所示的组件组成。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c49/b9f/396/c49b9f39652c0260a9e30ee4e5dea146.jpg"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">深度学习框架</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
的组件让我们分析这些组件。</font></font><br>
<br>
<h3><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">▍运算符</font></font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
“操作员”和“层”（层）的概念通常可以互换使用。</font><font style="vertical-align: inherit;">这些是任何神经网络的基本构建块。</font><font style="vertical-align: inherit;">运算符是转换数据的向量函数。</font><font style="vertical-align: inherit;">在经常使用的运算符中，可以区分诸如线性和卷积层，子采样层（合并），半线性（ReLU）和S形（Sigmoid）激活函数。</font></font><br>
<br>
<h3><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">▍优化器（optimizers）</font></font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
优化器是深度学习库的基础。</font><font style="vertical-align: inherit;">他们描述了使用某些标准并考虑优化目标来调整模型参数的方法。</font><font style="vertical-align: inherit;">在著名的优化器中，可以注意到SGD，RMSProp和Adam。</font></font><br>
<br>
<h3><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">▍损失函数</font></font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
损失函数是一种解析和微分的数学表达式，可用来替代解决问题时的优化目标。</font><font style="vertical-align: inherit;">例如，交叉熵函数和分段线性函数通常用于分类问题。</font></font><br>
<br>
<h3><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">▍初始化器</font></font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
初始化程序为模型参数提供初始值。</font><font style="vertical-align: inherit;">这些参数在训练开始时就是这些值。</font><font style="vertical-align: inherit;">初始化程序在神经网络的训练中起着重要作用，因为初始参数不成功可能意味着网络学习缓慢，或者根本无法学习。</font><font style="vertical-align: inherit;">有很多方法可以初始化神经网络的权重。</font><font style="vertical-align: inherit;">例如-您可以从正态分布中为它们分配较小的随机值。</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">这是一个</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">页面，您可以在其中了解不同类型的初始化程序。</font></font><br>
<br>
<h3><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">▍调节器</font></font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
正则化工具是避免网络重新训练并帮助网络获得泛化的工具。您可以以显式或隐式方式处理网络培训。显式方法涉及重量的结构限制。例如，将它们的L1-Norm和L2-Norm最小化，从而使重量值更好地分散和更均匀地分布。隐式方法由执行中间表示形式转换的专业运算符表示。这可以通过显式标准化（例如，使用数据包标准化技术（BatchNorm））或通过使用DropOut和DropConnect算法更改网络连接来完成。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
以上组件通常属于库的接口部分。</font><font style="vertical-align: inherit;">在这里，“界面部分”是指用户可以与之交互的实体。</font><font style="vertical-align: inherit;">他们为他提供了有效设计神经网络体系结构的便捷工具。</font><font style="vertical-align: inherit;">如果我们谈论库的内部机制，它们可以为损失函数的梯度自动计算提供支持，同时考虑模型的各种参数。</font><font style="vertical-align: inherit;">此技术通常称为自动微分（AD）。</font></font><br>
<br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">自动区分</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
每个深度学习库为用户提供了一些自动区分功能。这使他有机会专注于模型结构的描述（计算图），并将计算梯度的任务转移到AD模块。让我们举个例子，让我们知道一切的运作方式。假设我们要针对输入变量X 1和X 2计算以下函数的偏导数：</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Y = sin（x 1）+ X 3 * X 2 </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
下图是我</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">从这里</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">借来的，它</font><font style="vertical-align: inherit;">显示了计算图和使用链式法则的导数计算。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/605/635/189/605635189c56a2f87927ec1a0c5b6318.png"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">计算图和通过链式规则进行导数计算</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
您在这里看到的就像是自动微分的“反向模式”。</font><font style="vertical-align: inherit;">对于位于顶部的函数是损失函数的情况，众所周知的错误反向传播算法是上述算法的特例。</font><font style="vertical-align: inherit;">AD利用以下事实：任何复杂的函数都由基本算术运算和基本函数组成。</font><font style="vertical-align: inherit;">结果，可以通过将链式规则应用于这些操作来计算导数。</font></font><br>
<br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">实作</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
在上一节中，我们检查了创建深度学习库所必需的组件，该库专门用于创建和端对端训练神经网络。为了不使示例复杂化，我在这里模仿了</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Caffe</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">库的设计模式</font><font style="vertical-align: inherit;">。在这里，我们声明了两个抽象类- </font></font><code>Function</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">和</font></font><code>Optimizer</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。此外，还有一个class </font></font><code>Tensor</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">，它是一个简单的结构，其中包含两个多维NumPy数组。其中一个用于存储参数值，另一个用于存储其梯度。不同层（操作员）中的所有参数均为类型</font></font><code>Tensor</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。在继续之前，请看一下库的总体轮廓。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d05/98c/068/d0598c068139ecda1f2aacbd9ea5f068.jpg"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">库UML图</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
在编写本文时，该库包含线性层，ReLU激活功能，SoftMaxLoss层和SGD优化器的实现。结果，事实证明该库可用于训练由完全连接的层组成并使用非线性激活函数的分类模型。现在，让我们看一下有关抽象类的一些细节。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
抽象类</font></font><code>Function</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">为操作员提供接口。这是他的代码：</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword">class</span>&nbsp; <span class="hljs-title">Function</span>(<span class="hljs-params">object</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self</span>):</span>&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">raise</span> NotImplementedError<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backward</span>(<span class="hljs-params">self</span>):</span>&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">raise</span> NotImplementedError<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getParams</span>(<span class="hljs-params">self</span>):</span>&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span> []</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
所有运算符都是通过抽象类的继承实现的</font></font><code>Function</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。每个操作员必须提供方法</font></font><code>forward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">和</font><font style="vertical-align: inherit;">的实现</font></font><code>backward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。运算符可能包含</font></font><code>getParams()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">返回其参数（如果有）</font><font style="vertical-align: inherit;">的可选方法的实现</font><font style="vertical-align: inherit;">。该方法</font></font><code>forward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">接收输入数据，并由操作员返回其转换结果。此外，他解决了计算梯度所需的内部问题。该方法</font></font><code>backward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">接受相对于操作员的输出的损失函数的偏导数，并实现相对于操作员的输入数据和参数（如果有）的损失函数的偏导数的计算。注意方法</font></font><code>backward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">本质上讲，为我们的库提供了执行自动区分的能力。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
为了用一个特定的例子来处理所有这些，让我们看一下该函数的实现</font></font><code>Linear</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">：</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Linear</span>(<span class="hljs-params">Function</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,in_nodes,out_nodes</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.weights = Tensor((in_nodes,out_nodes))<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.bias&nbsp; &nbsp; = Tensor((<span class="hljs-number">1</span>,out_nodes))<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.type = <span class="hljs-string">'linear'</span><font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self,x</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;output = np.dot(x,self.weights.data)+self.bias.data<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.input = x&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span> output<font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backward</span>(<span class="hljs-params">self,d_y</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.weights.grad += np.dot(self.input.T,d_y)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.bias.grad&nbsp; &nbsp; += np.sum(d_y,axis=<span class="hljs-number">0</span>,keepdims=<span class="hljs-literal">True</span>)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;grad_input &nbsp; &nbsp; &nbsp; &nbsp; = np.dot(d_y,self.weights.data.T)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span> grad_input<font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getParams</span>(<span class="hljs-params">self</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span> [self.weights,self.bias]</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
该方法</font></font><code>forward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">实现视图的转换</font></font><code>Y = X*W+b</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">并返回结果。此外，这样可以节省输入值</font></font><code>X</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">的，因为它需要计算的偏导数</font></font><code>dY</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">的损失函数相对于输出值</font></font><code>Y</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">的方法</font></font><code>backward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。方法</font></font><code>backward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">接收的偏导数，相对于所述输入值计算</font></font><code>X</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">和所述参数</font></font><code>W</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">和</font></font><code>b</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。此外，它返回相对于输入值计算的偏导数</font></font><code>X</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">，它将被传输到上一层。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
抽象类</font></font><code>Optimizer</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">为优化器提供了一个接口：</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Optimizer</span>(<span class="hljs-params">object</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,parameters</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.parameters = parameters<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">step</span>(<span class="hljs-params">self</span>):</span>&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">raise</span> NotImplementedError<font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">zeroGrad</span>(<span class="hljs-params">self</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> self.parameters:<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;p.grad = <span class="hljs-number">0.</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
所有优化器都是通过从基类继承来实现的</font></font><code>Optimizer</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font><font style="vertical-align: inherit;">描述特定优化的类应提供该方法的实现</font></font><code>step()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font><font style="vertical-align: inherit;">该方法使用相对于损失函数的优化值计算出的偏导数来更新模型参数。</font><font style="vertical-align: inherit;">函数中提供了各种模型参数的链接</font></font><code>__init__()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font><font style="vertical-align: inherit;">请注意，用于重置梯度值的通用功能是在基类本身中实现的。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
现在，为了更好地理解所有这些，请考虑一个特定示例-随机梯度下降算法（SGD，随机梯度下降）的实现，该算法支持脉冲调整和权重降低：</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">SGD</span>(<span class="hljs-params">Optimizer</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,parameters,lr=<span class="hljs-number">.001</span>,weight_decay=<span class="hljs-number">0.0</span>,momentum = <span class="hljs-number">.9</span></span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;super().__init__(parameters)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.lr &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = lr<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.weight_decay = weight_decay<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.momentum &nbsp; &nbsp; = momentum<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.velocity &nbsp; &nbsp; = []<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> parameters:<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.velocity.append(np.zeros_like(p.grad))<font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">step</span>(<span class="hljs-params">self</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> p,v <span class="hljs-keyword">in</span> zip(self.parameters,self.velocity):<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v = self.momentum*v+p.grad+self.weight_decay*p.data<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;p.data=p.data-self.lr*v</code></pre><br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">真正问题的解决方案</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
现在，我们有了使用我们的库训练（深度）神经网络模型所需的一切。</font><font style="vertical-align: inherit;">为此，我们需要以下实体：</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">型号：计算图。</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">数据和目标值：用于网络培训的数据。</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">损失函数：替代优化目标。</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">优化器：一种更新模型参数的机制。</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
以下伪代码描述了典型的测试周期：</font></font><br>
<br>
<pre><code class="python hljs">model <span class="hljs-comment"># </span>
data,target <span class="hljs-comment"># </span>
loss_fn <span class="hljs-comment"># </span>
optim <span class="hljs-comment">#,         </span>
Repeat:<span class="hljs-comment">#   ,    ,     </span>
&nbsp;&nbsp;&nbsp;optim.zeroGrad() <span class="hljs-comment">#    </span>
&nbsp;&nbsp;&nbsp;output = model.forward(data) <span class="hljs-comment">#   </span>
&nbsp;&nbsp;&nbsp;loss &nbsp; = loss_fn(output,target) <span class="hljs-comment"># </span>
&nbsp;&nbsp;&nbsp;grad &nbsp; = loss.backward() <span class="hljs-comment">#      </span>
&nbsp;&nbsp;&nbsp;model.backward(grad) <span class="hljs-comment">#    </span>
&nbsp;&nbsp;&nbsp;optim.step() <span class="hljs-comment">#  </span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
尽管这在深度学习库中不是必需的，但将上述功能包含在单独的类中可能会很有用。</font><font style="vertical-align: inherit;">这将使我们在学习新模型时不必重复相同的步骤（此想法与</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Keras</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">之类的框架的高级抽象哲学相对应</font><font style="vertical-align: inherit;">）。</font><font style="vertical-align: inherit;">为了实现这一点，声明一个类</font></font><code>Model</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">：</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Model</span>():</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.computation_graph = []<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.parameters&nbsp; &nbsp; &nbsp; &nbsp; = []<font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">add</span>(<span class="hljs-params">self,layer</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.computation_graph.append(layer)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.parameters+=layer.getParams()<font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__innitializeNetwork</span>(<span class="hljs-params">self</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> self.computation_graph:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">if</span> f.type==<span class="hljs-string">'linear'</span>:<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;weights,bias = f.getParams()<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;weights.data = <span class="hljs-number">.01</span>*np.random.randn(weights.data.shape[<span class="hljs-number">0</span>],weights.data.shape[<span class="hljs-number">1</span>])<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bias.data&nbsp; &nbsp; = <span class="hljs-number">0.</span><font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fit</span>(<span class="hljs-params">self,data,target,batch_size,num_epochs,optimizer,loss_fn</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loss_history = []<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.__innitializeNetwork()<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;data_gen = DataGenerator(data,target,batch_size)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;itr = <span class="hljs-number">0</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(num_epochs):
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> X,Y <span class="hljs-keyword">in</span> data_gen:<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;optimizer.zeroGrad()<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> self.computation_graph: X=f.forward(X)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loss = loss_fn.forward(X,Y)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;grad = loss_fn.backward()<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> self.computation_graph[::<span class="hljs-number">-1</span>]: grad = f.backward(grad)&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loss_history+=[loss]<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(<span class="hljs-string">"Loss at epoch = {} and iteration = {}: {}"</span>.format(epoch,itr,loss_history[<span class="hljs-number">-1</span>]))<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;itr+=<span class="hljs-number">1</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;optimizer.step()<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span> loss_history<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span>(<span class="hljs-params">self,data</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;X = data<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> self.computation_graph: X = f.forward(X)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span> X</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
此类包含以下功能：</font></font><br>
<br>
<ul>
<li>  :  <code>add()</code>   ,    .        <code>computation_graph</code>.</li>
<li> : ,   ,       ,    .</li>
<li> :    <code>fit()</code>       .       ,    .</li>
<li>  :  <code>predict()</code>   ,       ,   .</li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
由于该课程不是深度学习系统的基本组成部分，因此我在单独的模块中实现了它</font></font><code>utilities.py</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font><font style="vertical-align: inherit;">请注意，该方法</font></font><code>fit()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用的类</font></font><code>DataGenerator</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">的实现在同一模块中。</font><font style="vertical-align: inherit;">此类仅是训练数据的包装，并且为每次训练迭代生成微型包装。</font></font><br>
<br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">模型训练</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
现在考虑使用上述库在其中训练神经网络模型的最后一段代码。</font><font style="vertical-align: inherit;">我将训练螺旋形排列的数据的多层网络。</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">该</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">出版物</font><font style="vertical-align: inherit;">提示我</font><font style="vertical-align: inherit;">。</font><font style="vertical-align: inherit;">在文件中可以找到用于生成此数据并对其进行可视化的代码</font></font><code>utilities.py</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/895/eb0/856/895eb085662368f9c1d171153a301fb0.png"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">三个类别的数据呈螺旋状排列，&nbsp;</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
上图显示了我们将在其上训练模型的数据的可视化。</font><font style="vertical-align: inherit;">该数据是非线性可分离的。</font><font style="vertical-align: inherit;">我们可以希望具有隐藏层的网络可以正确地找到非线性决策边界。</font><font style="vertical-align: inherit;">如果将我们讨论的所有内容放在一起，则会得到以下代码片段，可用于训练模型：</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">import</span> dl_numpy <span class="hljs-keyword">as</span> DL
<span class="hljs-keyword">import</span> utilities<font></font>
<font></font>
batch_size&nbsp; &nbsp; &nbsp; &nbsp; = <span class="hljs-number">20</span>
num_epochs&nbsp; &nbsp; &nbsp; &nbsp; = <span class="hljs-number">200</span>
samples_per_class = <span class="hljs-number">100</span>
num_classes &nbsp; &nbsp; &nbsp; = <span class="hljs-number">3</span>
hidden_units&nbsp; &nbsp; &nbsp; = <span class="hljs-number">100</span><font></font>
data,target &nbsp; &nbsp; &nbsp; = utilities.genSpiralData(samples_per_class,num_classes)<font></font>
model &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = utilities.Model()<font></font>
model.add(DL.Linear(<span class="hljs-number">2</span>,hidden_units))<font></font>
model.add(DL.ReLU())<font></font>
model.add(DL.Linear(hidden_units,num_classes))<font></font>
optim &nbsp; = DL.SGD(model.parameters,lr=<span class="hljs-number">1.0</span>,weight_decay=<span class="hljs-number">0.001</span>,momentum=<span class="hljs-number">.9</span>)<font></font>
loss_fn = DL.SoftmaxWithLoss()<font></font>
model.fit(data,target,batch_size,num_epochs,optim,loss_fn)<font></font>
predicted_labels = np.argmax(model.predict(data),axis=<span class="hljs-number">1</span>)<font></font>
accuracy &nbsp; &nbsp; &nbsp; &nbsp; = np.sum(predicted_labels==target)/len(target)<font></font>
print(<span class="hljs-string">"Model Accuracy = {}"</span>.format(accuracy))<font></font>
utilities.plot2DDataWithDecisionBoundary(data,target,model)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
下图显示了相同的数据以及训练模型的决定性边界。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/502/b0b/84f/502b0b84fee2c960c918494e8d63e33a.png"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">训练模型的数据和决策边界</font></font></font></i><br>
 <br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">摘要</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
鉴于深度学习模型的复杂性不断增加，趋势是增加各个库的功能并增加实现这些功能所需的代码量。但是，此类库的最基本功能仍可以以相对紧凑的形式实现。尽管我们创建的库可用于简单网络的端到端培训，但在许多方面，它仍然受到限制。我们正在谈论的功能方面的局限性使深度学习框架可以用于机器视觉，语音和文本识别等领域。当然，这种框架的可能性不受限制。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
我相信每个人都可以参与该</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">项目</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">，我们将在此处检查其代码，并作为练习向他们介绍他们希望在其中看到的内容。</font><font style="vertical-align: inherit;">您可以尝试以下一些机制来实现自己：</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">运算符：卷积，二次采样。</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">优化程序：Adam，RMSProp。</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">调节器：BatchNorm，DropOut。</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
我希望该材料至少可以让您从眼角看到深度学习图书馆的肠子中正在发生的事情。</font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">亲爱的读者们！</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">您使用什么深度学习库？</font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><img src="https://habrastorage.org/files/1ba/550/d25/1ba550d25e8846ce8805de564da6aa63.png"></a></div>
      
    </div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/zh-CN486686/">https://habr.com/ru/post/zh-CN486686/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN486676/index.html">FPGA渗透数据中心的必然性</a></li>
<li><a href="../zh-CN486678/index.html">ASP.NET Core中的石英</a></li>
<li><a href="../zh-CN486680/index.html">ML，VR和机器人（还有一点云）</a></li>
<li><a href="../zh-CN486682/index.html">Docker Compose：简化使用Makefile</a></li>
<li><a href="../zh-CN486684/index.html">我对那些相信TDD的价值被夸大的人的回答</a></li>
<li><a href="../zh-CN486688/index.html">Node.js，Tor，Puppeteer和Cheerio：匿名网络抓取</a></li>
<li><a href="../zh-CN486690/index.html">编写优质箭头功能的5个技巧</a></li>
<li><a href="../zh-CN486692/index.html">您可能从未使用过的Chrome控制台功能</a></li>
<li><a href="../zh-CN486694/index.html">OpenStreetMap第496号世界新闻（01/14/2020/20/01/2020）</a></li>
<li><a href="../zh-CN486702/index.html">2月3日至9日在莫斯科举行的数字活动</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>