<!doctype html>
<html class="no-js" lang="es">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üì≠ ‚ôèÔ∏è üîß Entrene a sus rivales de juegos inteligentes en Unity utilizando el m√©todo "juegue con usted mismo" utilizando ML-Agents üèè üéÑ üßúüèø</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hola Habr! 
 
 Como saben nuestros lectores habituales, hemos publicado libros largos y exitosos sobre Unity . Como parte del estudio del tema, nos in...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Entrene a sus rivales de juegos inteligentes en Unity utilizando el m√©todo "juegue con usted mismo" utilizando ML-Agents</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/507212/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hola Habr! </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Como saben nuestros lectores habituales, hemos </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">publicado libros</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> largos y exitosos </font><font style="vertical-align: inherit;">sobre </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Unity</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Como parte del estudio del tema, nos interes√≥, en particular, el </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">juego de herramientas ML-Agents</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Hoy traemos a su atenci√≥n una traducci√≥n de un art√≠culo del blog de Unity sobre c√≥mo entrenar efectivamente a los agentes de juegos utilizando el m√©todo "consigo mismo"; En particular, el art√≠culo ayuda a comprender por qu√© este m√©todo es m√°s efectivo que el aprendizaje reforzado tradicional. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/vb/sz/5m/vbsz5musziyq5yighwu_iq0eggu.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
¬°Disfruta leyendo!</font></font><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Luego, este art√≠culo ofrece una descripci√≥n general de la tecnolog√≠a de juego propio (jugar con uno mismo) y demuestra c√≥mo ayuda a proporcionar un entrenamiento estable y efectivo en el entorno de demostraci√≥n de f√∫tbol del juego de </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">herramientas ML-Agents</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En los entornos de demostraci√≥n de Tenis y F√∫tbol del Unity ML-Agents Toolkit, los agentes se enfrentan entre s√≠ como rivales. La formaci√≥n de agentes en un escenario tan competitivo es a veces una tarea muy poco trivial. De hecho, en versiones anteriores de ML-Agents Toolkit, para que el agente aprendiera con confianza, se requer√≠a un estudio serio del premio. En la </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">versi√≥n 0.14</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Se agreg√≥ una oportunidad que permite al usuario capacitar a los agentes utilizando el aprendizaje por refuerzo (RL) basado en el juego propio, un mecanismo que es crucial para lograr algunos de los resultados de aprendizaje de refuerzo m√°s </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">avanzados</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , como </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;">OpenAI Five</font></a><font style="vertical-align: inherit;"> y </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">AlphaStar de DeepMind</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . El juego personal en el trabajo enfrenta entre s√≠ las hip√≥stasis actuales y pasadas del agente. Por lo tanto, obtenemos un adversario para nuestro agente, que puede mejorar gradualmente utilizando algoritmos de aprendizaje de refuerzo tradicionales. </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Un agente completamente entrenado</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> puede competir con √©xito con jugadores humanos avanzados.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El juego personal proporciona un entorno de aprendizaje basado en los mismos principios que la competencia desde una perspectiva humana. Por ejemplo, una persona que aprende a jugar al tenis elegir√° entrenar a sus oponentes aproximadamente al mismo nivel que √©l, ya que un oponente demasiado fuerte o demasiado d√©bil no es tan conveniente para dominar el juego. Desde el punto de vista del desarrollo de sus propias habilidades, puede ser mucho m√°s valioso para un tenista novato vencer a los mismos principiantes, en lugar de, por ejemplo, un ni√±o en edad preescolar o Novak Djokovic. El primero ni siquiera podr√° golpear la pelota, y el segundo no le dar√° un servicio que pueda vencer. Cuando un principiante desarrolla suficiente fuerza, puede pasar al siguiente nivel o solicitar un torneo m√°s serio para jugar contra oponentes m√°s h√°biles.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En este art√≠culo, consideraremos algunas sutilezas t√©cnicas asociadas con la din√°mica del juego con nosotros mismos, y tambi√©n consideraremos ejemplos de trabajar en entornos virtuales Tenis y F√∫tbol, ‚Äã‚Äãrefactorizados de tal manera que ilustren el juego consigo mismos.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La historia de un juego contigo mismo en juegos</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El fen√≥meno de jugar con uno mismo tiene una larga historia, reflejada en la pr√°ctica de desarrollar agentes de juegos artificiales dise√±ados para competir con las personas en los juegos. </font><font style="vertical-align: inherit;">Uno de los primeros en usar este sistema fue Arthur Samuel, quien desarroll√≥ un simulador de ajedrez en la d√©cada de 1950 y public√≥ </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">este trabajo</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> en 1959. </font><font style="vertical-align: inherit;">Este sistema se convirti√≥ en el precursor de un resultado hist√≥rico en el aprendizaje de refuerzo logrado por Gerald Tesauro en TD-Gammon; </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">totales publicados</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">en 1995. TD-Gammon utiliz√≥ el algoritmo de diferencia de tiempo TD (Œª) con la funci√≥n de jugar consigo mismo para entrenar al agente a jugar backgammon para que pudiera competir con una persona profesional. En algunos casos, se ha observado que TD-Gammon tiene una visi√≥n m√°s segura de las posiciones que los jugadores de clase mundial. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Jugar contigo mismo se refleja en muchos de los logros ic√≥nicos asociados con RL. Es importante tener en cuenta que jugar contigo mismo ayud√≥ al desarrollo de agentes para jugar </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ajedrez e ir</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> con habilidades sobrehumanas, agentes de elite </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">DOTA 2</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , as√≠ como estrategias complejas y contra-estrategias en juegos como la </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">lucha libre</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y el </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">escondite.</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">En los resultados logrados jugando con uno mismo, a menudo se observa que los agentes del juego eligen estrategias que sorprenden a las personas expertas. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Jugar contigo mismo les da a los agentes una cierta creatividad que es independiente de la creatividad de los programadores. </font><font style="vertical-align: inherit;">El agente recibe solo las reglas del juego y, luego, informaci√≥n sobre si gan√≥ o perdi√≥. </font><font style="vertical-align: inherit;">Adem√°s, con base en estos principios b√°sicos, el agente debe desarrollar un comportamiento competente. </font><font style="vertical-align: inherit;">Seg√∫n el creador de TD-Gammon, este enfoque de aprendizaje libera, "... en el sentido de que el programa no est√° limitado por inclinaciones y prejuicios humanos, que pueden resultar err√≥neos y poco confiables". </font><font style="vertical-align: inherit;">Gracias a esta libertad, los agentes descubren estrategias de juego brillantes que cambian por completo la forma en que los ingenieros piensan sobre ciertos juegos.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Entrenamiento de refuerzo competitivo</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dentro del marco de la tarea tradicional de aprendizaje reforzado, el agente est√° tratando de desarrollar una l√≠nea de comportamiento que maximice la recompensa total. La se√±al gratificante codifica la tarea del agente; tal tarea puede ser, por ejemplo, trazar un curso o recolectar elementos. El comportamiento del agente est√° sujeto a restricciones ambientales. Tales, por ejemplo, la gravedad, los obst√°culos, as√≠ como la influencia relativa de las acciones tomadas por el propio agente, por ejemplo, la aplicaci√≥n de la fuerza para el propio movimiento. Estos factores limitan el comportamiento del agente y son fuerzas externas que debe aprender a manejar para recibir una alta recompensa. Por lo tanto, el agente compite con la din√°mica del entorno y debe moverse de un estado a otro precisamente para lograr la m√°xima recompensa.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/if/jf/xi/ifjfxil_iqm9pznr61qbpxujs38.png"><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El escenario t√≠pico de entrenamiento de refuerzo se muestra a la izquierda: el agente act√∫a en el entorno, se transfiere al siguiente estado y recibe una recompensa. El escenario de entrenamiento se muestra a la derecha, donde el agente compite con un rival que, desde el punto de vista del agente, es en realidad un elemento del entorno.</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
En el caso de los juegos competitivos, el agente compite no solo con la din√°mica del entorno, sino tambi√©n con otro agente (posiblemente intelectual). Podemos suponer que el oponente est√° integrado en el entorno, y sus acciones afectan directamente el siguiente estado que el agente "ve", as√≠ como la recompensa que recibir√°. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/zl/kv/xo/zlkvxoa9_bt0cdsthmddwll-fvc.png"><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ejemplo de tenis de ML-Agents Toolkit</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Considere la demostraci√≥n de ML-Agents Tennis. La raqueta azul (izquierda) es el agente de aprendizaje, y la p√∫rpura (derecha) es su oponente. Para lanzar la pelota sobre la red, el agente debe tener en cuenta la trayectoria de la pelota que vuela desde el oponente y hacer un ajuste en el √°ngulo y la velocidad de la pelota voladora, teniendo en cuenta las condiciones ambientales (gravedad). Sin embargo, en una competencia con un oponente, lanzar la pelota sobre la red es solo la mitad de la batalla. Un oponente fuerte puede responder con un golpe irresistible y, como resultado, el agente perder√°. Un oponente d√©bil puede golpear la pelota dentro de la red. Un oponente igual puede devolver el servicio y, por lo tanto, el juego continuar√°. En cualquier caso, tanto el siguiente estado como la recompensa correspondiente dependen tanto de las condiciones ambientales como del oponente. Sin embargo, en todas estas situaciones, el agente hace el mismo lanzamiento. Por lo tanto, como entrenamiento en juegos competitivos,y bombear comportamientos rivales por parte de un agente es un problema complejo.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Las consideraciones para un oponente adecuado no son triviales. Como se desprende de lo anterior, la fuerza relativa del oponente afecta significativamente el resultado de un juego en particular. Si el oponente es demasiado fuerte, entonces el agente puede tener dificultades para aprender a jugar desde cero. Por otro lado, si el oponente es demasiado d√©bil, entonces el agente puede aprender a ganar, pero estas habilidades pueden ser in√∫tiles en la competencia con un oponente m√°s fuerte o simplemente diferente. Por lo tanto, necesitamos un oponente que tenga aproximadamente la misma fuerza que el agente (inflexible, pero no insuperable). Adem√°s, dado que las habilidades de nuestro agente mejoran con cada juego completado, debemos aumentar la fuerza de su oponente en la misma medida. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/fn/vk/pt/fnvkptgsipqopi2iiyflsa5ec_a.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Cuando juegas contigo mismo, una instant√°nea del pasado o un agente en su estado actual es el oponente integrado en el entorno.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Aqu√≠ es donde el juego con nosotros es √∫til. El agente mismo satisface ambos requisitos para el oponente deseado. Definitivamente es aproximadamente igual en fuerza a s√≠ mismo, y sus habilidades mejoran con el tiempo. En este caso, la propia pol√≠tica del agente est√° integrada en el entorno (consulte la figura). Aquellos que est√°n familiarizados con la </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">educaci√≥n de complejidad gradualmente creciente</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (aprendizaje curricular), les muestran que podemos suponer que el sistema est√° desarrollando naturalmente el plan de estudios, despu√©s de lo cual, el agente aprende a luchar contra oponentes cada vez m√°s poderosos. ¬°En consecuencia, jugar contigo mismo te permite usar el entorno mismo para entrenar agentes competitivos para juegos competitivos!</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En las pr√≥ximas dos secciones, consideraremos m√°s detalles t√©cnicos de la capacitaci√≥n de agentes competitivos, en particular, con respecto a la implementaci√≥n y el uso del juego con uno mismo en el Kit de herramientas de ML-Agents.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Consideraciones pr√°cticas</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Surgen algunos problemas pr√°cticos con respecto al marco para jugar contigo mismo. </font><font style="vertical-align: inherit;">En particular, es posible volver a entrenar, en el que el agente aprende a ganar solo con un cierto estilo de juego, as√≠ como la inestabilidad inherente al proceso de aprendizaje, que puede surgir debido a la inestabilidad de la funci√≥n de transici√≥n (es decir, debido al cambio constante de los oponentes). </font><font style="vertical-align: inherit;">El primer problema surge porque queremos que nuestros agentes tengan un conocimiento general y la capacidad de luchar contra oponentes de diferentes tipos.</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El segundo problema puede ilustrarse en el entorno de tenis: diferentes oponentes golpear√°n la pelota a diferentes velocidades y en diferentes √°ngulos. Desde el punto de vista del agente de aprendizaje, esto significa que, a medida que aprende, las mismas decisiones conducir√°n a resultados diferentes y, en consecuencia, el agente estar√° en diferentes situaciones posteriores. En el aprendizaje de refuerzo tradicional, las funciones de transici√≥n estacionarias est√°n impl√≠citas. Desafortunadamente, habiendo preparado una selecci√≥n de varios oponentes para el agente a fin de resolver el primer problema, nosotros, siendo descuidados, podemos agravar el segundo.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para hacer frente a esto, mantendremos un amortiguador con las pol√≠ticas de agentes anteriores, de las cuales elegiremos rivales potenciales para nuestro "estudiante" a largo plazo. </font><font style="vertical-align: inherit;">Al elegir un agente de pol√≠ticas anteriores, obtenemos para √©l una selecci√≥n de oponentes diversos. </font><font style="vertical-align: inherit;">Adem√°s, al permitir que el agente entrene con un oponente fijo durante mucho tiempo, estabilizamos la funci√≥n de transici√≥n y creamos un entorno de aprendizaje m√°s consistente. </font><font style="vertical-align: inherit;">Finalmente, estos aspectos algor√≠tmicos se pueden controlar mediante hiperpar√°metros, que se analizan en la siguiente secci√≥n.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Detalles de implementaci√≥n y uso</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Al elegir hiperpar√°metros para jugar con nosotros mismos, en primer lugar, tenemos en cuenta un compromiso entre el nivel del oponente, la universalidad de la pol√≠tica final y la estabilidad del entrenamiento. Entrenar en competencia con un grupo de oponentes que cambian lentamente o no cambian en absoluto, lo que significa que dan una dispersi√≥n m√°s peque√±a de resultados, es un proceso m√°s estable que entrenar en competencia con muchos oponentes diversos que cambian r√°pidamente. Los hiperpar√°metros disponibles le permiten controlar con qu√© frecuencia se guardar√° la pol√≠tica actual del agente para su uso posterior como uno de los oponentes en la muestra, con qu√© frecuencia se guardar√° el nuevo oponente, posteriormente se elegir√° para el combate, con qu√© frecuencia se seleccionar√° el nuevo oponente, el n√∫mero de oponentes guardados, as√≠ como la probabilidadque en este caso, el estudiante tendr√° que jugar contra su propio alter ego, y no contra un oponente seleccionado del grupo.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En los juegos competitivos, el premio "acumulativo" emitido por el medio ambiente tal vez no sea la m√©trica m√°s informativa para seguir el progreso del aprendizaje. El hecho es que el premio acumulativo depende completamente del nivel del oponente. Un agente con cierta habilidad de juego recibir√° una recompensa mayor o menor, dependiendo de un oponente menos h√°bil o m√°s h√°bil, respectivamente. Proponemos la implementaci√≥n </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">del sistema de clasificaci√≥n ELO</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , que le permite calcular la habilidad relativa del juego de dos jugadores de una determinada poblaci√≥n cuando juegan con una cantidad cero. Durante una sola carrera de entrenamiento, este valor deber√≠a aumentar constantemente. Puede rastrearlo, junto con otras m√©tricas de aprendizaje, por ejemplo, el premio general, utilizando </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">TensorBoard</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Jugando contigo mismo en el f√∫tbol</font></font></h3> <br>
<img src="https://habrastorage.org/webt/gl/m_/rh/glm_rhop1whkyve0i-wpubxfpqw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Las √∫ltimas versiones de ML-Agent Toolkit no incluyen pol√≠ticas de agente para el entorno de aprendizaje de f√∫tbol, ‚Äã‚Äãya que el proceso de entrenamiento confiable no se cre√≥ en √©l. </font><font style="vertical-align: inherit;">Sin embargo, usando el juego con nosotros mismos y algunas refactorizaciones, podemos entrenar al agente en comportamientos no triviales. </font><font style="vertical-align: inherit;">El cambio m√°s significativo es la eliminaci√≥n de las "posiciones de juego" de las caracter√≠sticas del agente. </font><font style="vertical-align: inherit;">Anteriormente en el entorno de f√∫tbol, ‚Äã‚Äãel "portero" y el "delantero" se destacaban claramente, por lo que todo el juego parec√≠a m√°s l√≥gico. </font><font style="vertical-align: inherit;">En </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">este video</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Se presenta un nuevo entorno en el que se puede ver c√≥mo se forma espont√°neamente el comportamiento del rol, en el que algunos agentes comienzan a actuar como atacantes y otros como porteros. ¬°Ahora los propios agentes est√°n aprendiendo a jugar estos puestos! La funci√≥n de recompensa para los cuatro agentes se define como +1.0 para un gol anotado y -1.0 para un gol concedido, con una penalizaci√≥n adicional de -0.0003 por paso; esta penalizaci√≥n se proporciona para estimular a los agentes a atacar.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Aqu√≠ enfatizamos una vez m√°s que los agentes en el entorno de aprendizaje de Soccer aprenden el comportamiento cooperativo, y para esto, no se utiliza ning√∫n algoritmo expl√≠cito relacionado con el comportamiento de m√∫ltiples agentes o la asignaci√≥n de roles. </font><font style="vertical-align: inherit;">Este resultado demuestra que el agente puede ser entrenado en comportamientos complejos utilizando algoritmos relativamente simples, siempre que la tarea est√© bien formulada. </font><font style="vertical-align: inherit;">La condici√≥n m√°s importante para esto es que los agentes pueden observar a sus compa√±eros de equipo, es decir, reciben informaci√≥n sobre la posici√≥n relativa del compa√±ero de equipo. </font><font style="vertical-align: inherit;">Forzando una pelea agresiva por el bal√≥n, el agente indirectamente le dice al compa√±ero de equipo que debe moverse en defensa. </font><font style="vertical-align: inherit;">Por el contrario, alej√°ndose en defensa, el agente provoca que un compa√±ero de equipo ataque.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Que sigue</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Si alguna vez ha utilizado alguna de las nuevas funciones de esta versi√≥n, cu√©ntenos sobre ellas. </font><font style="vertical-align: inherit;">Llamamos su atenci√≥n a la p√°gina de </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">problemas de GitHub de ML-Agents</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , donde puede hablar sobre los errores encontrados, as√≠ como a la p√°gina de </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">foros de Unity ML-Agents</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , donde se discuten preguntas y problemas generales.</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es507200/index.html">7 maneras en que los cient√≠ficos de datos te enga√±an</a></li>
<li><a href="../es507202/index.html">Encuentro de Avito Analytics</a></li>
<li><a href="../es507204/index.html">Cocina interior de dise√±o industrial: del boceto al producto en caja</a></li>
<li><a href="../es507206/index.html">Arquitectura Y messenger</a></li>
<li><a href="../es507210/index.html">El rendimiento de Java moderno cuando se trabaja con grandes cantidades de datos, parte 2</a></li>
<li><a href="../es507214/index.html">C√≥mo crear y modificar formularios PDF interactivos, o la nueva habilidad ABBYY FineReader PDF</a></li>
<li><a href="../es507218/index.html">L√©ame o por qu√© el texto no se lee hasta el final</a></li>
<li><a href="../es507222/index.html">¬øPor qu√© todos deber√≠an usar m√°scaras?</a></li>
<li><a href="../es507224/index.html">C√≥mo eliminar los puntos ciegos con pruebas visuales</a></li>
<li><a href="../es507226/index.html">OCR para PDF en .NET: c√≥mo extraer texto de documentos PDF inaccesibles</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>