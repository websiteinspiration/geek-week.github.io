<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🎉 🎛️ 🤵🏿 US Department of Defense: Ethics for AI and Unmanned Vehicles 🧓🏽 🌳 💎</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ethics AI is a hot and relevant topic these days. And it is right. 
 
 With the heyday of AI systems and their very widespread occurrence, there are r...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>US Department of Defense: Ethics for AI and Unmanned Vehicles</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/itelma/blog/492730/"><img src="https://habrastorage.org/webt/je/r-/a0/jer-a0fwwrd7ose8dodxkcm6nq0.jpeg" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ethics AI is a hot and relevant topic these days. </font><font style="vertical-align: inherit;">And it is right. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
With the heyday of AI systems and their very widespread occurrence, there are reasonable concerns that these computer systems are simply thrown into this world without understanding the possible consequences for society. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
There is serious concern that the new artificial intelligence systems have </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">prejudices built into them and are prone</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> to actions that </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;">are</font></a><font style="vertical-align: inherit;"> toxic in terms of ethics and society. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
When it comes to the ethics of artificial intelligence, I argue that there are at least two main aspects (or rules):</font></font><br>
<br>
<ol>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">AI system behavior should be acceptable to society</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> AI system designers and the systems themselves must enforce rule 1.</font></font></li>
</ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Perhaps the essence of the first rule is obvious - the meaning of AI ethics is that the behavior of these systems should comply with the ethical standards of society. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
It will be shown below that compliance with the first rule is more complicated than it might seem.</font></font><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The importance of rule 2 is to clarify that the parties responsible for ensuring full compliance with the ethics of AI are made up of people who design, build, and distribute these systems. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
If this seems obvious to you, then do not rush to make judgments based on "common sense." </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Around the AI ​​systems, there is a kind of aura, as if they are autonomous and capable of self-determination. Thus, when the AI ​​system commits a kind of </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">manifestation of racial prejudice</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , some make a logical mistake saying that the AI ​​did it, without blaming those who created and implemented this system. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Some developers and manufacturers of these systems would certainly prefer you </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">not to look in their direction.</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">when the AI ​​system does something wrong. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As a result, those who bear true responsibility can make a surprised face </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">and cunningly shy away from their duties</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , saying that the AI ​​is to blame. These people will behave as if they are just watching the train crash, even if it was they who put it on the rails and started it without any control or verification. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Of course, some will defend their system that made a mistake. They will say that they tried to prevent all failures, but damn it, </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">something was missing</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . In a sense, these people will try to make themselves “victims,” contrasting themselves with those who have suffered from AI. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Do not get fooled by this. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In fact, please do not get fooled by this.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In short, the second rule is as important as the first. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Without bringing people accountable for the actions of AI created by them, we will all sit in a puddle. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
I’m sure that the creators will easily get away with their promises related </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">to the development of their new AI systems</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . These people will praise their products, hoping for their success, and they will also pretend that they have nothing to do with applications whose ethical mistakes have fallen on us all. All of this will encourage the creation of AI systems that do not have moral responsibility. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Another area in which ethics of AI can be particularly difficult is AI for military purposes.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Most people will agree that we need to abide by certain AI ethical standards when creating and using military intelligence systems. The knowledge and experience that we gain in this way can be used for commercial and industrial use of AI. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The U.S. Department of Defense recently released a </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">set of AI principles</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
If you look at these principles in isolation (which I will do in this article), that is, without taking into account the military and defensive contexts, you can see that they are equally applicable to any commercial or industry AI systems.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
I will tell you about these principles, and use my favorite area for this - the emergence of real unmanned vehicles. This will allow me to illustrate how applicable the latest set of principles of AI ethics (including non-military use). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
According to media reports, the Pentagon published this latest set of AI ethical standards, and, citing Air Force Lieutenant Jack Jack Shenhan and Director of the Joint AI Center, </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">aptly stated</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : “AI is a powerful developing and effective technology that rapidly transforms culture, society and ultimately even war. "How good or bad these changes depend on our approach to adopting and using these systems."</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As I have repeatedly said, the approach of society to the development of AI will determine the fate of this system - whether it will </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">work for the good</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , or it </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">will (or will become over time) a failure with deplorable consequences</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Developers and manufacturers of AI systems must be seriously pressured. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
For clarity, I note that AI systems will not work for the good simply by their nature (there are no axioms and prerequisites that could guarantee this). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In order to fully study this issue, we will consider my concept, which is a four-square scheme, which I call </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">the AI ​​ethics consequences scheme</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Conventionally, we denote the upper part of the abscissa axis as “AI for good,” and the lower as “AI for harm." </font><font style="vertical-align: inherit;">Similarly, we divide the ordinate axis, the left side is designated as "Intentionally", and the right side is "Unintentionally". </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Thus we get 4 squares:</font></font><br>
<br>
<ol>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Good AI - Intentionally</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">AI to the detriment - Intentionally</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> AI for Good - Unintentionally</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> AI to the detriment - Inadvertently</font></font></li>
</ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
I understand that some of you may be horrified from simplifying such a weighty issue to a simple four-square layout, but sometimes a global perspective can be useful - with it you can see the forest among the trees. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The first square describes a good AI that was created with good intentions. This AI fulfills its goals without any ethical violations, and I believe that this square is the standard that all intelligent systems should strive for. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The second square is a malicious AI. Unfortunately, such systems will be created and released (by criminals or terrorists), but I hope that we will be ready for this, and the principles of AI ethics will help us detect their appearance.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The third square is random luck. He describes a situation in which someone created an intelligent system that suddenly turned out to be good. Perhaps its creator did not think about the positive direction of its development, but even if it is just a successful combination of circumstances, it is still a success, because for us the result is the basis. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The fourth square is the most insidious, and it is he who causes the most anxiety (with the exception of the creepy second square). As a rule, systems that should work for the good fall here, but during development, negative qualities accumulate in them that overlap all the good.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In general, we will be surrounded by systems in which good and bad traits will be mixed little by little. </font><font style="vertical-align: inherit;">The situation can be described as follows: developers will strive to create a good AI that will be useful, but in the process of creating </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">unintentional errors</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> will accumulate in the system </font><font style="vertical-align: inherit;">, which will lead to adverse consequences. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Consider two important questions:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">How can we understand or discover that an intelligent system may contain features of poor AI?</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">How can developers be aware of how to create such systems in order to catch errors and prevent the emergence of systems whose operation can lead to negative consequences?</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Answer: strictly abide by the rules taken from the correct and substantial set of principles of AI ethics. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Speaking of AI ethics guidelines, a document issued by the Department of Defense contains five basic principles, and each of them expresses important ideas that all developers and distributors of AI should pay attention to, as well as everyone who uses or will use these systems. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In general, AI ethics guidelines are useful to all interested parties, not just developers. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The best way to learn the principles from a document from the Department of Defense is to demonstrate them in relation to existing intelligent systems. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
It is worth considering the urgent question: </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">are the principles from the code of the Ministry of Defense applicable to</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">truly unmanned cars</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ? </font><font style="vertical-align: inherit;">If applicable, how should they be used? </font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
They are really applicable, let's take a look at how they work.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Unmanned vehicle levels</font></font></h3><br>
<img src="https://habrastorage.org/getpro/habr/post_images/6b1/9d2/599/6b19d25991a4964cfe26a5827542933b.png" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
It’s important to clarify what I mean when I talk </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">about fully unmanned vehicles</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> with AI. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Real unmanned vehicles are vehicles in which the AI ​​manages itself without any human assistance. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Such vehicles are assigned to </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">levels 4 and 5</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , while cars that require human participation for co-driving are usually assigned to </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">levels 2 or 3</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Cars in which driving with the help of a person is called semi-autonomous, and usually they contain many additional functions that are referred to as ADAS (advanced driver assistance systems). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
So far, there is no fully unmanned vehicle level 5. Today we even</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">we don’t know whether this will be achieved</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , and how long it will take. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Meanwhile, work is underway in level 4 area. Very narrow and selective tests are carried out on public roads, although there is debate about the admissibility of such tests (some believe that people participating in tests on roads and highways act as </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">guinea pigs, which can survive or die in each test</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ) . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Since semi-autonomous cars require a human driver, the adoption of such cars by the masses will not be much different from driving ordinary cars, there is nothing new that could be said about them in the context of our topic (although, as you will soon see, the points that will be considered further applicable to them).</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In the case of semi-autonomous cars, it is important that the public is warned about the alarming aspect that has arisen recently - despite people who continue to publish videos about how they fall asleep while driving cars of 2 or 3 levels, we all need to remember that the driver cannot be distracted from driving a semi-autonomous car. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
You are responsible for actions to control the vehicle level 2 or 3, regardless of the level of automation.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Unmanned vehicles and AI ethics</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In vehicles of level 4 and 5, a person does not take part in the management, all people in these vehicles will be passengers, and AI will drive. As a result, many intelligent systems will be built into these level vehicles - an indisputable fact. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The following question arises - will the AI ​​that drives the unmanned vehicle be limited to any guidelines or requirements on the ethics of AI (you can look at my </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">analysis of the ethics of AI for unmanned vehicles by this link</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , as well as </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">tips on developing this ethics</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Recall that there is no default system that would require AI to comply with ethical standards or a moral contract with society.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
People themselves must embed such beliefs or principles into intelligent systems, and AI developers must do this explicitly and with wide eyes. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Unfortunately, some act the other way round - they close their eyes tightly or fall under the influence of hypnosis, and instead of the above actions, they look fascinated at the potential prize that their project will bring (or the supposed noble goal). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
I warned that some AI systems will lead to </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">“noble corruption”</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> from those who develop them. The result will be as follows - those involved in the development will be so absorbed in the potential of well-meaning AI that they will selectively ignore, miss or reduce attention to considerations about the ethics of AI (see link). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Here are five points in the set.</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">US Department of Defense AI Ethics Principles</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> :</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Principle One: Responsibility</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
“Defense Department staff will carry out an appropriate level of assessment and attention, while remaining responsible for the development, deployment and use of AI capabilities.” </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
To begin with, we’ll extract from this wording the part that speaks of “Ministry staff” and replace it with “All staff”. You got a manual that is suitable for any organization working with AI, especially commercial. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Automakers and technology firms working with unmanned vehicles should carefully read the wording of the principle, carefully following its words. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
I’m talking about this, as some continue to argue that the AI-based drone will act on its own, and therefore there will be a lot of concern about who will carry</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">liability for errors in the behavior of the vehicle</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> that will result in personal injury or death. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In short, </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">people will be responsible</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
These may be people who developed an AI-based drone — an automaker or a technology firm. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Understand also that I constantly say that the responsible people include </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">both developers and those who deploy these AI systems</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , which means that working with these systems and installing them is just as important as working with developers . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Why? </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Because people who deploy intelligent systems can also make mistakes that lead to adverse consequences. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
For example, suppose that</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">a ridesharing company</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> has acquired a fleet of unmanned vehicles and uses them for joint trips of its customers. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
So far, so good. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
But let's say that this company is not able to </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">properly service these unmanned vehicles</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Due to lack of maintenance or neglect of instructions, unmanned vehicles do not work. </font><font style="vertical-align: inherit;">In this case, we could find fault with the original developer, but it is more worth asking questions to those who deployed the AI ​​system and </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">did not show due care and responsibility in their work</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Principle Two: Impartiality</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
“The Ministry will take steps to minimize the appearance of unintentional bias in the operation of AI systems.” </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Once again, take the term “Ministry” in the wording and replace it with “Developers and employees installing AI systems.” You will receive a principle that is universal and suitable for everyone. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In the case of unmanned vehicles, there is a fear that the AI ​​system may </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">react</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> differently </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;">to pedestrians depending on their race</font></a><font style="vertical-align: inherit;"> , thus subtly and inexorably demonstrating racial prejudice. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Another problem is that the fleet of unmanned vehicles can cause cars to avoid certain areas. This will lead to geographic exclusion, which will ultimately rob people </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">with mobility problems.</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">free access to unmanned vehicles. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As a result, there is a significant chance that full-fledged unmanned vehicles will become mired in prejudices of one form or another, and instead of raising their hands and saying that this is life, developers and users should strive to minimize, mitigate or reduce any such prejudice.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Principle Three: Traceability</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
“Ministry projects in the field of AI will be developed and deployed in such a way that the relevant personnel have a proper understanding of the technologies, development processes and operational methods applicable to the field of AI, including through transparent and verifiable methods, data sources, as well as design and documentation ”. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Just remove from the wording all references to the ministry or, if you want, you can think of it as any department in any company or organization - this will make the principle widely applicable to everyone. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
There is much inherent in this principle, but for brevity, I will focus only on one aspect that you may not have noticed. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The item with “data sources” aims to disseminate </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">machine learning</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> and</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">deep learning</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
When you work with machine / deep learning, you usually need to collect a large set of data, and then use </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">special algorithms and models to find patterns</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Similar to the preceding paragraph on hidden prejudice, if the data collected is initially biased in some way, then patterns will potentially highlight these prejudices. As a result, when the AI ​​system operates in real time, it will translate these biases into reality. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Even more frightening is that AI developers may not understand what is happening, and those who install and deploy these systems will not be able to figure it out either. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A drone trained on crazy and hostile traffic in conditional New York could potentially take over</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aggressive driving style</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> typical of a driver from New York. </font><font style="vertical-align: inherit;">Imagine that you took this system and use it in US cities where the driving style is more calm and measured. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
This principle clearly indicates that developers and staff responsible for installation and deployment should be careful about the data sources, transparency, and auditing of such systems.</font></font><br>
<br>
<h3><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Principle Four: Reliability</font></font></i></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
“Ministry projects in the field of AI will have clearly defined uses, and the safety, reliability and effectiveness of such projects will be tested and verified within the described uses throughout the life cycle.” </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Follow the same steps as in the previous paragraphs: remove the word “ministry” and apply this paragraph as if it applies to any organization and its department. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Most (or perhaps all) will agree that the success or failure of unmanned </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">vehicles depends on their safety</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">safety</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , </font><font style="vertical-align: inherit;">and efficiency. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
I hope this principle is obvious because of its explicit applicability to AI-based cars.</font></font><br>
<br>
<h3><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Principle Five: Manageability</font></font></i></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
“The Ministry will design and develop AI-related projects to carry out their intended functions, while being able to detect inadvertent consequences in advance and avoid them, as well as the ability to disable or deactivate deployed systems that exhibit abnormal behavior.” </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As usual, remove the word "ministry" and apply this paragraph to anything. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The first part of this principle underlies my topic of detecting and preventing unexpected consequences, especially adverse ones. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
This is pretty obvious in the case of unmanned vehicles.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
There is also a subtle point regarding unmanned vehicles, which includes the ability to disable or deactivate the deployed AI system, which has shown unexpected behavior - there are many options and difficulties, and this is not just clicking on the “red” shutdown button located outside the car ( </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">as some people think</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> )</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Conclusion</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We usually expect the Department of Defense to not reveal its cards. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In this case, the ethics manual of AI should not be kept secret. </font><font style="vertical-align: inherit;">Indeed, it is very pleasant and commendable that the Ministry of Defense published this document. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In the end, it is one thing to glue a tiger out of paper, and another to see it in the real world. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
All developers and staff responsible for installation and deployment should carefully and thoughtfully study this AI ethics manual, as well as similar (many) ethical principles from other sources.</font></font><br>
<br>
<hr><br>
<img src="https://habrastorage.org/webt/4m/5z/_p/4m5z_pc9zhjja8pmtwxvaihckfe.png" alt="image"><br>
<br>
<div class="spoiler"><b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">About ITELMA</font></font></b><div class="spoiler_text">  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=">automotive</a> .     2500 ,    650 . <br>
<br>
, ,          .        ( 30,     ),   -, -,  - (DSP-)  .<br>
<br>
        ,  .   ,  ,    ,       .     ,      automotive.    , , .</div></div><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Read more useful articles:</font></font></b><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[Forecast] Transport of the future ( </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">short-term</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">medium-term</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">long-term</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> horizons)</font></font></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=">      DEF CON 2018-2019 </a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=">[] Motornet —      </a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=">  16     ,     8 </a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=">  </a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=">   open source</a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=">McKinsey:       automotive</a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=">       </a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=">   </a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=">      …</a></li>
</ul></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../en492718/index.html">Security through user restriction or how to create a vulnerability</a></li>
<li><a href="../en492720/index.html">diskussion: project file service</a></li>
<li><a href="../en492724/index.html">Björn Straustrup Answers Top 5 C ++ Questions with Stack Overflow</a></li>
<li><a href="../en492726/index.html">OS Sivelkiriya: software development process</a></li>
<li><a href="../en492728/index.html">Asterisk and sending missed in Telegram / Slack / E-mail</a></li>
<li><a href="../en492742/index.html">Visualize Node JS application data with Prometheus + Grafana</a></li>
<li><a href="../en492744/index.html">Which system will be the loudest, and which will give “absolute” silence: two interesting scientific projects</a></li>
<li><a href="../en492746/index.html">How to avoid programming outrage? Integrator Tips</a></li>
<li><a href="../en492748/index.html">Profession DevOps-engineer: a view of the system administrator</a></li>
<li><a href="../en492754/index.html">Why JavaScript devours HTML: code examples</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>