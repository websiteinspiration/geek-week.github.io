<!doctype html>
<html class="no-js" lang="zh-CN">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>⛄️ ▪️ 👨🏾‍⚖️ BERT，ELMO和Co.的图片（NLP的转移培训如何来了） 👐🏻 🗄️ 👈🏽</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="2018年是旨在解决文本处理问题（或更准确地说，处理自然语言（NLP））的机器学习模型发展的转折点。关于如何呈现单词和句子以最准确地提取它们的语义和它们之间的关系的概念性理解正在迅速发展。此外，NLP社区推广了非常强大的工具，这些工具可以在其模型和管道中免费下载和使用。这个转折点也称为NLP的Ima...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>BERT，ELMO和Co.的图片（NLP的转移培训如何来了）</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/487358/"><p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2018年是旨在解决文本处理问题（或更准确地说，处理自然语言（NLP））的机器学习模型发展的转折点。</font><font style="vertical-align: inherit;">关于如何呈现单词和句子以最准确地提取它们的语义和它们之间的关系的概念性理解正在迅速发展。</font><font style="vertical-align: inherit;">此外，NLP社区推广了非常强大的工具，这些工具可以在其模型和管道中免费下载和使用。</font><font style="vertical-align: inherit;">这个转折点也称为</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">NLP的ImageNet时刻</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">，指的是几年前的时刻，当时类似的发展大大加速了计算机视觉问题领域中机器学习的发展。</font></font></p><br>
<p><img src="https://habrastorage.org/webt/uh/cd/qv/uhcdqv--w2t4i8srv9rtzjgk9ac.png" alt="贝尔·乌尔姆菲特·埃尔莫变压器"></p><br>
<p><em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">（ULM-FiT与Korzhik无关，但没有发生更好的事情）</font></font></em></p><a name="habracut"></a><br>
<p>        –  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"></a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow">BERT'</a>, ,  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"> </a>    NLP. BERT –  ,        NLP-.    ,  ,                BERT',        .                    ,   ,   ,        .</p><br>
<p><img src="https://habrastorage.org/webt/pz/zk/xy/pzzkxyzmqf21r5rik00228zntwm.png" alt="伯特转移学习"></p><br>
<p><em> BERT'.  1:   (   );  2:   .</em></p><br>
<p>BERT      ,  NLP-, ,   : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow">Semi-supervised Sequence learning</a> ( – <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow">Andrew Dai</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow">Quoc Le</a>), <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow">ELMo</a> ( – <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow">Matthew Peters</a>    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow">AI2</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow">UW CSE</a>), <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow">ULMFiT</a> ( –  fast.ai <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow">Jeremy Howard</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow">Sebastian Ruder</a>), <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow">OpenAI Transformer</a> ( –  OpenAI <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow">Radford</a>, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow">Narasimhan</a>, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow">Salimans</a>,  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow">Sutskever</a>)   (<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow">Vaswani et al</a>).</p><br>
<p>  ,     ,     BERT'.   ,     ,       .</p><br>
<h1 id="primer-klassifikaciya-predlozheniy">:  </h1><br>
<p>    BERT –    .      :</p><br>
<p><img src="https://habrastorage.org/webt/mx/eo/u_/mxeou__qytr_9_2m6pxjo2icemc.png" alt="伯特分类垃圾邮件"></p><br>
<p>   ,   ,    (classifier)       BERT'    .       (fine-tuning),      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow">Semi-supervised Sequence Learning</a>   ULMFiT.</p><br>
<p> ,    :   ,         .        .               («»  « »).</p><br>
<p><img src="https://habrastorage.org/webt/hy/qr/pa/hyqrpadlqytj81eqk3xtkyr3fcu.png" alt="垃圾邮件标记的数据集"></p><br>
<p>   BERT':</p><br>
<ul>
<li><strong>  (sentiment analysis)</strong><br>
<ul>
<li>:   /. : / </li>
<li>  : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow">SST</a></li>
</ul></li>
<li><strong>  (fact-checking)</strong>:<br>
<ul>
<li>: . : «» (Claim)  « » (Not Claim)</li>
<li> / :<br>
<ul>
<li>:    (Claim sentence). : «»  «»</li>
</ul></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow">Full Fact</a> – ,      .     ,        ,      ( , , ,    )</li>
<li>: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow">     </a></li>
</ul></li>
</ul><br>
<h1 id="arhitektura-modeli"> </h1><br>
<p>,         BERT', ,   .</p><br>
<p><img src="https://habrastorage.org/webt/i3/u4/fq/i3u4fq9cclcq0-zeqfk56b5y78i.png" alt="伯特基地伯特大"></p><br>
<p>     BERT'  :</p><br>
<ul>
<li>BERT BASE () –       OpenAI Transformer; </li>
<li>BERT LARGE () –   ,     (state of the art),   .</li>
</ul><br>
<p> , BERT –     . . <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="> </a>,      –   BERT’   ,      .</p><br>
<p><img src="https://habrastorage.org/webt/6k/ce/se/6kcesezyar2zqppjc31sfkcsxak.png" alt="bert-base-bert-large-encoders"></p><br>
<p>   BERT'      (    « » (Transformer Blocks)): 12     24  .          (768  1024   )   «»  (attention heads)(12  16 ),     ,     (6  , 512  , 8 «» ).</p><br>
<h3 id="vhody-modeli"> </h3><br>
<p><img src="https://habrastorage.org/webt/ed/7k/go/ed7kgoai63syz-koc-_tlqs0gwk.png" alt="伯特输入输出"></p><br>
<p>        [CLS]  ,     . CLS     .</p><br>
<p>  ,     , BERT     ,       .       (self-attention)       ,      .</p><br>
<p><img src="https://habrastorage.org/webt/jp/kd/qs/jpkdqszmo06ogw7xbfk1tqmz0kw.png" alt="伯特编码器输入"></p><br>
<p>   ,         (   ,    ).        .</p><br>
<h3 id="vyhody-modeli"> </h3><br>
<p>        hidden_size (768    BERT').    ,    ,       (      [CLS]).</p><br>
<p><img src="https://habrastorage.org/webt/at/9b/xe/at9bxefqh-vnkxlc-xkuxlgi13s.png" alt="伯特输出向量"></p><br>
<p>           .      ,          .</p><br>
<p><img src="https://habrastorage.org/webt/ee/lg/99/eelg99xutp6h7oztqyyz3hv-5e4.png" alt="伯特分类器"></p><br>
<p>       (,       «», « », « », «»  .),               .</p><br>
<h1 id="paralleli-so-svertochnymi-setyami">   </h1><br>
<p> ,     ,      ,        VGGNet     .</p><br>
<p><img src="https://habrastorage.org/webt/sl/37/yf/sl37yfo6xriqw24ule31ukksi8q.png" alt="vgg-net-classifier"></p><br>
<h1 id="novaya-era-embeddingov">  </h1><br>
<p>         .   ,    NLP-     ,   :        Word2Vec  GloVe.    ,    ,     ,  .</p><br>
<h3 id="kratkiy-obzor-mehanizma-embeddingov-slov">    </h3><br>
<p>              ,        . Word2Vec ,      ( ),     ,         (..                ,  «» – «»  «» – «»),       (, ,    «»  «»  ,   «»  «»).</p><br>
<p>  ,     ,     ,            .        ,      Word2Vec  GloVe.      GloVe   «stick» (   – 200):</p><br>
<p><img src="https://habrastorage.org/webt/l1/u-/ad/l1u-admk5irbjkb__sq90albkx0.png" alt="手套嵌入"></p><br>
<p><em>  «stick»   GloVe –   200     (  2   ).</em></p><br>
<p>                .</p><br>
<p><img src="https://habrastorage.org/webt/gz/ji/ee/gzjieex8v-pmouar89ocbbzan-e.png" alt="向量框"></p><br>
<h3 id="elmo-kontekst-imeet-znachenie">ELMo:   </h3><br>
<p>    GloVe,   «stick»        . « », –   NLP- (  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow">Peters et. al., 2017</a>, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow">McCann et. al., 2017</a>   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow">Peters et. al., 2018    ELMo</a>). – « «stick»       ,   .         ,     –  ,      ,    ?».     (contextualized word-embeddings).</p><br>
<p><img src="https://habrastorage.org/webt/jr/rt/hb/jrrthbwj9xdzd4vgs5ckjgayv0m.png" alt="Elmo-embeding-robin-williams"></p><br>
<p><em>            .</em></p><br>
<p> ,     , ELMo    ,       .        (bi-directional LSTM),       .</p><br>
<p><img src="https://habrastorage.org/webt/bf/yw/qq/bfywqqcnnk6cw6hr3l3fbl-xq-i.png" alt="词嵌入"></p><br>
<p>ELMo         NLP.  ELMo LSTM          ,        ,     .</p><br>
<p>   ELMo?</p><br>
<p>   ELMo          – ,    (language modeling).  ,        ,         .</p><br>
<p><img src="https://habrastorage.org/webt/mq/j1/bo/mqj1bozk08fff_cglqbdcatcuao.png" alt="伯特语言建模"></p><br>
<p><em>    ELMo:     «Let's stick to»,        –   .            . ,         .  ,    , , , «hang»,       «out» (   «hang out»),   «camera».</em></p><br>
<p>  ,      LSTM  -  ELMo.       ,       .</p><br>
<p>  , ELMo        LSTM – ,    «»    ,    .</p><br>
<p><img src="https://habrastorage.org/webt/0w/re/y4/0wrey4vtsshgd7wcgvi_k_cguzk.png" alt="elmo-向前-向后语言模型嵌入"></p><br>
<p><em><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"> </a>  ELMo</em></p><br>
<p>ELMo        (  )   (    ).</p><br>
<p><img src="https://habrastorage.org/webt/d8/dq/ch/d8dqchc0wxmoxmg2e79fnahk3ru.png" alt="电磁嵌入"></p><br>
<h1 id="ulm-fit-vnedryaya-transfernoe-obuchenie-v-nlp">ULM-FiT:     NLP</h1><br>
<p>ULM-FiT      ,        –       . ULM-FiT              .</p><br>
<p>NLP      , ,   ,       .</p><br>
<h1 id="transformer-vyhodya-za-predely-lstm-setey">:    LSTM </h1><br>
<p>   ,    ,   ,        ,     NLP-    LSTM.          .</p><br>
<p> -       .         ?         ,        (..     ,     )?</p><br>
<h1 id="openai-transformer-predvaritelnoe-obuchenie-dekodera-transformera-dlya-yazykovogo-modelirovaniya">OpenAI Transformer:       </h1><br>
<p> ,     ,          NLP-.        .    :    ,        (  ).</p><br>
<p><img src="https://habrastorage.org/webt/3p/-d/q3/3p-dq3wsky9bqnz6mdfv6-y-r2o.png" alt="openai-transformer-1"></p><br>
<p><em>OpenAI Transformer     </em></p><br>
<p>   12  .          -  ,     . ,         (   ).</p><br>
<p>                :   ,     .    7       .       , ..          ,         – ,   ,        .</p><br>
<p><img src="https://habrastorage.org/webt/8d/l0/4k/8dl04ko7gbfw9kiq_jlg5ve281w.png" alt="openai变压器语言建模"></p><br>
<p><em>OpenAI Transformer             7000 </em></p><br>
<h1 id="transfernoe-obuchenie-v-prikladnyh-zadachah">    </h1><br>
<p>,  OpenAI Transformer           ,         .      (    «»   « »):</p><br>
<p><img src="https://habrastorage.org/webt/-l/fo/hr/-lfohrojznururmmrni-ykoqnog.png" alt="openai变压器句分类"></p><br>
<p>  OpenAI    ,        .               :</p><br>
<p><img src="https://habrastorage.org/webt/mb/aw/na/mbawnanchccikwp7z4hhe1n0tbi.png" alt="openai输入转换"></p><br>
<p>,   ?</p><br>
<h1 id="bert-ot-dekoderov-k-enkoderam">BERT:    </h1><br>
<p>OpenAI Transformer      ,    .  -       LSTM  .   ELMo  ,  OpenAI Transformer    .         ,      ,    ( – «   ,    »)?</p><br>
<blockquote>«  », –      .</blockquote><br>
<h3 id="maskirovannaya-yazykovaya-model-masked-language-model">   (masked language model)</h3><br>
<blockquote>«    », –  .<br>
« !» –  . – « ,            .»<br>
«   », –   .</blockquote><p><img src="https://habrastorage.org/webt/z7/m-/qm/z7m-qmmtz724m8viviqejgzsmjs.png" alt="BERT语言建模遮罩lm"></p><br>
<p><em>    BERT  «»   15%          .</em></p><br>
<p>        –   ,  BERT ,    «  » (masked language model)     (   «-»).</p><br>
<p>  15%  , BERT      ,      .             .</p><br>
<h3 id="zadachi-dvuh-predlozheniy">  </h3><br>
<p>     OpenAI Transformer,   ,        -      (,       ?                ,       ?).</p><br>
<p> ,  BERT         ,     :    (  );  ,      ?</p><br>
<p><img src="https://habrastorage.org/webt/rz/hr/jf/rzhrjfq5iyequzyykq9tp0urdic.png" alt="伯特下句预测"></p><br>
<p><em> ,   BERT    –     .     , .. BERT  WordPieces,       –        .</em></p><br>
<h3 id="modeli-dlya-konkretnyh-zadach">   </h3><br>
<p>   BERT'        .</p><br>
<p><img src="https://habrastorage.org/webt/03/8i/a7/038ia7qjndp3qhcz8pdkcd_14nw.png" alt="伯特任务"></p><br>
<p><em>a)    : MNLI, QQP, QNLI, STS-B, MRPC, RTE, SWAG; b)    : SST-2, CoLA; c) - : SQuAD v1.1; d)    : CoNLL-2003 NER.</em></p><br>
<h3 id="bert-dlya-izvlecheniya-priznakov">BERT   </h3><br>
<p>   –     BERT.      ELMo,     BERT'    .         – ,          , , ,    (named-entity recognition).</p><br>
<p><img src="https://habrastorage.org/webt/ob/pa/a6/obpaa6snqryacqb9vbyaahue7zc.png" alt="伯特位嵌入"></p><br>
<p>       ?    .    6  (    ,   96,4):</p><br>
<p><img src="https://habrastorage.org/webt/ir/vr/sv/irvrsv9mefroz7io6ilnjng3fo4.png" alt="伯特特征提取上下文嵌入"></p><br>
<h1 id="test-drayv-berta">- BERT'</h1><br>
<p>   BERT   –   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow">BERT FineTuning with Cloud TPUs</a>,   Google Colab.       Cloud TPU,    , ..  BERT'     TPU,    CPU  GPU.</p><br>
<p>  –     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"> BERT'</a>:</p><br>
<ul>
<li>   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow">modeling.py</a> (class BertModel)      .</li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow">run_classifier.py</a> –    .         .       , .  create_model()   .</li>
<li>     ,     BERT'     ,    ,      102 .</li>
<li>BERT     .      WordPieces. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow">tokenization.py</a> – ,     WordPieces,   BERT'.</li>
</ul><br>
<p>    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow">PyTorch- BERT'</a>.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow">AllenNLP</a>      BERT'a   .</p><br>
<h1 id="avtory"></h1><br>
<ul>
<li><strong> </strong> — <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow">Jay Alammar</a></li>
<li><strong></strong> — <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="> </a></li>
<li><strong>  </strong> — <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="> </a></li>
</ul></div>
      
    </div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/zh-CN487358/">https://habr.com/ru/post/zh-CN487358/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN487340/index.html">正如我们所做的那样，下一个聊天机器人的设计师。第1部分</a></li>
<li><a href="../zh-CN487342/index.html">第2课-会议和活动</a></li>
<li><a href="../zh-CN487346/index.html">网络异常分析工具的使用案例：泄漏检测</a></li>
<li><a href="../zh-CN487348/index.html">优先任务排名-IL TEMPO应用</a></li>
<li><a href="../zh-CN487356/index.html">你要改变吗？再想一想</a></li>
<li><a href="../zh-CN487360/index.html">数据匿名化不能保证您完全匿名</a></li>
<li><a href="../zh-CN487362/index.html">Angular 9现在可用-常春藤到了</a></li>
<li><a href="../zh-CN487366/index.html">恐慌状态：对2019-nCoV冠状病毒流行情况的清晰观察</a></li>
<li><a href="../zh-CN487368/index.html">混合销售团队。人与AI在同一团队中工作</a></li>
<li><a href="../zh-CN487370/index.html">基于msgr.ru数据的房地产市场分析</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>