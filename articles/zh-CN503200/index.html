<!doctype html>
<html class="no-js" lang="zh-CN">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🧙 📑 🍮 YOLOv4-Microsoft COCO数据集上最准确的实时神经网络 ♟️ 🏪 🤜🏿</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Darknet YOLOv4比Google TensorFlow EfficientDet和FaceBook Pytorch / Detectron RetinaNet / MaskRCNN更快/更准确。
 
 关于介质的同一文章：介质
 代码：github.com/AlexeyAB/darknet...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>YOLOv4-Microsoft COCO数据集上最准确的实时神经网络</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/503200/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Darknet YOLOv4比Google TensorFlow EfficientDet和FaceBook Pytorch / Detectron RetinaNet / MaskRCNN更快/更准确。</font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">关于介质的同一文章</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">：</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">介质</font></font></a><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">代码</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">：</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/AlexeyAB/darknet </font></font></a><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">文章</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">：</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">arxiv.org/abs/2004.10934</font></font></a><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/3h/nc/sr/3hncsroz9wt8u3ycqskubgu1xk8.gif"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
我们将展示比较和使用神经网络检测物体的一些细微差别。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
我们的目标是开发一种用于实际产品的对象检测算法，而不仅仅是推动科学发展。</font><font style="vertical-align: inherit;">YOLOv4神经网络（608x608）的准确度是Microsoft-COCO-testdev 43.5％AP / 65.7％AP50。</font><font style="vertical-align: inherit;">在Tesla V100上为</font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">62 FPS</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> -YOLOv4（608x608批次= 1）- </font><font style="vertical-align: inherit;">在RTX 2080 Ti上</font><font style="vertical-align: inherit;">使用Darknet- </font><font style="vertical-align: inherit;">frame </font></font><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">400 FPS</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> -YOLOv4（416x416批次= 4）-在TensorRT + tkDNN上使用</font></font><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">32 FPS</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> -YOLOv4（416x416批次= 1） Jetson AGX Xavier-使用TensorRT + tkDNN</font></font><br>
<br>
<img src="https://habrastorage.org/webt/p_/ep/cl/p_epcl_aaw_trgeltekatagtqkg.jpeg"> <br>
<a name="habracut"></a><br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/1_SiUOYUoOI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">首先，一些有用的链接。</font></font></h3><br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">您可以在本文中阅读YOLOv4中使用的功能的详细说明：</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">medium.com/@jonathan_hui/yolov4-c9901eaa8e61</font></font></a></li>
<li>  YOLOv4: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://lutzroeder.github.io/netron/%3Furl%3D" rel="nofollow">lutzroeder.github.io/netron/?url=https%3A%2F%2Fraw.githubusercontent.com%2FAlexeyAB%2Fdarknet%2Fmaster%2Fcfg%2Fyolov4.cfg</a></li>
<li>     YOLOv4  GPU   Google-cloud  Jupyter Notebook –      ,   - «Open in Playground»,         [ ] –    ,  ,  ,    5    : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow">colab.research.google.com/drive/12QusaaRj_lUwCGDvQNfICpa7kA7_a2dE</a>      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow">www.youtube.com/watch?v=mKAEGSxwOAY</a></li>
<li>  Darknet   : <br>
 — <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow">  Darknet YOLOv4</a><br>
 — <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow">  Darknet YOLOv4</a><br>
 </li>
</ul><br>
<h3>   </h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
我们的YOLOv4神经网络和我们自己的Darknet DL框架（C / C ++ / CUDA）在FPS速度和AP50：在Microsoft COCO数据集上的准确度优于DL框架和神经网络：Google TensorFlow EfficientDet，FaceBook Detectron RetinaNet / MaskRCNN，PyTorch Yolov3-ASFF和其他许多产品... YOLOv4在Microsoft COCO测试中以62 FPS TitanV或34 FPS RTX 2070的速度达到了43.5％AP / 65.7％AP50的精度。与其他现代探测器不同，YOLOv4可以训练任何具有以下能力的人具有8-16 GB VRAM的nVidia游戏图形卡的任何人。现在，不仅大型公司可以在数百个GPU / TPU上训练神经网络，以使用大型微型批次来实现更高的精度，所以我们将人工智能的控制权返回给普通用户，因为对于YOLOv4，不需要大型微型批次，可以限制为2-8。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
YOLOV4最适合实时使用，因为网络位于</font><font style="vertical-align: inherit;">AP（准确性）/ FPS（速度）图中</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">的帕累托最优曲线</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">上。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/2k/77/as/2k77aszzprngk0qmtistcehkz8c.png"><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">许多神经网络的准确性（AP）和速度（FPS）图表，用于检测在GPU TitanV / TeslaV100，TitanXP / TeslaP100，TitanX / TeslaM40上测量的对象，以显示AP50的两个主要准确性指标：95和AP50</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
为了公平起见，我们从文章和仅在具有相同架构的GPU上进行比较。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
大多数实际任务对检测器具有最低的必要要求-这些是可接受的最低精度和速度。通常，对于实时系统，最低允许速度为30 FPS（每秒帧）或更高。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
从图中可以看出，在FPS 30或更高的实时系统中：</font></font><br>
<br>
<ul>
<li> YOLOv4-608   RTX 2070  <b>450$</b> (34 FPS)   <b>43.5% AP / 65.7% AP50</b></li>
<li> EfficientDet-D2   TitanV  <b>2250$</b> (42 FPS)   <b>43.0% AP / 62.3% AP50</b></li>
<li> EfficientDet-D0   RTX 2070  <b>450$</b> (34 FPS)   <b>33.8% AP / 52.2% AP50</b></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
那些。</font><font style="vertical-align: inherit;">与EfficientDet-D2（Google-TensorFlow）相比，</font><font style="vertical-align: inherit;">YOLOv4所需的</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">设备便宜5倍，</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">且精度更高。您可以使用EfficientDet-D0（Google-TensorFlow），则设备成本相同，但准确性将降低10％AP。</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
另外，某些工业系统在散热或使用被动冷却系统方面有限制-在这种情况下，即使有钱也不能使用TitanV。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
在使用TensorRT + tkDNN的RTX 2080 Ti GPU上使用YOLOv4（416x416）时，我们的速度提高了2倍，而在batch = 4时，速度提高了3x-4倍-坦白地说，我们不在arxiv上的文章中介绍这些结果。单位：</font></font><br>
<img src="https://habrastorage.org/webt/ci/j7/uq/cij7uqas0ypsjcpsfkhvdxuyxzs.png"><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
YOLOv4神经网络（416x416）FP16（张量核心）批处理= </font><font style="vertical-align: inherit;">1达到32 FPS计算器nVidia Jetson AGX Xavier使用库+ tkDNN TensorRT：</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/ceccocats/tkDNN</font></font></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
速度稍慢，使得使用CUDA编译的OpenCV-dnn库：</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">docs .opencv.org / master / da / d9d / tutorial_dnn_yolo.html</font></font></a><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
有时，在使用大批量时或在使用专用软件（TensorRT）进行测试时，可以指示文章中某些神经网络的速度（FPS），这可以优化网络并显示出增加的FPS值。将TRT上的某些网络与没有TRT的其他网络进行比较是不公平的。与批次= 1相比，使用较大的批次大小会增加FPS，但也会增加延迟（而不是减少延迟）。如果批次= 1的网络显示40 FPS，而批次= 32的网络显示60 FPS，则对于批次= 1，延迟将为25ms，对于批次= 32，延迟为〜500ms，因为每秒仅处理约2个数据包（每个32个图像），这就是为什么在许多工业系统中不使用批处理= 32的原因。因此，我们仅在批处理= 1且不使用TensorRT的情况下比较了图上的结果。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
任何过程都可以由人或计算机控制。当计算机系统由于低速而造成较大的延迟而犯了太多错误时，则不能委托它完全控制动作，在这种情况下，该人控制流程，并且计算机系统仅给出提示-这是一个推荐系统-该人可以工作，并且仅该系统告诉哪里出错了。当系统快速且高精度地工作时，这样的系统可以独立控制过程，只有人来照料。因此，准确性和系统速度始终很重要。如果您觉得YOLOv4 416x416的120 FPS对于您的任务而言太高了，并且最好更慢，更准确地采用该算法，那么在实际任务中，您很有可能会使用比Tesla V100 250瓦弱的功能，例如RTX 2060 / Jetson-Xavier 30-80 Watt，在这种情况下，您将在YOLOv4 416x416上获得30 FPS，而其他神经网络则以1-15 FPS或根本无法启动。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">训练各种神经网络的特征</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
您必须在多个Tesla V100 32GB GPU上以mini-batch = 128大小训练EfficientDet，而YOLOv4仅在一个mini-batch = 8 =批/细分的Tesla V100 32GB GPU上进行训练，并且可以在常规游戏中进行训练显卡8-16GB GPU-VRAM。</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
下一个细微差别是训练神经网络以检测其自身对象的难度。</font><font style="vertical-align: inherit;">无论在相同的1080 Ti GPU上训练其他网络多少时间，您都不会获得上图所示的准确性。</font><font style="vertical-align: inherit;">大多数网络（EfficientDet，ASFF等）都需要在4-128个GPU上进行训练（使用syncBN具有较大的迷你批处理大小），并且有必要针对每种网络分辨率进行新的训练，而不能同时满足这两个条件，就不可能达到它们声明的AP或AP50精度。</font></font><br>
 <br>
<img src="https://habrastorage.org/webt/p4/sx/p3/p4sxp3ewxd9owskis23n6dyrv58.jpeg"><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">您可以在其他检测器中看到物体的检测精度与最小批量大小的相关性，即 </font><font style="vertical-align: inherit;">使用128个视频卡而不是8个视频卡，学习速度提高了10倍，最终精度提高了1.5 AP-MegDet：大型微型批处理对象检测器</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">arxiv.org/abs/1711.07240</font></font></a></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Yolo ASFF：</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">arxiv.org/abs/1911.09516</font></font></a> <br>
<blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">继[43]之后，我们在训练过程中引入了一些技巧，例如混合算法[12]，余弦[26]学习率进度表和同步批量归一化技术[30]。</font></font></blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
EfficientDet：</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">arxiv.org/abs/1911.09070</font></font></a> <br>
<blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">在每次每次卷积后，以批次范数衰减0.99和epsilon 1e-3添加同步的批次归一化。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
每个模型在32个TPUv3内核上训练了300个时期，批次总大小为128。</font></font></blockquote><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow">cloud.google.com/tpu/docs/types-zones#europe</a><br>
<blockquote>v3-32 TPU type (v3) – 32 TPU v3 cores – 512 GiB Total TPU memory</blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
您必须使用512 GB TPU / GPU-RAM来训练EfficientDet模型，并在批次= 128时进行同步批次归一化，而mini-batch = 8且仅32 GB GPU-RAM用于训练YOLOv4。尽管如此，尽管YOLOv4仅训练了1次，每1个GPU的分辨率为512x512（Tesla V100 32GB / 16GB），但它比公共网络更快/更准确。同时，使用较小的微型批处理大小和GPU-VRAM不会像其他神经网络那样导致准确性的急剧下降：</font></font><br>
<br>
<img src="https://habrastorage.org/webt/xi/ol/rs/xiolrsvx4vzpjvahb6kvambdvgq.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">来源：</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">arxiv.org/abs/2004.10934</font></font></a></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
因此，您可以在PC上本地训练人工智能，而无需下载数据集到云-这可以确保对您的个人数据的保护，并为所有人提供人工智能培训。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
只需训练一次网络分辨率为512x512的网络，然后就可以在以下范围内使用不同的网络分辨率：[416x416-512x512-608x608]。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
大多数其他模型需要针对每种网络分辨率分别进行每次训练，因此，训练时间要长很多倍。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">物体检测算法的测量精度特征</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
您总是可以找到一张图像，其中一种算法效果不佳，另一种算法效果很好，反之亦然。因此，为了测试检测算法，使用了约20,000张图像和80种不同类型的对象的大集合-MSCOCO test-dev数据集。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
因此，算法不会尝试仅记住每个图像的哈希值以及其上的坐标（过度拟合），因此始终会在算法和训练过程中未看到的图像和标签上检查对象检测的准确性-这确保了算法可以检测到图像/视频上的对象没看过。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
因此，在公共领域，没有人会犯错，在公共领域，您只检测到test-dev测试图像，然后将结果发送到CodaLab评估服务器，在该服务器上，程序本身会将您的结果与任何人都无法访问的测试注释进行比较。 。</font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">MSCOCO数据集包括3个部分</font></font></a><br>
<br>
<ol>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">教程：120,000张图像和一个json文件，其中包含每个对象的坐标</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">验证集：5,000个图像和一个json文件，其中包含每个对象的坐标</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">测试套件：41,000个不带对象坐标的jpg图像（其中一些图像用于确定任务的准确性：对象检测，实例分割，关键点等）</font></font></li>
</ol><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">YOLOv4开发的特点</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
在开发YOLOv4时，我必须在C / C ++ / CUDA上同时开发YOLOv4神经网络和Darknet框架。</font><font style="vertical-align: inherit;">因为 </font><font style="vertical-align: inherit;">在Darknet中，没有自动区分和自动执行链规则的功能，因此必须手动实施所有渐变。</font><font style="vertical-align: inherit;">另一方面，我们可以摆脱严格遵循链条规则，改变反向传播并尝试非常平凡的事情来提高学习稳定性和准确性。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">创建神经网络时的其他发现</font></font></h3><br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">并非总是最好的用于对对象进行分类的网络将最好地用作用于检测对象的网络的骨干网</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用经过训练的权重具有提高分类精度的功能可能会对检测器精度产生不利影响（在某些网络上）</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">并非各种研究中陈述的所有功能都会提高网络准确性。</font></font></li>
<li>                .</li>
<li>      BFLOPS  ,   BFLOPS    </li>
<li>                  ,     receptive field     ,       stride=2 / conv3x3,    weights (filters)      . </li>
</ul><br>
<h3>   YOLOv4</h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
使用受过训练的YOLOv4模型的对象检测内置于OpenCV-dnn库</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/opencv/opencv/issues/17148中，</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">因此您可以直接从OpenCV使用YOLOv4，而无需使用Darknet框架。</font><font style="vertical-align: inherit;">OpenCV库支持在CPU，GPU（nVidia GPU），VPU（Intel Myriad X）上实现神经网络。</font><font style="vertical-align: inherit;">更多详细信息：</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">docs.opencv.org/master/da/d9d/tutorial_dnn_yolo.html </font></font></a><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">OpenCV</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">（dnn）框架：</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">C ++示例：</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/opencv/opencv/blob/master/samples/dnn/object_detection.cpp</font></font></a></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Python示例：</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/opencv/opencv/blob/master/samples/dnn/object_detection.py</font></font></a></li>
</ul><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Darknet</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">框架：</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用YOLOv4来检测对象的说明：</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/AlexeyAB/darknet#how-to-use-on-the-command-line</font></font></a></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">训练神经网络以检测其自身对象的说明：</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/AlexeyAB/darknet#how-to-train-to-detect-your-custom-objects</font></font></a></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">在ILSVRC2012数据集（ImageNet）上训练CSPDarknet53分类器的说明：</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/AlexeyAB/darknet/wiki/Train-Classifier-on-ImageNet-</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">（ILSVRC2012）</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">在MS COCO数据集上训练YOLOv4的说明：</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/AlexeyAB/darknet/wiki/Train-Detector-on-MS-COCO-</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">（trainvalno5k-2014）- </font><font style="vertical-align: inherit;">数据集</font></font></li>
</ul><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">tkDNN + TensorRT-</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用YOLOv4进行对象检测的最大速度：TensorRT + tkDNN </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/ceccocats/tkDNN</font></font></a><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">RTX 2080 Ti上为400 FPS-YOLOv4（416x416批次= 4）</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">在Jetson AGX Xavier上为32 FPS-YOLOv4（416x416批次= 1）</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
可以扩展使用YOLOv4来检测3D旋转Bboxes或关键点/面部标志，例如：</font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/ouyanghuiyu/darknet_face_with_landmark</font></font></a><br>
<br>
<img src="https://habrastorage.org/webt/z7/vs/dv/z7vsdvhcpfbrgmdv1byhbpzd1cu.jpeg"></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN503178/index.html">Apple Mac和精美的设备。LTO，SAS，光纤通道，eSATA</a></li>
<li><a href="../zh-CN503182/index.html">“我是公司中的第一个盲人开发人员。” 第1部分</a></li>
<li><a href="../zh-CN503184/index.html">我们邀请您参加Zabbix在线会议</a></li>
<li><a href="../zh-CN503192/index.html">oVirt在2小时内。第4部分。基本操作</a></li>
<li><a href="../zh-CN503196/index.html">常春藤联盟提供450道免费课程</a></li>
<li><a href="../zh-CN503204/index.html">如何在Android 10上闪烁小米Redmi 4 Prime / Pro / Premium</a></li>
<li><a href="../zh-CN503208/index.html">什么时候是最佳投资时间？</a></li>
<li><a href="../zh-CN503210/index.html">网络钓鱼站点可以根除吗？</a></li>
<li><a href="../zh-CN503212/index.html">30个生活技巧，可完成在线课程</a></li>
<li><a href="../zh-CN503214/index.html">使用ElasticSearch对高负载项目进行负载优化</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>