<!doctype html>
<html class="no-js" lang="de">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>📬 👩🏾‍⚕️ 🤾🏿 ML, VR & Robots (und ein bisschen Cloud) 🤟🏻 🧠 ♨️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo alle zusammen! 
 
 Ich möchte über ein sehr langweiliges Projekt sprechen, bei dem sich Robotik, maschinelles Lernen (und zusammen ist dies Robo...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>ML, VR & Robots (und ein bisschen Cloud)</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/recognitor/blog/486680/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hallo alle zusammen! </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ich möchte über ein sehr langweiliges Projekt sprechen, bei dem sich Robotik, maschinelles Lernen (und zusammen ist dies Roboterlernen), virtuelle Realität und ein bisschen Cloud-Technologie kreuzten. </font><font style="vertical-align: inherit;">Und das alles macht tatsächlich Sinn. </font><font style="vertical-align: inherit;">Schließlich ist es sehr praktisch, in einen Roboter einzusteigen, zu zeigen, was zu tun ist, und dann anhand der gespeicherten Daten Gewichte auf dem ML-Server zu trainieren. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Unter dem Schnitt werden wir erzählen, wie es jetzt funktioniert, und einige Details zu jedem der Aspekte, die entwickelt werden mussten.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/t0/kx/wi/t0kxwiicqakswvomnrwz-iycc2o.jpeg"><br>
<a name="habracut"></a><br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wozu</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Für den Anfang lohnt es sich, ein wenig zu enthüllen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Es scheint, dass mit Deep Learning bewaffnete Roboter Menschen überall von ihren Jobs verdrängen werden. </font><font style="vertical-align: inherit;">In der Tat ist nicht alles so glatt. </font><font style="vertical-align: inherit;">Wo Aktionen streng wiederholt werden, sind Prozesse bereits sehr gut automatisiert. </font><font style="vertical-align: inherit;">Wenn es sich um „intelligente Roboter“ handelt, dh um Anwendungen, bei denen Computer Vision und Algorithmen bereits ausreichen. </font><font style="vertical-align: inherit;">Es gibt aber auch viele äußerst komplizierte Geschichten. </font><font style="vertical-align: inherit;">Roboter können mit der Vielfalt der Objekte, mit denen sie umgehen müssen, und der Vielfalt der Umgebung kaum fertig werden.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wichtige Punkte</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Es gibt drei wichtige Dinge in Bezug auf die Implementierung, die noch nicht überall zu finden sind: </font></font><br>
<br>
<ul>
<li>       (data-driven learning). ..   ,    ,     ,    . ,     .</li>
<li>   ()    </li>
<li>  -  (Human-machine collaboration) </li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das zweite ist ebenfalls wichtig, da wir im Moment eine Änderung der Lernansätze, der dahinter stehenden Algorithmen und der Computerwerkzeuge beobachten werden. </font><font style="vertical-align: inherit;">Wahrnehmungs- und Steuerungsalgorithmen werden flexibler. </font><font style="vertical-align: inherit;">Ein Roboter-Upgrade kostet Geld. </font><font style="vertical-align: inherit;">Und der Rechner kann effizienter eingesetzt werden, wenn mehrere Roboter gleichzeitig bedient werden. </font><font style="vertical-align: inherit;">Dieses Konzept nennt man „Cloud Robotics“. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Mit letzterem ist alles einfach - AI ist derzeit nicht ausreichend entwickelt, um 100% Zuverlässigkeit und Genauigkeit in allen Situationen zu bieten, die vom Unternehmen gefordert werden. </font><font style="vertical-align: inherit;">Daher wird der Supervisor-Bediener, der manchmal Robotern helfen kann, nicht verletzt.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Planen</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Zunächst zu einer Software- / Netzwerkplattform, die alle beschriebenen Funktionen bietet: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/fd/6y/fi/fd6yfi2svby-3l7hkn9lxxt3neo.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Komponenten:</font></font><br>
<br>
<ol>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Der Roboter sendet einen 3D-Videostream an den Server und erhält als Antwort die Kontrolle. </font></font></li>
<li>    :  - ,      (, , , )</li>
<li>  ML  ( ),        ,   ,  .      —  3D   ,     . </li>
<li> -  ,  3D       ,   UI   .   — . </li>
</ol><br>
<h4> </h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Es gibt zwei Funktionsmodi des Roboters: automatisch und manuell. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Im manuellen Modus arbeitet der Roboter, wenn der ML-Dienst noch nicht geschult ist. </font><font style="vertical-align: inherit;">Dann wechselt der Roboter entweder auf Wunsch des Bedieners von automatisch zu manuell (ich habe beim Beobachten des Roboters merkwürdige Verhaltensweisen festgestellt) oder wenn ML-Dienste selbst eine Anomalie feststellen. </font><font style="vertical-align: inherit;">Über die Erkennung von Anomalien wird später berichtet - dies ist ein sehr wichtiger Teil, ohne den es unmöglich ist, den vorgeschlagenen Ansatz anzuwenden. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die Entwicklung der Kontrolle ist wie folgt:</font></font><br>
<br>
<ol>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Aufgabe für den Roboter wird in lesbaren Begriffen formuliert und Leistungsindikatoren beschrieben.</font></font><br>
</li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Der Bediener stellt in VR eine Verbindung zum Roboter her und führt die Aufgabe für einige Zeit innerhalb des vorhandenen Workflows aus</font></font><br>
</li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Der ML-Teil wird auf die empfangenen Daten trainiert</font></font><br>
</li>
<li>      ,     ML             <br>
</li>
<li>              <br>
</li>
</ol><br>
<h4>       3D</h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Sehr oft verwenden Roboter die ROS-Umgebung (Robot Operating System), die in der Tat ein Framework für die Verwaltung von „Knoten“ (Knoten) ist, von denen jeder einen Teil der Funktionalität des Roboters bereitstellt. Im Allgemeinen ist dies eine relativ bequeme Methode zum Programmieren von Robotern, die in gewisser Weise der Microservice-Architektur von Webanwendungen ähnelt. Der Hauptvorteil von ROS ist der Industriestandard, und es wird bereits eine große Anzahl von Modulen benötigt, um einen Roboter zu erstellen. Sogar Industrieroboterarme können ein ROS-Schnittstellenmodul haben. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Am einfachsten ist es, ein Brückenmodell zwischen unserem Serverteil und ROS zu erstellen. Zum Beispiel </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">wie z</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Jetzt verwendet unser Projekt eine weiterentwickelte Version des ROS- „Knotens“, der sich anmeldet und den Mikrodienst des Registers abfragt, mit dem ein Relay-Server ein bestimmter Roboter verbinden kann. Der Quellcode dient nur als Beispiel für Anweisungen zur Installation des ROS-Moduls. Wenn Sie dieses Framework (ROS) beherrschen, sieht zunächst alles ziemlich unfreundlich aus, aber die Dokumentation ist ziemlich gut, und nach ein paar Wochen beginnen Entwickler, seine Funktionalität recht sicher zu nutzen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Interessant - das Problem der Komprimierung des 3D-Datenstroms, der direkt auf dem Roboter erzeugt werden muss.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Es ist nicht so einfach, die Tiefenkarte zu komprimieren. </font><font style="vertical-align: inherit;">Selbst bei einer geringen Komprimierung des RGB-Streams ist eine sehr schwerwiegende lokale Helligkeitsverzerrung von wahr in Pixel an den Rändern oder beim Bewegen von Objekten zulässig. </font><font style="vertical-align: inherit;">Das Auge merkt das fast nicht, aber sobald die gleichen Verzerrungen in der Tiefenkarte erlaubt sind, wird beim Rendern von 3D alles sehr schlecht: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/kv/1l/6-/kv1l6-qgxaqxhsf1a-zl67hymtk.png"><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
(aus dem </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Artikel</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ) </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Diese Defekte an den Rändern verderben die 3D-Szene stark, weil </font><font style="vertical-align: inherit;">Es liegt nur viel Müll in der Luft.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wir haben begonnen, Frame-für-Frame-Komprimierung zu verwenden - JPEG für RGB und PNG für eine Tiefenkarte mit kleinen Hacks. </font><font style="vertical-align: inherit;">Diese Methode komprimiert den 30-FPS-Stream für eine 3D-Scannerauflösung von 640 x 480 bei 25 Mbit / s. </font><font style="vertical-align: inherit;">Eine bessere Komprimierung kann auch bereitgestellt werden, wenn der Datenverkehr für die Anwendung kritisch ist. </font><font style="vertical-align: inherit;">Es gibt kommerzielle 3D-Stream-Codecs, mit denen dieser Stream auch komprimiert werden kann.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Kontrolle der virtuellen Realität</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nachdem wir den Referenzrahmen der Kamera und des Roboters </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">kalibriert haben</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (und bereits </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;">einen Artikel über die Kalibrierung geschrieben haben</font></a><font style="vertical-align: inherit;"> ), kann der Roboterarm in der virtuellen Realität gesteuert werden. </font><font style="vertical-align: inherit;">Der Controller stellt sowohl die Position in 3D XYZ als auch die Ausrichtung ein. </font><font style="vertical-align: inherit;">Für einige Roboruk sind nur 3 Koordinaten ausreichend, aber bei einer großen Anzahl von Freiheitsgraden muss auch die Ausrichtung des von der Steuerung festgelegten Werkzeugs übertragen werden. </font><font style="vertical-align: inherit;">Darüber hinaus gibt es genügend Steuerungen an den Steuerungen, um Roboterbefehle auszuführen, z. B. das Ein- und Ausschalten der Pumpe, die Steuerung des Greifers und andere. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Zunächst wurde beschlossen, das JavaScript-Framework für Virtual-Reality-A-Frames zu verwenden, das auf der WebVR-Engine basiert. </font><font style="vertical-align: inherit;">Die ersten Ergebnisse (Videodemonstration am Ende des Artikels für den 4-Koordinaten-Arm) wurden auf dem A-Rahmen erzielt.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Tatsächlich stellte sich heraus, dass WebVR (oder A-Frame) aus mehreren Gründen eine erfolglose Lösung war:</font></font><br>
<br>
<ul>
<li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Kompatibilität hauptsächlich mit FireFox</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , während das A-Frame-Framework in FireFox keine Texturressourcen freigab (der Rest der Browser wurde damit fertig), bis der Speicherverbrauch 16 GB erreichte</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">begrenzte Interaktion mit VR-Controllern und Helm. </font><font style="vertical-align: inherit;">So war es beispielsweise nicht möglich, zusätzliche Markierungen hinzuzufügen, mit denen Sie beispielsweise die Position der Ellbogen des Bedieners festlegen können.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Anwendung erforderte Multithreading oder mehrere Prozesse. </font><font style="vertical-align: inherit;">In einem Thread / Prozess war es notwendig, die Videobilder zu entpacken, in einem anderen - Zeichnen. </font><font style="vertical-align: inherit;">Infolgedessen wurde alles durch Arbeiter organisiert, aber die Auspackzeit erreichte 30 ms, und das Rendern in VR sollte mit einer Frequenz von 90 FPS erfolgen.</font></font><br>
</li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
All diese Mängel führten dazu, dass das Rendern des Rahmens in den zugewiesenen 10 ms keine Zeit hatte und es in VR sehr unangenehme Zuckungen gab. Wahrscheinlich konnte alles überwunden werden, aber die Identität jedes Browsers war etwas nervig. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Jetzt haben wir uns entschlossen, zum C # -, OpenTK- und C # -Port der OpenVR-Bibliothek zu wechseln. Es gibt noch eine Alternative - Einheit. Sie schreiben, dass Unity für Anfänger ist ... aber schwierig. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das Wichtigste, was gefunden und bekannt sein musste, um Freiheit zu erlangen:</font></font><br>
<br>
<pre><code class="cs hljs">VRTextureBounds_t bounds = <span class="hljs-keyword">new</span> VRTextureBounds_t() { uMin = <span class="hljs-number">0</span>, vMin = <span class="hljs-number">0</span>, uMax = <span class="hljs-number">1f</span>, vMax = <span class="hljs-number">1f</span> }; <font></font>
OpenVR.Compositor.Submit(EVREye.Eye_Left, <span class="hljs-keyword">ref</span> leftTexture, <span class="hljs-keyword">ref</span> bounds, EVRSubmitFlags.Submit_Default);<font></font>
OpenVR.Compositor.Submit(EVREye.Eye_Right, <span class="hljs-keyword">ref</span> rightTexture, <span class="hljs-keyword">ref</span> bounds, EVRSubmitFlags.Submit_Default);
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
(Dies ist der Code zum Senden von zwei Texturen an das linke und rechte Auge des Helms) </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
d.h. </font><font style="vertical-align: inherit;">Zeichnen Sie OpenGL in die Textur, die verschiedene Augen sehen, und senden Sie es an eine Brille. </font><font style="vertical-align: inherit;">Joy kannte keine Grenzen, als sich herausstellte, dass das linke Auge mit Rot und das rechte mit Blau gefüllt war. </font><font style="vertical-align: inherit;">Nur ein paar Tage und jetzt wurde die über webSocket kommende Tiefen- und RGB-Karte in 10 ms anstelle von 30 in JS auf das polygonale Modell übertragen. </font><font style="vertical-align: inherit;">Und dann fragen Sie einfach die Koordinaten und Tasten der Controller ab, geben Sie das Ereignissystem für die Tasten ein, verarbeiten Sie Benutzerklicks, geben Sie die Zustandsmaschine für die Benutzeroberfläche ein und jetzt können Sie ein Glas vom Espresso nehmen:</font></font><br>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/EuBNaGZctf8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Jetzt ist die Qualität des Realsense D435 etwas deprimierend, aber sie wird vergehen, sobald wir mindestens </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">einen so interessanten 3D-Scanner von Microsoft</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> installieren </font><font style="vertical-align: inherit;">, dessen Punktwolke viel genauer ist.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Serverseite</font></font></h4><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Relay-Server</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Das Hauptfunktionselement ist das Server-Relay (Server in der Mitte), das vom Roboter einen Videostream mit 3D-Bildern und Sensorablesungen sowie dem Zustand des Roboters empfängt und unter den Verbrauchern verteilt. Eingabedaten - gepackte Frames und Sensorwerte über TCP / IP. Die Verteilung an Verbraucher erfolgt über Web-Sockets (ein sehr praktischer Mechanismus für das Streaming an mehrere Verbraucher, einschließlich eines Browsers). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Darüber hinaus speichert der Staging-Server den Datenstrom im S3-Cloud-Speicher, damit er später für Schulungen verwendet werden kann. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Jeder Relay-Server unterstützt die http-API, mit der Sie den aktuellen Status ermitteln können. Dies ist praktisch, um aktuelle Verbindungen zu überwachen.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die Weiterleitungsaufgabe ist sowohl aus rechnerischer als auch aus verkehrstechnischer Sicht recht schwierig. Daher folgten wir hier der Logik, dass Relay-Server auf einer Vielzahl von Cloud-Servern bereitgestellt werden. Und das bedeutet, dass Sie nachverfolgen müssen, wer wo eine Verbindung herstellt (insbesondere wenn sich Roboter und Bediener in verschiedenen Regionen befinden). </font></font><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Registrieren</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Die zuverlässigste Version ist jetzt für jeden Roboter schwer festzulegen, mit welchen Servern er eine Verbindung herstellen kann (Redundanz schadet nicht). Der ML-Verwaltungsdienst ist dem Roboter zugeordnet. Er fragt den Relay-Server ab, um festzustellen, mit welchem ​​der Roboter verbunden ist, und stellt eine Verbindung mit dem entsprechenden Server her, wenn er natürlich über genügend Rechte dafür verfügt. Die Anwendung des Bedieners funktioniert auf ähnliche Weise.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das angenehmste! Aufgrund der Tatsache, dass die Ausbildung von Robotern ein Dienst ist, ist der Dienst nur für uns im Inneren sichtbar. So kann sein Frontend für uns so bequem wie möglich sein! Jene. Es handelt sich um eine Konsole im Browser (es gibt eine </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">TerminalJS-</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Bibliothek </font><font style="vertical-align: inherit;">, die </font><font style="vertical-align: inherit;">in ihrer Einfachheit </font><font style="vertical-align: inherit;">sehr </font><font style="vertical-align: inherit;">schön </font><font style="vertical-align: inherit;">ist und sehr einfach zu ändern ist, wenn Sie zusätzliche Funktionen wie die automatische TAB-Vervollständigung oder das Abspielen des Anrufverlaufs wünschen). Sie sieht folgendermaßen aus: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/wl/vl/yo/wlvlyojuytiepjvhzexlxr-ixcq.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dies ist natürlich ein separates Diskussionsthema, warum die Befehlszeile so gemütlich. Übrigens ist es besonders praktisch, Unit-Tests eines solchen Frontends durchzuführen.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Zusätzlich zur http-API implementiert dieser Dienst einen Mechanismus zum Registrieren von Benutzern mit temporären Token, Anmelde- / Abmeldeoperatoren, Administratoren und Robotern, Sitzungsunterstützung und Sitzungsverschlüsselungsschlüsseln für die Verkehrsverschlüsselung zwischen dem Relay-Server und dem Roboter. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
All dies geschieht in Python mit Flask - ein sehr enger Stapel für ML-Entwickler (d. H. Uns). </font><font style="vertical-align: inherit;">Ja, außerdem ist die vorhandene CI / CD-Infrastruktur für Microservices mit Flask befreundet.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Verzögerungsproblem</font></font></h4> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wenn wir die Manipulatoren in Echtzeit steuern möchten, ist die minimale Verzögerung äußerst nützlich. </font><font style="vertical-align: inherit;">Wenn die Verzögerung zu groß wird (mehr als 300 ms), ist es sehr schwierig, die Manipulatoren basierend auf dem Bild im virtuellen Helm zu steuern. </font><font style="vertical-align: inherit;">In unserer Lösung beträgt die Verzögerung aufgrund der Frame-für-Frame-Komprimierung (dh ohne Pufferung) und ohne Verwendung von Standardtools wie GStreamer selbst unter Berücksichtigung des Zwischenservers etwa 150 bis 200 ms. </font><font style="vertical-align: inherit;">Die Übertragungszeit über das Netzwerk von ihnen beträgt etwa 80 ms. </font><font style="vertical-align: inherit;">Der Rest der Verzögerung wird durch die Realsense D435-Kamera und die begrenzte Aufnahmefrequenz verursacht.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dies ist natürlich ein Problem in voller Höhe, das im Verfolgungsmodus auftritt, wenn der Manipulator in seiner Realität in der virtuellen Realität ständig der Steuerung des Bedieners folgt. </font><font style="vertical-align: inherit;">In der Art, sich zu einem bestimmten Punkt XYZ zu bewegen, verursacht die Verzögerung keine Probleme für den Bediener.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ML Teil</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Es gibt zwei Arten von Dienstleistungen: Management und Schulung. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Der Trainingsdienst sammelt die im S3-Speicher gespeicherten Daten und beginnt mit dem erneuten Training der Modellgewichte. </font><font style="vertical-align: inherit;">Am Ende des Trainings werden Gewichte an den Managementdienst gesendet. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Der Verwaltungsdienst unterscheidet sich in Bezug auf Eingabe- und Ausgabedaten nicht von der Anwendung des Bedieners. </font><font style="vertical-align: inherit;">Ebenso der Eingangs-RGBD-Stream (RGB + Depth), die Sensorwerte und der Roboterstatus, die Ausgangssteuerbefehle. </font><font style="vertical-align: inherit;">Aufgrund dieser Identität erscheint es möglich, im Rahmen des Konzepts des „datengesteuerten Trainings“ zu trainieren.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Der Zustand des Roboters (und die Sensorwerte) sind eine Schlüsselgeschichte für ML. Es definiert den Kontext. Beispielsweise verfügt ein Roboter über eine Zustandsmaschine, die für seinen Betrieb charakteristisch ist und weitgehend bestimmt, welche Art von Steuerung erforderlich ist. Diese 2 Werte werden zusammen mit jedem Frame übertragen: der Betriebsmodus und der Zustandsvektor des Roboters. </font></font><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Und ein wenig zum Training:</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
In der Demonstration am Ende des Artikels ging es </font><i><font style="vertical-align: inherit;">darum</font></i><font style="vertical-align: inherit;"> , ein Objekt (einen Kinderwürfel) in einer 3D-Szene zu finden. Dies ist eine grundlegende Aufgabe für Bestückungsanwendungen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das Training basierte auf einem Paar von Vorher-Nachher-Bildern und der Zielbezeichnung, die mit manueller Steuerung erhalten wurden: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/n6/v2/s3/n6v2s3v59u7eumxhdh3gxucngxq.jpeg"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Aufgrund des Vorhandenseins von zwei Tiefenkarten war es einfach, die Maske des im Bild bewegten Objekts zu berechnen: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/6a/vm/k-/6avmk-ue2jfndgwzy08zdb0qtqe.jpeg"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Außerdem werden xyz auf die Kameraebene projiziert und Sie können die Nachbarschaft des erfassten Objekts auswählen:</font></font><br>
<br>
<img src="https://habrastorage.org/webt/ha/_s/o-/ha_so-yh4xmxwsp4dkoo42tct9i.jpeg"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Eigentlich mit dieser Nachbarschaft und wird funktionieren. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Zuerst erhalten wir XY durch Training. Unet ein Faltungsnetzwerk für die Würfelsegmentierung. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dann müssen wir die Tiefe bestimmen und verstehen, ob das Bild vor uns abnormal ist. </font><font style="vertical-align: inherit;">Dies erfolgt mit einem Auto-Encoder in RGB und einem bedingten Auto-Encoder in der Tiefe. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Modellarchitektur für das Training von Auto-Encodern: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/xf/q4/ex/xfq4exzon5b63cucrz6aqi8kflm.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die Logik der Arbeit:</font></font><br>
<br>
<ol>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Suchen Sie auf der „Wärmekarte“ nach einem Maximum (bestimmen Sie die Winkelkoordinaten u = x / zv = y / z des Objekts), das den Schwellenwert überschreitet</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dann rekonstruiert der Auto-Encoder die Nachbarschaft des gefundenen Punktes für alle Hypothesen in der Tiefe (mit einem gegebenen Schritt von min_depth bis max_depth) und wählt die Tiefe aus, bei der die Diskrepanz zwischen Rekonstruktion und Eingabe minimal ist</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mit den Winkelkoordinaten u, v und der Tiefe können Sie die Koordinaten x, y, z erhalten</font></font></li>
</ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ein Beispiel für die Auto-Encoder-Rekonstruktion einer Karte von Würfeltiefen mit einer korrekt definierten Tiefe: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/av/1t/bu/av1tbu5dyvflda-_-lpbdw0t0qs.jpeg"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Zum Teil basiert die Idee einer Tiefensuchmethode auf einem </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Artikel über Sätze von Auto-Encodern</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dieser Ansatz eignet sich gut für Objekte mit verschiedenen Formen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Im Allgemeinen gibt es jedoch viele verschiedene Ansätze, um ein XYZ-Objekt aus einem RGBD-Bild zu finden. Natürlich ist es in der Praxis und bei einer großen Datenmenge erforderlich, die genaueste Methode zu wählen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Es gab auch die Aufgabe, Anomalien zu erkennen. Dazu benötigen wir ein Segmentierungs-Faltungsnetzwerk, um aus den verfügbaren Masken zu lernen. Anhand dieser Maske können Sie dann die Genauigkeit der Auto-Encoder-Rekonstruktion in der Tiefenkarte und im RGB bewerten. Aufgrund dieser Diskrepanz kann man über das Vorhandensein einer Anomalie entscheiden.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Aufgrund dieser Methode ist es möglich, das Auftreten von zuvor nicht sichtbaren Objekten im Rahmen zu erkennen, die dennoch vom primären Suchalgorithmus erkannt werden.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Demonstration</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das Überprüfen und Debuggen der gesamten erstellten Softwareplattform wurde am Stand durchgeführt:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3D-Kamera Realsense D435</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">4 koordinieren Dobot Magier</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">VR Helm HTC Vive</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Server in der Yandex Cloud (reduziert die Latenz im Vergleich zur AWS Cloud)</font></font></li>
</ul><br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/G5rBhaxHL_E" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In dem Video lernen wir, wie man einen Würfel in einer 3D-Szene findet, indem man eine Aufgabe in VR Pick &amp; Place ausführt. </font><font style="vertical-align: inherit;">Etwa 50 Beispiele reichten für das Training auf einem Würfel. </font><font style="vertical-align: inherit;">Dann ändert sich das Objekt und es werden ungefähr 30 weitere Beispiele gezeigt. </font><font style="vertical-align: inherit;">Nach der Umschulung kann der Roboter ein neues Objekt finden. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Der gesamte Vorgang dauerte ungefähr 15 Minuten, von denen ungefähr die Hälfte das Gewicht des Modells trainierte. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Und in diesem Video steuert YuMi in VR. </font><font style="vertical-align: inherit;">Um zu lernen, wie Objekte bearbeitet werden, müssen Sie die Ausrichtung und Position des Werkzeugs bewerten. </font><font style="vertical-align: inherit;">Die Mathematik basiert auf einem ähnlichen Prinzip, befindet sich jedoch derzeit in der Test- und Entwicklungsphase.</font></font><br>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/Q0yTey1srBM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fazit</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Big Data und Deep Learning sind nicht alles. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wir ändern den Lernansatz und bewegen uns dahin, wie Menschen neue Dinge lernen - indem wir wiederholen, was sie sehen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Der mathematische Apparat „unter der Haube“, den wir für reale Anwendungen entwickeln werden, zielt auf das Problem der kontextsensitiven Interpretation und Kontrolle ab. </font><font style="vertical-align: inherit;">Der Kontext hier sind natürliche Informationen, die von Robotersensoren verfügbar sind, oder externe Informationen über den aktuellen Prozess. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Und je mehr technologische Prozesse wir beherrschen, desto mehr wird die Struktur des „Gehirns in den Wolken“ entwickelt und seine Einzelteile trainiert. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Stärken dieses Ansatzes:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">die Möglichkeit zu lernen, wie man variable Objekte manipuliert </font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Lernen in einer sich ändernden Umgebung (z. B. mobile Roboter)</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">schlecht strukturierte Aufgaben</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">kurze Markteinführungszeit; </font><font style="vertical-align: inherit;">Sie können das Ziel auch im manuellen Modus mit den Operatoren ausführen</font></font><br>
</li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Verjährung:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bedarf an zuverlässigem und gutem Internet</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zusätzliche Methoden sind erforderlich, um eine hohe Genauigkeit zu erzielen, z. B. Kameras im Manipulator selbst</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wir arbeiten derzeit daran, unseren Ansatz auf die Standard-Bestückungsaufgabe verschiedener Objekte anzuwenden. </font><font style="vertical-align: inherit;">Aber es scheint uns (natürlich!), Dass er zu mehr fähig ist. </font><font style="vertical-align: inherit;">Irgendwelche Ideen, wo Sie sich sonst noch versuchen können? </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Vielen Dank für Ihre Aufmerksamkeit!</font></font></div>
      
    </div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de486680/">https://habr.com/ru/post/de486680/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de486670/index.html">Электронный паспорт РФ, 2020-ая часть марлезонского балета</a></li>
<li><a href="../de486672/index.html">OpenVINO Hackathon: Erkennen von Stimme und Emotion auf dem Raspberry Pi</a></li>
<li><a href="../de486674/index.html">Die Verdauung von frischen Materialien aus der Welt des Frontends für die letzte Woche Nr. 400 (27. Januar - 2. Februar 2020)</a></li>
<li><a href="../de486676/index.html">Unvermeidlichkeit der FPGA-Penetration in Rechenzentren</a></li>
<li><a href="../de486678/index.html">Quarz in ASP.NET Core</a></li>
<li><a href="../de486682/index.html">Docker Compose: Mit Makefile vereinfachen</a></li>
<li><a href="../de486684/index.html">Meine Antwort an diejenigen, die glauben, dass der Wert von TDD übertrieben ist</a></li>
<li><a href="../de486686/index.html">Informationen zum Implementieren einer Deep-Learning-Bibliothek in Python</a></li>
<li><a href="../de486688/index.html">Node.js, Tor, Puppenspieler und Cheerio: anonymes Web-Scraping</a></li>
<li><a href="../de486690/index.html">5 Tipps zum Schreiben von Qualitätspfeilfunktionen</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>