<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§µüèΩ üóúÔ∏è üê¥ Instead of 100 application launches - one autotest, or how to save a QA engineer 20 years of life üó∫Ô∏è üõèÔ∏è üïù</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello everyone, my name is Evgeny Demidenko. For the past few years, I have been developing an automated game testing system at Pixonic. Today I wante...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Instead of 100 application launches - one autotest, or how to save a QA engineer 20 years of life</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/pixonic/blog/503704/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hello everyone, my name is Evgeny Demidenko. For the past few years, I have been developing an automated game testing system at Pixonic. Today I wanted to share our experience in developing, supporting and using such a system on the War Robots project. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
To begin with, we‚Äôll figure out what we are automating with this system. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
First of all, these are regression UI-testing, testing of core-gameplay and automation of benchmarks. All three systems as a whole make it possible to reduce the burden on the QA department before releases, be more confident in large-scale and deep refactoring, and constantly maintain an overall assessment of the performance of the application, as well as its individual parts. Another point I want to note is automation of the routine, for example - testing of any hypotheses.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/x-/p8/li/x-p8lisenhooudwb_qghpki8oew.gif" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
I will give a few numbers. Now more than 600 UI tests and about 100 core tests have been written for War Robots. On this project alone, we made about a million launches of our test scripts, each of which took about 80 seconds. If we checked these scenarios manually, we would have spent at least five minutes each. In addition, we launched more than 700 thousand benchmarks. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Of the platforms, we use Android and iOS - only 12 devices in the park. Two programmers are involved in system development and support, and one QA engineer is writing and analyzing tests.</font></font><br>
<br>
<a name="habracut"></a><br>
<img src="https://habrastorage.org/webt/uf/yw/0w/ufyw0wcb6rt75dkydnmcqfg2zly.jpeg"><br>
<br>
<img src="https://habrastorage.org/webt/yt/x1/9g/ytx19guugufvzvniufmworpua6e.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As for the software stack, we use NUnit in our database, but not for unit tests, but for integration and system tests. </font><font style="vertical-align: inherit;">For core gameplay and build verification tests, we use the built-in solution from Unity - Unity Test Tools. </font><font style="vertical-align: inherit;">For writing and analyzing reports after these tests, the Allure Test Report from Yandex is used, as well as </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">TeamCity</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - as a system of continuous integration for the application build, server deployment and run tests. </font><font style="vertical-align: inherit;">We use the Nexus Repository and the PostgreSQL database to store our artifacts.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/u6/3q/mo/u63qmozjx0tp1pcl6ym9ykqnrwa.jpeg"><br>
<br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">How do you create, analyze and run tests</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Suppose we want to write a simple test that in the game settings window will check the icon for turning the sound on and off. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
So, we wrote a test and committed it to a specific branch in our test repository. We chose the tests that we want to run, chose a build to run, or maybe a specific commit, on which the build will be assembled. Now run the test, wait a while and get the result. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/yh/lh/zh/yhlhzh-ks_cwipqs8r0rz6sgcdk.jpeg"><br>
<br>
<img src="https://habrastorage.org/webt/58/g2/vc/58g2vcabu81mbxprwuoalnf9p34.jpeg"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In this case, 575 tests were launched, of which 97% were successful. It took us about three hours to complete all the tests. For comparison, the same tests, if done manually, would take at least 50 hours of continuous operation. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
So what happened to those 3% of the tests that failed? </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We open a specific test and see a message that an error occurred while matching screenshots.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/sk/ae/tl/skaetlvcp3-ivmp5aqkzhmolbac.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Then we open the screenshot, which at that moment was on the device, and we see that zones that do not correspond to the original are marked with red pixels. </font><font style="vertical-align: inherit;">For comparison, we give him. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/am/h2/vn/amh2vnpzgkyhqtkrfsnski3y1sy.png"><br>
<br>
<img src="https://habrastorage.org/webt/s3/7o/zl/s37ozlopkihi4vpcpwrbslws05i.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Naturally, after this, the QA engineer must either make a bug that the behavior of the build does not correspond to the game design document, or update the original screenshots, because the game design document has changed, and now these elements will not be in the game.</font></font><br>
<br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">It looks cool. </font><font style="vertical-align: inherit;">Why is all this necessary?</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Some time ago, on the War Robots project, we needed to do one small refactoring. </font><font style="vertical-align: inherit;">It consisted of rewriting some pieces of code for firing weapons - in particular, machine guns.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
During testing, we found one interesting nuance: the rate of machine guns depended directly on FPS. Such a bug would be unrealistic to detect during manual testing: firstly, due to the features of the network calculation of damage on the project, and secondly, due to the fact that the War Robots application is quite well optimized and at that time it ran on all devices with approximately the same FPS - 30 frames / s. Of course, there were small deviations, but they were not enough to notice an increase in damage from firing weapons during manual testing. Then we asked ourselves: how many such bugs do we still have and how many can appear during refactoring?</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Since we didn‚Äôt want to reduce the number of tests, but rather increase it, since we had planned major updates and an increase in the number of content, we did not want to grow horizontally and increase the number of QA department employees. </font><font style="vertical-align: inherit;">Instead, we planned vertical growth with a reduction in the routine of current employees and making their lives easier during the integration testing of new content.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/sh/yf/_w/shyf_wvrx43ugfrojh0q57lozoa.jpeg"><br>
<br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">What tools do we use</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
When we first started to automate tests, we first of all drew attention to the Unity Integration Test Tools, which was built-in at that time in Unity at that time. We wrote several UIs and core tests on it, finished the refactoring that we started earlier, and were satisfied with it, because the solution already worked, which means that our assumptions were correct, and we had to move on. The only negative of this solution, but very significant for us, was that tests could not be run on mobile devices.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Thus, we came up with the idea of ‚Äã‚Äãusing the Appium framework. This is a fork of another well-known testing framework - Selenium. It, in turn, is perhaps the most famous framework for testing web applications, the main concept of which is to work with UI elements, get their coordinates and organize input into these UI elements. Appium adopted this concept and, in addition to the existing web drivers in Selenium, also added iOS and Android drivers: they use native test frameworks for each of these platforms. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Since there are no native UI elements in Unity, and there is only one UI element in which the picture is rendered, I had to add in addition to the Appium UnityDriver, which allows you to work with the scene hierarchy, get scene objects, and much more.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
At that moment, a QA engineer had already appeared on the project, things started to flow, the number of test scenarios began to grow significantly, which we gradually automated. We started to launch them on devices, and in general, our work already looked about the way we wanted. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In the future, in addition to UI tests, more core tests and other tools based on our system began to appear, as a result of which we ran into performance and quality of work on various devices, added support for several more devices, parallel tests, and also abandoned Appium in benefit of your own framework. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/lv/mn/sv/lvmnsvmwhtnp3ewe2mpofdem-e4.jpeg"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The only problem that remained with us - and still is - was the UI hierarchy. Because if a hierarchy changes in a scene due to UI refactoring or work on the scene, this needs to be supported in tests.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
After the next innovations and revisions, the architecture of the entire system began to look as follows. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/yu/52/ho/yu52hohx3nmookj9vajusrl7uyg.jpeg"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We take the War Robots build, take our tests, which are in a separate repository, add some parameters to run there that allow us to configure the launch of tests in each case, and send it all to the TeamCity agent on a remote PC. The TeamCity agent launches our tests, passes them the ‚ÄúRobots‚Äù build and launch parameters, after which the tests begin to work and ‚Äúcommunicate‚Äù independently with the devices that are connected to the TeamCity agent by wire: put builds on them, run them, execute certain scripts, delete builds, restart the application and so on.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Since the tests and the application itself run on physically different devices - on a mobile phone and Mac mini - we needed to implement communication between our framework, War Robots API and Unity API. </font><font style="vertical-align: inherit;">We have added a small UDP server to the application, which receives commands from the framework and communicates with the application API and Unity through handlers. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/sq/rt/qf/sqrtqfu4f-swgarbf1zgsysocea.jpeg"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The main task of our framework is to organize the work of tests: the correct preparation, completion and management of devices. </font><font style="vertical-align: inherit;">In particular, parallelization to speed up work, the right choice of devices and screenshots, communication with the build. </font><font style="vertical-align: inherit;">After completing the tests, our framework should save all the generated artifacts and generate a report.</font></font><br>
<br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tips for choosing devices</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Separately, I want to pay attention to the choice of devices for testing. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Considerable attention should be paid to hubs. If you want to run benchmarks on your devices - especially if they are Android devices - they will run out of power. Hubs must provide the necessary power for the devices used. There is another very subtle feature: some hubs have active power, and this power turns off after power surges, after which it is turned on only by physical pressing a button. We have such hubs, and this is very inconvenient.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
If you want to run regression UI testing and test logic on devices, do not take different devices. </font><font style="vertical-align: inherit;">Take the same devices - better the most productive ones you can afford, because in this way you will save time on device brakes, the convenience of working with them, and the behavior of the application on all devices will be the same. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A separate issue is the use of cloud farms. </font><font style="vertical-align: inherit;">We do not use them yet, although we have conducted research on them: what they are, how much they cost and how to run our tests on them - but so far we have enough of our in-house device park to cover our requests.</font></font><br>
<br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Test Reporting</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
After the tests are completed, we generate an allure report, which includes all the artifacts that were created during our test. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The main ‚Äúworkhorse" for analyzing what happened and identifying the causes of the crash during the test is the logs. First of all, we collect them from our framework, which tells us about the state of the script and what happened in this script. We divide the logs into the system (more detailed) and the log for QA (more compact and convenient for analysis). We also collect system logs from devices (for example, logcat) and logs from a Unity application.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
During the fall of the tests, we also take a screenshot to understand what was happening on the devices at the time of the fall, record a video to understand what happened before the crash, and try to collect information about the status of the device, such as our server pings and ifconfig information, to the maximum to understand if the device has an IP. </font><font style="vertical-align: inherit;">You will be surprised, however, if you launch the application manually 50 times, everything will be fine with it, but if you run it 50 thousand times in automatic mode, you will find that the Internet on the device may be lost, and it will not be clear during the test, whether there was a connection before and after the fall. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We also collect a list of processes, battery power, temperature, and generally everything that we can reach.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/du/vk/o3/duvko3ve8ep_2fsl2wcm5m129wk.jpeg"><br>
<br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">What are good screenshots and videos</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Some time ago, our QA engineer suggested, in addition to taking screenshots in the fall, in certain places in the tests to compare these screenshots with the templates that are in our repository. Thus, he proposed saving time on the number of test runs and reducing the size of the code base. That is, with one test we could check the logic and the visual part. From the point of view of the concept of unit testing, this is not very correct, because in one test we should not test several hypotheses. But this is a deliberate step: we know how to analyze all this correctly, so we ventured to add similar functionality.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
First of all, we thought about adding libraries to match screenshots, but we realized that using images with different resolutions is not very reliable, so we stopped on devices with the same resolution and just compare the images with a certain threshold pixel by pixel.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/mk/er/0l/mker0lc9w32qaca97jmykzsdae8.jpeg"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A very interesting effect of using screenshot matching is that if some process is difficult to automate, we will automate it as far as it turns out, and then we simply look at the screenshots manually. This is exactly what we did with test localization. We received a request to test the localization of our applications, so we started looking at libraries that allow text recognition - but we realized that it was rather unreliable, and as a result we wrote several scripts that ‚Äúwalk‚Äù across different screens and cause different pop- ups, and at this moment screenshots are created. Before starting such a script, we change the locale on the device, run the script, take screenshots, change the locale again and run the script again. Thus, all tests are run at night,so that in the morning the QA engineer can look at 500 screenshots and immediately analyze if there are problems with localization somewhere. Yes, screenshots still need to be watched, but this is much faster than manually passing through all the screens on the device.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Sometimes screenshots and logs are not enough: something strange starts to happen on the devices, but since they are located remotely, you cannot go and evaluate what happened there. </font><font style="vertical-align: inherit;">Moreover, it is sometimes unclear what happened literally a few moments before the test fell. </font><font style="vertical-align: inherit;">Therefore, we added a video recording from the device, which starts with the start of the test and is saved only in the event of a fall. </font><font style="vertical-align: inherit;">With the help of such videos it is very convenient to track application crashes and freezes.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/tv/yi/jk/tvyijk8fgyl3_sgkyykx4tdjc2g.jpeg"><br>
<br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">What else can our system do?</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Some time ago, from the QA testing department, we received a request to develop a tool for collecting metrics during manual playtests. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
What is it for? </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
This is necessary so that QA engineers, after a manual playtest, can additionally analyze the behavior of FPS and memory consumption in the application, simultaneously looking at screenshots and videos reflecting what was happening on this device.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The system developed by us worked as follows. The QA engineer launched War Robots on the device, turned on the record of the playbench session ‚Äî our analogue of the gamebench ‚Äî played the playtest, then clicked ‚Äúend the playbench session‚Äù, the generated report was saved in the repository, after which the engineer with the data for this playtest could reach his working machines and see the report: what were the drawdowns on FPS, what memory consumption, what was happening on the device.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We also automated the launch of benchmarks on the War Robots project, essentially just wrapping the existing benchmarks in an automatic launch. The result of benchmarks is usually one digit. In our case, this is usually the average FPS per benchmark. In addition to the automatic launch, we decided to add another playbench session and thus received not just a specific figure, how the benchmark worked, but also information thanks to which we can analyze what happened to the benchmark at that moment.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We should also mention the pull request test. </font><font style="vertical-align: inherit;">This time it was more to help the client development team, rather than QA-engineers. </font><font style="vertical-align: inherit;">We run the so-called build verification test for each pull request. </font><font style="vertical-align: inherit;">You can run them both on devices and in the Unity editor to speed up the work of checking logic. </font><font style="vertical-align: inherit;">We also run a set of core-tests in separate branches, where a kind of redesign of some elements or code refactoring takes place.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/6x/xj/t8/6xxjt8np3nxlfacdadgagz-qxry.jpeg"><br>
<br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">And other useful features</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In the end, I want to dwell on some interesting cases that we have met in the past few years. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
One of the most interesting cases that appeared recently with us is benchmarks during fights with bots.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
For the new project, Pixonic Dino Squad developed a system in which the QA engineer could play a play test with bots, so as not to wait for his colleagues, but to test some hypothesis. Our QA engineer, in turn, asked to add the ability not only to play with bots, but also so that bots can play with each other. Thus, we simply launch the application, and at this moment the bot starts playing with other bots. At the same time, all the interaction is network, with real servers, just instead of players playing a computer. All this is wrapped in benchmarks and a playbench session with triggers for night starts. Thus, at night we start several battles between bots and bots, at this time FPS and memory consumption are written, screenshots are taken and videos are recorded. In the morning, a QA engineer comes and can see,what playtests were held and what happened on them.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Also worth checking out is texture leakage. </font><font style="vertical-align: inherit;">This is a kind of sub-analysis of memory usage - but here we mainly check the use, for example, of garage textures in battle. </font><font style="vertical-align: inherit;">Accordingly, in the battle there should not be atlases that are used in the garage, and when we exit the battle, the textures that were used in the battle should not remain in memory. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
An interesting side effect of our system is that almost from the very beginning of its use, we tracked the loading time of the application. </font><font style="vertical-align: inherit;">In the case of War Robots, this time is not strong, but it is constantly growing, because new content is being added, and the quality of this content is improving - but we can keep this parameter under control and always be aware of its size.</font></font><br>
<br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Instead of a conclusion</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In the end, I would like to draw attention to the problems that we have that we know about and that we would like to solve in the first place. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/s3/8o/ow/s38oowcmbceqawngd_udbsvt3eq.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The first and most painful is UI changes. Since we are working with a black box, we do not embed anything in the War Robots application except our server - that is, we test everything in the same way as a QA engineer would test. But somehow we need to access the elements in the scene. And we find them along the absolute path. Thus, when something changes on the stage, especially at a high level of hierarchy, we have to support these changes in a large number of tests. Unfortunately, we can‚Äôt do anything about it right now. Of course, there are some solutions, but they bring their additional problems.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The second big problem is infrastructure. As I said, if you run your application 50 times with your hands, you will not notice most of the problems that will come to light if you run your application 50 thousand times. Those problems that can be easily solved in manual mode - for example, reinstalling builds or restarting the Internet - will turn out to be a real pain in automation, because all these problems must be correctly handled, an error message displayed, and provided that they can occur at all. In particular, we need to determine why the tests fell: because of a malfunctioning logic or some kind of infrastructure problem, or for any other reason. There are a lot of problems with low-end devices: they do not have builds, the Internet falls off, devices freeze, crash, do not turn on, are quickly discharged, and so on.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
I would also like to interact with native UIs, but so far we do not have such an opportunity. </font><font style="vertical-align: inherit;">We know how to do this, but the presence of other requests for functionality does not allow us to get to this. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
And personally, my desire is to comply with the standards that exist in the industry, but this is also in the plans for the future, maybe even this year.</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../en503690/index.html">Kubernetes best practices. Kubernetes Zero Downtime Cluster Upgrade</a></li>
<li><a href="../en503696/index.html">Gears of war: when mechanical analog computers ruled the sea</a></li>
<li><a href="../en503698/index.html">Remote accounting - as a business benefit</a></li>
<li><a href="../en503700/index.html">Creating React Forms in 2020</a></li>
<li><a href="../en503702/index.html">Using Raw Data in Google Analytics in practice</a></li>
<li><a href="../en503706/index.html">Why are these annoying weather rules needed to launch and land rockets?</a></li>
<li><a href="../en503710/index.html">PyDoma [PyData Moscow Meetup # 12]: May 26, 2020</a></li>
<li><a href="../en503712/index.html">Status and performance of Kubernetes persistent storage solutions</a></li>
<li><a href="../en503716/index.html">Do you need clouds in space</a></li>
<li><a href="../en503718/index.html">Robot lawyer, garbage collection application and much more: online tube hackathon with online pizza conquers the crisis</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>