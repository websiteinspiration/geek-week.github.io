<!doctype html>
<html class="no-js" lang="fr">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🐛 👨‍🎨 👩🏼‍🎤 Random Forest, la méthode des principaux composants et l'optimisation des hyperparamètres: un exemple de résolution du problème de classification en Python 🚻 Ⓜ️ ⬅️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Les spécialistes du traitement et de l'analyse des données disposent de nombreux outils pour créer des modèles de classification. L'une des méthodes l...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Random Forest, la méthode des principaux composants et l'optimisation des hyperparamètres: un exemple de résolution du problème de classification en Python</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ruvds/blog/488342/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Les spécialistes du traitement et de l'analyse des données disposent de nombreux outils pour créer des modèles de classification. </font><font style="vertical-align: inherit;">L'une des méthodes les plus populaires et les plus fiables pour développer de tels modèles est d'utiliser l'algorithme Random Forest (RF). </font><font style="vertical-align: inherit;">Afin d'essayer d'améliorer les performances d'un modèle construit à l'aide de l'algorithme </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">RF</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , vous pouvez utiliser l'optimisation de l'hyperparamètre du modèle ( </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hyperparameter Tuning</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , HT). </font><font style="vertical-align: inherit;">
De plus, il existe une approche répandue selon laquelle les données, avant d'être transférées au modèle, sont traitées à l'aide de l' </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;">analyse en composantes principales</font></a></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><img src="https://habrastorage.org/webt/tt/m5/h7/ttm5h7jbbx2u2wuc1var1azxwew.jpeg"></a><br>
<br><font style="vertical-align: inherit;"></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, PCA). </font><font style="vertical-align: inherit;">Mais vaut-il la peine d'utiliser? </font><font style="vertical-align: inherit;">Le but principal de l'algorithme RF n'est-il pas d'aider l'analyste à interpréter l'importance des traits?</font></font><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Oui, l'utilisation de l'algorithme PCA peut conduire à une légère complication de l'interprétation de chaque «caractéristique» dans l'analyse de «l'importance des caractéristiques» du modèle RF. Cependant, l'algorithme PCA réduit la dimension de l'espace des fonctionnalités, ce qui peut entraîner une diminution du nombre de fonctionnalités qui doivent être traitées par le modèle RF. Veuillez noter que le volume des calculs est l'un des principaux inconvénients de l'algorithme de forêt aléatoire (c'est-à-dire qu'il peut prendre beaucoup de temps pour terminer le modèle). L'application de l'algorithme PCA peut être une partie très importante de la modélisation, en particulier dans les cas où ils fonctionnent avec des centaines voire des milliers de fonctionnalités. Par conséquent, si la chose la plus importante est de simplement créer le modèle le plus efficace, et en même temps vous pouvez sacrifier la précision de la détermination de l'importance des attributs, alors l'ACP peut valoir la peine d'être essayée.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Maintenant au point. </font><font style="vertical-align: inherit;">Nous travaillerons avec un </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ensemble de données sur le cancer du sein</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;">Scikit-learn «cancer du sein»</font></a><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Nous allons créer trois modèles et comparer leur efficacité. </font><font style="vertical-align: inherit;">À savoir, nous parlons des modèles suivants:</font></font><br>
<br>
<ol>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Le modèle de base basé sur l'algorithme RF (nous abrégerons ce modèle RF).</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Le même modèle que le n ° 1, mais dans lequel une réduction de la dimension de l'espace caractéristique est appliquée en utilisant la méthode des composants principaux (RF + PCA).</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Le même modèle que le n ° 2, mais construit en utilisant l'optimisation hyperparamétrique (RF + PCA + HT).</font></font></li>
</ol><br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1. Importer des données</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pour commencer, chargez les données et créez une trame de données Pandas. </font><font style="vertical-align: inherit;">Étant donné que nous utilisons un ensemble de données «jouet» pré-autorisé de Scikit-learn, nous pouvons déjà commencer le processus de modélisation. </font><font style="vertical-align: inherit;">Mais même lorsque vous utilisez de telles données, il est recommandé de toujours commencer à travailler en effectuant une analyse préliminaire des données à l'aide des commandes suivantes appliquées au bloc de données ( </font></font><code>df</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">):</font></font><br>
<br>
<ul>
<li><code>df.head()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - pour jeter un oeil à la nouvelle trame de données et voir si elle ressemble à celle attendue.</font></font></li>
<li><code>df.info()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- pour découvrir les caractéristiques des types de données et le contenu des colonnes. </font><font style="vertical-align: inherit;">Il peut être nécessaire d'effectuer une conversion de type de données avant de continuer.</font></font></li>
<li><code>df.isna()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- pour vous assurer qu'il n'y a pas de valeurs dans les données </font></font><code>NaN</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Les valeurs correspondantes, le cas échéant, peuvent devoir être traitées d'une manière ou d'une autre, ou, si nécessaire, il peut être nécessaire de supprimer des lignes entières du bloc de données.</font></font></li>
<li><code>df.describe()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - connaître les valeurs minimales, maximales et moyennes des indicateurs dans les colonnes, connaître les indicateurs du carré moyen et de l'écart probable dans les colonnes.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dans notre jeu de données, une colonne </font></font><code>cancer</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(cancer) est la variable cible dont nous voulons prédire la valeur à l'aide du modèle. </font></font><code>0</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">signifie «pas de maladie». </font></font><code>1</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- "la présence de la maladie".</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_breast_cancer<font></font>
columns = [<span class="hljs-string">'mean radius'</span>, <span class="hljs-string">'mean texture'</span>, <span class="hljs-string">'mean perimeter'</span>, <span class="hljs-string">'mean area'</span>, <span class="hljs-string">'mean smoothness'</span>, <span class="hljs-string">'mean compactness'</span>, <span class="hljs-string">'mean concavity'</span>, <span class="hljs-string">'mean concave points'</span>, <span class="hljs-string">'mean symmetry'</span>, <span class="hljs-string">'mean fractal dimension'</span>, <span class="hljs-string">'radius error'</span>, <span class="hljs-string">'texture error'</span>, <span class="hljs-string">'perimeter error'</span>, <span class="hljs-string">'area error'</span>, <span class="hljs-string">'smoothness error'</span>, <span class="hljs-string">'compactness error'</span>, <span class="hljs-string">'concavity error'</span>, <span class="hljs-string">'concave points error'</span>, <span class="hljs-string">'symmetry error'</span>, <span class="hljs-string">'fractal dimension error'</span>, <span class="hljs-string">'worst radius'</span>, <span class="hljs-string">'worst texture'</span>, <span class="hljs-string">'worst perimeter'</span>, <span class="hljs-string">'worst area'</span>, <span class="hljs-string">'worst smoothness'</span>, <span class="hljs-string">'worst compactness'</span>, <span class="hljs-string">'worst concavity'</span>, <span class="hljs-string">'worst concave points'</span>, <span class="hljs-string">'worst symmetry'</span>, <span class="hljs-string">'worst fractal dimension'</span>]<font></font>
dataset = load_breast_cancer()<font></font>
data = pd.DataFrame(dataset[<span class="hljs-string">'data'</span>], columns=columns)<font></font>
data[<span class="hljs-string">'cancer'</span>] = dataset[<span class="hljs-string">'target'</span>]<font></font>
display(data.head())<font></font>
display(data.info())<font></font>
display(data.isna().sum())<font></font>
display(data.describe())</code></pre><br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/bce/fb8/b60/bcefb8b60462b7658b40e1e56f7744ab.png"></div><br>
<i><font color="#999999">      .       .  , cancer,   ,    . 0  « ». 1 — « »</font></i><br>
 <br>
<h2><font color="#3AC1EF">2.        </font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Maintenant, divisez les données à l'aide de la fonction Scikit-learn </font></font><code>train_test_split</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Nous voulons donner au modèle autant de données d'entraînement que possible. Cependant, nous devons disposer de suffisamment de données pour tester le modèle. En général, nous pouvons dire que, à mesure que le nombre de lignes de l'ensemble de données augmente, la quantité de données pouvant être considérée comme éducative augmente également. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Par exemple, s'il y a des millions de lignes, vous pouvez diviser l'ensemble en mettant en surbrillance 90% des lignes pour les données d'apprentissage et 10% pour les données de test. Mais l'ensemble de données de test ne contient que 569 lignes. Et ce n'est pas tant pour la formation et le test du modèle. Par conséquent, afin d'être juste par rapport aux données de formation et de vérification, nous diviserons l'ensemble en deux parties égales - 50% - données de formation et 50% - données de vérification. Nous installons</font></font><code>stratify=y</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> pour garantir que les ensembles de données de formation et de test ont le même rapport de 0 et 1 que l'ensemble de données d'origine.</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<font></font>
X = data.drop(<span class="hljs-string">'cancer'</span>, axis=<span class="hljs-number">1</span>)&nbsp;&nbsp;<font></font>
y = data[<span class="hljs-string">'cancer'</span>]&nbsp;<font></font>
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="hljs-number">0.50</span>, random_state = <span class="hljs-number">2020</span>, stratify=y)</code></pre><br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3. Mise à l'échelle des données</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Avant de procéder à la modélisation, vous devez «centrer» et «normaliser» les données en les </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">mettant</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> à l' </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;">échelle</font></a><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">La mise à l'échelle est effectuée du fait que différentes quantités sont exprimées dans différentes unités. </font><font style="vertical-align: inherit;">Cette procédure vous permet d'organiser un «combat loyal» entre les signes pour déterminer leur importance. </font><font style="vertical-align: inherit;">De plus, nous convertissons </font></font><code>y_train</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">le type de données Pandas </font></font><code>Series</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">en tableau NumPy afin que plus tard le modèle puisse fonctionner avec les cibles correspondantes.</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler<font></font>
ss = StandardScaler()<font></font>
X_train_scaled = ss.fit_transform(X_train)<font></font>
X_test_scaled = ss.transform(X_test)<font></font>
y_train = np.array(y_train)</code></pre><br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">4. Formation du modèle de base (modèle n ° 1, RF)</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Créez maintenant le modèle numéro 1. </font><font style="vertical-align: inherit;">Dans ce document, nous rappelons que seul l'algorithme Random Forest est utilisé. </font><font style="vertical-align: inherit;">Il utilise toutes les fonctionnalités et est configuré en utilisant les valeurs par défaut (des détails sur ces paramètres peuvent être trouvés dans la documentation de </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">sklearn.ensemble.RandomForestClassifier</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ). </font><font style="vertical-align: inherit;">Initialisez le modèle. </font><font style="vertical-align: inherit;">Après cela, nous la formerons sur les données à l'échelle. </font><font style="vertical-align: inherit;">La précision du modèle peut être mesurée sur les données d'entraînement:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier
<span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> recall_score<font></font>
rfc = RandomForestClassifier()<font></font>
rfc.fit(X_train_scaled, y_train)<font></font>
display(rfc.score(X_train_scaled, y_train))<font></font>
<span class="hljs-comment"># 1.0</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Si nous voulons savoir quelles caractéristiques sont les plus importantes pour le modèle RF dans la prédiction du cancer du sein, nous pouvons visualiser et quantifier les indicateurs de l'importance des signes en faisant référence à l'attribut </font></font><code>feature_importances_</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">:</font></font><br>
<br>
<pre><code class="python hljs">feats = {}
<span class="hljs-keyword">for</span> feature, importance <span class="hljs-keyword">in</span> zip(data.columns, rfc_1.feature_importances_):<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;feats[feature] = importance<font></font>
importances = pd.DataFrame.from_dict(feats, orient=<span class="hljs-string">'index'</span>).rename(columns={<span class="hljs-number">0</span>: <span class="hljs-string">'Gini-Importance'</span>})<font></font>
importances = importances.sort_values(by=<span class="hljs-string">'Gini-Importance'</span>, ascending=<span class="hljs-literal">False</span>)<font></font>
importances = importances.reset_index()<font></font>
importances = importances.rename(columns={<span class="hljs-string">'index'</span>: <span class="hljs-string">'Features'</span>})<font></font>
sns.set(font_scale = <span class="hljs-number">5</span>)<font></font>
sns.set(style=<span class="hljs-string">"whitegrid"</span>, color_codes=<span class="hljs-literal">True</span>, font_scale = <span class="hljs-number">1.7</span>)<font></font>
fig, ax = plt.subplots()<font></font>
fig.set_size_inches(<span class="hljs-number">30</span>,<span class="hljs-number">15</span>)<font></font>
sns.barplot(x=importances[<span class="hljs-string">'Gini-Importance'</span>], y=importances[<span class="hljs-string">'Features'</span>], data=importances, color=<span class="hljs-string">'skyblue'</span>)<font></font>
plt.xlabel(<span class="hljs-string">'Importance'</span>, fontsize=<span class="hljs-number">25</span>, weight = <span class="hljs-string">'bold'</span>)<font></font>
plt.ylabel(<span class="hljs-string">'Features'</span>, fontsize=<span class="hljs-number">25</span>, weight = <span class="hljs-string">'bold'</span>)<font></font>
plt.title(<span class="hljs-string">'Feature Importance'</span>, fontsize=<span class="hljs-number">25</span>, weight = <span class="hljs-string">'bold'</span>)<font></font>
display(plt.show())<font></font>
display(importances)</code></pre><br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a02/a8f/cd2/a02a8fcd28f87af338f364a70faeca3e.jpg"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Visualisation de «l'importance» des signes</font></font></font></i><br>
 <br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/830/176/e1f/830176e1fc9ce63bfedf2d727619253b.png"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Indicateurs de signification</font></font></font></i><br>
<br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">5. La méthode des principaux composants</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Voyons maintenant comment nous pouvons améliorer le modèle RF de base. En utilisant la technique de réduction de la dimension de l'espace d'entité, il est possible de présenter l'ensemble de données initial à travers moins de variables et en même temps de réduire la quantité de ressources informatiques nécessaires pour assurer le fonctionnement du modèle. À l'aide de l'ACP, vous pouvez étudier la variance cumulée de l'échantillon de ces caractéristiques afin de comprendre quelles caractéristiques expliquent la plupart de la variance dans les données. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nous initialisons l'objet PCA ( </font></font><code>pca_test</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">), indiquant le nombre de composants (fonctionnalités) à prendre en compte. Nous avons fixé cet indicateur à 30 afin de voir la variance expliquée de tous les composants générés avant de décider du nombre de composants dont nous avons besoin. Ensuite, nous transférons aux </font></font><code>pca_test</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">données mises </font><font style="vertical-align: inherit;">à l' </font><font style="vertical-align: inherit;">échelle</font></font><code>X_train</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">en utilisant la méthode </font></font><code>pca_test.fit()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Après cela, nous visualisons les données.</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns
<span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> PCA<font></font>
pca_test = PCA(n_components=<span class="hljs-number">30</span>)<font></font>
pca_test.fit(X_train_scaled)<font></font>
sns.set(style=<span class="hljs-string">'whitegrid'</span>)<font></font>
plt.plot(np.cumsum(pca_test.explained_variance_ratio_))<font></font>
plt.xlabel(<span class="hljs-string">'number of components'</span>)<font></font>
plt.ylabel(<span class="hljs-string">'cumulative explained variance'</span>)<font></font>
plt.axvline(linewidth=<span class="hljs-number">4</span>, color=<span class="hljs-string">'r'</span>, linestyle = <span class="hljs-string">'--'</span>, x=<span class="hljs-number">10</span>, ymin=<span class="hljs-number">0</span>, ymax=<span class="hljs-number">1</span>)<font></font>
display(plt.show())<font></font>
evr = pca_test.explained_variance_ratio_<font></font>
cvr = np.cumsum(pca_test.explained_variance_ratio_)<font></font>
pca_df = pd.DataFrame()<font></font>
pca_df[<span class="hljs-string">'Cumulative Variance Ratio'</span>] = cvr<font></font>
pca_df[<span class="hljs-string">'Explained Variance Ratio'</span>] = evr<font></font>
display(pca_df.head(<span class="hljs-number">10</span>))</code></pre><br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6eb/65f/acc/6eb65facc6c8b05f1e910d3b2b676d5e.jpg"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Après que le nombre de composants utilisés dépasse 10, l'augmentation de leur nombre n'augmente pas considérablement la variance expliquée</font></font></font></i><br>
 <br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f12/e3c/915/f12e3c915d761e1d4623051dac74cd8d.png"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ce bloc de données contient des indicateurs tels que le ratio de variance cumulée (taille cumulée de la variance expliquée des données) et le ratio de variance expliquée (contribution de chaque composant au volume total de la variance expliquée)</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
.Si vous regardez le bloc de données ci-dessus, il s'avère que l'utilisation de l'ACP pour passer de 30 variables à 10 aux composants permet d'expliquer 95% de la dispersion des données. Les 20 autres composantes représentent moins de 5% de la variance, ce qui signifie que nous pouvons les refuser. Suivant cette logique, nous utilisons le PCA pour réduire le nombre de composants de 30 à 10 pour</font></font><code>X_train</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">et</font></font><code>X_test</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Nous écrivons ces ensembles de données de «dimension réduite» créés artificiellement dans</font></font><code>X_train_scaled_pca</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">et en</font><font style="vertical-align: inherit;">dedans</font></font><code>X_test_scaled_pca</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font><br>
<br>
<pre><code class="python hljs">pca = PCA(n_components=<span class="hljs-number">10</span>)<font></font>
pca.fit(X_train_scaled)<font></font>
X_train_scaled_pca = pca.transform(X_train_scaled)<font></font>
X_test_scaled_pca = pca.transform(X_test_scaled)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Chaque composante est une combinaison linéaire de variables sources avec des «pondérations» correspondantes. </font><font style="vertical-align: inherit;">Nous pouvons voir ces «poids» pour chaque composant en créant un bloc de données.</font></font><br>
<br>
<pre><code class="python hljs">pca_dims = []
<span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>, len(pca_df)):<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;pca_dims.append(<span class="hljs-string">'PCA Component {}'</span>.format(x))<font></font>
pca_test_df = pd.DataFrame(pca_test.components_, columns=columns, index=pca_dims)<font></font>
pca_test_df.head(<span class="hljs-number">10</span>).T</code></pre><br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/086/a28/ae4/086a28ae45e9048811cf813d4868902e.png"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cadre de données d'informations sur les composants</font></font></font></i><br>
 <br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">6. Formation au modèle RF de base après application de la méthode des composants principaux aux données (modèle n ° 2, RF + PCA)</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Maintenant , </font><font style="vertical-align: inherit;">nous pouvons passer à une </font><font style="vertical-align: inherit;">autre base de </font><font style="vertical-align: inherit;">données modèle RF </font></font><code>X_train_scaled_pca</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">et </font></font><code>y_train</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">et peut savoir de </font><font style="vertical-align: inherit;">savoir s'il y a une amélioration de la précision des prévisions émises par le modèle.</font></font><br>
<br>
<pre><code class="python hljs">rfc = RandomForestClassifier()<font></font>
rfc.fit(X_train_scaled_pca, y_train)<font></font>
display(rfc.score(X_train_scaled_pca, y_train))<font></font>
<span class="hljs-comment"># 1.0</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Les modèles se comparent ci-dessous.</font></font><br>
<br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">7. Optimisation des hyperparamètres. </font><font style="vertical-align: inherit;">Round 1: RandomizedSearchCV</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Après avoir traité les données en utilisant la méthode du composant principal, vous pouvez essayer d'utiliser l' </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">optimisation des hyperparamètres du</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> modèle afin d'améliorer la qualité des prédictions produites par le modèle RF. Les hyperparamètres peuvent être considérés comme quelque chose comme des «paramètres» du modèle. Les paramètres parfaits pour un ensemble de données ne fonctionneront pas pour un autre - c'est pourquoi vous devez les optimiser. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Vous pouvez commencer avec l'algorithme RandomizedSearchCV, qui vous permet d'explorer assez approximativement un large éventail de valeurs. Des descriptions de tous les hyperparamètres pour les modèles RF peuvent être trouvées </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ici</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Au cours du travail, nous générons une entité </font></font><code>param_dist</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">qui contient, pour chaque hyperparamètre, une plage de valeurs à tester. Ensuite, nous initialisons l'objet.</font></font><code>rs</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">en utilisant la fonction </font></font><code>RandomizedSearchCV()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, en lui passant le modèle RF </font></font><code>param_dist</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, le nombre d'itérations et le nombre de </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">validations croisées</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> qui doivent être effectuées. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
L'hyperparamètre </font></font><code>verbose</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">permet de contrôler la quantité d'informations affichées par le modèle lors de son fonctionnement (comme la sortie d'informations lors de la formation du modèle). </font><font style="vertical-align: inherit;">L'hyperparamètre </font></font><code>n_jobs</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">vous permet de spécifier le nombre de cœurs de processeur que vous devez utiliser pour garantir le fonctionnement du modèle. </font><font style="vertical-align: inherit;">La définition </font></font><code>n_jobs</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">d'une valeur </font></font><code>-1</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">entraînera un modèle plus rapide, car cela utilisera tous les cœurs de processeur. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nous serons engagés dans la sélection des hyperparamètres suivants:</font></font><br>
<br>
<ul>
<li><code>n_estimators</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - le nombre d '"arbres" dans la "forêt aléatoire".</font></font></li>
<li><code>max_features</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - le nombre d'entités pour sélectionner le fractionnement.</font></font></li>
<li><code>max_depth</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - profondeur maximale des arbres.</font></font></li>
<li><code>min_samples_split</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - le nombre minimum d'objets nécessaires pour qu'un nœud d'arbre se divise.</font></font></li>
<li><code>min_samples_leaf</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - le nombre minimum d'objets dans les feuilles.</font></font></li>
<li><code>bootstrap</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - utiliser pour construire des arbres de sous-échantillons avec retour.</font></font></li>
</ul><br>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> RandomizedSearchCV<font></font>
n_estimators = [int(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> np.linspace(start = <span class="hljs-number">100</span>, stop = <span class="hljs-number">1000</span>, num = <span class="hljs-number">10</span>)]<font></font>
max_features = [<span class="hljs-string">'log2'</span>, <span class="hljs-string">'sqrt'</span>]<font></font>
max_depth = [int(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> np.linspace(start = <span class="hljs-number">1</span>, stop = <span class="hljs-number">15</span>, num = <span class="hljs-number">15</span>)]<font></font>
min_samples_split = [int(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> np.linspace(start = <span class="hljs-number">2</span>, stop = <span class="hljs-number">50</span>, num = <span class="hljs-number">10</span>)]<font></font>
min_samples_leaf = [int(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> np.linspace(start = <span class="hljs-number">2</span>, stop = <span class="hljs-number">50</span>, num = <span class="hljs-number">10</span>)]<font></font>
bootstrap = [<span class="hljs-literal">True</span>, <span class="hljs-literal">False</span>]<font></font>
param_dist = {<span class="hljs-string">'n_estimators'</span>: n_estimators,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'max_features'</span>: max_features,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'max_depth'</span>: max_depth,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'min_samples_split'</span>: min_samples_split,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'min_samples_leaf'</span>: min_samples_leaf,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'bootstrap'</span>: bootstrap}<font></font>
rs = RandomizedSearchCV(rfc_2,&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;param_dist,&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;n_iter = <span class="hljs-number">100</span>,&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;cv = <span class="hljs-number">3</span>,&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;verbose = <span class="hljs-number">1</span>,&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;n_jobs=<span class="hljs-number">-1</span>,&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;random_state=<span class="hljs-number">0</span>)<font></font>
rs.fit(X_train_scaled_pca, y_train)<font></font>
rs.best_params_<font></font>
<span class="hljs-comment"># {'n_estimators': 700,</span>
<span class="hljs-comment"># 'min_samples_split': 2,</span>
<span class="hljs-comment"># 'min_samples_leaf': 2,</span>
<span class="hljs-comment"># 'max_features': 'log2',</span>
<span class="hljs-comment"># 'max_depth': 11,</span>
<span class="hljs-comment"># 'bootstrap': True}</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Avec les valeurs des paramètres </font></font><code>n_iter = 100</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">et </font></font><code>cv = 3</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, nous avons créé 300 modèles RF, en choisissant aléatoirement des combinaisons des hyper </font><font style="vertical-align: inherit;">paramètres </font><font style="vertical-align: inherit;">présentés ci-dessus. </font><font style="vertical-align: inherit;">Nous pouvons nous référer à l'attribut </font></font><code>best_params_ </code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">pour plus d'informations sur un ensemble de paramètres qui vous permet de créer le meilleur modèle. </font><font style="vertical-align: inherit;">Mais à ce stade, cela ne nous donne peut-être pas les données les plus intéressantes sur les plages de paramètres qui méritent d'être explorées lors de la prochaine phase d'optimisation. </font><font style="vertical-align: inherit;">Afin de découvrir dans quelle plage de valeurs il vaut la peine de poursuivre la recherche, nous pouvons facilement obtenir une trame de données contenant les résultats de l'algorithme RandomizedSearchCV.</font></font><br>
<br>
<pre><code class="python hljs">rs_df = pd.DataFrame(rs.cv_results_).sort_values(<span class="hljs-string">'rank_test_score'</span>).reset_index(drop=<span class="hljs-literal">True</span>)<font></font>
rs_df = rs_df.drop([<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'mean_fit_time'</span>,&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'std_fit_time'</span>,&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'mean_score_time'</span>,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'std_score_time'</span>,&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'params'</span>,&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'split0_test_score'</span>,&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'split1_test_score'</span>,&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'split2_test_score'</span>,&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'std_test_score'</span>],<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;axis=<span class="hljs-number">1</span>)<font></font>
rs_df.head(<span class="hljs-number">10</span>)</code></pre><br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/617/b8c/20b/617b8c20b787acc3c76c23d9235b4b5a.png"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Résultats de l'algorithme RandomizedSearchCV</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Nous allons maintenant créer des graphiques à barres sur lesquels, sur l'axe X, sont les valeurs d'hyperparamètre, et sur l'axe Y sont les valeurs moyennes affichées par les modèles. </font><font style="vertical-align: inherit;">Cela permettra de comprendre quelles valeurs des hyperparamètres affichent en moyenne leurs meilleures performances.</font></font><br>
<br>
<pre><code class="python hljs">fig, axs = plt.subplots(ncols=<span class="hljs-number">3</span>, nrows=<span class="hljs-number">2</span>)<font></font>
sns.set(style=<span class="hljs-string">"whitegrid"</span>, color_codes=<span class="hljs-literal">True</span>, font_scale = <span class="hljs-number">2</span>)<font></font>
fig.set_size_inches(<span class="hljs-number">30</span>,<span class="hljs-number">25</span>)<font></font>
sns.barplot(x=<span class="hljs-string">'param_n_estimators'</span>, y=<span class="hljs-string">'mean_test_score'</span>, data=rs_df, ax=axs[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>], color=<span class="hljs-string">'lightgrey'</span>)<font></font>
axs[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>].set_ylim([<span class="hljs-number">.83</span>,<span class="hljs-number">.93</span>])axs[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>].set_title(label = <span class="hljs-string">'n_estimators'</span>, size=<span class="hljs-number">30</span>, weight=<span class="hljs-string">'bold'</span>)<font></font>
sns.barplot(x=<span class="hljs-string">'param_min_samples_split'</span>, y=<span class="hljs-string">'mean_test_score'</span>, data=rs_df, ax=axs[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>], color=<span class="hljs-string">'coral'</span>)<font></font>
axs[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>].set_ylim([<span class="hljs-number">.85</span>,<span class="hljs-number">.93</span>])axs[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>].set_title(label = <span class="hljs-string">'min_samples_split'</span>, size=<span class="hljs-number">30</span>, weight=<span class="hljs-string">'bold'</span>)<font></font>
sns.barplot(x=<span class="hljs-string">'param_min_samples_leaf'</span>, y=<span class="hljs-string">'mean_test_score'</span>, data=rs_df, ax=axs[<span class="hljs-number">0</span>,<span class="hljs-number">2</span>], color=<span class="hljs-string">'lightgreen'</span>)<font></font>
axs[<span class="hljs-number">0</span>,<span class="hljs-number">2</span>].set_ylim([<span class="hljs-number">.80</span>,<span class="hljs-number">.93</span>])axs[<span class="hljs-number">0</span>,<span class="hljs-number">2</span>].set_title(label = <span class="hljs-string">'min_samples_leaf'</span>, size=<span class="hljs-number">30</span>, weight=<span class="hljs-string">'bold'</span>)<font></font>
sns.barplot(x=<span class="hljs-string">'param_max_features'</span>, y=<span class="hljs-string">'mean_test_score'</span>, data=rs_df, ax=axs[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>], color=<span class="hljs-string">'wheat'</span>)<font></font>
axs[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>].set_ylim([<span class="hljs-number">.88</span>,<span class="hljs-number">.92</span>])axs[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>].set_title(label = <span class="hljs-string">'max_features'</span>, size=<span class="hljs-number">30</span>, weight=<span class="hljs-string">'bold'</span>)<font></font>
sns.barplot(x=<span class="hljs-string">'param_max_depth'</span>, y=<span class="hljs-string">'mean_test_score'</span>, data=rs_df, ax=axs[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>], color=<span class="hljs-string">'lightpink'</span>)<font></font>
axs[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>].set_ylim([<span class="hljs-number">.80</span>,<span class="hljs-number">.93</span>])axs[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>].set_title(label = <span class="hljs-string">'max_depth'</span>, size=<span class="hljs-number">30</span>, weight=<span class="hljs-string">'bold'</span>)<font></font>
sns.barplot(x=<span class="hljs-string">'param_bootstrap'</span>,y=<span class="hljs-string">'mean_test_score'</span>, data=rs_df, ax=axs[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>], color=<span class="hljs-string">'skyblue'</span>)<font></font>
axs[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>].set_ylim([<span class="hljs-number">.88</span>,<span class="hljs-number">.92</span>])<font></font>
axs[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>].set_title(label = <span class="hljs-string">'bootstrap'</span>, size=<span class="hljs-number">30</span>, weight=<span class="hljs-string">'bold'</span>)<font></font>
plt.show()</code></pre><br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/418/311/ba6/418311ba6c38bfcebbf152af810d6b58.jpg"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Analyse des valeurs des hyperparamètres</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Si nous analysons les graphiques ci-dessus, nous pouvons remarquer des choses intéressantes qui parlent de la façon dont, en moyenne, chaque valeur d'un hyperparamètre affecte le modèle.</font></font><br>
<br>
<ul>
<li><code>n_estimators</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">: des valeurs de 300, 500, 700, apparemment, montrent les meilleurs résultats moyens.</font></font></li>
<li><code>min_samples_split</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">: Les petites valeurs comme 2 et 7 semblent donner les meilleurs résultats. </font><font style="vertical-align: inherit;">La valeur 23 semble également bonne. Vous pouvez examiner plusieurs valeurs de cet hyperparamètre au-delà de 2, ainsi que plusieurs valeurs d'environ 23.</font></font></li>
<li><code>min_samples_leaf</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">: On a le sentiment que les petites valeurs de cet hyperparamètre donnent de meilleurs résultats. </font><font style="vertical-align: inherit;">Cela signifie que nous pouvons expérimenter des valeurs comprises entre 2 et 7.</font></font></li>
<li><code>max_features</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">: l'option </font></font><code>sqrt</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">donne le résultat moyen le plus élevé.</font></font></li>
<li><code>max_depth</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">: il n'y a pas de relation claire entre la valeur de l'hyperparamètre et le résultat du modèle, mais on a le sentiment que les valeurs 2, 3, 7, 11, 15 semblent bonnes.</font></font></li>
<li><code>bootstrap</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">: la valeur </font></font><code>False</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">affiche le meilleur résultat moyen.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Maintenant, en utilisant ces résultats, nous pouvons passer au deuxième cycle d'optimisation des hyperparamètres. </font><font style="vertical-align: inherit;">Cela réduira la gamme de valeurs qui nous intéresse.</font></font><br>
<br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">8. Optimisation des hyperparamètres. </font><font style="vertical-align: inherit;">Tour 2: GridSearchCV (préparation finale des paramètres pour le modèle n ° 3, RF + PCA + HT)</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Après avoir appliqué l'algorithme RandomizedSearchCV, nous utiliserons l'algorithme GridSearchCV pour effectuer une recherche plus précise de la meilleure combinaison d'hyperparamètres. Les mêmes hyperparamètres sont étudiés ici, mais maintenant nous appliquons une recherche plus «approfondie» pour leur meilleure combinaison. En utilisant l'algorithme GridSearchCV, chaque combinaison d'hyperparamètres est examinée. Cela nécessite beaucoup plus de ressources de calcul que l'utilisation de l'algorithme RandomizedSearchCV lorsque nous définissons indépendamment le nombre d'itérations de recherche. Par exemple, la recherche de 10 valeurs pour chacun des 6 hyperparamètres avec validation croisée en 3 blocs nécessitera 10 × 3 ou 3 000 000 de sessions de formation sur modèle. C'est pourquoi nous utilisons l'algorithme GridSearchCV après, après avoir appliqué RandomizedSearchCV, nous avons rétréci les plages de valeurs des paramètres étudiés.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Donc, en utilisant ce que nous avons découvert à l'aide de RandomizedSearchCV, nous examinons les valeurs des hyperparamètres qui se sont le mieux montrées:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> GridSearchCV<font></font>
n_estimators = [<span class="hljs-number">300</span>,<span class="hljs-number">500</span>,<span class="hljs-number">700</span>]<font></font>
max_features = [<span class="hljs-string">'sqrt'</span>]<font></font>
max_depth = [<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">7</span>,<span class="hljs-number">11</span>,<span class="hljs-number">15</span>]<font></font>
min_samples_split = [<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">22</span>,<span class="hljs-number">23</span>,<span class="hljs-number">24</span>]<font></font>
min_samples_leaf = [<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>,<span class="hljs-number">7</span>]<font></font>
bootstrap = [<span class="hljs-literal">False</span>]<font></font>
param_grid = {<span class="hljs-string">'n_estimators'</span>: n_estimators,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'max_features'</span>: max_features,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'max_depth'</span>: max_depth,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'min_samples_split'</span>: min_samples_split,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'min_samples_leaf'</span>: min_samples_leaf,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'bootstrap'</span>: bootstrap}<font></font>
gs = GridSearchCV(rfc_2, param_grid, cv = <span class="hljs-number">3</span>, verbose = <span class="hljs-number">1</span>, n_jobs=<span class="hljs-number">-1</span>)<font></font>
gs.fit(X_train_scaled_pca, y_train)<font></font>
rfc_3 = gs.best_estimator_<font></font>
gs.best_params_<font></font>
<span class="hljs-comment"># {'bootstrap': False,</span>
<span class="hljs-comment"># 'max_depth': 7,</span>
<span class="hljs-comment"># 'max_features': 'sqrt',</span>
<span class="hljs-comment"># 'min_samples_leaf': 3,</span>
<span class="hljs-comment"># 'min_samples_split': 2,</span>
<span class="hljs-comment"># 'n_estimators': 500}</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ici, nous appliquons la validation croisée à 3 blocs pour 540 (3 x 1 x 5 x 6 x 6 x 1) sessions de formation de modèle, ce qui donne 1620 sessions de formation de modèle. </font><font style="vertical-align: inherit;">Et maintenant, après avoir utilisé RandomizedSearchCV et GridSearchCV, nous pouvons nous tourner vers l'attribut </font></font><code>best_params_</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">pour savoir quelles valeurs d'hyperparamètres permettent au modèle de mieux fonctionner avec l'ensemble de données à l'étude (ces valeurs peuvent être vues au bas du bloc de code précédent) . </font><font style="vertical-align: inherit;">Ces paramètres sont utilisés pour créer le modèle numéro 3.</font></font><br>
<br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">9. Évaluation de la qualité des modèles sur les données de vérification</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Vous pouvez maintenant évaluer les modèles créés sur les données de vérification. </font><font style="vertical-align: inherit;">À savoir, nous parlons de ces trois modèles décrits au tout début du matériau. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Découvrez ces modèles:</font></font><br>
<br>
<pre><code class="python hljs">y_pred = rfc.predict(X_test_scaled)<font></font>
y_pred_pca = rfc.predict(X_test_scaled_pca)<font></font>
y_pred_gs = gs.best_estimator_.predict(X_test_scaled_pca)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Créez des matrices d'erreurs pour les modèles et découvrez dans quelle mesure chacune d'elles est capable de prédire le cancer du sein:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> confusion_matrix<font></font>
conf_matrix_baseline = pd.DataFrame(confusion_matrix(y_test, y_pred), index = [<span class="hljs-string">'actual 0'</span>, <span class="hljs-string">'actual 1'</span>], columns = [<span class="hljs-string">'predicted 0'</span>, <span class="hljs-string">'predicted 1'</span>])<font></font>
conf_matrix_baseline_pca = pd.DataFrame(confusion_matrix(y_test, y_pred_pca), index = [<span class="hljs-string">'actual 0'</span>, <span class="hljs-string">'actual 1'</span>], columns = [<span class="hljs-string">'predicted 0'</span>, <span class="hljs-string">'predicted 1'</span>])<font></font>
conf_matrix_tuned_pca = pd.DataFrame(confusion_matrix(y_test, y_pred_gs), index = [<span class="hljs-string">'actual 0'</span>, <span class="hljs-string">'actual 1'</span>], columns = [<span class="hljs-string">'predicted 0'</span>, <span class="hljs-string">'predicted 1'</span>])<font></font>
display(conf_matrix_baseline)<font></font>
display(<span class="hljs-string">'Baseline Random Forest recall score'</span>, recall_score(y_test, y_pred))<font></font>
display(conf_matrix_baseline_pca)<font></font>
display(<span class="hljs-string">'Baseline Random Forest With PCA recall score'</span>, recall_score(y_test, y_pred_pca))<font></font>
display(conf_matrix_tuned_pca)<font></font>
display(<span class="hljs-string">'Hyperparameter Tuned Random Forest With PCA Reduced Dimensionality recall score'</span>, recall_score(y_test, y_pred_gs))</code></pre><br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f48/a9e/92f/f48a9e92fd5fdca613d6073e00bae2c6.png"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Résultats des travaux des trois modèles</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Ici, la «complétude» métrique (rappel) est évaluée. </font><font style="vertical-align: inherit;">Le fait est que nous avons affaire à un diagnostic de cancer. </font><font style="vertical-align: inherit;">Par conséquent, nous sommes extrêmement intéressés à minimiser les prévisions fausses négatives émises par les modèles. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Compte tenu de cela, nous pouvons conclure que le modèle RF de base a donné les meilleurs résultats. </font><font style="vertical-align: inherit;">Son taux d'exhaustivité était de 94,97%. </font><font style="vertical-align: inherit;">Dans l'ensemble de données de test, il y avait un record de 179 patients atteints de cancer. </font><font style="vertical-align: inherit;">Le modèle en a trouvé 170.</font></font><br>
<br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sommaire</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Cette étude fournit une observation importante. </font><font style="vertical-align: inherit;">Parfois, le modèle RF, qui utilise la méthode des composants principaux et l'optimisation à grande échelle des hyperparamètres, peut ne pas fonctionner aussi bien que le modèle le plus ordinaire avec des paramètres standard. </font><font style="vertical-align: inherit;">Mais ce n'est pas une raison pour se limiter aux modèles les plus simples. </font><font style="vertical-align: inherit;">Sans essayer différents modèles, il est impossible de dire lequel affichera le meilleur résultat. </font><font style="vertical-align: inherit;">Et dans le cas des modèles utilisés pour prédire la présence de cancer chez les patients, nous pouvons dire que meilleur est le modèle - plus de vies peuvent être sauvées. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Chers lecteurs! </font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Quelles tâches résolvez-vous en utilisant des méthodes d'apprentissage automatique?</font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><img src="https://habrastorage.org/files/1ba/550/d25/1ba550d25e8846ce8805de564da6aa63.png"></a></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr488330/index.html">Anglais avec George Karlin: nous analysons l'ingénieux stand-up sur les unités phraséologiques</a></li>
<li><a href="../fr488332/index.html">Zéro, un, deux, Freddy viendra te chercher</a></li>
<li><a href="../fr488336/index.html">Conseils d'utilisation de l'algorithme de réduction de la fonction d'onde</a></li>
<li><a href="../fr488338/index.html">Stages Google: Zurich, Londres et Silicon Valley</a></li>
<li><a href="../fr488340/index.html">Profession: Développeur Backend</a></li>
<li><a href="../fr488346/index.html">Installation de ou-tools avec SCIP et GLPK dans un environnement virtuel Python 3.7 sous Linux</a></li>
<li><a href="../fr488348/index.html">Webinaire «Les dix principaux défis agiles et les moyens de les surmonter en une heure» 17 février à 20 h, heure de Moscou</a></li>
<li><a href="../fr488352/index.html">Comparaison des coûts VDI: sur site et cloud public</a></li>
<li><a href="../fr488356/index.html">Formation à l'Université technique maritime de Saint-Pétersbourg pour les produits Dassault Systèmes</a></li>
<li><a href="../fr488360/index.html">Mythes du Big Data et culture numérique</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>