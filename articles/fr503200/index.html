<!doctype html>
<html class="no-js" lang="fr">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👩🏽‍⚕️ 🕺🏻 👩‍👩‍👦 YOLOv4 - le réseau neuronal en temps réel le plus précis du jeu de données Microsoft COCO ✌🏾 🧕🏾 🙏🏼</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Darknet YOLOv4 est plus rapide / plus précis que Google TensorFlow EfficientDet et FaceBook Pytorch / Detectron RetinaNet / MaskRCNN. 
 
 Le même arti...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>YOLOv4 - le réseau neuronal en temps réel le plus précis du jeu de données Microsoft COCO</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/503200/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Darknet YOLOv4 est plus rapide / plus précis que Google TensorFlow EfficientDet et FaceBook Pytorch / Detectron RetinaNet / MaskRCNN. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Le même article sur medium</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">medium </font></font></a><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Code</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/AlexeyAB/darknet </font></font></a><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Article</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">arxiv.org/abs/2004.10934</font></font></a><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/3h/nc/sr/3hncsroz9wt8u3ycqskubgu1xk8.gif"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nous montrerons quelques nuances de comparaison et d'utilisation de réseaux de neurones pour détecter des objets. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Notre objectif était de développer un algorithme de détection d'objets pour une utilisation dans des produits réels, et pas seulement de faire avancer la science. </font><font style="vertical-align: inherit;">La précision du réseau de neurones YOLOv4 (608x608) est de 43,5% AP / 65,7% AP50 Microsoft-COCO-testdev. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">62 FPS</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - YOLOv4 (608x608 batch = 1) sur Tesla V100 - en utilisant Darknet-framework </font></font><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">400 FPS</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - YOLOv4 (416x416 batch = 4) sur RTX 2080 Ti - en utilisant TensorRT + tkDNN </font></font><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">32 FPS</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - YOLOv4 (416x416 batch = 1) sur Jetson AGX Xavier - en utilisant TensorRT + tkDNN</font></font><br>
<br>
<img src="https://habrastorage.org/webt/p_/ep/cl/p_epcl_aaw_trgeltekatagtqkg.jpeg"> <br>
<a name="habracut"></a><br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/1_SiUOYUoOI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tout d'abord, quelques liens utiles.</font></font></h3><br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Vous pouvez lire une description détaillée des fonctionnalités utilisées dans YOLOv4 dans cet article: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">medium.com/@jonathan_hui/yolov4-c9901eaa8e61</font></font></a></li>
<li>  YOLOv4: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://lutzroeder.github.io/netron/%3Furl%3D" rel="nofollow">lutzroeder.github.io/netron/?url=https%3A%2F%2Fraw.githubusercontent.com%2FAlexeyAB%2Fdarknet%2Fmaster%2Fcfg%2Fyolov4.cfg</a></li>
<li>     YOLOv4  GPU   Google-cloud  Jupyter Notebook –      ,   - «Open in Playground»,         [ ] –    ,  ,  ,    5    : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow">colab.research.google.com/drive/12QusaaRj_lUwCGDvQNfICpa7kA7_a2dE</a>      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow">www.youtube.com/watch?v=mKAEGSxwOAY</a></li>
<li>  Darknet   : <br>
 — <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow">  Darknet YOLOv4</a><br>
 — <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow">  Darknet YOLOv4</a><br>
 </li>
</ul><br>
<h3>   </h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Notre réseau de neurones YOLOv4 et notre propre framework Darknet DL (C / C ++ / CUDA) sont meilleurs en vitesse FPS et en précision AP50: 95 et AP50 sur les jeux de données Microsoft COCO que les frameworks DL et réseaux de neurones: Google TensorFlow EfficientDet, FaceBook Detectron RetinaNet / MaskRCNN, PyTorch Yolov3-ASFF, et bien d'autres ... YOLOv4 atteint une précision de 43,5% AP / 65,7% AP50 sur le test Microsoft COCO à une vitesse de 62 FPS TitanV ou 34 FPS RTX 2070. Contrairement à d'autres détecteurs modernes, YOLOv4 peut entraîner n'importe qui avec celui qui a la carte graphique de jeu nVidia avec 8-16 Go de VRAM. Désormais, non seulement les grandes entreprises peuvent former un réseau de neurones sur des centaines de GPU / TPU pour utiliser de grandes tailles de mini-lots afin d'obtenir une plus grande précision, nous renvoyons donc le contrôle de l'intelligence artificielle aux utilisateurs ordinaires, car pour YOLOv4, un grand mini-lot n'est pas nécessaire,peut être limité à une taille de 2 à 8.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
YOLOV4 est optimal pour une utilisation en temps réel, car le réseau se trouve </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">sur la courbe d'optimalité de Pareto</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> dans le </font><b><font style="vertical-align: inherit;">graphique</font></b><font style="vertical-align: inherit;"> AP (précision) / FPS (vitesse). </font></font><br>
<br>
<img src="https://habrastorage.org/webt/2k/77/as/2k77aszzprngk0qmtistcehkz8c.png"><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Graphiques de précision (AP) et de vitesse (FPS) de nombreux réseaux de neurones pour détecter des objets mesurés sur des GPU TitanV / TeslaV100, TitanXP / TeslaP100, TitanX / TeslaM40 pour les deux principaux indicateurs de précision AP50: 95 et AP50</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Pour une comparaison équitable, nous prenons des données d'articles et comparer uniquement sur le GPU avec la même architecture. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La plupart des tâches pratiques ont les exigences minimales nécessaires pour les détecteurs - ce sont la précision et la vitesse minimales acceptables. Habituellement, la vitesse minimale autorisée de 30 FPS (images par seconde) et plus pour les systèmes en temps réel. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Comme le montrent les graphiques, dans les systèmes en temps réel avec FPS 30 ou plus:</font></font><br>
<br>
<ul>
<li> YOLOv4-608   RTX 2070  <b>450$</b> (34 FPS)   <b>43.5% AP / 65.7% AP50</b></li>
<li> EfficientDet-D2   TitanV  <b>2250$</b> (42 FPS)   <b>43.0% AP / 62.3% AP50</b></li>
<li> EfficientDet-D0   RTX 2070  <b>450$</b> (34 FPS)   <b>33.8% AP / 52.2% AP50</b></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ceux. YOLOv4 nécessite </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">un équipement 5 fois moins cher</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> et plus précis que EfficientDet-D2 (Google-TensorFlow). Vous pouvez utiliser EfficientDet-D0 (Google-TensorFlow) alors le coût de l'équipement sera le même, mais la précision sera de 10% AP inférieure. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
De plus, certains systèmes industriels ont des limites sur la dissipation thermique ou sur l'utilisation d'un système de refroidissement passif - dans ce cas, vous ne pouvez pas utiliser TitanV même avec de l'argent. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Lorsque vous utilisez YOLOv4 (416x416) sur un GPU RTX 2080 Ti en utilisant TensorRT + tkDNN, nous atteignons une vitesse 2x plus rapide, et lorsque vous utilisez batch = 4, il est 3x-4x fois plus rapide - pour une comparaison honnête, nous ne présentons pas ces résultats dans un article sur arxiv. org:</font></font><br>
<img src="https://habrastorage.org/webt/ci/j7/uq/cij7uqas0ypsjcpsfkhvdxuyxzs.png"><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Réseau de neurones YOLOv4 (416x416) FP16 (Tensor Cores) batch = </font><font style="vertical-align: inherit;">1 atteint à 32 FPS calculator nVidia Jetson AGX Xavier en utilisant les bibliothèques + tkDNN TensorRT: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/ceccocats/tkDNN la</font></font></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
vitesse légèrement plus lente donne la bibliothèque OpenCV-dnn compilée avec CUDA: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">docs .opencv.org / master / da / d9d / tutorial_dnn_yolo.html</font></font></a><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Parfois, la vitesse (FPS) de certains réseaux de neurones dans les articles est indiquée lors de l'utilisation d'une taille de lot élevée ou lors de tests avec un logiciel spécialisé (TensorRT), qui optimise le réseau et affiche une valeur FPS accrue. La comparaison de certains réseaux sur TRT avec d'autres réseaux sans TRT n'est pas juste. L'utilisation d'une taille de lot élevée augmente le FPS, mais augmente également la latence (plutôt que de la diminuer) par rapport à batch = 1. Si le réseau avec batch = 1 affiche 40 FPS, et avec batch = 32, il affiche 60 FPS, le délai sera de 25 ms pour batch = 1 et de ~ 500 ms pour batch = 32, car seuls ~ 2 paquets (32 images chacun) seront traités par seconde, c'est pourquoi l'utilisation de batch = 32 n'est pas acceptable dans de nombreux systèmes industriels. Par conséquent, nous avons comparé les résultats sur les graphiques uniquement avec batch = 1 et sans utiliser TensorRT.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Tout processus peut être contrôlé par des personnes ou par des ordinateurs. Lorsqu'un système informatique agit avec un retard important en raison de la faible vitesse et fait trop d'erreurs, vous ne pouvez pas vous voir confier un contrôle complet sur les actions, dans ce cas, la personne contrôle le processus, et le système informatique ne donne que des indices - il s'agit d'un système de recommandation - la personne travaille et le système uniquement indique où des erreurs ont été commises. Lorsque le système fonctionne rapidement et avec une grande précision, un tel système peut contrôler le processus de manière indépendante et une personne ne s'en occupe que. Par conséquent, la précision et la vitesse du système sont toujours importantes. S'il vous semble que 120 FPS pour YOLOv4 416x416 est trop pour votre tâche, et qu'il vaut mieux prendre l'algorithme plus lentement et plus précisément, alors très probablement dans les tâches réelles, vous utiliserez quelque chose de plus faible que le Tesla V100 250 Watt,par exemple, RTX 2060 / Jetson-Xavier 30-80 Watt, dans ce cas, vous obtiendrez 30 FPS sur YOLOv4 416x416 et d'autres réseaux de neurones à 1-15 FPS ou ne démarrerez pas du tout.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Caractéristiques de la formation de divers réseaux de neurones</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Vous devez former EfficientDet avec un mini-lot = taille 128 sur plusieurs GPU Tesla V100 32 Go, tandis que YOLOv4 a été formé sur un seul GPU Tesla V100 32 Go avec mini-lot = 8 = lot / subdivisions, et peut être formé sur un jeu régulier carte graphique 8-16 Go GPU-VRAM. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La nuance suivante est la difficulté de former un réseau de neurones pour détecter ses propres objets. </font><font style="vertical-align: inherit;">Peu importe combien de temps vous entraînez d'autres réseaux sur le même GPU 1080 Ti, vous n'obtiendrez pas la précision indiquée dans le graphique ci-dessus. </font><font style="vertical-align: inherit;">La plupart des réseaux (EfficientDet, ASFF, ...) doivent être formés sur 4 à 128 GPU (avec une grande taille de mini-lot utilisant syncBN) et il est nécessaire de s'entraîner à chaque fois pour chaque résolution de réseau, sans remplir les deux conditions, il est impossible d'atteindre la précision AP ou AP50 déclarée par eux.</font></font><br>
 <br>
<img src="https://habrastorage.org/webt/p4/sx/p3/p4sxp3ewxd9owskis23n6dyrv58.jpeg"><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Vous pouvez voir la dépendance de la précision de détection des objets sur la taille du mini-lot dans d'autres détecteurs, c'est-à-dire </font><font style="vertical-align: inherit;">en utilisant 128 cartes vidéo au lieu de 8 cartes vidéo et la vitesse d'apprentissage est 10 fois plus élevée et la précision finale est 1,5 AP plus élevée - MegDet: un grand détecteur d'objets en mini-lots </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">arxiv.org/abs/1711.07240</font></font></a></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Yolo ASFF: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">arxiv.org/abs/1911.09516</font></font></a> <br>
<blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Après [43], nous introduisons un sac de trucs dans le processus de formation, tels que l'algorithme de mixage [12], le calendrier des taux d'apprentissage cosinus [26] et la technique de normalisation des lots synchronisés [30].</font></font></blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
EfficientDet: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">arxiv.org/abs/1911.09070</font></font></a> <br>
<blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La normalisation par lots synchronisée est ajoutée après chaque convolution avec la décroissance de la norme de lot 0,99 et epsilon 1e-3. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Chaque modèle est formé de 300 époques avec une taille totale de lot de 128 sur 32 cœurs TPUv3.</font></font></blockquote><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow">cloud.google.com/tpu/docs/types-zones#europe</a><br>
<blockquote>v3-32 TPU type (v3) – 32 TPU v3 cores – 512 GiB Total TPU memory</blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Vous devez utiliser 512 Go de TPU / GPU-RAM pour entraîner le modèle EfficientDet avec une normalisation de lot synchronisée à lot = 128, tandis que le mini-lot = 8 et seulement 32 Go de RAM-GPU ont été utilisés pour entraîner YOLOv4. Malgré cela, YOLOv4 est plus rapide / plus précis que les réseaux publics, bien qu'il ne soit formé qu'une seule fois avec une résolution de 512x512 par GPU (Tesla V100 32 Go / 16 Go). Dans le même temps, l'utilisation de la plus petite taille de mini-lot et du GPU-VRAM ne conduit pas à une perte de précision aussi dramatique que dans d'autres réseaux de neurones: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/xi/ol/rs/xiolrsvx4vzpjvahb6kvambdvgq.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Source: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">arxiv.org/abs/2004.10934</font></font></a></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Vous pouvez donc former l'intelligence artificielle localement sur votre PC, au lieu de télécharger Ensemble de données vers le cloud - cela garantit la protection de vos données personnelles et rend la formation en intelligence artificielle accessible à tous.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Il suffit de former notre réseau une fois avec une résolution de réseau 512x512, puis il peut être utilisé avec différentes résolutions de réseau dans la plage: [416x416 - 512x512 - 608x608]. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La plupart des autres modèles doivent être formés à chaque fois séparément pour chaque résolution de réseau, pour cette raison, la formation prend beaucoup plus de temps.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Caractéristiques de mesure de la précision des algorithmes de détection d'objets</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Vous pouvez toujours trouver une image sur laquelle un algorithme fonctionnera mal, et un autre algorithme fonctionnera bien, et vice versa. Par conséquent, pour tester les algorithmes de détection, un large ensemble de ~ 20 000 images et 80 types d'objets différents est utilisé - MSCOCO test-dev dataset. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pour que l'algorithme n'essaie pas de se souvenir simplement du hachage de chaque image et des coordonnées sur celle-ci (sur-ajustement), la précision de la détection d'objet est toujours vérifiée sur les images et les étiquettes que l'algorithme n'a pas vues pendant la formation - cela garantit que l'algorithme peut détecter des objets sur les images / vidéos qu'il jamais vu.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pour que personne ne puisse se tromper dans le calcul de la précision, dans le domaine public, il n'y a que des images de test-dev sur lesquelles vous détectez et envoyez les résultats au serveur d'évaluation CodaLab, sur lequel le programme lui-même compare vos résultats avec des annotations de test qui ne sont accessibles à personne. . </font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">L'ensemble de données MSCOCO se compose de 3 parties</font></font></a><br>
<br>
<ol>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tutoriel: 120 000 images et un fichier json avec les coordonnées de chaque objet</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Set de validation: 5000 images et un fichier json avec les coordonnées de chaque objet</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Suite de tests: 41 000 images jpg sans les coordonnées des objets (certaines de ces images sont utilisées pour déterminer la précision des tâches: détection d'objets, segmentation d'instances, points clés, ...)</font></font></li>
</ol><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Caractéristiques du développement de YOLOv4</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Lors du développement de YOLOv4, j'ai dû développer moi-même le réseau de neurones YOLOv4 et le framework Darknet en C / C ++ / CUDA. </font><font style="vertical-align: inherit;">Parce que </font><font style="vertical-align: inherit;">dans Darknet il n'y a pas de différenciation automatique et d'exécution automatique de la règle de chaîne, alors tous les gradients doivent être implémentés manuellement. </font><font style="vertical-align: inherit;">D'un autre côté, nous pouvons déroger au strict respect de la règle de la chaîne, changer la rétropropagation et essayer des choses très simples pour augmenter la stabilité et la précision de l'apprentissage.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Résultats supplémentaires lors de la création de réseaux de neurones</font></font></h3><br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pas toujours le meilleur réseau pour classer les objets sera le meilleur en tant que colonne vertébrale pour le réseau utilisé pour détecter les objets</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">L'utilisation de poids entraînés avec des fonctionnalités qui ont une précision accrue dans la classification peut affecter la précision du détecteur (sur certains réseaux)</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Toutes les fonctionnalités mentionnées dans diverses études n'améliorent pas la précision du réseau.</font></font></li>
<li>                .</li>
<li>      BFLOPS  ,   BFLOPS    </li>
<li>                  ,     receptive field     ,       stride=2 / conv3x3,    weights (filters)      . </li>
</ul><br>
<h3>   YOLOv4</h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La détection d'objets à l'aide de modèles YOLOv4 formés est intégrée à la bibliothèque OpenCV-dnn </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/opencv/opencv/issues/17148</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> afin que vous puissiez utiliser YOLOv4 directement à partir d'OpenCV sans utiliser le framework Darknet. </font><font style="vertical-align: inherit;">La bibliothèque OpenCV prend en charge la mise en œuvre de réseaux de neurones sur le processeur, le GPU (nVidia GPU), le VPU (Intel Myriad X). </font><font style="vertical-align: inherit;">Plus de détails: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">docs.opencv.org/master/da/d9d/tutorial_dnn_yolo.html Framework </font></font></a><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">OpenCV</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (dnn):</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Exemple C ++: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/opencv/opencv/blob/master/samples/dnn/object_detection.cpp</font></font></a></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Exemple Python: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/opencv/opencv/blob/master/samples/dnn/object_detection.py</font></font></a></li>
</ul><br>
<b><font style="vertical-align: inherit;"></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Framework </font><b><font style="vertical-align: inherit;">Darknet</font></b><font style="vertical-align: inherit;"> :</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Instructions d'utilisation de YOLOv4 pour détecter des objets: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/AlexeyAB/darknet#how-to-use-on-the-command-line</font></font></a></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Instructions pour entraîner un réseau de neurones à détecter ses propres objets: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/AlexeyAB/darknet#how-to-train-to-detect-your-custom-objects</font></font></a></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Instructions pour la formation du classificateur CSPDarknet53 sur le jeu de données ILSVRC2012 (ImageNet): </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/AlexeyAB/darknet/wiki/Train-Classifier-on-ImageNet-</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (ILSVRC2012)</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Instructions pour la formation de YOLOv4 sur le jeu de données MS COCO: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/AlexeyAB/darknet/wiki/Train-Detector-on-MS-COCO-</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (trainvalno5k-2014) -dataset</font></font></li>
</ul><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">tkDNN + TensorRT</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - Vitesse maximale de détection d'objet à l'aide de YOLOv4: TensorRT + tkDNN </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/ceccocats/tkDNN</font></font></a><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">400 FPS - YOLOv4 (416x416 batch = 4) sur RTX 2080 Ti</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">32 FPS - YOLOv4 (416x416 batch = 1) sur Jetson AGX Xavier</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
L'utilisation de YOLOv4 peut être étendue pour détecter des boîtes 3D tournées ou des points clés / points de repère faciaux, par exemple: </font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/ouyanghuiyu/darknet_face_with_landmark</font></font></a><br>
<br>
<img src="https://habrastorage.org/webt/z7/vs/dv/z7vsdvhcpfbrgmdv1byhbpzd1cu.jpeg"></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr503182/index.html">"Je suis le premier développeur aveugle de mon entreprise." Partie 1</a></li>
<li><a href="../fr503184/index.html">Nous vous invitons à la réunion en ligne Zabbix</a></li>
<li><a href="../fr503192/index.html">oVirt en 2 heures. Partie 4. Opérations de base</a></li>
<li><a href="../fr503194/index.html">ISA ne pardonne pas les erreurs</a></li>
<li><a href="../fr503196/index.html">450 cours gratuits de la Ivy League</a></li>
<li><a href="../fr503204/index.html">Comment flasher Xiaomi Redmi 4 Prime / Pro / Premium sur Android 10</a></li>
<li><a href="../fr503208/index.html">Quel est le meilleur moment pour investir?</a></li>
<li><a href="../fr503210/index.html">Les sites de phishing peuvent-ils être éradiqués?</a></li>
<li><a href="../fr503212/index.html">30 astuces pour terminer le cours en ligne</a></li>
<li><a href="../fr503214/index.html">Optimisation de la charge sur un projet Highload avec ElasticSearch</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>