<!doctype html>
<html class="no-js" lang="de">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🍟 👩🏿 🤘🏾 Spracherkennung: Ein sehr kurzer Einführungskurs 👩🏼‍🤝‍👨🏽 📊 👫</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Es ist fast unmöglich, dem Laien so einfach wie möglich von der Arbeit der Computer-Spracherkennung zu erzählen und sie in Text umzuwandeln. Keine ein...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Spracherkennung: Ein sehr kurzer Einführungskurs</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/toshibarus/blog/490732/"><img src="https://habrastorage.org/webt/tz/sh/ll/tzshllxzf2iddwai7sredy3edie.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Es ist fast unmöglich, dem Laien so einfach wie möglich von der Arbeit der Computer-Spracherkennung zu erzählen und sie in Text umzuwandeln. </font><font style="vertical-align: inherit;">Keine einzige Geschichte darüber ist vollständig ohne komplexe Formeln und mathematische Begriffe. </font><font style="vertical-align: inherit;">Wir werden versuchen, so klar und leicht wie möglich zu erklären, wie Ihr Smartphone Sprache versteht, wann Autos gelernt haben, eine menschliche Stimme zu erkennen und in welchen unerwarteten Bereichen diese Technologie eingesetzt wird. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Notwendige Warnung: Wenn Sie Entwickler oder insbesondere Mathematiker sind, ist es unwahrscheinlich, dass Sie etwas Neues aus der Post lernen und sich sogar über den unzureichenden wissenschaftlichen Charakter des Materials beschweren. </font><font style="vertical-align: inherit;">Unser Ziel ist es, die nicht eingeweihten Leser auf einfachste Weise mit Sprachtechnologien vertraut zu machen und zu erklären, wie und warum Toshiba die Schaffung ihrer Sprach-KI aufgenommen hat.</font></font><br>
<a name="habracut"></a><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wichtige Meilensteine ​​in der Geschichte der Spracherkennung</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die Geschichte der Erkennung menschlicher Sprache durch elektronische Maschinen begann etwas früher als üblich: In den meisten Fällen ist es üblich, ab 1952 herunterzuzählen, aber tatsächlich war eines der ersten Geräte, das auf Sprachbefehle reagierte, der Televox-Roboter, über </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">den wir bereits geschrieben haben</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Der Herbert Televox-Roboter wurde 1927 in den USA entwickelt und war ein einfaches Gerät, bei dem verschiedene Relais auf Geräusche mit unterschiedlichen Frequenzen reagierten. Der Roboter hatte drei Stimmgabeln, von denen jede für ihren Ton verantwortlich war. Je nachdem, welche Stimmgabel funktionierte, wurde das eine oder andere Relais aktiviert.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/90/pq/4i/90pq4ixpys3c8uevjp-ovvydfny.jpeg" alt="Bild"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tatsächlich befand sich die gesamte „Füllung“ von Televox, einschließlich des Befehlserkennungssystems, auf einem Gestell im Bereich des Körpers des „Roboters“. Es war unmöglich, den Deckel zu schließen, da sonst die Stimmgabeln Geräusche nicht richtig „hören“ konnten. Quelle: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Acme Telepictures / Wikimedia.</font></font></a></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Es war möglich, mit Televox als separate Signale mit einer Pfeife und in kurzen verbalen Hinweisen zu kommunizieren - ihre Stimmgabeln wurden auch in einer Folge von Tönen angeordnet. Der Schöpfer des Roboters Roy Wensley führte für diese Zeit sogar eine fantastische Demonstration durch und sagte den Befehl „Sesam, offen“, durch den Televox das Relais einschaltete, das für das Öffnen der Tür verantwortlich war. Keine digitale Technologie, neuronale Netze, KI und maschinelles Lernen - nur analoge Technologie!</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die nächste Schlüsselerfindung, die den Weg für eine echte Erkennung menschlicher Sprache ebnete, war die Audrey-Maschine, die 1952 in der Bell Labs Innovation Forge entwickelt wurde. Der riesige Audrey verbrauchte viel Strom und hatte die Größe eines guten Schranks, aber seine gesamte Funktionalität bestand darin, gesprochene Zahlen von null bis neun zu erkennen. Nur zehn Worte, ja, aber vergessen wir nicht, dass Audrey eine analoge Maschine war. </font></font><br>
<img src="https://habrastorage.org/webt/vd/1q/eb/vd1qebrer6czotgwty3tdyfp15i.png" alt="Bild"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Leider hat die Geschichte keine öffentlichen Fotografien von Audrey erhalten, es gibt nur ein Konzept. Einfach auf Papier, schwer zu übersetzen - nach den Erinnerungen von Zeitgenossen besetzten Audrey-Komponenten einen ganzen Schrank. Quelle: Bell Labs</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Es funktionierte so: Der Ansager sprach Zahlen in das Mikrofon, wobei zwischen den Wörtern Intervalle von mindestens 350 ms eingehalten wurden. Audrey wandelte die von ihm gehörten Geräusche in elektrische Signale um und verglich sie mit im analogen Speicher aufgezeichneten Samples. Nach den Ergebnissen des Vergleichs hat das Auto die Nummer auf dem Armaturenbrett hervorgehoben. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Es war ein Durchbruch, aber es gab keinen wirklichen Vorteil von Audrey - die Maschine erkannte die Stimme ihres Schöpfers mit einer Genauigkeit von 97%, andere speziell ausgebildete Sprecher erhielten eine Genauigkeit von 70-80%. Fremde, die Audrey zum ersten Mal kontaktierten, egal wie sehr sie es versuchten, sahen ihre Nummer in nur 50% der Fälle auf der Anzeigetafel.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Trotz der revolutionären Ergebnisse seiner Zeit fand Audrey keine praktische Anwendung und konnte sie auch nicht finden. </font><font style="vertical-align: inherit;">Es wurde angenommen, dass das System anstelle von Telefonisten angepasst werden könnte, aber dennoch waren die menschlichen Dienste bequemer, schneller und viel zuverlässiger als Audrey.</font></font><br>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/rQco1sa9AwU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Präsentation ähnlich wie Audrey, nur viel kleinere Maschinen - IBM Shoebox. </font><font style="vertical-align: inherit;">Die Geschwindigkeit des Schuhkartons ist deutlich sichtbar. </font><font style="vertical-align: inherit;">Die Maschine könnte auch einfache mathematische Operationen der Addition und Subtraktion ausführen</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In den frühen 1960er Jahren wurden in Japan, Großbritannien, den USA und sogar der UdSSR Arbeiten zur Erstellung von Spracherkennungsgeräten durchgeführt, bei denen ein sehr wichtiger Algorithmus für die dynamische Transformation der Zeitachse (DTW) erfunden wurde, mit dessen Hilfe ein System mit etwa 200 Wörtern erstellt werden konnte. Alle Entwicklungen waren jedoch ähnlich, und das Erkennungsprinzip wurde zu einem allgemeinen Nachteil: Wörter wurden als integrale Klangfingerabdrücke wahrgenommen und dann anhand der Stichprobenbasis (Wörterbuch) überprüft. Änderungen der Geschwindigkeit, des Timbres und der Klarheit der Aussprache von Wörtern beeinträchtigten die Erkennungsqualität erheblich. Wissenschaftler haben eine neue Aufgabe: der Maschine beizubringen, einzelne Geräusche, Phoneme oder Silben zu hören und daraus Wörter zu machen. Ein solcher Ansatz würde es ermöglichen, den Effekt des Lautsprecherwechsels auszugleichen, wenn der Erkennungspegel je nach Sprecher stark variiert.</font></font><br>
<br>
<i> —     ,           . ,   « »  «»       «».   «»   « »  « »      «»,    —  «».  ,  ,   . </i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
1971 startete die Agentur für fortgeschrittene Forschungsprojekte des Verteidigungsministeriums (DARPA) ein Fünfjahresprogramm mit einem Budget von 15 Millionen US-Dollar, bei dem die Aufgabe darin bestand, ein Anerkennungssystem zu schaffen, das mindestens 1000 Wörter kannte. 1976 führte die Carnegie Mellon University die Harpyie ein, die ein Wörterbuch mit 1011 Wörtern bedienen kann. Harpyie verglich die vollständig gehörten Wörter nicht mit den Samples, sondern teilte sie in Allophone ein (ein Sample des Klangs eines Phonems in Abhängigkeit von den ihn umgebenden Buchstaben). Dies war ein weiterer Erfolg, der bestätigte, dass die Zukunft in der Erkennung einzelner Phoneme und nicht ganzer Wörter liegt. Zu den Nachteilen von Harpyie gehörte jedoch eine äußerst geringe korrekte Erkennung von Allophonen (Aussprachen von Phonemen) - etwa 47%. Bei einem so hohen Fehler stieg der Fehleranteil nach dem Umfang des Wörterbuchs.</font></font><br>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/32KKg3aP3Vw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Beschreibung, wie Harpyie funktioniert. Video des Programms überlebte nicht.</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Die Erfahrung von Harpy hat gezeigt, dass das Erstellen von Wörterbüchern mit ganzheitlichen Klangfingerabdrücken nutzlos ist - es erhöht nur die Erkennungszeit und verringert die Genauigkeit drastisch, sodass Forscher auf der ganzen Welt einen anderen Weg eingeschlagen haben - das Erkennen von Phonemen. Mitte der 1980er Jahre konnte die IBM Tangora-Maschine lernen, die Sprache eines Sprechers mit Akzent, Dialekt und Aussprache zu verstehen. Es war lediglich eine 20-minütige Schulung erforderlich, in der eine Datenbank mit Phonemen und Allophonproben gesammelt wurde. Die Verwendung des </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hidden-Markov-Modells</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> erhöhte auch das IBM Tangora-Vokabular auf beeindruckende 20.000 Wörter - 20-mal mehr als Harpy und ist bereits mit dem Vokabular des Teenagers vergleichbar.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Alle Spracherkennungssysteme der 1950er bis Mitte der 1990er Jahre wussten nicht, wie man die natürliche gesprochene Sprache einer Person liest - sie mussten die Wörter separat aussprechen und zwischen ihnen pausieren. Ein wahrhaft revolutionäres Ereignis war die Einführung des in den 1980er Jahren entwickelten Hidden-Markov-Modells - eines statistischen Modells, das auf der Grundlage der bekannten präzise Annahmen über unbekannte Elemente aufbaute. Einfach ausgedrückt, mit nur wenigen erkannten Phonemen in einem Wort wählt das versteckte Markov-Modell die fehlenden Phoneme sehr genau aus, wodurch die Genauigkeit der Spracherkennung erheblich erhöht wird.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
1996 erschien das erste kommerzielle Programm, das nicht einzelne Wörter, sondern einen kontinuierlichen Fluss natürlicher Sprache unterscheiden konnte - IBM MedSpeak / Radiology. IBM war ein spezialisiertes Produkt, das in der Medizin verwendet wurde, um die Ergebnisse einer Röntgenaufnahme, die von einem Arzt während der Studie durchgeführt wurde, kurz zu beschreiben. Hier reichte die Leistung von Computern schließlich aus, um einzelne Wörter "on the fly" zu erkennen. Außerdem sind die Algorithmen perfekter geworden, und die korrekte Erkennung von Mikropausen zwischen den gesprochenen Wörtern ist aufgetreten.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Der erste universelle Motor zur Erkennung natürlicher Sprache war 1997 das Programm Dragon NaturallySpeaking. Bei der Arbeit mit ihr musste der Ansager (d. H. Der Benutzer) keine Schulung absolvieren oder mit einem bestimmten Vokabular arbeiten, da im Fall von MedSpeak jede Person, sogar ein Kind, mit NaturallySpeaking arbeiten konnte und das Programm keine Ausspracheregeln festlegte. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/xj/m6/w-/xjm6w-kpgryltox7wquuvpl8db4.png" alt="Bild"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Trotz der Einzigartigkeit von Dragon NaturallySpeaking zeigten IT-Browser keine große Begeisterung für das Erkennen natürlicher Sprache. Unter den Mängeln wurden Erkennungsfehler und eine fehlerhafte Verarbeitung der an das Programm selbst gerichteten Befehle festgestellt. Quelle: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">itWeek</font></font></a></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Es ist bemerkenswert, dass die Erkennungs-Engine bereits in den 1980er Jahren bereit war, aber aufgrund der unzureichenden Computerleistung hatte die Entwicklung von Dragon Systems (jetzt im Besitz von Nuance Communications) keine Zeit, die Leerzeichen zwischen Wörtern im laufenden Betrieb zu bestimmen, die für die Erkennung natürlicher Sprache erforderlich sind. </font><font style="vertical-align: inherit;">Ohne dies könnten die Wörter "während der Behandlung" beispielsweise vom Computer als "verkrüppelt" gehört werden. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Vor uns lag die wachsende Beliebtheit von Spracherkennungssystemen, neuronalen Netzen, das Aufkommen der Google-Sprachsuche auf Mobilgeräten und schließlich Siris Sprachassistent, der nicht nur Sprache in Text umwandelte, sondern auch auf auf natürliche Weise konstruierte Anfragen angemessen reagierte.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wie kann man hören, was gesagt wurde und wie man an das Unhörbare denkt?</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Heutzutage ist das beste Werkzeug zum Erstellen einer Spracherkennungsmaschine das wiederkehrende neuronale Netzwerk (RNN), auf dem alle modernen Dienste zum Erkennen von Sprache, Musik, Bildern, Gesichtern, Objekten und Text basieren. Mit RNN können Sie Wörter mit äußerster Genauigkeit verstehen und das wahrscheinlichste Wort im Kontext des Kontexts vorhersagen, wenn es nicht erkannt wurde. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die zeitliche Klassifizierung des neuronalen Netzwerks des Modells (CTC) wählt einzelne Phoneme im aufgezeichneten Audiostream (Wort, Phrase) aus und ordnet sie in der Reihenfolge an, in der sie ausgesprochen wurden. Nach wiederholter Analyse identifiziert CTC bestimmte Phoneme sehr deutlich, und ihre Textaufzeichnung wird mit der Datenbank von Wörtern im neuronalen Netzwerk verglichen und dann in ein erkanntes Wort umgewandelt.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Neuronale Netze werden so genannt, weil das Prinzip ihrer Arbeit der Arbeit des menschlichen Gehirns ähnlich ist. Das neuronale Netzwerktraining ist dem menschlichen Training sehr ähnlich. Damit ein sehr kleines Kind beispielsweise lernen kann, Autos zu erkennen und von Motorrädern zu unterscheiden, muss es mindestens mehrmals auf verschiedene Autos aufmerksam gemacht werden und jedes Mal das entsprechende Wort aussprechen: Dies ist groß und rot - das Auto und dieses niedrige Schwarz - das Auto, aber dies und das sind Motorräder. Irgendwann entdeckt das Kind Muster und gemeinsame Zeichen für verschiedene Autos und lernt, richtig zu erkennen, wo sich das Auto befindet, wo sich der Jeep befindet, wo sich das Motorrad befindet und wo sich das ATV befindet, auch wenn es im Vorbeigehen auf einem Werbeplakat auf der Straße zu sehen ist. Auf die gleiche Weise muss das neuronale Netzwerk anhand von Beispielen trainiert werden, sodass Hunderte und Tausende von Aussprachevarianten für jedes Wort, jeden Buchstaben und jedes Phonem „untersucht“ werden müssen.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ein wiederkehrendes neuronales Netzwerk für die Spracherkennung ist gut, da es nach einem langen Training der Basis verschiedener Aussprachen lernt, Phoneme von Wörtern zu unterscheiden und daraus Wörter zu machen, unabhängig von der Qualität und Art der Aussprache. Und sogar mit hoher Genauigkeit im Kontext des Wortes „ausdenken“, Wörter, die aufgrund von Hintergrundgeräuschen oder unscharfer Aussprache nicht eindeutig erkannt werden konnten. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
RNN-Vorhersagen weisen jedoch eine Nuance auf: Ein wiederkehrendes neuronales Netzwerk kann ein fehlendes Wort nur dann „ausdenken“, wenn es sich auf den engsten Kontext von etwa fünf Wörtern stützt. Außerhalb dieses Bereichs wird keine Analyse durchgeführt. Und manchmal ist er so notwendig! Zur Anerkennung haben wir zum Beispiel den Satz „Der große russische Dichter Alexander Sergejewitsch </font><i><font style="vertical-align: inherit;">Puschkin</font></i><font style="vertical-align: inherit;"> “ ausgesprochen</font></font><i><font style="vertical-align: inherit;"></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">”, In dem das Wort“ Puschkin ”(speziell in Kursivschrift) so unhörbar gesagt wurde, dass die KI es nicht genau erkennen konnte. Ein wiederkehrendes neuronales Netzwerk, das auf den während des Trainings gesammelten Erfahrungen basiert, könnte jedoch darauf hindeuten, dass das Wort "Puschkin" am häufigsten neben den Wörtern "Russisch", "Dichter", "Alexander" und "Sergejewitsch" vorkommt. Dies ist eine ziemlich einfache Aufgabe für ein RNN, das in russischen Texten geschult ist, da wir in einem sehr spezifischen Kontext Annahmen mit höchster Genauigkeit treffen können.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Und wenn der Kontext vage ist? Nehmen Sie einen anderen Text, in dem ein Wort nicht erkannt werden kann: „Unser Alles, Alexander Sergejewitsch Puschkin, ist in der Blüte seines Lebens nach einem Duell mit Dantes auf tragische Weise gestorben. Das Puschkin-Theaterfestival ist nach dem Dichter benannt. “ Wenn Sie das Wort "Puschkinsky" entfernen, kann RNN es aufgrund des Kontextes des Vorschlags einfach nicht erraten, da nur ein Theaterfestival und ein Verweis auf den Namen eines unbekannten Dichters erwähnt werden - es gibt unzählige Möglichkeiten! </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Hier kommt die 1997 erstellte Long Short Term Memory (LSTM) -Architektur für wiederkehrende neuronale Netze (ein </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;">ausführlicher Artikel über LSTM</font></a><font style="vertical-align: inherit;"> ) ins </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Spiel.</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">) Es wurde speziell entwickelt, um die RNN-Fähigkeit hinzuzufügen, den Kontext zu berücksichtigen, der von dem zu verarbeitenden Ereignis entfernt ist. Die Ergebnisse der Lösung früherer Probleme (dh der Worterkennung) durchlaufen den gesamten Erkennungsprozess, unabhängig davon, wie lange der Monolog dauert, und werden in jedem Zweifelsfall berücksichtigt. Darüber hinaus hat der Entfernungsabstand fast keinen Einfluss auf die Effizienz der Architektur. Mit Hilfe von LSTM berücksichtigt ein Wortnetzwerk bei Bedarf alle im Rahmen der Aufgabe verfügbaren Erfahrungen: In unserem Beispiel wird RNN den vorherigen Satz betrachten und feststellen, dass Puschkin und Dantes früher erwähnt wurden. Daher weist „Mit dem Namen des Dichters“ höchstwahrscheinlich auf einen von ihnen hin. Da es keine Beweise für die Existenz des Dantes Theaterfestivals gibt,Wir sprechen über Puschkinski (zumal der Klangabdruck eines nicht erkannten Wortes sehr ähnlich ist) - ein solches Festival war die Basis für das Training des neuronalen Netzwerks.</font></font><br>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/P325_hrGsDI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Geständnis eines Sprachassistenten." </font><font style="vertical-align: inherit;">Wenn ein gut ausgebildetes neuronales Netzwerk ins Spiel kommt, kann ein Sprachassistent genau herausfinden, was mit „grünen Hausschuhen“ zu tun ist.</font></font></i><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wie macht die Spracherkennung die Welt zu einem besseren Ort?</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In jedem Fall ist die Anwendung anders - sie hilft jemandem, mit Gadgets zu kommunizieren, und </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">laut PricewaterhouseCooper geben</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> mehr als die Hälfte der Smartphone-Benutzer Sprachbefehle an Geräte - bei Erwachsenen (25-49 Jahre) sogar der Prozentsatz derjenigen, die ständig Sprachschnittstellen verwenden höher als bei jungen Menschen (18-25) - 65% gegenüber 59%. </font><font style="vertical-align: inherit;">Und in Russland haben mindestens einmal mindestens 71% der Bevölkerung mit Siri, Google Assitant oder Alice kommuniziert. </font><font style="vertical-align: inherit;">45 Millionen Russen kommunizieren ständig mit Yandex von Alice, wobei Yandex.Maps / Yandex.Navigator nur 30% der Anfragen ausmacht.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die Spracherkennung hilft wirklich jemandem bei der Arbeit - zum Beispiel, wie oben erwähnt, für Ärzte: In der Medizin wird die Erkennung seit 1996 (als IBM MedSpeak herauskam) verwendet, um Anamnese aufzuzeichnen und Bilder zu studieren - ein Arzt kann weiterarbeiten, ohne durch Aufzeichnungen in abgelenkt zu werden Computer oder Papierkarte. Die Arbeit am Diktat in der Medizin wird übrigens nicht nur im Westen durchgeführt - in Russland gibt es ein Voice2Med-Programm des „Zentrums für Sprachtechnologien“.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Es gibt andere Beispiele, einschließlich unserer eigenen. Die Organisation eines Toshiba-Geschäfts beinhaltet die vollständige Einbeziehung, dh gleiche Rechte und Chancen für Menschen mit verschiedenen Gesundheitszuständen, einschließlich für Mitarbeiter mit Hörbehinderungen. Wir haben ein Unternehmensprogramm namens Universal Design Advisor System, in dem Menschen mit verschiedenen Arten von Behinderungen an der Entwicklung von Toshiba-Produkten teilnehmen und Vorschläge zur Verbesserung ihres Komforts für Menschen mit Behinderungen machen. Das heißt, wir gehen nicht davon aus, wie wir es besser machen können, sondern arbeiten mit echten Erfahrungen und Mitarbeiterbewertungen.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Vor einigen Jahren standen wir am Hauptsitz von Toshiba in Japan vor einer sehr interessanten Aufgabe, die die Entwicklung eines neuen Spracherkennungssystems erforderte. Während des Betriebs des Universal Design Advisor Systems haben wir einen wichtigen Einblick erhalten: Mitarbeiter mit Hörbehinderung möchten in Echtzeit an Diskussionen bei Besprechungen und Vorträgen teilnehmen und sich nicht darauf beschränken, die verarbeiteten Niederschriften Stunden oder Tage später zu lesen. Das Starten der Spracherkennung über ein Smartphone führt in solchen Fällen zu einem sehr schwachen Ergebnis. Daher mussten Toshiba-Spezialisten mit der Entwicklung eines speziellen Erkennungssystems beginnen. Und natürlich sind wir sofort auf Probleme gestoßen.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Konversation unterscheidet sich erheblich von schriftlicher Sprache - wir sprechen nicht so, wie wir Briefe schreiben, und eine echte Konversation, die in Text übersetzt wird, sieht sehr schlampig und sogar unlesbar aus. Das heißt, selbst wenn wir Gespräche auf dem Morgengleiter mit hoher Genauigkeit in Text umwandeln, erhalten wir einen inkohärenten Mischmasch voller parasitärer Wörter, Interjektionen und nachdenklicher „aaa“, „uh“ und „mmm“. Um die Transkription unnötiger Geräusche, Wörter und Ausdrucksformen von Emotionen im Text zu vermeiden, haben wir uns entschlossen, eine KI zu entwickeln, die in der Lage ist, nicht immer notwendige Elemente der Umgangssprache so unverkennbar wie möglich zu erkennen, einschließlich der emotionalen Färbung einiger Wörter (zum Beispiel „Ja, gut“ kann nach Skepsis klingen oder wie aufrichtige Überraschung, und dies sind buchstäblich entgegengesetzte Bedeutungen).</font></font><br>
<br>
<img src="https://habrastorage.org/webt/6z/vv/od/6zvvodwnihcvdqdqfv4uuprbtb4.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Es sieht aus wie ein Laptop mit einer Reihe von Peripheriegeräten zur Spracherkennung mit Toshiba AI (links) und einer Anwendung mit den Ergebnissen für Endgeräte (rechts). Quelle: Toshiba</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
LSTM hat sich hier als nützlich erwiesen, ohne die die Erkennungsgenauigkeit nicht ausreichte, um den empfangenen Text mühelos lesen und verstehen zu können. Darüber hinaus war LSTM nicht nur für die genauere Vorhersage von Wörtern im Kontext nützlich, sondern auch für die korrekte Verarbeitung von Pausen in der Mitte von Sätzen und Interjektionsparasiten - dafür haben wir dem neuronalen Netzwerk diese Parasiten und Pausen beigebracht, die für die Umgangssprache natürlich sind.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bedeutet dies, dass das neuronale Netzwerk jetzt Interjektionen aus Transkripten entfernen kann? </font><font style="vertical-align: inherit;">Ja, das kann es, aber das ist nicht nötig. </font><font style="vertical-align: inherit;">Tatsache ist, dass (eine weitere Erkenntnis) Menschen mit Hörbehinderungen geführt werden, auch durch die Bewegungen der Lippen des Sprechers. </font><font style="vertical-align: inherit;">Wenn sich die Lippen bewegen, der diesen Bewegungen entsprechende Text jedoch nicht auf dem Bildschirm angezeigt wird, besteht das Gefühl, dass das Erkennungssystem einen Teil des Gesprächs verpasst hat. </font><font style="vertical-align: inherit;">Das heißt, für jemanden, der nicht hören kann, ist es wichtig, so viele Informationen wie möglich über das Gespräch zu erhalten, einschließlich unglücklicher Pausen und Mejometia. </font><font style="vertical-align: inherit;">Daher belässt die Toshiba-Engine diese Elemente im Transkript, verringert jedoch in Echtzeit die Helligkeit der Buchstaben, wodurch deutlich wird, dass dies optionale Details zum Verständnis des Textes sind.</font></font><br>
<br>
<div class="oembed"><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">https://www.toshiba-clip.com/de/detail/7655</font></font></a></div><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">So sieht das Erkennungsergebnis im laufenden Betrieb auf dem Clientgerät aus. </font><font style="vertical-align: inherit;">Die Teile des Monologs, die nicht aussagekräftig sind, sind grau gestrichen.</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Jetzt arbeitet Toshiba AI mit englischer, japanischer und chinesischer Sprache, und sogar eine Übersetzung zwischen Sprachen im laufenden Betrieb ist möglich. </font><font style="vertical-align: inherit;">Es ist nicht erforderlich, es für Kurzschrift im laufenden Betrieb zu verwenden - die KI kann für die Arbeit mit Sprachassistenten angepasst werden, die schließlich lernen, Interjektionen, Pausen und Stottern angemessen wahrzunehmen, wenn eine Person einen Befehl ausspricht. </font><font style="vertical-align: inherit;">Im März 2019 wurde das System erfolgreich eingesetzt, um der in Japan ausgestrahlten IPSJ National Convention Untertitel hinzuzufügen. </font><font style="vertical-align: inherit;">In naher Zukunft - die Umwandlung der Toshiba AI in einen öffentlichen Dienst und Erfahrungen mit der Implementierung der Spracherkennung in der Produktion.</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de490720/index.html">Motor! oder Was ist Spielphysik?</a></li>
<li><a href="../de490722/index.html">Gender Urlaub in der IT. Wie zu beachten</a></li>
<li><a href="../de490726/index.html">Authentifizierung auf Netzwerkgeräten über SSH mit öffentlichen Schlüsseln</a></li>
<li><a href="../de490728/index.html">Sicherheitswoche 10: RSA-Konferenz und Sensibilisierung für Cybersicherheit</a></li>
<li><a href="../de490730/index.html">Intel x86 Root of Trust: Vertrauensverlust</a></li>
<li><a href="../de490734/index.html">Ostereier auf topografischen Karten der Schweiz</a></li>
<li><a href="../de490736/index.html">9 klare Werkzeuge zum Lernen und Pumpen des englischen Wortschatzes</a></li>
<li><a href="../de490738/index.html">Lisk-Substitutionsprinzip</a></li>
<li><a href="../de490740/index.html">Das Defektieren von *** s ist nicht nur eine Randomisierung</a></li>
<li><a href="../de490742/index.html">Eine neue Ära in der Robotik hat begonnen</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>