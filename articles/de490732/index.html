<!doctype html>
<html class="no-js" lang="de">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸŸ ğŸ‘©ğŸ¿ ğŸ¤˜ğŸ¾ Spracherkennung: Ein sehr kurzer EinfÃ¼hrungskurs ğŸ‘©ğŸ¼â€ğŸ¤â€ğŸ‘¨ğŸ½ ğŸ“Š ğŸ‘«</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Es ist fast unmÃ¶glich, dem Laien so einfach wie mÃ¶glich von der Arbeit der Computer-Spracherkennung zu erzÃ¤hlen und sie in Text umzuwandeln. Keine ein...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Spracherkennung: Ein sehr kurzer EinfÃ¼hrungskurs</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/toshibarus/blog/490732/"><img src="https://habrastorage.org/webt/tz/sh/ll/tzshllxzf2iddwai7sredy3edie.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Es ist fast unmÃ¶glich, dem Laien so einfach wie mÃ¶glich von der Arbeit der Computer-Spracherkennung zu erzÃ¤hlen und sie in Text umzuwandeln. </font><font style="vertical-align: inherit;">Keine einzige Geschichte darÃ¼ber ist vollstÃ¤ndig ohne komplexe Formeln und mathematische Begriffe. </font><font style="vertical-align: inherit;">Wir werden versuchen, so klar und leicht wie mÃ¶glich zu erklÃ¤ren, wie Ihr Smartphone Sprache versteht, wann Autos gelernt haben, eine menschliche Stimme zu erkennen und in welchen unerwarteten Bereichen diese Technologie eingesetzt wird. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Notwendige Warnung: Wenn Sie Entwickler oder insbesondere Mathematiker sind, ist es unwahrscheinlich, dass Sie etwas Neues aus der Post lernen und sich sogar Ã¼ber den unzureichenden wissenschaftlichen Charakter des Materials beschweren. </font><font style="vertical-align: inherit;">Unser Ziel ist es, die nicht eingeweihten Leser auf einfachste Weise mit Sprachtechnologien vertraut zu machen und zu erklÃ¤ren, wie und warum Toshiba die Schaffung ihrer Sprach-KI aufgenommen hat.</font></font><br>
<a name="habracut"></a><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wichtige Meilensteine â€‹â€‹in der Geschichte der Spracherkennung</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die Geschichte der Erkennung menschlicher Sprache durch elektronische Maschinen begann etwas frÃ¼her als Ã¼blich: In den meisten FÃ¤llen ist es Ã¼blich, ab 1952 herunterzuzÃ¤hlen, aber tatsÃ¤chlich war eines der ersten GerÃ¤te, das auf Sprachbefehle reagierte, der Televox-Roboter, Ã¼ber </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">den wir bereits geschrieben haben</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Der Herbert Televox-Roboter wurde 1927 in den USA entwickelt und war ein einfaches GerÃ¤t, bei dem verschiedene Relais auf GerÃ¤usche mit unterschiedlichen Frequenzen reagierten. Der Roboter hatte drei Stimmgabeln, von denen jede fÃ¼r ihren Ton verantwortlich war. Je nachdem, welche Stimmgabel funktionierte, wurde das eine oder andere Relais aktiviert.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/90/pq/4i/90pq4ixpys3c8uevjp-ovvydfny.jpeg" alt="Bild"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">TatsÃ¤chlich befand sich die gesamte â€FÃ¼llungâ€œ von Televox, einschlieÃŸlich des Befehlserkennungssystems, auf einem Gestell im Bereich des KÃ¶rpers des â€Robotersâ€œ. Es war unmÃ¶glich, den Deckel zu schlieÃŸen, da sonst die Stimmgabeln GerÃ¤usche nicht richtig â€hÃ¶renâ€œ konnten. Quelle: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Acme Telepictures / Wikimedia.</font></font></a></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Es war mÃ¶glich, mit Televox als separate Signale mit einer Pfeife und in kurzen verbalen Hinweisen zu kommunizieren - ihre Stimmgabeln wurden auch in einer Folge von TÃ¶nen angeordnet. Der SchÃ¶pfer des Roboters Roy Wensley fÃ¼hrte fÃ¼r diese Zeit sogar eine fantastische Demonstration durch und sagte den Befehl â€Sesam, offenâ€œ, durch den Televox das Relais einschaltete, das fÃ¼r das Ã–ffnen der TÃ¼r verantwortlich war. Keine digitale Technologie, neuronale Netze, KI und maschinelles Lernen - nur analoge Technologie!</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die nÃ¤chste SchlÃ¼sselerfindung, die den Weg fÃ¼r eine echte Erkennung menschlicher Sprache ebnete, war die Audrey-Maschine, die 1952 in der Bell Labs Innovation Forge entwickelt wurde. Der riesige Audrey verbrauchte viel Strom und hatte die GrÃ¶ÃŸe eines guten Schranks, aber seine gesamte FunktionalitÃ¤t bestand darin, gesprochene Zahlen von null bis neun zu erkennen. Nur zehn Worte, ja, aber vergessen wir nicht, dass Audrey eine analoge Maschine war. </font></font><br>
<img src="https://habrastorage.org/webt/vd/1q/eb/vd1qebrer6czotgwty3tdyfp15i.png" alt="Bild"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Leider hat die Geschichte keine Ã¶ffentlichen Fotografien von Audrey erhalten, es gibt nur ein Konzept. Einfach auf Papier, schwer zu Ã¼bersetzen - nach den Erinnerungen von Zeitgenossen besetzten Audrey-Komponenten einen ganzen Schrank. Quelle: Bell Labs</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Es funktionierte so: Der Ansager sprach Zahlen in das Mikrofon, wobei zwischen den WÃ¶rtern Intervalle von mindestens 350 ms eingehalten wurden. Audrey wandelte die von ihm gehÃ¶rten GerÃ¤usche in elektrische Signale um und verglich sie mit im analogen Speicher aufgezeichneten Samples. Nach den Ergebnissen des Vergleichs hat das Auto die Nummer auf dem Armaturenbrett hervorgehoben. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Es war ein Durchbruch, aber es gab keinen wirklichen Vorteil von Audrey - die Maschine erkannte die Stimme ihres SchÃ¶pfers mit einer Genauigkeit von 97%, andere speziell ausgebildete Sprecher erhielten eine Genauigkeit von 70-80%. Fremde, die Audrey zum ersten Mal kontaktierten, egal wie sehr sie es versuchten, sahen ihre Nummer in nur 50% der FÃ¤lle auf der Anzeigetafel.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Trotz der revolutionÃ¤ren Ergebnisse seiner Zeit fand Audrey keine praktische Anwendung und konnte sie auch nicht finden. </font><font style="vertical-align: inherit;">Es wurde angenommen, dass das System anstelle von Telefonisten angepasst werden kÃ¶nnte, aber dennoch waren die menschlichen Dienste bequemer, schneller und viel zuverlÃ¤ssiger als Audrey.</font></font><br>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/rQco1sa9AwU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PrÃ¤sentation Ã¤hnlich wie Audrey, nur viel kleinere Maschinen - IBM Shoebox. </font><font style="vertical-align: inherit;">Die Geschwindigkeit des Schuhkartons ist deutlich sichtbar. </font><font style="vertical-align: inherit;">Die Maschine kÃ¶nnte auch einfache mathematische Operationen der Addition und Subtraktion ausfÃ¼hren</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In den frÃ¼hen 1960er Jahren wurden in Japan, GroÃŸbritannien, den USA und sogar der UdSSR Arbeiten zur Erstellung von SpracherkennungsgerÃ¤ten durchgefÃ¼hrt, bei denen ein sehr wichtiger Algorithmus fÃ¼r die dynamische Transformation der Zeitachse (DTW) erfunden wurde, mit dessen Hilfe ein System mit etwa 200 WÃ¶rtern erstellt werden konnte. Alle Entwicklungen waren jedoch Ã¤hnlich, und das Erkennungsprinzip wurde zu einem allgemeinen Nachteil: WÃ¶rter wurden als integrale KlangfingerabdrÃ¼cke wahrgenommen und dann anhand der Stichprobenbasis (WÃ¶rterbuch) Ã¼berprÃ¼ft. Ã„nderungen der Geschwindigkeit, des Timbres und der Klarheit der Aussprache von WÃ¶rtern beeintrÃ¤chtigten die ErkennungsqualitÃ¤t erheblich. Wissenschaftler haben eine neue Aufgabe: der Maschine beizubringen, einzelne GerÃ¤usche, Phoneme oder Silben zu hÃ¶ren und daraus WÃ¶rter zu machen. Ein solcher Ansatz wÃ¼rde es ermÃ¶glichen, den Effekt des Lautsprecherwechsels auszugleichen, wenn der Erkennungspegel je nach Sprecher stark variiert.</font></font><br>
<br>
<i> â€”     ,           . ,   Â« Â»  Â«Â»       Â«Â».   Â«Â»   Â« Â»  Â« Â»      Â«Â»,    â€”  Â«Â».  ,  ,   . </i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
1971 startete die Agentur fÃ¼r fortgeschrittene Forschungsprojekte des Verteidigungsministeriums (DARPA) ein FÃ¼nfjahresprogramm mit einem Budget von 15 Millionen US-Dollar, bei dem die Aufgabe darin bestand, ein Anerkennungssystem zu schaffen, das mindestens 1000 WÃ¶rter kannte. 1976 fÃ¼hrte die Carnegie Mellon University die Harpyie ein, die ein WÃ¶rterbuch mit 1011 WÃ¶rtern bedienen kann. Harpyie verglich die vollstÃ¤ndig gehÃ¶rten WÃ¶rter nicht mit den Samples, sondern teilte sie in Allophone ein (ein Sample des Klangs eines Phonems in AbhÃ¤ngigkeit von den ihn umgebenden Buchstaben). Dies war ein weiterer Erfolg, der bestÃ¤tigte, dass die Zukunft in der Erkennung einzelner Phoneme und nicht ganzer WÃ¶rter liegt. Zu den Nachteilen von Harpyie gehÃ¶rte jedoch eine Ã¤uÃŸerst geringe korrekte Erkennung von Allophonen (Aussprachen von Phonemen) - etwa 47%. Bei einem so hohen Fehler stieg der Fehleranteil nach dem Umfang des WÃ¶rterbuchs.</font></font><br>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/32KKg3aP3Vw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Beschreibung, wie Harpyie funktioniert. Video des Programms Ã¼berlebte nicht.</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Die Erfahrung von Harpy hat gezeigt, dass das Erstellen von WÃ¶rterbÃ¼chern mit ganzheitlichen KlangfingerabdrÃ¼cken nutzlos ist - es erhÃ¶ht nur die Erkennungszeit und verringert die Genauigkeit drastisch, sodass Forscher auf der ganzen Welt einen anderen Weg eingeschlagen haben - das Erkennen von Phonemen. Mitte der 1980er Jahre konnte die IBM Tangora-Maschine lernen, die Sprache eines Sprechers mit Akzent, Dialekt und Aussprache zu verstehen. Es war lediglich eine 20-minÃ¼tige Schulung erforderlich, in der eine Datenbank mit Phonemen und Allophonproben gesammelt wurde. Die Verwendung des </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hidden-Markov-Modells</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> erhÃ¶hte auch das IBM Tangora-Vokabular auf beeindruckende 20.000 WÃ¶rter - 20-mal mehr als Harpy und ist bereits mit dem Vokabular des Teenagers vergleichbar.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Alle Spracherkennungssysteme der 1950er bis Mitte der 1990er Jahre wussten nicht, wie man die natÃ¼rliche gesprochene Sprache einer Person liest - sie mussten die WÃ¶rter separat aussprechen und zwischen ihnen pausieren. Ein wahrhaft revolutionÃ¤res Ereignis war die EinfÃ¼hrung des in den 1980er Jahren entwickelten Hidden-Markov-Modells - eines statistischen Modells, das auf der Grundlage der bekannten prÃ¤zise Annahmen Ã¼ber unbekannte Elemente aufbaute. Einfach ausgedrÃ¼ckt, mit nur wenigen erkannten Phonemen in einem Wort wÃ¤hlt das versteckte Markov-Modell die fehlenden Phoneme sehr genau aus, wodurch die Genauigkeit der Spracherkennung erheblich erhÃ¶ht wird.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
1996 erschien das erste kommerzielle Programm, das nicht einzelne WÃ¶rter, sondern einen kontinuierlichen Fluss natÃ¼rlicher Sprache unterscheiden konnte - IBM MedSpeak / Radiology. IBM war ein spezialisiertes Produkt, das in der Medizin verwendet wurde, um die Ergebnisse einer RÃ¶ntgenaufnahme, die von einem Arzt wÃ¤hrend der Studie durchgefÃ¼hrt wurde, kurz zu beschreiben. Hier reichte die Leistung von Computern schlieÃŸlich aus, um einzelne WÃ¶rter "on the fly" zu erkennen. AuÃŸerdem sind die Algorithmen perfekter geworden, und die korrekte Erkennung von Mikropausen zwischen den gesprochenen WÃ¶rtern ist aufgetreten.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Der erste universelle Motor zur Erkennung natÃ¼rlicher Sprache war 1997 das Programm Dragon NaturallySpeaking. Bei der Arbeit mit ihr musste der Ansager (d. H. Der Benutzer) keine Schulung absolvieren oder mit einem bestimmten Vokabular arbeiten, da im Fall von MedSpeak jede Person, sogar ein Kind, mit NaturallySpeaking arbeiten konnte und das Programm keine Ausspracheregeln festlegte. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/xj/m6/w-/xjm6w-kpgryltox7wquuvpl8db4.png" alt="Bild"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Trotz der Einzigartigkeit von Dragon NaturallySpeaking zeigten IT-Browser keine groÃŸe Begeisterung fÃ¼r das Erkennen natÃ¼rlicher Sprache. Unter den MÃ¤ngeln wurden Erkennungsfehler und eine fehlerhafte Verarbeitung der an das Programm selbst gerichteten Befehle festgestellt. Quelle: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">itWeek</font></font></a></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Es ist bemerkenswert, dass die Erkennungs-Engine bereits in den 1980er Jahren bereit war, aber aufgrund der unzureichenden Computerleistung hatte die Entwicklung von Dragon Systems (jetzt im Besitz von Nuance Communications) keine Zeit, die Leerzeichen zwischen WÃ¶rtern im laufenden Betrieb zu bestimmen, die fÃ¼r die Erkennung natÃ¼rlicher Sprache erforderlich sind. </font><font style="vertical-align: inherit;">Ohne dies kÃ¶nnten die WÃ¶rter "wÃ¤hrend der Behandlung" beispielsweise vom Computer als "verkrÃ¼ppelt" gehÃ¶rt werden. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Vor uns lag die wachsende Beliebtheit von Spracherkennungssystemen, neuronalen Netzen, das Aufkommen der Google-Sprachsuche auf MobilgerÃ¤ten und schlieÃŸlich Siris Sprachassistent, der nicht nur Sprache in Text umwandelte, sondern auch auf auf natÃ¼rliche Weise konstruierte Anfragen angemessen reagierte.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wie kann man hÃ¶ren, was gesagt wurde und wie man an das UnhÃ¶rbare denkt?</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Heutzutage ist das beste Werkzeug zum Erstellen einer Spracherkennungsmaschine das wiederkehrende neuronale Netzwerk (RNN), auf dem alle modernen Dienste zum Erkennen von Sprache, Musik, Bildern, Gesichtern, Objekten und Text basieren. Mit RNN kÃ¶nnen Sie WÃ¶rter mit Ã¤uÃŸerster Genauigkeit verstehen und das wahrscheinlichste Wort im Kontext des Kontexts vorhersagen, wenn es nicht erkannt wurde. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die zeitliche Klassifizierung des neuronalen Netzwerks des Modells (CTC) wÃ¤hlt einzelne Phoneme im aufgezeichneten Audiostream (Wort, Phrase) aus und ordnet sie in der Reihenfolge an, in der sie ausgesprochen wurden. Nach wiederholter Analyse identifiziert CTC bestimmte Phoneme sehr deutlich, und ihre Textaufzeichnung wird mit der Datenbank von WÃ¶rtern im neuronalen Netzwerk verglichen und dann in ein erkanntes Wort umgewandelt.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Neuronale Netze werden so genannt, weil das Prinzip ihrer Arbeit der Arbeit des menschlichen Gehirns Ã¤hnlich ist. Das neuronale Netzwerktraining ist dem menschlichen Training sehr Ã¤hnlich. Damit ein sehr kleines Kind beispielsweise lernen kann, Autos zu erkennen und von MotorrÃ¤dern zu unterscheiden, muss es mindestens mehrmals auf verschiedene Autos aufmerksam gemacht werden und jedes Mal das entsprechende Wort aussprechen: Dies ist groÃŸ und rot - das Auto und dieses niedrige Schwarz - das Auto, aber dies und das sind MotorrÃ¤der. Irgendwann entdeckt das Kind Muster und gemeinsame Zeichen fÃ¼r verschiedene Autos und lernt, richtig zu erkennen, wo sich das Auto befindet, wo sich der Jeep befindet, wo sich das Motorrad befindet und wo sich das ATV befindet, auch wenn es im Vorbeigehen auf einem Werbeplakat auf der StraÃŸe zu sehen ist. Auf die gleiche Weise muss das neuronale Netzwerk anhand von Beispielen trainiert werden, sodass Hunderte und Tausende von Aussprachevarianten fÃ¼r jedes Wort, jeden Buchstaben und jedes Phonem â€untersuchtâ€œ werden mÃ¼ssen.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ein wiederkehrendes neuronales Netzwerk fÃ¼r die Spracherkennung ist gut, da es nach einem langen Training der Basis verschiedener Aussprachen lernt, Phoneme von WÃ¶rtern zu unterscheiden und daraus WÃ¶rter zu machen, unabhÃ¤ngig von der QualitÃ¤t und Art der Aussprache. Und sogar mit hoher Genauigkeit im Kontext des Wortes â€ausdenkenâ€œ, WÃ¶rter, die aufgrund von HintergrundgerÃ¤uschen oder unscharfer Aussprache nicht eindeutig erkannt werden konnten. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
RNN-Vorhersagen weisen jedoch eine Nuance auf: Ein wiederkehrendes neuronales Netzwerk kann ein fehlendes Wort nur dann â€ausdenkenâ€œ, wenn es sich auf den engsten Kontext von etwa fÃ¼nf WÃ¶rtern stÃ¼tzt. AuÃŸerhalb dieses Bereichs wird keine Analyse durchgefÃ¼hrt. Und manchmal ist er so notwendig! Zur Anerkennung haben wir zum Beispiel den Satz â€Der groÃŸe russische Dichter Alexander Sergejewitsch </font><i><font style="vertical-align: inherit;">Puschkin</font></i><font style="vertical-align: inherit;"> â€œ ausgesprochen</font></font><i><font style="vertical-align: inherit;"></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">â€, In dem das Wortâ€œ Puschkin â€(speziell in Kursivschrift) so unhÃ¶rbar gesagt wurde, dass die KI es nicht genau erkennen konnte. Ein wiederkehrendes neuronales Netzwerk, das auf den wÃ¤hrend des Trainings gesammelten Erfahrungen basiert, kÃ¶nnte jedoch darauf hindeuten, dass das Wort "Puschkin" am hÃ¤ufigsten neben den WÃ¶rtern "Russisch", "Dichter", "Alexander" und "Sergejewitsch" vorkommt. Dies ist eine ziemlich einfache Aufgabe fÃ¼r ein RNN, das in russischen Texten geschult ist, da wir in einem sehr spezifischen Kontext Annahmen mit hÃ¶chster Genauigkeit treffen kÃ¶nnen.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Und wenn der Kontext vage ist? Nehmen Sie einen anderen Text, in dem ein Wort nicht erkannt werden kann: â€Unser Alles, Alexander Sergejewitsch Puschkin, ist in der BlÃ¼te seines Lebens nach einem Duell mit Dantes auf tragische Weise gestorben. Das Puschkin-Theaterfestival ist nach dem Dichter benannt. â€œ Wenn Sie das Wort "Puschkinsky" entfernen, kann RNN es aufgrund des Kontextes des Vorschlags einfach nicht erraten, da nur ein Theaterfestival und ein Verweis auf den Namen eines unbekannten Dichters erwÃ¤hnt werden - es gibt unzÃ¤hlige MÃ¶glichkeiten! </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Hier kommt die 1997 erstellte Long Short Term Memory (LSTM) -Architektur fÃ¼r wiederkehrende neuronale Netze (ein </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;">ausfÃ¼hrlicher Artikel Ã¼ber LSTM</font></a><font style="vertical-align: inherit;"> ) ins </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Spiel.</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">) Es wurde speziell entwickelt, um die RNN-FÃ¤higkeit hinzuzufÃ¼gen, den Kontext zu berÃ¼cksichtigen, der von dem zu verarbeitenden Ereignis entfernt ist. Die Ergebnisse der LÃ¶sung frÃ¼herer Probleme (dh der Worterkennung) durchlaufen den gesamten Erkennungsprozess, unabhÃ¤ngig davon, wie lange der Monolog dauert, und werden in jedem Zweifelsfall berÃ¼cksichtigt. DarÃ¼ber hinaus hat der Entfernungsabstand fast keinen Einfluss auf die Effizienz der Architektur. Mit Hilfe von LSTM berÃ¼cksichtigt ein Wortnetzwerk bei Bedarf alle im Rahmen der Aufgabe verfÃ¼gbaren Erfahrungen: In unserem Beispiel wird RNN den vorherigen Satz betrachten und feststellen, dass Puschkin und Dantes frÃ¼her erwÃ¤hnt wurden. Daher weist â€Mit dem Namen des Dichtersâ€œ hÃ¶chstwahrscheinlich auf einen von ihnen hin. Da es keine Beweise fÃ¼r die Existenz des Dantes Theaterfestivals gibt,Wir sprechen Ã¼ber Puschkinski (zumal der Klangabdruck eines nicht erkannten Wortes sehr Ã¤hnlich ist) - ein solches Festival war die Basis fÃ¼r das Training des neuronalen Netzwerks.</font></font><br>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/P325_hrGsDI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"GestÃ¤ndnis eines Sprachassistenten." </font><font style="vertical-align: inherit;">Wenn ein gut ausgebildetes neuronales Netzwerk ins Spiel kommt, kann ein Sprachassistent genau herausfinden, was mit â€grÃ¼nen Hausschuhenâ€œ zu tun ist.</font></font></i><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wie macht die Spracherkennung die Welt zu einem besseren Ort?</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In jedem Fall ist die Anwendung anders - sie hilft jemandem, mit Gadgets zu kommunizieren, und </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">laut PricewaterhouseCooper geben</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> mehr als die HÃ¤lfte der Smartphone-Benutzer Sprachbefehle an GerÃ¤te - bei Erwachsenen (25-49 Jahre) sogar der Prozentsatz derjenigen, die stÃ¤ndig Sprachschnittstellen verwenden hÃ¶her als bei jungen Menschen (18-25) - 65% gegenÃ¼ber 59%. </font><font style="vertical-align: inherit;">Und in Russland haben mindestens einmal mindestens 71% der BevÃ¶lkerung mit Siri, Google Assitant oder Alice kommuniziert. </font><font style="vertical-align: inherit;">45 Millionen Russen kommunizieren stÃ¤ndig mit Yandex von Alice, wobei Yandex.Maps / Yandex.Navigator nur 30% der Anfragen ausmacht.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die Spracherkennung hilft wirklich jemandem bei der Arbeit - zum Beispiel, wie oben erwÃ¤hnt, fÃ¼r Ã„rzte: In der Medizin wird die Erkennung seit 1996 (als IBM MedSpeak herauskam) verwendet, um Anamnese aufzuzeichnen und Bilder zu studieren - ein Arzt kann weiterarbeiten, ohne durch Aufzeichnungen in abgelenkt zu werden Computer oder Papierkarte. Die Arbeit am Diktat in der Medizin wird Ã¼brigens nicht nur im Westen durchgefÃ¼hrt - in Russland gibt es ein Voice2Med-Programm des â€Zentrums fÃ¼r Sprachtechnologienâ€œ.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Es gibt andere Beispiele, einschlieÃŸlich unserer eigenen. Die Organisation eines Toshiba-GeschÃ¤fts beinhaltet die vollstÃ¤ndige Einbeziehung, dh gleiche Rechte und Chancen fÃ¼r Menschen mit verschiedenen GesundheitszustÃ¤nden, einschlieÃŸlich fÃ¼r Mitarbeiter mit HÃ¶rbehinderungen. Wir haben ein Unternehmensprogramm namens Universal Design Advisor System, in dem Menschen mit verschiedenen Arten von Behinderungen an der Entwicklung von Toshiba-Produkten teilnehmen und VorschlÃ¤ge zur Verbesserung ihres Komforts fÃ¼r Menschen mit Behinderungen machen. Das heiÃŸt, wir gehen nicht davon aus, wie wir es besser machen kÃ¶nnen, sondern arbeiten mit echten Erfahrungen und Mitarbeiterbewertungen.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Vor einigen Jahren standen wir am Hauptsitz von Toshiba in Japan vor einer sehr interessanten Aufgabe, die die Entwicklung eines neuen Spracherkennungssystems erforderte. WÃ¤hrend des Betriebs des Universal Design Advisor Systems haben wir einen wichtigen Einblick erhalten: Mitarbeiter mit HÃ¶rbehinderung mÃ¶chten in Echtzeit an Diskussionen bei Besprechungen und VortrÃ¤gen teilnehmen und sich nicht darauf beschrÃ¤nken, die verarbeiteten Niederschriften Stunden oder Tage spÃ¤ter zu lesen. Das Starten der Spracherkennung Ã¼ber ein Smartphone fÃ¼hrt in solchen FÃ¤llen zu einem sehr schwachen Ergebnis. Daher mussten Toshiba-Spezialisten mit der Entwicklung eines speziellen Erkennungssystems beginnen. Und natÃ¼rlich sind wir sofort auf Probleme gestoÃŸen.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Konversation unterscheidet sich erheblich von schriftlicher Sprache - wir sprechen nicht so, wie wir Briefe schreiben, und eine echte Konversation, die in Text Ã¼bersetzt wird, sieht sehr schlampig und sogar unlesbar aus. Das heiÃŸt, selbst wenn wir GesprÃ¤che auf dem Morgengleiter mit hoher Genauigkeit in Text umwandeln, erhalten wir einen inkohÃ¤renten Mischmasch voller parasitÃ¤rer WÃ¶rter, Interjektionen und nachdenklicher â€aaaâ€œ, â€uhâ€œ und â€mmmâ€œ. Um die Transkription unnÃ¶tiger GerÃ¤usche, WÃ¶rter und Ausdrucksformen von Emotionen im Text zu vermeiden, haben wir uns entschlossen, eine KI zu entwickeln, die in der Lage ist, nicht immer notwendige Elemente der Umgangssprache so unverkennbar wie mÃ¶glich zu erkennen, einschlieÃŸlich der emotionalen FÃ¤rbung einiger WÃ¶rter (zum Beispiel â€Ja, gutâ€œ kann nach Skepsis klingen oder wie aufrichtige Ãœberraschung, und dies sind buchstÃ¤blich entgegengesetzte Bedeutungen).</font></font><br>
<br>
<img src="https://habrastorage.org/webt/6z/vv/od/6zvvodwnihcvdqdqfv4uuprbtb4.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Es sieht aus wie ein Laptop mit einer Reihe von PeripheriegerÃ¤ten zur Spracherkennung mit Toshiba AI (links) und einer Anwendung mit den Ergebnissen fÃ¼r EndgerÃ¤te (rechts). Quelle: Toshiba</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
LSTM hat sich hier als nÃ¼tzlich erwiesen, ohne die die Erkennungsgenauigkeit nicht ausreichte, um den empfangenen Text mÃ¼helos lesen und verstehen zu kÃ¶nnen. DarÃ¼ber hinaus war LSTM nicht nur fÃ¼r die genauere Vorhersage von WÃ¶rtern im Kontext nÃ¼tzlich, sondern auch fÃ¼r die korrekte Verarbeitung von Pausen in der Mitte von SÃ¤tzen und Interjektionsparasiten - dafÃ¼r haben wir dem neuronalen Netzwerk diese Parasiten und Pausen beigebracht, die fÃ¼r die Umgangssprache natÃ¼rlich sind.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bedeutet dies, dass das neuronale Netzwerk jetzt Interjektionen aus Transkripten entfernen kann? </font><font style="vertical-align: inherit;">Ja, das kann es, aber das ist nicht nÃ¶tig. </font><font style="vertical-align: inherit;">Tatsache ist, dass (eine weitere Erkenntnis) Menschen mit HÃ¶rbehinderungen gefÃ¼hrt werden, auch durch die Bewegungen der Lippen des Sprechers. </font><font style="vertical-align: inherit;">Wenn sich die Lippen bewegen, der diesen Bewegungen entsprechende Text jedoch nicht auf dem Bildschirm angezeigt wird, besteht das GefÃ¼hl, dass das Erkennungssystem einen Teil des GesprÃ¤chs verpasst hat. </font><font style="vertical-align: inherit;">Das heiÃŸt, fÃ¼r jemanden, der nicht hÃ¶ren kann, ist es wichtig, so viele Informationen wie mÃ¶glich Ã¼ber das GesprÃ¤ch zu erhalten, einschlieÃŸlich unglÃ¼cklicher Pausen und Mejometia. </font><font style="vertical-align: inherit;">Daher belÃ¤sst die Toshiba-Engine diese Elemente im Transkript, verringert jedoch in Echtzeit die Helligkeit der Buchstaben, wodurch deutlich wird, dass dies optionale Details zum VerstÃ¤ndnis des Textes sind.</font></font><br>
<br>
<div class="oembed"><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">https://www.toshiba-clip.com/de/detail/7655</font></font></a></div><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">So sieht das Erkennungsergebnis im laufenden Betrieb auf dem ClientgerÃ¤t aus. </font><font style="vertical-align: inherit;">Die Teile des Monologs, die nicht aussagekrÃ¤ftig sind, sind grau gestrichen.</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Jetzt arbeitet Toshiba AI mit englischer, japanischer und chinesischer Sprache, und sogar eine Ãœbersetzung zwischen Sprachen im laufenden Betrieb ist mÃ¶glich. </font><font style="vertical-align: inherit;">Es ist nicht erforderlich, es fÃ¼r Kurzschrift im laufenden Betrieb zu verwenden - die KI kann fÃ¼r die Arbeit mit Sprachassistenten angepasst werden, die schlieÃŸlich lernen, Interjektionen, Pausen und Stottern angemessen wahrzunehmen, wenn eine Person einen Befehl ausspricht. </font><font style="vertical-align: inherit;">Im MÃ¤rz 2019 wurde das System erfolgreich eingesetzt, um der in Japan ausgestrahlten IPSJ National Convention Untertitel hinzuzufÃ¼gen. </font><font style="vertical-align: inherit;">In naher Zukunft - die Umwandlung der Toshiba AI in einen Ã¶ffentlichen Dienst und Erfahrungen mit der Implementierung der Spracherkennung in der Produktion.</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de490720/index.html">Motor! oder Was ist Spielphysik?</a></li>
<li><a href="../de490722/index.html">Gender Urlaub in der IT. Wie zu beachten</a></li>
<li><a href="../de490726/index.html">Authentifizierung auf NetzwerkgerÃ¤ten Ã¼ber SSH mit Ã¶ffentlichen SchlÃ¼sseln</a></li>
<li><a href="../de490728/index.html">Sicherheitswoche 10: RSA-Konferenz und Sensibilisierung fÃ¼r Cybersicherheit</a></li>
<li><a href="../de490730/index.html">Intel x86 Root of Trust: Vertrauensverlust</a></li>
<li><a href="../de490734/index.html">Ostereier auf topografischen Karten der Schweiz</a></li>
<li><a href="../de490736/index.html">9 klare Werkzeuge zum Lernen und Pumpen des englischen Wortschatzes</a></li>
<li><a href="../de490738/index.html">Lisk-Substitutionsprinzip</a></li>
<li><a href="../de490740/index.html">Das Defektieren von *** s ist nicht nur eine Randomisierung</a></li>
<li><a href="../de490742/index.html">Eine neue Ã„ra in der Robotik hat begonnen</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>