<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>💪 👗 👆🏾 Event2Mind for the Russian language. How we taught the model to read between the lines and understand the intentions of the interlocutor 🕋 🥋 🏟️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The ability of a model to recognize the interlocutor’s intentions, that is, to understand why a person has performed one or another action, is applica...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Event2Mind for the Russian language. How we taught the model to read between the lines and understand the intentions of the interlocutor</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/sberbank/blog/507178/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The ability of a model to recognize the interlocutor’s intentions, that is, to understand why a person has performed one or another action, is applicable in a large number of applied NLP tasks. For example, chatbots, voice assistants and other dialogue systems will allow you to emotionally respond to the interlocutor’s statements, show understanding, sympathy and other emotions. In addition, the intention of recognizing the problem - this is another step toward understanding human speech ( </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">human Understanding</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ). </font><font style="vertical-align: inherit;">
Several attempts have already been made to solve this problem in one form or another. For example, on </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;">NLP-progress</font></a></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><img src="https://habrastorage.org/webt/uc/aq/ek/ucaqekbmpy4zkuanbo2a6qn_vei.jpeg"></a><br>
<br><font style="vertical-align: inherit;"></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The latest commonsense reasoning advances are published. The weakness of most existing models is that they are based on a supervised approach, that is, they require large, labeled datasets for training. And due to the specificity of the task, the markup is often very non-standard and quite complex. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
For English, there are a number of cases and benchmark'ov, but for the Russian language, the situation with the data is much sadder. The lack of labeled data for the Russian is often one of the main obstacles that prevents the Russification of working English models. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In this post, we will describe how we created a dataset for the Common Sense Reasoning task in one of its possible formulations proposed in the article </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">event2mind</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , and also adapted the English model</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">event2mind</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> from </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">AllenNLP</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> for the Russian language.</font></font><a name="habracut"></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">First, let's talk a bit about what the Common Sense Reasoning task is. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In fact, it would be more correct to consider this as a whole line of tasks aimed at recognizing the intentions and emotions of the actor. She doesn’t have a single wording, and in this post we will take as a basis this version of it proposed by the event2mind authors: in a short text in a free form containing some action or event (for example, </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">“PersonX eats breakfast in the morning”</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ), determine intentions the subject ( </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">“X wants to satisfy hunger”</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ), his emotions / reactions ( </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">“X feels satiated, full”</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ) and possible emotions / reactions of other participants in the event, if any. Figure 1 illustrates this.</font></font><br>
<br>
<p><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Figure 1. The task of Commonsense Reasoning is to determine the intentions, emotions / reactions of the subject and emotions / reactions of others from the short text-event.</font></font></i><br>
</p><div style="text-align:center;"><img src="https://habrastorage.org/webt/fj/nc/xi/fjncxilwif9qw4monmkb7po4j4q.png" width="400"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In the original English article, the authors proposed an event2mind model for solving this problem, as well as a large marked-out case for its training for the English language. </font><font style="vertical-align: inherit;">Our task was to Russify this model, adapting it to the Russian language. </font><font style="vertical-align: inherit;">We then wanted to embed this model in the chatbot in order to teach him to understand the user's intentions and respond correctly to emotions. </font><font style="vertical-align: inherit;">To do this, we collected a dataset for the Russian language, in a format similar to English, and also trained and tested several models of the event2mind architecture on the collected data.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Data, data and again data</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
So, our task was to assemble a body of marked-up texts in free form in a format suitable for training event2mind. </font><font style="vertical-align: inherit;">For simplicity, we will call such short texts simply </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">events</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">At the same time, we tried to maximize the common sense reasoning horizons of the model, collecting </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">events</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> on a variety of topics from everyday life.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Part one. </font><font style="vertical-align: inherit;">Crowdsourced corpus</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
At the first stage, we had to collect a sufficient number of "raw" events for subsequent marking. </font><font style="vertical-align: inherit;">We took three data sources as a basis:</font></font><br>
<br>
<ol>
<li>      .    50      ,   «», «   », «-», «», «»  .       «  »     .       ,        ,       .  ,    ,   « »  « »?             .</li>
<li>  .       1512 .</li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Texts from </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SynTagRus</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , which is part of the Russian National Corps and contains literary texts along with news.</font></font></li>
</ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In the next step, we extracted events from the collected texts. </font><font style="vertical-align: inherit;">Events for training event2mind can be defined as a combination of a verb predicate with the arguments included in the component of the verb. </font><font style="vertical-align: inherit;">To search for such patterns, the </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">UdPipe</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> syntax parser was used </font><font style="vertical-align: inherit;">, with the help of which we selected patterns of the form </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">verb + dependent words in the syntax tree</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , such as in Figure 2, which satisfied one of the following rules:</font></font><br>
<br>
<i><ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">nsubj + root + obj</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">nsubj + root + iobj</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">nsubj + advmod + root</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">nsubj + root + case + obl</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">etc.</font></font></li>
</ul></i><br>
<p><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Figure 2. Syntactic patterns used to extract events from texts</font></font></i><br>
<br>
</p><div style="text-align:center;"><img src="https://habrastorage.org/webt/8u/72/5q/8u725q7ffw8qa2ercspo8bgi8fa.png" width="300"></div><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Selected events are depersonalized. By analogy with the original event2mind, all the actors and named entities were replaced with uniform PersonX, as well as PersonY and PersonZ, if the proposal mentions more than one actor. For recognition of named entities ( </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Named Entity Recognition</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ) and further replacement, we again used UdPipe: in events that correspond to the patterns above, we depersonalized tokens marked with PROPN or PRONOUN tags. In conclusion, we excluded events that did not contain animated subjects. For example, by this criterion the sentence </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">“It is raining”</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> was eliminated </font><font style="vertical-align: inherit;">. As a result, only events with animated named entities (person named entities) entered the corpus.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
After depersonalization and filtering, we used the frequency analysis and </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Levenshtein distance</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> to select the most common </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">events</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> and filter non-standard examples that were encountered only once. </font><font style="vertical-align: inherit;">Firstly, we took all the events that met in the initial sample more than once. </font><font style="vertical-align: inherit;">For the remainder of the data, we calculated the pairwise distances of Levenshtein</font></font><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi><mo stretchy=&quot;false&quot;>(</mo><mi>p</mi><mi>h</mi><mi>r</mi><mi>a</mi><mi>s</mi><msub><mi>e</mi><mn>1</mn></msub><mo>,</mo><mi>p</mi><mi>h</mi><mi>r</mi><mi>a</mi><mi>s</mi><msub><mi>e</mi><mn>2</mn></msub><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="20.456ex" height="2.634ex" viewBox="0 -809.3 8807.5 1134.2" role="img" focusable="false" style="vertical-align: -0.755ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/sberbank/blog/507178/&amp;usg=ALkJrhjynsM26F9lxRRXKQFVghp-2h-8PA#MJMATHI-4C" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/sberbank/blog/507178/&amp;usg=ALkJrhjynsM26F9lxRRXKQFVghp-2h-8PA#MJMAIN-28" x="681" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/sberbank/blog/507178/&amp;usg=ALkJrhjynsM26F9lxRRXKQFVghp-2h-8PA#MJMATHI-70" x="1071" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/sberbank/blog/507178/&amp;usg=ALkJrhjynsM26F9lxRRXKQFVghp-2h-8PA#MJMATHI-68" x="1574" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/sberbank/blog/507178/&amp;usg=ALkJrhjynsM26F9lxRRXKQFVghp-2h-8PA#MJMATHI-72" x="2151" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/sberbank/blog/507178/&amp;usg=ALkJrhjynsM26F9lxRRXKQFVghp-2h-8PA#MJMATHI-61" x="2602" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/sberbank/blog/507178/&amp;usg=ALkJrhjynsM26F9lxRRXKQFVghp-2h-8PA#MJMATHI-73" x="3132" y="0"></use><g transform="translate(3601,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/sberbank/blog/507178/&amp;usg=ALkJrhjynsM26F9lxRRXKQFVghp-2h-8PA#MJMATHI-65" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/sberbank/blog/507178/&amp;usg=ALkJrhjynsM26F9lxRRXKQFVghp-2h-8PA#MJMAIN-31" x="659" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/sberbank/blog/507178/&amp;usg=ALkJrhjynsM26F9lxRRXKQFVghp-2h-8PA#MJMAIN-2C" x="4521" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/sberbank/blog/507178/&amp;usg=ALkJrhjynsM26F9lxRRXKQFVghp-2h-8PA#MJMATHI-70" x="4967" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/sberbank/blog/507178/&amp;usg=ALkJrhjynsM26F9lxRRXKQFVghp-2h-8PA#MJMATHI-68" x="5470" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/sberbank/blog/507178/&amp;usg=ALkJrhjynsM26F9lxRRXKQFVghp-2h-8PA#MJMATHI-72" x="6047" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/sberbank/blog/507178/&amp;usg=ALkJrhjynsM26F9lxRRXKQFVghp-2h-8PA#MJMATHI-61" x="6498" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/sberbank/blog/507178/&amp;usg=ALkJrhjynsM26F9lxRRXKQFVghp-2h-8PA#MJMATHI-73" x="7028" y="0"></use><g transform="translate(7497,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/sberbank/blog/507178/&amp;usg=ALkJrhjynsM26F9lxRRXKQFVghp-2h-8PA#MJMATHI-65" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/sberbank/blog/507178/&amp;usg=ALkJrhjynsM26F9lxRRXKQFVghp-2h-8PA#MJMAIN-32" x="659" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/sberbank/blog/507178/&amp;usg=ALkJrhjynsM26F9lxRRXKQFVghp-2h-8PA#MJMAIN-29" x="8417" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>L</mi><mo stretchy="false">(</mo><mi>p</mi><mi>h</mi><mi>r</mi><mi>a</mi><mi>s</mi><msub><mi>e</mi><mn>1</mn></msub><mo>,</mo><mi>p</mi><mi>h</mi><mi>r</mi><mi>a</mi><mi>s</mi><msub><mi>e</mi><mn>2</mn></msub><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-1">L(phrase_1,phrase_2)</script><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">,), selected pairs for which it did not exceed 5 and took a shorter sentence from each pair. With this method, we were guided by the following consideration: if for a couple of events their Levenshtein distance is small (in this case, threshold 5), then these sentences differ very slightly, for example, in the form of a verb or an adjective. In fact, these are variations of the same </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">event</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . And we chose a shorter sentence from the pair because it would rather contain the initial forms of words (they are often shorter, although not always). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
After collecting the data, the events were to be marked out, highlighting in them the intentions of PersonX, his emotions / reactions, as well as the emotions / reactions of PersonY and PersonZ, if any. To do this, we created a task in </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Yandex.Tolok</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">You can see an example from it in Figure 3. For each event we asked markers:</font></font><br>
<br>
<i><ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Does it contain a meaningful event?</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">whether the text of the event can understand the intentions of the actor;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">whether the event can understand the emotions / reactions of the actor;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">whether this event can cause a surrounding reaction.</font></font></li>
</ul></i><br>
<p><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Figure 3. Example task from Yandex.Tolki</font></font></i><br>
<br>
</p><div style="text-align:center;"><img src="https://habrastorage.org/webt/pd/ef/i8/pdefi8wjewqbjvqzofnavnjz9po.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
To facilitate the work of the tolokers, we prepared tips for them - answers to the trial model event2mind, trained in the translated English case event2mind. </font><font style="vertical-align: inherit;">So they could only verify the answers of the “draft” model and, if possible, propose their own. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
It is worth mentioning that since the translated data turned out to be quite raw, the model trained on them turned out to be weak. </font><font style="vertical-align: inherit;">She did not reach a full-fledged model, but in some cases she was able to accurately guess the emotions and intentions of the subjects. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As a result, we were able to collect 6756 events on various everyday topics.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Part 2. Translated English corpus </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In addition to using the labeled Russian dataset, we partially translated the English corpus with the help of </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Google translator</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , and then filtered and corrected the result. Why was this necessary? It is clear that automatic translation is slightly worse than the original Russian data marked up manually.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The fact is that markup of such a dataset is a labor-intensive business requiring a large amount of resources, money and funds. We simply did not have the opportunity to mark out the case, comparable in size to English, which consists of 46 thousand examples. Since the data set assembled in Russian turned out to be smaller, we decided to evaluate whether there is enough data for training. To do this, we trained the English model on parts of the original case and measured how the quality changes depending on the size of the training dataset. The results are shown in the table. The quality for </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">intent</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> and emotions / reactions ( </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">react</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ) was evaluated, by analogy with the original article, by the metric </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">recall @ 10</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> on validation. </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">recall @ 10</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">reflects the proportion of cases when the true answer - </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">golden standard</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - falls into the top 10 predictions of the model. </font><font style="vertical-align: inherit;">The metric varies from 0 to 1, the more the better.</font></font><br>
<br>
<p><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Table 1. The dependence of the quality of the English model on the size of the case for training</font></font></i><br>
<br>
</p><div style="text-align:center;"><img src="https://habrastorage.org/webt/cz/bj/bn/czbjbnmgz2upluaiscw_0yafz-g.png" width="400"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Immediately we can say that 5000 examples are not enough for a complete model training. However, even with 30,000 examples, loss and recall practically do not differ from the results on the full amount of data. It turns out that the 7000 examples we marked out are not enough for training the model and it is necessary to somehow increase the size of the training sample. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
To do this, we prepared an additional corpus, obtained from English using the automatic </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Google translation by Translator</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . As noted above, in the automatic translation of the entire corpus, some translations turned out to be incorrect or completely lost their meaning. Therefore, we selected that part of the English data that was translated most adequately. Initially, the English corps was assembled from several sources: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ROC Story training set</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">the GoogleSyntactic N-grams</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">the Spinn3r corpus,</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> and idioms. </font><font style="vertical-align: inherit;">However, offers from some sources were easier to translate than from others. </font><font style="vertical-align: inherit;">For example, an adequate translation of idioms without manual editing was beyond the power of a computer. </font><font style="vertical-align: inherit;">Therefore, we took only examples from the ROC-story. </font><font style="vertical-align: inherit;">According to the results of the original article (see table 2), this source has an annotator consistency coefficient ( </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cohen's kappa coefficient</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ) of 0.57. </font><font style="vertical-align: inherit;">And this, most likely, indicates that the events from there are easier to understand and markup, and therefore less prone to errors in the translation.</font></font><br>
<br>
<p><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Table 2 Data and Cohen's kappa coefficient for different sources in the English case</font></font></i><br>
</p><div style="text-align:center;"><img src="https://habrastorage.org/webt/un/ix/s8/unixs8bvmj4oz5d8usft0ak5aro.png" width="350"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
After that, we filtered out the selected data, removed examples from which in which English words and examples which were translated incorrectly remained after the translation. </font><font style="vertical-align: inherit;">The rest was edited by the editors. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
It is worth noting that, despite all the tricks, filtering and editing, the translated case still lags behind the “quality” of sentences from the dataset marked by tolokers. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As a result, we managed to collect 23,409 translated </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">events</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , and the volume of the combined corps, taking into account the marked up Russian part, amounted to 30165. This, as we found out during the experiments, should have been enough to train the Russian model.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">And now - to the experiments! </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
So, the data is collected, it's time to move on to model training and experimentation. </font><font style="vertical-align: inherit;">The event2mind model is a neural network architecture of the form </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">encoder-decoder</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> with one encoder and three decoders for each type of prediction (see Figure 4): the intent of the subject, his emotions / reactions and the emotions / reactions of other participants in the event, if any ( </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">subject's intent, subjects's reaction</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> and other </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">events participants' reactions</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ). </font><font style="vertical-align: inherit;">Original sentences are vectorized using one of the methods of vector representations of words (for example, </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">word2vec</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> or </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">fasttext</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ) and encoded using an encoder into a vector</font></font><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>h</mi><mi>E</mi></msup><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mi>H</mi></msup></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="9.037ex" height="2.509ex" viewBox="0 -970.7 3890.9 1080.4" role="img" focusable="false" style="vertical-align: -0.255ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/sberbank/blog/507178/&amp;usg=ALkJrhjynsM26F9lxRRXKQFVghp-2h-8PA#MJMATHI-68" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/sberbank/blog/507178/&amp;usg=ALkJrhjynsM26F9lxRRXKQFVghp-2h-8PA#MJMATHI-45" x="815" y="513"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/sberbank/blog/507178/&amp;usg=ALkJrhjynsM26F9lxRRXKQFVghp-2h-8PA#MJMAIN-2208" x="1494" y="0"></use><g transform="translate(2440,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/sberbank/blog/507178/&amp;usg=ALkJrhjynsM26F9lxRRXKQFVghp-2h-8PA#MJAMS-52" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/sberbank/blog/507178/&amp;usg=ALkJrhjynsM26F9lxRRXKQFVghp-2h-8PA#MJMATHI-48" x="1021" y="581"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>h</mi><mi>E</mi></msup><mo>∈</mo><msup><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">R</mi></mrow><mi>H</mi></msup></math></span></span><script type="math/tex" id="MathJax-Element-2">h^E\in \mathbb{R}^H</script><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. And then, using three RNN decoders, predictions are generated. Thanks to this, the model can generate answers even for intentions and reactions that it had not seen before. </font></font><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Figure 4. Architecture of the event2mind model</font></font></i><br>
<br>
<p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/i8/ot/dg/i8otdgafbpp00ssxbvxp73kbrpy.png" width="500"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
For experiments, we used the integrated corpus for the Russian language, marked up and translated parts. </font><font style="vertical-align: inherit;">And to make the distribution of Russian and translated examples more even, we additionally mixed the data. </font><font style="vertical-align: inherit;">Note that we also tried to train the model only on labeled data, but due to the small volume of the dataset, it showed very poor results. </font><font style="vertical-align: inherit;">We tested various layers in the encoder - LSTM and GRU, and also tried various vector representations - </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">fasttext</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> and </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">word2vec</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> with </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">RusVectores</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">The results are shown in Table 3, the results for intent and react, as previously calculated using the </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">recall @ 10</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> metric </font><font style="vertical-align: inherit;">.</font></font><br>
<br>
<p><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Table 3. Model results for the Russian language, intent and react were evaluated by recall @ 10</font></font></i><br>
</p><div style="text-align:center;"><img src="https://habrastorage.org/webt/dz/ar/c0/dzarc0uafcs3cokc0ya0dhcvpdu.png" width="600"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
So, what conclusions can be drawn from the results of experiments? </font><font style="vertical-align: inherit;">Firstly, word2vec embeddings turned out to be slightly better than fasttext. </font><font style="vertical-align: inherit;">At the same time, fasttext embeddings trained at ruscorpora proved to be better than trained at araneum. </font><font style="vertical-align: inherit;">Secondly, it can be noted that when using word2vec, GRU in the encoder is better than LSTM. </font><font style="vertical-align: inherit;">And finally, the best model (areneum word2vec + GRU) practically repeats the results for the English language.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">And finally - let's look at real examples! </font></font></h2><br>
<p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/0t/0o/y7/0t0oy7dcsauyxp6d-xsitnkroli.png" width="600"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Very good! </font><font style="vertical-align: inherit;">Intentions look very believable and really reflect some kind of universal human concepts and knowledge. </font><font style="vertical-align: inherit;">But the model coped with emotions / reactions a little worse, they turned out to be more primitive and monotonous. </font><font style="vertical-align: inherit;">Perhaps this is due to the fact that emotions depend on a larger volume of text and a short event text is not enough to determine it correctly.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Instead of a conclusion</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
So, we have created a Russian-language building for training the event2mind model. </font><font style="vertical-align: inherit;">In addition, we conducted experiments that showed that event2mind architecture works for the Russian language, which by its nature is grammatically more complicated than English. </font><font style="vertical-align: inherit;">Despite the complexity of the Russian language, it was possible to achieve a quality comparable to English. </font><font style="vertical-align: inherit;">The best model and data are posted in the </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">repository</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<p><img src="https://habrastorage.org/webt/p3/yn/_k/p3yn_k4sbghk8xbj1nt_3rlvxrk.png" width="700"></p><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Such a large project became possible only thanks to the joint efforts of our team. </font><font style="vertical-align: inherit;">In the adaptation of event2mind for the Russian language also took part</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" class="user_link"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">alenusch</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> and </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" class="user_link"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">onetwotrickster</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../en507166/index.html">Experience in using Rutoken technology for registering and authorizing users in the system (part 3)</a></li>
<li><a href="../en507170/index.html">Are fourteen people viewing this product for sure?</a></li>
<li><a href="../en507172/index.html">What they do at the faculty of low-temperature energy at ITMO University</a></li>
<li><a href="../en507174/index.html">Does the Armed Forces of Russia and other structures of the Ministry of Defense of the Russian Federation need a standard for describing algorithms?</a></li>
<li><a href="../en507176/index.html">MMS system in the data center: how we automated maintenance management</a></li>
<li><a href="../en507182/index.html">Game of Motivations. Part 2</a></li>
<li><a href="../en507188/index.html">Better than silicon: scientists have obtained semiconductor material with more advanced characteristics</a></li>
<li><a href="../en507190/index.html">Bot in no code tools. Implementation details</a></li>
<li><a href="../en507194/index.html">Tester masks (questions for a successful transition to a tester personality disorder)</a></li>
<li><a href="../en507196/index.html">What is the difference between Data Analytics and Statistics</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>