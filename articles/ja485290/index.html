<!doctype html>
<html class="no-js" lang="ja">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ‘¨ğŸ»â€ğŸŒ¾ â¹ï¸ ğŸ‘°ğŸ¾ ã‚·ãƒ³ãƒ—ãƒ«ãªBERTè’¸ç•™ã‚¬ã‚¤ãƒ‰ ğŸ˜ ğŸ§‘ğŸ¿â€ğŸ¤â€ğŸ§‘ğŸ¼ ğŸš´ğŸ»</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="æ©Ÿæ¢°å­¦ç¿’ã«èˆˆå‘³ãŒã‚ã‚‹å ´åˆã¯ã€ãŠãã‚‰ãBERTã¨ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã«ã¤ã„ã¦èã„ãŸã“ã¨ãŒã‚ã‚‹ã§ã—ã‚‡ã†ã€‚
 

BERTã¯GoogleãŒæä¾›ã™ã‚‹è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚Šã€æœ€æ–°ã®çµæœã‚’å¤šæ•°ã®ã‚¿ã‚¹ã‚¯ã«å¤§å¹…ãªãƒãƒ¼ã‚¸ãƒ³ã§ç¤ºã—ã¦ã„ã¾ã™ã€‚BERTã€ãŠã‚ˆã³ä¸€èˆ¬ã«ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã¯ã€è‡ªç„¶è¨€èªå‡¦ç†ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆNLPï¼‰ã®é–‹ç™ºã«ãŠ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>ã‚·ãƒ³ãƒ—ãƒ«ãªBERTè’¸ç•™ã‚¬ã‚¤ãƒ‰</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/avito/blog/485290/"><p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">æ©Ÿæ¢°å­¦ç¿’ã«èˆˆå‘³ãŒã‚ã‚‹å ´åˆã¯ã€ãŠãã‚‰ãBERTã¨ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã«ã¤ã„ã¦èã„ãŸã“ã¨ãŒã‚ã‚‹ã§ã—ã‚‡ã†ã€‚</font></font></p><br>
<p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">BERTã¯GoogleãŒæä¾›ã™ã‚‹è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚Šã€æœ€æ–°ã®çµæœã‚’å¤šæ•°ã®ã‚¿ã‚¹ã‚¯ã«å¤§å¹…ãªãƒãƒ¼ã‚¸ãƒ³ã§ç¤ºã—ã¦ã„ã¾ã™ã€‚</font><font style="vertical-align: inherit;">BERTã€ãŠã‚ˆã³ä¸€èˆ¬ã«ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã¯ã€è‡ªç„¶è¨€èªå‡¦ç†ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆNLPï¼‰ã®é–‹ç™ºã«ãŠã‘ã‚‹ã¾ã£ãŸãæ–°ã—ã„ã‚¹ãƒ†ãƒƒãƒ—ã«ãªã£ã¦ã„ã¾ã™ã€‚</font><font style="vertical-align: inherit;">ãã‚Œã‚‰ã«é–¢ã™ã‚‹è¨˜äº‹ã¨ã•ã¾ã–ã¾ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®ã€Œåœ°ä½ã€ã¯</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ã€Papers With Codeã®Webã‚µã‚¤ãƒˆã«ã‚ã‚Š</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ã¾ã™ã€‚</font></font></p><br>
<p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">BERTã«ã¯1ã¤ã®å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚ç”£æ¥­ã‚·ã‚¹ãƒ†ãƒ ã§ã®ä½¿ç”¨ã«ã¯å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚</font><font style="vertical-align: inherit;">BERTãƒ™ãƒ¼ã‚¹ã«ã¯110Mã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã€BERTãƒ©ãƒ¼ã‚¸-340MãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚</font><font style="vertical-align: inherit;">ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒéå¸¸ã«å¤šã„ãŸã‚ã€ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’æºå¸¯é›»è©±ãªã©ã®ãƒªã‚½ãƒ¼ã‚¹ãŒé™ã‚‰ã‚Œã¦ã„ã‚‹ãƒ‡ãƒã‚¤ã‚¹ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã“ã¨ã¯å›°é›£ã§ã™ã€‚</font><font style="vertical-align: inherit;">ã•ã‚‰ã«ã€æ¨è«–ã®æ™‚é–“ãŒé•·ã„ãŸã‚ã€ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯å¿œç­”é€Ÿåº¦ãŒé‡è¦ãªå ´åˆã«ã¯é©ã—ã¦ã„ã¾ã›ã‚“ã€‚</font><font style="vertical-align: inherit;">ã—ãŸãŒã£ã¦ã€BERTã‚’åŠ é€Ÿã™ã‚‹æ–¹æ³•ã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ã¯éå¸¸ã«ãƒ›ãƒƒãƒˆãªãƒˆãƒ”ãƒƒã‚¯ã§ã™ã€‚</font></font></p><br>
<p> &nbsp;      .      ,   .      - .    &nbsp;  BERT &nbsp;   . &nbsp;  ,        &nbsp; BERT,        .</p><br>
<p><img src="https://habrastorage.org/webt/c_/po/z2/c_poz2e3dkggx7ekt3gn_9wva3g.png"></p><a name="habracut"></a><br>
<h2 id="knowledge-distillation-kak-metod-uskoreniya-neyronnyh-setey">Knowledge distillation     </h2><br>
<p>   /  .    ,   ,  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=">&nbsp; Intento  </a>. </p><br>
<p>      :</p><br>
<ol>
<li>  .</li>
<li>  (quantization, pruning).</li>
<li>Knowledge distillation.</li>
</ol><br>
<p>       , &nbsp;  .       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=">&nbsp; â€œModel Compressionâ€</a>.   :    ,     -    . &nbsp;    BERT,  â€”   .</p><br>
<h2 id="zadacha"></h2><br>
<p>   &nbsp;  .    SST-2 &nbsp;  , &nbsp;   &nbsp;NLP.</p><br>
<p>       &nbsp;IMDb &nbsp; &nbsp;  â€”   . &nbsp;      accuracy.</p><br>
<h2 id="obuchenie-bert-based-modeli-ili-uchitelya"> BERT-based   Â«Â»</h2><br>
<p>    Â«Â» BERT-based ,   .      â€”   &nbsp;BERT     ,    &nbsp;.</p><br>
<p> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="> tranformers</a>    ,        BertForSequenceClassification. &nbsp; ,          <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://towardsdatascience.com/"> Thilina Rajapakse  Towards Data Science</a>. </p><br>
<p> ,      BertForSequenceClassification. &nbsp;  num_labels=2,   &nbsp;  .      &nbsp; Â«Â».</p><br>
<h2 id="obuchenie-uchenika"> Â«Â»</h2><br>
<p>&nbsp;     :  ,  ,  .  &nbsp;    BiLSTM. &nbsp;  BiLSTM  BERT. </p><br>
<p>  &nbsp;   ,    &nbsp; .  &nbsp;   â€”       &nbsp;.    &nbsp;-n    &nbsp;     : â€œpadâ€ â€” Â«-Â»,      ,  â€œunkâ€ â€” &nbsp; &nbsp; .   &nbsp;    &nbsp;torchtext.    &nbsp;    .<br>
&nbsp;</p><br>
<pre><code class="python hljs"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> torchtext <span class="hljs-keyword">import</span> data<font></font>
<font></font>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_vocab</span>(<span class="hljs-params">X</span>):</span>
    X_split = [t.split() <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> X]<font></font>
    text_field = data.Field()<font></font>
    text_field.build_vocab(X_split, max_size=<span class="hljs-number">10000</span>)
    <span class="hljs-keyword">return</span> text_field<font></font>
<font></font>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">pad</span>(<span class="hljs-params">seq, max_len</span>):</span>
    <span class="hljs-keyword">if</span> len(seq) &lt; max_len:<font></font>
        seq = seq + [<span class="hljs-string">'&lt;pad&gt;'</span>] * (max_len - len(seq))
    <span class="hljs-keyword">return</span> seq[<span class="hljs-number">0</span>:max_len]<font></font>
<font></font>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">to_indexes</span>(<span class="hljs-params">vocab, words</span>):</span>
    <span class="hljs-keyword">return</span> [vocab.stoi[w] <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> words]<font></font>
<font></font>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">to_dataset</span>(<span class="hljs-params">x, y, y_real</span>):</span><font></font>
    torch_x = torch.tensor(x, dtype=torch.long)<font></font>
    torch_y = torch.tensor(y, dtype=torch.float)<font></font>
    torch_real_y = torch.tensor(y_real, dtype=torch.long)<font></font>
    <span class="hljs-keyword">return</span> TensorDataset(torch_x, torch_y, torch_real_y)</code></pre><br>
<h3 id="model-bilstm"> BiLSTM</h3><br>
<p> &nbsp;   :</p><br>
<pre><code class="python hljs"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn
<span class="hljs-keyword">from</span> torch.autograd <span class="hljs-keyword">import</span> Variable<font></font>
<font></font>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">SimpleLSTM</span>(<span class="hljs-params">nn.Module</span>):</span><font></font>
<font></font>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers,
                 bidirectional, dropout, batch_size, device=None</span>):</span><font></font>
        super(SimpleLSTM, self).__init__()<font></font>
        self.batch_size = batch_size<font></font>
        self.hidden_dim = hidden_dim<font></font>
        self.n_layers = n_layers<font></font>
        self.embedding = nn.Embedding(input_dim, embedding_dim)<font></font>
<font></font>
        self.rnn = nn.LSTM(embedding_dim,<font></font>
                           hidden_dim,<font></font>
                           num_layers=n_layers,<font></font>
                           bidirectional=bidirectional,<font></font>
                           dropout=dropout)<font></font>
<font></font>
        self.fc = nn.Linear(hidden_dim * <span class="hljs-number">2</span>, output_dim)<font></font>
        self.dropout = nn.Dropout(dropout)<font></font>
        self.device = self.init_device(device)<font></font>
        self.hidden = self.init_hidden()<font></font>
<font></font>
<span class="hljs-meta">    @staticmethod</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">init_device</span>(<span class="hljs-params">device</span>):</span>
        <span class="hljs-keyword">if</span> device <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:
            <span class="hljs-keyword">return</span> torch.device(<span class="hljs-string">'cuda'</span>)
        <span class="hljs-keyword">return</span> device<font></font>
<font></font>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">init_hidden</span>(<span class="hljs-params">self</span>):</span>
        <span class="hljs-keyword">return</span> (Variable(torch.zeros(<span class="hljs-number">2</span> * self.n_layers, self.batch_size, self.hidden_dim).to(self.device)),<font></font>
                Variable(torch.zeros(<span class="hljs-number">2</span> * self.n_layers, self.batch_size, self.hidden_dim).to(self.device)))<font></font>
<font></font>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, text, text_lengths=None</span>):</span><font></font>
        self.hidden = self.init_hidden()<font></font>
        x = self.embedding(text)<font></font>
        x, self.hidden = self.rnn(x, self.hidden)<font></font>
        hidden, cell = self.hidden<font></font>
        hidden = self.dropout(torch.cat((hidden[<span class="hljs-number">-2</span>, :, :], hidden[<span class="hljs-number">-1</span>, :, :]), dim=<span class="hljs-number">1</span>))<font></font>
        x = self.fc(hidden)<font></font>
<font></font>
        <span class="hljs-keyword">return</span> x</code></pre><br>
<h3 id="obuchenie"></h3><br>
<p>&nbsp;      (batch_size, output_dim). &nbsp;    logloss. &nbsp;PyTorch   BCEWithLogitsLoss,     -. ,  .</p><br>
<pre><code class="python hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">loss</span>(<span class="hljs-params">self, output, bert_prob, real_label</span>):</span><font></font>
    criterion = torch.nn.BCEWithLogitsLoss()<font></font>
    <span class="hljs-keyword">return</span> criterion(output, real_label.float())</code></pre><br>
<p> &nbsp;  :</p><br>
<pre><code class="python hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_optimizer</span>(<span class="hljs-params">model</span>):</span><font></font>
    optimizer = torch.optim.Adam(model.parameters())<font></font>
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, <span class="hljs-number">2</span>, gamma=<span class="hljs-number">0.9</span>)
    <span class="hljs-keyword">return</span> optimizer, scheduler<font></font>
<font></font>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">epoch_train_func</span>(<span class="hljs-params">model, dataset, loss_func, batch_size</span>):</span>
    train_loss = <span class="hljs-number">0</span><font></font>
    train_sampler = RandomSampler(dataset)<font></font>
    data_loader = DataLoader(dataset, sampler=train_sampler,<font></font>
                             batch_size=batch_size,<font></font>
                             drop_last=<span class="hljs-literal">True</span>)<font></font>
    model.train()<font></font>
    optimizer, scheduler = get_optimizer(model)<font></font>
    <span class="hljs-keyword">for</span> i, (text, bert_prob, real_label) <span class="hljs-keyword">in</span> enumerate(tqdm(data_loader, desc=<span class="hljs-string">'Train'</span>)):<font></font>
        text, bert_prob, real_label = to_device(text, bert_prob, real_label)<font></font>
        model.zero_grad()<font></font>
        output = model(text.t(), <span class="hljs-literal">None</span>).squeeze(<span class="hljs-number">1</span>)<font></font>
        loss = loss_func(output, bert_prob, real_label)<font></font>
        loss.backward()<font></font>
        optimizer.step()<font></font>
        train_loss += loss.item()<font></font>
    scheduler.step()<font></font>
    <span class="hljs-keyword">return</span> train_loss / len(data_loader)</code></pre><br>
<p>    :</p><br>
<pre><code class="python hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">epoch_evaluate_func</span>(<span class="hljs-params">model, eval_dataset, loss_func, batch_size</span>):</span><font></font>
    eval_sampler = SequentialSampler(eval_dataset)<font></font>
    data_loader = DataLoader(eval_dataset, sampler=eval_sampler,<font></font>
                             batch_size=batch_size,<font></font>
                             drop_last=<span class="hljs-literal">True</span>)<font></font>
<font></font>
    eval_loss = <span class="hljs-number">0.0</span><font></font>
    model.eval()<font></font>
    <span class="hljs-keyword">for</span> i, (text, bert_prob, real_label) <span class="hljs-keyword">in</span> enumerate(tqdm(data_loader, desc=<span class="hljs-string">'Val'</span>)):<font></font>
        text, bert_prob, real_label = to_device(text, bert_prob, real_label)<font></font>
        output = model(text.t(), <span class="hljs-literal">None</span>).squeeze(<span class="hljs-number">1</span>)<font></font>
        loss = loss_func(output, bert_prob, real_label)<font></font>
        eval_loss += loss.item()<font></font>
<font></font>
    <span class="hljs-keyword">return</span> eval_loss / len(data_loader)</code></pre><br>
<p>    , &nbsp;   &nbsp; :</p><br>
<pre><code class="python hljs"><span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> (TensorDataset, random_split,<font></font>
                              RandomSampler, DataLoader,<font></font>
                              SequentialSampler)<font></font>
<span class="hljs-keyword">from</span> torchtext <span class="hljs-keyword">import</span> data
<span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<font></font>
<font></font>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">device</span>():</span>
    <span class="hljs-keyword">return</span> torch.device(<span class="hljs-string">"cuda"</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">"cpu"</span>)<font></font>
<font></font>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">to_device</span>(<span class="hljs-params">text, bert_prob, real_label</span>):</span><font></font>
    text = text.to(device())<font></font>
    bert_prob = bert_prob.to(device())<font></font>
    real_label = real_label.to(device())<font></font>
    <span class="hljs-keyword">return</span> text, bert_prob, real_label<font></font>
<font></font>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">LSTMBaseline</span>(<span class="hljs-params">object</span>):</span>
    vocab_name = <span class="hljs-string">'text_vocab.pt'</span>
    weights_name = <span class="hljs-string">'simple_lstm.pt'</span><font></font>
<font></font>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, settings</span>):</span><font></font>
        self.settings = settings<font></font>
        self.criterion = torch.nn.BCEWithLogitsLoss().to(device())<font></font>
<font></font>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">loss</span>(<span class="hljs-params">self, output, bert_prob, real_label</span>):</span>
        <span class="hljs-keyword">return</span> self.criterion(output, real_label.float())<font></font>
<font></font>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">model</span>(<span class="hljs-params">self, text_field</span>):</span><font></font>
        model = SimpleLSTM(<font></font>
            input_dim=len(text_field.vocab),<font></font>
            embedding_dim=<span class="hljs-number">64</span>,<font></font>
            hidden_dim=<span class="hljs-number">128</span>,<font></font>
            output_dim=<span class="hljs-number">1</span>,<font></font>
            n_layers=<span class="hljs-number">1</span>,<font></font>
            bidirectional=<span class="hljs-literal">True</span>,<font></font>
            dropout=<span class="hljs-number">0.5</span>,<font></font>
            batch_size=self.settings[<span class="hljs-string">'train_batch_size'</span>])
        <span class="hljs-keyword">return</span> model<font></font>
<font></font>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span>(<span class="hljs-params">self, X, y, y_real, output_dir</span>):</span>
        max_len = self.settings[<span class="hljs-string">'max_seq_length'</span>]<font></font>
        text_field = get_vocab(X)<font></font>
<font></font>
        X_split = [t.split() <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> X]<font></font>
        X_pad = [pad(s, max_len) <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> tqdm(X_split, desc=<span class="hljs-string">'pad'</span>)]<font></font>
        X_index = [to_indexes(text_field.vocab, s) <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> tqdm(X_pad, desc=<span class="hljs-string">'to index'</span>)]<font></font>
<font></font>
        dataset = to_dataset(X_index, y, y_real)<font></font>
        val_len = int(len(dataset) * <span class="hljs-number">0.1</span>)<font></font>
        train_dataset, val_dataset = random_split(dataset, (len(dataset) - val_len, val_len))<font></font>
<font></font>
        model = self.model(text_field)<font></font>
        model.to(device())<font></font>
<font></font>
        self.full_train(model, train_dataset, val_dataset, output_dir)<font></font>
        torch.save(text_field, os.path.join(output_dir, self.vocab_name))<font></font>
<font></font>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">full_train</span>(<span class="hljs-params">self, model, train_dataset, val_dataset, output_dir</span>):</span><font></font>
        train_settings = self.settings<font></font>
        num_train_epochs = train_settings[<span class="hljs-string">'num_train_epochs'</span>]<font></font>
        best_eval_loss = <span class="hljs-number">100000</span>
        <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(num_train_epochs):<font></font>
            train_loss = epoch_train_func(model, train_dataset, self.loss, self.settings[<span class="hljs-string">'train_batch_size'</span>])<font></font>
            eval_loss = epoch_evaluate_func(model, val_dataset, self.loss, self.settings[<span class="hljs-string">'eval_batch_size'</span>])<font></font>
<font></font>
            <span class="hljs-keyword">if</span> eval_loss &lt; best_eval_loss:<font></font>
                best_eval_loss = eval_loss<font></font>
                torch.save(model.state_dict(), os.path.join(output_dir, self.weights_name))</code></pre><br>
<h3 id="distillyaciya"></h3><br>
<p>     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=">&nbsp;    </a>.    , Â«Â»     Â«Â».    ? &nbsp;    - &nbsp; .    â€”    &nbsp;  . ,        ,    &nbsp; . </p><br>
<p>&nbsp;   &nbsp;   ,    &nbsp; Â«Â» â€” MSE   .</p><br>
<p><img src="https://habrastorage.org/webt/hx/_d/w9/hx_dw9ypkcwc_fhgui_jcappoo4.png"></p><br>
<p>      :     &nbsp;1 &nbsp;2    .</p><br>
<pre><code class="python hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">loss</span>(<span class="hljs-params">self, output, bert_prob, real_label</span>):</span>
    a = <span class="hljs-number">0.5</span><font></font>
    criterion_mse = torch.nn.MSELoss()<font></font>
    criterion_ce = torch.nn.CrossEntropyLoss()<font></font>
    <span class="hljs-keyword">return</span> a*criterion_ce(output, real_label) + (<span class="hljs-number">1</span>-a)*criterion_mse(output, bert_prob)</code></pre><br>
<p>   ,   ,     loss: </p><br>
<pre><code class="python hljs">
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">LSTMDistilled</span>(<span class="hljs-params">LSTMBaseline</span>):</span>
    vocab_name = <span class="hljs-string">'distil_text_vocab.pt'</span>
    weights_name = <span class="hljs-string">'distil_lstm.pt'</span><font></font>
<font></font>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, settings</span>):</span><font></font>
        super(LSTMDistilled, self).__init__(settings)<font></font>
        self.criterion_mse = torch.nn.MSELoss()<font></font>
        self.criterion_ce = torch.nn.CrossEntropyLoss()<font></font>
        self.a = <span class="hljs-number">0.5</span><font></font>
<font></font>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">loss</span>(<span class="hljs-params">self, output, bert_prob, real_label</span>):</span>
        <span class="hljs-keyword">return</span> self.a * self.criterion_ce(output, real_label) + (<span class="hljs-number">1</span> - self.a) * self.criterion_mse(output, bert_prob)<font></font>
<font></font>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">model</span>(<span class="hljs-params">self, text_field</span>):</span><font></font>
        model = SimpleLSTM(<font></font>
            input_dim=len(text_field.vocab),<font></font>
            embedding_dim=<span class="hljs-number">64</span>,<font></font>
            hidden_dim=<span class="hljs-number">128</span>,<font></font>
            output_dim=<span class="hljs-number">2</span>,<font></font>
            n_layers=<span class="hljs-number">1</span>,<font></font>
            bidirectional=<span class="hljs-literal">True</span>,<font></font>
            dropout=<span class="hljs-number">0.5</span>,<font></font>
            batch_size=self.settings[<span class="hljs-string">'train_batch_size'</span>])
        <span class="hljs-keyword">return</span> model</code></pre><br>
<p>  ,     Â«Â». </p><br>
<h3 id="sravnenie-modeley"> </h3><br>
<p>&nbsp;     &nbsp;SST-2  &nbsp;a=0,     , &nbsp;  . Accuracy   ,  &nbsp;BERT,     BiLSTM.</p><br>
<p><img src="https://habrastorage.org/webt/0m/y6/g7/0my6g7eahypj6eq3o52arxxldxq.png"></p><br>
<p>    &nbsp;,  &nbsp;     &nbsp;a=0,5.</p><br>
<p>   loss  accuracy &nbsp; LSTM  .  &nbsp; loss,   , &nbsp;-     . </p><br>
<p><img src="https://habrastorage.org/webt/lk/bg/rn/lkbgrnank6obtu7pewsoalba9yk.png"></p><br>
<p> &nbsp;:</p><br>
<p><img src="https://habrastorage.org/webt/gl/u5/7g/glu57giappdlhkc31wlpdulsdkc.png"></p><br>
<p> BiLSTM   . ,      ,     .    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=">   </a>.</p><br>
<h2 id="zaklyuchenie"></h2><br>
<p>&nbsp;        .      &nbsp; .  &nbsp;    &nbsp;  . &nbsp;  &nbsp;  ,     &nbsp;,    .</p></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../ja485278/index.html">ç¥ã®æ‰‹ã€‚ã‚¯ãƒ¼ãƒãƒ³ãƒ˜ãƒ«ãƒ—</a></li>
<li><a href="../ja485280/index.html">è¡Œã Fakedb ãƒ†ã‚¹ãƒˆã§ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¨ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³</a></li>
<li><a href="../ja485284/index.html">SPIKEâ„¢PrimeLEGOÂ®Educationã®æ©Ÿèƒ½</a></li>
<li><a href="../ja485286/index.html">å•†å“ã¾ãŸã¯å°ã•ãªè‡ªå‹•åŒ–ã‚ªãƒ¼ãƒ‰ã®é‡ã•</a></li>
<li><a href="../ja485288/index.html">ã‚¤ãƒ³ãƒ‡ã‚£ãƒ¼ã‚ºã‚²ãƒ¼ãƒ ã‚’å«Œã„</a></li>
<li><a href="../ja485294/index.html">2020å¹´ã®Node.jsã«é–¢ã™ã‚‹æœ€æ–°ã®ã‚³ãƒ¼ã‚¹</a></li>
<li><a href="../ja485298/index.html">ç¥ç§˜çš„ãªLyXãƒ—ãƒ­ã‚°ãƒ©ãƒ ã€‚ãƒ‘ãƒ¼ãƒˆ4</a></li>
<li><a href="../ja485300/index.html">Redis RCEã‚’æ‚ªç”¨ã™ã‚‹H2Minerãƒ¯ãƒ¼ãƒ ã®æ–°ãŸãªç™ºç”ŸãŒç™ºè¦‹ã•ã‚Œã¾ã—ãŸ</a></li>
<li><a href="../ja485304/index.html">ã„ãã¤ã‹ã®iframeãƒˆãƒªãƒƒã‚¯</a></li>
<li><a href="../ja485316/index.html">ã™ã¹ã¦ã®Google SERPãŒåºƒå‘Šã®ã‚ˆã†ã«è¦‹ãˆã‚‹</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>