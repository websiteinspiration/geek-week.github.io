<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèΩ‚Äçü§ù‚Äçüë©üèº üë©‚Äç‚úàÔ∏è ü§πüèº Why hyperconvergence? Cisco HyperFlex Overview and Tests ‚úäüèΩ ‚ö™Ô∏è üëºüèæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In IT, the main thing is three letters
 The task of any IT infrastructure is to provide a reliable platform for the company's business processes. It i...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Why hyperconvergence? Cisco HyperFlex Overview and Tests</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/cti/blog/497024/"><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In IT, the main thing is three letters</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The task of any IT infrastructure is to provide a reliable platform for the company's business processes. It is traditionally believed that the quality of the information technology infrastructure is assessed according to three main parameters: accessibility, security, reliability. However, the assessment for this triple is in no way connected with the business and the direct income / loss of the company. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Three main letters rule IT. If the letters ‚ÄúRUB‚Äù are not at the head of the IT hierarchy, then you are building your IT infrastructure incorrectly. Of course, it is difficult to build IT directly, starting only from income / expenses, therefore there is a hierarchy of ‚Äúthree letters‚Äù - from the most important to the more private. SLA, RPO, RTO, GRC - all this is known to industry experts and has long been used in building infrastructures. Unfortunately, not always linking these indicators into an end-to-end hierarchy.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/oj/wm/6n/ojwm6n6_0mqymccxwojckh8q4fi.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Many companies today are building infrastructure for the future using yesterday‚Äôs technology on yesterday‚Äôs architecture. </font><font style="vertical-align: inherit;">And at the same time, the accelerating development of IT shows that modern services are fundamentally changing not only business but also society - people of the digital age are used to the fact that a few seconds are enough to access any information. </font><font style="vertical-align: inherit;">IT from an incomprehensible technology has become commonplace for the masses, such as a burger or coffee shop. </font><font style="vertical-align: inherit;">This has added new extremely important three letters to IT. </font><font style="vertical-align: inherit;">These letters - TTM (Time to market) - the time before the launch of a productive service on the market.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/ot/k1/ns/otk1nsmpnwofr1v2bzlexp5a9j8.png"><br>
<a name="habracut"></a><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sds</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
On the other hand, a kraken rose from the depths of technology, turning over traditional IT and lifestyle. As the computing power of x86 processors grew, software storage systems became the first tentacle. Classic storage systems were very specific pieces of iron filled with ‚Äúcustom silicon‚Äù, various proprietary hardware accelerators, and specialized software. And it was administered by a specially trained person who was practically worshiped in the company as a priest of a dark cult. To expand the storage system operating in the company was a whole project, with a lot of calculations and approvals - after all, it‚Äôs expensive!</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The high cost and complexity spurred the creation of software storage systems on top of the usual x86 hardware with a common general purpose OS - Windows, Linux, FreeBSD or Solaris. Only software remained from the complex custom hardware, working not even in the kernel, but at the user level. The first software systems were of course quite simple and limited in functionality, often they were specialized niche solutions, but time passed. And now even large storage system vendors have begun to abandon specialized hardware solutions - TTM for such systems could no longer withstand competition, and the cost of the error became very high. In fact, with rare exceptions, even classic storage systems by 2020 became the most common x86 servers, just with beautiful plastic muzzles and a bunch of disk shelves.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The second tentacle of the approaching kraken is the appearance and massive adoption by the market of flash memory technology, which has become a concrete pillar breaking the back of an elephant.</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The performance of magnetic disks has not changed for many years and the processors of storage controllers completely coped with hundreds of disks. </font><font style="vertical-align: inherit;">But alas, the quantity will sooner or later turn into quality - and the storage system is already at an average level, not to mention the initial one, it has a upper limit on the meaningful number of flash drives. </font><font style="vertical-align: inherit;">With a certain amount (literally from ten disks), the system performance does not stop growing, but it can also begin to decline due to the need to process an ever larger volume. </font><font style="vertical-align: inherit;">After all, the processing power and throughput of the controllers does not change with increasing capacity. </font><font style="vertical-align: inherit;">The solution, in theory, was the emergence of scale-out systems that can assemble many independent shelves with disks and processor resources into a single cluster that looks from the outside as a single multi-controller storage system. </font><font style="vertical-align: inherit;">There was only one step left.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hyper convergence</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The most obvious step into the future was the unification of previously disparate data storage and processing points. In other words, why not implement distributed storage not on separate servers, but directly on the virtualization hosts, thereby refusing a special storage network and dedicated hardware, and thus combining functions. The kraken woke up. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
But let me say, you see, because combination is convergence. Where did this stupid prefix hyper come from?</font></font><br>
<blockquote>        .        +  + .    .    ,      ‚Äú ‚Äù.<br>
‚Ä¶<br>
 ,  ,            ,        .       ‚Äî       SDS.<br>
<br>
:<br>
<br>
<ul>
<li>  ‚Äî , ,       ,   /.    .</li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Converged system - all from one source, one support, one partner number. </font><font style="vertical-align: inherit;">Not to be confused with self-assembly from one vendor.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
And it turns out that the term for our converged architecture is already taken. </font><font style="vertical-align: inherit;">Exactly the same situation as with the supervisor. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Hyperconverged System - A converged system with converged architecture.</font></font></blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The definitions are taken from the article ‚Äú </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">General Theory and Archeology of Virtualization</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ‚Äù, in the writing of which I took a lively part. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
What gives the hyperconverged approach in the application to the three letters mentioned?</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Start with a minimum volume (and minimum cost)</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Storage capacity grows with computing power</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Each node of the system is its controller - and the problem of the ‚Äúglass ceiling‚Äù is removed (disks can, but the controller no longer exists)</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Storage management simplified dramatically</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
For the last paragraph, hyperconverged systems are very disliked by old-mode storage administrators who are used to administer queues on Fiber Channel ports. </font><font style="vertical-align: inherit;">Space is allocated in just a few clicks of the mouse from the virtual infrastructure management console. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In other words, only clouds are faster than hyperconverged systems in launching a product, but clouds are not suitable for everyone and / or not always. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
If you are a techie administrator and read up to here - rejoice, the general words have ended and now I will tell you about my personal view of the Cisco Hyperflex system, which I got in tenacious paws for conducting various tests on it.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cisco Hyperflex</font></font></h2><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Why Cisco</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Cisco is known primarily as the dominant vendor in the network equipment market, but at the same time it is quite widely present in other segments of the data center market, offering both server and hyperconverged solutions, as well as automation and control systems. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Surprisingly, by 2020, there are still people: ‚ÄúCisco servers? And who does she take them from? ‚Äù</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Cisco began to deal with servers already in 2009, choosing the path of actively growing blade solutions at that time. The idea of ‚Äã‚ÄãCisco was to implement the approach of anonymous calculators. The result was a UCS (Unified Computing System) system consisting of two specialized switches (they were called Fabric Interconnect), and from 1 to 20 chassis (8 half-sized blades) or up to 160 servers. At the same time, the chassis became generally stupid with a piece of iron with power, all the logic and switching are made in Fabric Interconnect; chassis is just a way to host servers and connect them to the system. Fabric Interconnect is fully responsible for all server interactions with the outside world ‚Äî Ethernet, FC, and management. It would seem that the blades and blades, what is there, except for external switching, and not like everyone else in the chassis.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A key moment in the implementation of those same ‚Äúanonymous calculators‚Äù. As part of the Cisco UCS concept, servers have no personality other than a serial number. Neither MAC, nor WWN, nor anything else. The UCS management system powered by Fabric Interconnect is based on server profiles and templates. After connecting a bundle of servers in the chassis, they need to be assigned an appropriate profile, within which all identifying addresses and identifiers are set. Of course, if you have only a dozen servers, then the game would not be worth it. But when there are at least two, or even three dozen of them, this is a serious advantage. It becomes easy and quick to migrate configurations, or, more importantly, to replicate server configurations in the right amount, apply the changes immediately to a large number of servers,essentially managing a set of servers (for example, a virtualization farm) as a single entity. The approach proposed within the UCS system allows, with the right approach, to seriously simplify the life of administrators, increase flexibility and significantly reduce risks, so UCS blades literally in 2-3 years have become the best-selling blade platform in the Western Hemisphere, and today they are globally one of two dominant platforms, along with HPE.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
It quickly became clear that the same approach based on a universal factory with integrated management based on policies and templates is fully in demand and applies not only to blades, but also to rack servers. And in this sense, the Cisco rack-mount servers connected to Fabric Interconnect get all the same benefits that make blades so popular. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Today I will talk about HyperFlex, a Cisco hyperconverged solution built on rack-mount servers connected to Fabric Interconnect. What makes HyperFlex interesting and worth considering in the review:</font></font><br>
<br>
<ul>
<li>  Cisco   ,  , ¬´¬ª ‚Äì        ,      HyperFlex;     , ,   ,         HyperFlex    ;</li>
<li>   ‚Äì          ;  HyperFlex  ,    ,      ;          ,       .</li>
<li> ¬´  ¬ª ‚Äî     ¬´ ¬ª,       ,       ;</li>
<li>           Fabric Interconnect    Cisco   -,     SAN ,       native FC;</li>
<li>    ‚Äú‚Äù ‚Äì  ,  ,     ;</li>
<li>   Cisco      ,   ,            ,      ;</li>
<li>  ,      , Cisco        HCI,      , HyperFlex      ,   ,   .</li>
</ul><br>
<h3> </h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
HyperFlex is a true hyperconverged system with dedicated controller VMs. </font><font style="vertical-align: inherit;">Let me remind you that the main advantage of such an architecture is its potential portability for different hypervisors. </font><font style="vertical-align: inherit;">Today, Cisco has implemented support for VMware ESXi and Microsoft Hyper-V, but it is possible that one of the KVM options will appear as its popularity grows in the corporate segment. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Consider the mechanism of work on the example of ESXi. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Devices using VM_DIRECT_PATH technology ‚Äî cache disk and storage level disks ‚Äî are directly thrown to the controller VM (hereinafter CVM). </font><font style="vertical-align: inherit;">Therefore, we exclude the effect of the hypervisor disk stack on performance. </font><font style="vertical-align: inherit;">Additional VIB packets are installed in the hypervisor itself:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">IO Visor: provides the mount point for the NFS datastore for the hypervisor</font></font></li>
<li>VAAI:   VMware  API  ¬´ ¬ª </li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Virtual disk blocks are distributed evenly across all hosts in a cluster with relatively little granularity. When the VM on the host performs some disk operations, through the disk stack of the hypervisor the operation goes to the datastore, then to IO Visor, and then it turns to the CVM responsible for these blocks. In this case, CVM can be located on any host in the cluster. Given the very limited resources of IO Visor, there are of course no metadata tables and the choice is mathematically determined. Next, the CVM that the request came to processes it. In the case of reading, it sends data either from one of the cache levels (RAM, write cache, read cache) or from the disks of its host. In the case of recording, it writes to the local journal, and duplicates the operation for one (RF2) or two (RF3) CVM.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/k_/hq/eq/k_hqeqytlifhbevdrf8r7mk_rv0.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Perhaps this is quite enough to understand the principle of work within the framework of this publication, otherwise I will take bread from Cisco trainers, and I will be ashamed. </font><font style="vertical-align: inherit;">Not really, but still enough.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Question about synthetic tests</font></font></h3><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
- Navigator, appliances! </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
- 36! </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
- What is 36? </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
- What about appliances? </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Something like this today looks like most synthetic tests of storage systems. Why is that? </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Until relatively recently, most storage systems were flat with uniform access. What does this mean? </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The total available disk space was collected from disks with the same characteristics. For example, 300 15k drives. And the performance was the same throughout the space. With the advent of tiered storage technology, storage systems have become non-flat - performance varies within a single disk space. And it‚Äôs not just different, but also unpredictable, depending on the algorithms and capabilities of a particular storage model.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
And everything would not be so interesting if hyperconverged systems with data localization did not appear. </font><font style="vertical-align: inherit;">In addition to the unevenness of the disk space itself (tirings, flash caches), there is also uneven access to it - depending on whether one of the data copies is located on the local disks of the node or it must be accessed over the network. </font><font style="vertical-align: inherit;">All this leads to the fact that the numbers of synthetic tests can be absolutely any, and not speak about anything practically meaningful. </font><font style="vertical-align: inherit;">For example, the fuel consumption of a car according to an advertising brochure that you can never achieve in real life.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Question about sizing</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The flip side of the synthetic test numbers was sizing numbers and specifications from under the presale keyboard. Presales in this case are divided into two categories - some just stupidly hammer your TK into the vendor‚Äôs configurator, and the second will take it themselves, because they understand how it works. But with the second you will have to consider in detail what you wrote in your TK. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As you know, without a clear TK - the result of HZ. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/nk/li/uq/nkliuqyn2iv0ogsigw-nd_m94ky.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
From practical experience - when sizing a rather heavy hyperconverged system in a competition with one of the customers, I personally, after the pilot, took the load indicators from the system and compared them with what was written in the TOR. It turned out like in a joke:</font></font><br>
<blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- Rabinovich, is it true that you did win a million in the lottery? </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 - Oh, who told you that? </font><font style="vertical-align: inherit;">Not a million, but ten rubles, not in the lottery, but in preference, and did not win, but lost.</font></font></blockquote><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In other words, the classic GIGO situation - Garbage In Garbage Out - Garbage inlet = Garbage in the output. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Practical applicable sizing for hyperconvergence is almost guaranteed to be of two types: take us with a margin, or for a long time we will drive a pilot and take indicators.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
There is one more point with sizing and evaluation of specifications. Different systems are built differently and work differently with disks; their controllers interact differently. Therefore, it is practically pointless to compare ‚Äúhead-to-head‚Äù according to the specifications the number and volume of disks. You have some kind of TK, within which you understand the level of load. And then there is a certain number of gearboxes, within which you are offered various systems that meet the requirements for performance and reliability. What is the fundamental difference, how much does a disk cost and what type in system 1, and that in system 2 there are more / less of them if both of them successfully cope with the task.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Since performance is often determined by controllers living on the same hosts as virtual machines, for some types of loads it can quite significantly swim simply because processors with different frequencies are located in different clusters, all other things being equal. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In other words, even the most experienced presale architect-archmage will not tell you the specification more precisely than you formulate the requirements, and more precisely, than ‚Äúwell, somewhere SAM-VOSEM‚Äù without pilot projects.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/79/su/eq/79sueqtcnez6nz-kop4mp1i4qlq.png"><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">About snapshots</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
HyperFlex can do its native snapshots of virtual machines using Redirect-on-Write technology. And here it is necessary to stop separately to consider different technologies of snapshots. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Initially, there were snapshots of the Copy-on-Write (CoW) type, and VMware vSphere native snapshots can be taken as a classic example. The principle of operation is the same with vmdk on top of VMFS or NFS, which with native file systems such as VSAN. After creating a CoW snapshot, the original data (blocks or vmdk files) is frozen, and when you try to write to frozen blocks, a copy is created and the data is written to a new block / file (delta file for vmdk). As a result, as the snapshot tree grows, the number of ‚Äúspurious‚Äù disk accesses that do not carry any productive meaning increases, and</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">performance drops / delays grow</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Then Redirect-on-Write (RoW) snapshots were invented, in which instead of creating copies of blocks with data, a copy of metadata is created, and the record just goes on without delays and additional readings and checks. With the correct implementation of RoW snapshots have almost zero effect on the performance of the disk system. The second effect of working with metadata instead of the live data itself is not only the instant creation of snapshots, but also VM clones, which immediately after creation do not take up space at all (we do not consider system overhead for VM service files).</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
And the third, key point that radically distinguishes RoW from CoW snapshots for productive systems is the instant removal of snapshot. It would seem that this is so? However, you need to remember how CoW snapshots work and that removing a snapshot is not really a delta removal, but its commit. And here the time of her commit is extremely dependent on the size of the accumulated delta and the performance of the disk system. RoW snapshots are committed instantly simply because no matter how many terabytes of difference accumulate, deleting (committing) RoW snapshots is an update of the metadata table.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
And here an interesting application of RoW snapshots appears - drop the RPO to values ‚Äã‚Äãof tens of minutes. Making backups every 30 minutes is almost impossible in the general case, and in most cases they are done once a day, which gives an RPO of 24 hours. But at the same time, we can just do RoW snapshots on a schedule, bringing the RPO to 15-30 minutes, and store them for a day or two. No penalty to performance, spending only capacity. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
But there are some nuances.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
For proper operation of native snapshots and integration with VMware, HyperFlex requires an official snapshot called Sentinel. Sentinel snapshot is created automatically when you first create a snapshot for a given VM through HXConnect, and you should not delete it, you should not "go back" to it, you just need to put up with the fact that in the interface in the list of snapshots this is the first service snapshot of Sentinel. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/6v/xv/fk/6vxvfkz6yuhzkylorocntklep1w.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
HyperFlex snapshots can run in crash-consistent mode or in application-consistent mode. The second type involves "flushing buffers" inside the VM, it requires VMTools, and it starts if the "Quiesce" checkbox is checked in the HXConnect snapshot menu.</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In addition to HyperFlex snapshots, no one prohibits the use of "native" VMware snapshots. </font><font style="vertical-align: inherit;">It is worthwhile for a specific virtual machine to determine which snapshots you will use, and in the future to focus on this technology, ‚Äúnot disturbing‚Äù different snapshots for one VM. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As part of the test, I tried to create snapshots and check their FIO. </font><font style="vertical-align: inherit;">And yet, yes, I can confirm that snapshots are really RoW, they do not affect performance. </font><font style="vertical-align: inherit;">Snapshots really are created quickly (a few seconds depending on the load profile and the size of the dataset), I can give the following recommendation based on the results: if your load has a lot of random write operations, you should start creating a snapshot from the HXConnect interface, with the ‚ÄúQuiesce‚Äù checkmark and with a preliminary the presence of a Sentinel snapshot.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tests</font></font></h2><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Test platform</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The following platform fell into tenacious paws:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">4 x C220 M4 (2630v4 10c x 2.20 GHz, 256, 800 + 6 * 960)</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">vSphere 6.7</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">HX Data Platform 4.0.2</font></font></li>
</ul><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Clear patch test</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
What kind of testing without CrystalDisk? That's right, this can‚Äôt be, normal guys always start a crystallized disk! Well, if it‚Äôs necessary, then it is necessary. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/hp/tl/x5/hptlx5eb-ojjdbwet5l87h6wjx4.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
For the crystal disk, a specially created VM with 2 vCPU 4GB and Windows 7 on board was created. Oh, and I got sick of putting patches on it, I'll tell you! The test was carried out in the best traditions of the best houses in London and Paris - namely, just one virtual disk next-next-finish was added without any thoughts and the test was launched. Yes, and by the way, of course CrystalDiskMark itself is not involved in testing, it is just an interface, but directly loads the disk system with the well-known DiskSpd package included in the kit. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ns/wt/i9/nswti9u6hmne-srf37nniqjevzq.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
What struck me literally - for some reason, all skipped the choice of units in the upper right corner. And alle op!</font></font><br>
<br>
<img src="https://habrastorage.org/webt/kg/3y/th/kg3ythef08dpqq3zcsrr20wmq8w.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Listen, honestly, I didn‚Äôt expect 75 thousand IOPS and more than 1 gigabyte per second from the micromachine in next-next-finish mode! </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
To put it mildly, not every company in Russia has loads that exceed these indicators in total. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Further tests were carried out using VMware HCI Bench and Nutanix XRay, as ‚Äúideologically hostile‚Äù to HyperFlex, and accordingly, it was expected that we would not take prisoners. The numbers turned out to be extremely close, so the results from the XRay package were taken as a basis simply because it has a more convenient reporting system and ready-made load templates. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
For those who do not trust anyone and want total control over the process, I remind you of my article on building your own system to generate the load on a hyperconverged platform - "</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Performance Testing giperkonvergentnyh systems and SDS with their own hands</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> "</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Achtung! </font><font style="vertical-align: inherit;">Uwaga! </font><font style="vertical-align: inherit;">Pozor!</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
All further results and their interpretations are the opinion of the author of the article, and are given by themselves in the framework of the study of the system. </font><font style="vertical-align: inherit;">Most of the tests are bare synthetics and are applicable only for understanding the limit indicators in extreme and degenerate cases, which you will never achieve in real life.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">FourCorners Microbenchmark</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The 4-sided microtest is designed to evaluate the system ‚Äúfast‚Äù for the ultimate theoretical performance and peak performance of the controllers. The practical application for this test is to check the system immediately after launch for any configuration and environment errors, especially network errors. Those. if you regularly run such systems, then you just know what numbers you should expect ‚Äúif all is well‚Äù. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/im/wl/70/imwl70bqxpwhlum10vhlhm1kbk0.png"><br>
<br>
<img src="https://habrastorage.org/webt/61/xf/ss/61xfss1sc92seaez55-aq_shz2i.png"><br>
<br>
<img src="https://habrastorage.org/webt/4x/38/l7/4x38l7clygg4y5lavoks-lecrfs.png"><br>
<br>
<img src="https://habrastorage.org/webt/si/he/kg/sihekgnrrioansimgwwetryowme.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Final numbers: 280k / 174k IOPS, 3.77 / 1.72 GBps (read / write) </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
How did our controllers behave?</font></font><br>
<br>
<img src="https://habrastorage.org/webt/-_/co/qv/-_coqvha1rwj6l78ng3fl_kowoi.png"><br>
<br>
<img src="https://habrastorage.org/webt/xu/hd/8s/xuhd8szlg-hrwoyeasjl-nqybbm.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
From which it can be noted that the total resource consumption for 4 controllers and 4 VM loads was 49 cores of 2.2. According to VMware statistics, the CPU utilization of the controllers was up to 80%, i.e. in fact, performance was limited by the performance of controllers, and specifically processors. The speed of sequential operations specifically rested against the speed of 10G network. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Let's try again. Peak performance on a small 4-node cluster with not the fastest 2.2GHz processors is almost 300 thousand IOPS at 4U heights. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The conversation ‚Äúhere we have 10, 20 or even 40% more / less‚Äù is practically meaningless due to the order of numbers. The same as starting to measure "and I can have a car 240, I have 280" despite the fact that the limit is 80.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
280k / 4 nodes gives a peak performance of 70k / node, which for example exceeds the numbers from the VMware VSAN calculator, which assumes that the AF node issues no more than 46k per disk group. </font><font style="vertical-align: inherit;">In our case, here in VMware terminology there is just one disk group, which actually runs at x1.8.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Datastore block size effect</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
When creating a HyperFlex datastore, you can choose the data block size - 4k or 8k. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
What will it affect? </font><font style="vertical-align: inherit;">Run the same quadrangular test. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/70/rc/vf/70rcvfpf8-l_zewvlxiawo6lgmm.png"><br>
<br>
<img src="https://habrastorage.org/webt/gd/s6/ys/gds6ysghq6zmspmh1aiprlngtni.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
If the picture is almost identical with reading, then the record on the contrary matters. </font><font style="vertical-align: inherit;">The quadrangular test uses an 8k load. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Total numbers: 280k / 280k, 172-158k / 200-180k (4k 8k). </font><font style="vertical-align: inherit;">When the block size matches, + 15% of write performance is obtained. </font><font style="vertical-align: inherit;">If you expect a significant amount of recording with a small block (4k) in the load - create a datastore for this particular load with a 4k block, otherwise use 8k.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">OLTP Simulator</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A much closer picture to reality is given by another test. </font><font style="vertical-align: inherit;">As part of it, two generators are launched with a profile close to a transactional DBMS and a load level of 6000 + 400 IOPS. </font><font style="vertical-align: inherit;">Here, the delay is measured, which should remain at a stable low level. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/aw/tb/ty/awtbtyfcnnow6yydbgr28dpgiqc.png"><br>
<br>
<img src="https://habrastorage.org/webt/o9/kd/m1/o9kdm1zv2lksnqi1g7qk8o5gl7m.png"><br>
<br>
<img src="https://habrastorage.org/webt/cp/3n/oh/cp3noh9aqrp7y0kfrdgehpd19xe.png"><br>
<br>
<img src="https://habrastorage.org/webt/rn/qh/0j/rnqh0jmup291dks_4ir4eg1ssu8.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The delay for the VM load was 1.07 / 1.08 ms. </font><font style="vertical-align: inherit;">All in all a great result, but let's add some heat!</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Database Colocation: High Intensity</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
How the transactional base will behave, depending on the delays, if suddenly a noisy consecutive neighbor is formed. </font><font style="vertical-align: inherit;">Well, very noisy. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/0s/wg/1w/0swg1wykmap5tqivoahd3qymfbe.png"><br>
<br>
<img src="https://habrastorage.org/webt/kr/uy/xh/kruyxhmfiswvbc2tue7whwpwu60.png"><br>
<br>
<img src="https://habrastorage.org/webt/jw/3l/ib/jw3libpbrez473tlexzbilmepyy.png"><br>
<br>
<img src="https://habrastorage.org/webt/p1/4g/2j/p14g2jumznxoh8oyw2y1f3slyge.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
So, the OLTP base on node 1 generates 4200 IOPS at 0.85 ms delay. </font><font style="vertical-align: inherit;">What happens after a DSS system suddenly begins to consume resources in sequential operations? </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Two generators on nodes 2 and 3 load the platform at 1.18 / 1.08 GBps, respectively, those 2.26 GBps in total. </font><font style="vertical-align: inherit;">The delay on OLTP of course grows and becomes less flat, but the average value remains 1.85ms, and the base receives its 4200 IOPS without any problems.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Snapshot impact</font></font></h3><br>
<img src="https://habrastorage.org/webt/n0/eo/w8/n0eow8pfcfna7kqwhrt0xblqq8g.png"><br>
<br>
<img src="https://habrastorage.org/webt/-a/0h/dl/-a0hdlvysga1zwnpwtyvb1zq1_c.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The system sequentially takes several snapshots once an hour on an OLTP base. </font><font style="vertical-align: inherit;">There is nothing surprising in the schedule, and moreover, this is generally an indicator of how VMware classic snapshots work, since Nutanix XRay does not know how to work with native snapshots except for its own. </font><font style="vertical-align: inherit;">You do not need to use vSphere snapshots on a regular basis, because not all yogurts are equally useful. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
HyperFlex native snapshots work much better, use them and your hair will become soft and silky!</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Big data ingestion</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
How will HyperFlex digest a large amount of data uploaded sequentially? </font><font style="vertical-align: inherit;">Well let's say 1TB. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/0f/ll/m7/0fllm7dze4walp1c2xff9msn-wg.png"><br>
<br>
<img src="https://habrastorage.org/webt/vx/go/ig/vxgoigfhqqpqguabodmsmklgmma.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The test took 27 minutes, including cloning, tuning and starting the generators.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Throughput scalability</font></font></h3><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Now, gradually load the entire cluster and look at the steady numbers. To start with random reading, then writing. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/iu/kb/jy/iukbjyoms8btipiqv8ckw7-oikg.png"><br>
<br>
<img src="https://habrastorage.org/webt/zw/zj/gy/zwzjgyqeu9wn4ofiatztt5umy70.png"><br>
<br>
<img src="https://habrastorage.org/webt/zd/eu/r2/zdeur21emet_sylypa1p7btvg9c.png"><br>
<br>
<img src="https://habrastorage.org/webt/_u/z1/_p/_uz1_p1ht5ll4bihnaeuxj_znoa.png"><br>
<br>
<img src="https://habrastorage.org/webt/-c/ms/o4/-cmso46htkcgb6ixeetyowyninq.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We are seeing a stable picture with a gradual decrease in the performance of the machine load from 78k to 55-57k IOPS, with smooth shelves. At the same time, there is a steady increase in overall performance from 78 to 220k IOPS. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/to/cj/w0/tocjw0zga3ug1recbqb8oa133sq.png"><br>
<br>
<img src="https://habrastorage.org/webt/3z/nt/df/3zntdf3yigfx_acxr0ra6n5nwfa.png"><br>
<br>
<img src="https://habrastorage.org/webt/od/4x/jh/od4xjhnjcvxthkezbcrncc33c5a.png"><br>
<br>
<img src="https://habrastorage.org/webt/mu/zj/mz/muzjmzjtko49unkx1iwzzm_zzo0.png"><br>
<br>
<img src="https://habrastorage.org/webt/v4/wr/0j/v4wr0jbbzvpfz94majmfgisny1m.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Recording is a little less smooth, but still stable shelves from 64k to 19-21k per car. At the same time, the load on the controllers is much lower. If during reading the total processor load level increased from 44 to 109, then on recording from 57 to 73 GHz.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Here you can observe the simplest and most obvious example of the features of hyperconverged systems - the only consumer is simply not able to completely utilize all the resources of the system, and when the load is added, there is no significant drop in performance. </font><font style="vertical-align: inherit;">The drop that we are witnessing is already the result of extreme synthetic loads designed to squeeze everything to the last drop, which is almost never the case in a normal product.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Breaking OLTP</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
By this time, it became even boring how predictable HyperFlex was. </font><font style="vertical-align: inherit;">Urgent need to break something! </font></font><br>
<br>
<img src="https://habrastorage.org/webt/nw/rq/da/nwrqdazu_dn_gpyg6cdu3z0paju.png"><br>
<br>
<img src="https://habrastorage.org/webt/wr/fw/ks/wrfwkshatwbjd5gr2rvvtevdf8s.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The red dot marks the moment the controller VM shuts down on one of the hosts with a load. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Since by default rebuild in HyperFlex starts immediately only when the disk is lost, and when the node is lost, the timeout is 2 hours, the moment of forced rebuild is marked with a green dot.</font></font><br>
<br>
<pre><code class="plaintext hljs">login as: admin<font></font>
 HyperFlex StorageController 4.0(2a)<font></font>
admin@192.168.***.***'s password:<font></font>
&lt;b&gt;admin@SpringpathController0VY9B6ERXT:~$&lt;/b&gt; stcli rebalance status<font></font>
rebalanceStatus:<font></font>
    percentComplete: 0<font></font>
    rebalanceState: cluster_rebalance_not_running<font></font>
rebalanceEnabled: True<font></font>
&lt;b&gt;admin@SpringpathController0VY9B6ERXT:~$&lt;/b&gt; stcli rebalance start -f<font></font>
msgstr: Successfully started rebalance<font></font>
params:<font></font>
msgid: Successfully started rebalance<font></font>
&lt;b&gt;admin@SpringpathController0VY9B6ERXT:~$&lt;/b&gt; stcli rebalance status<font></font>
rebalanceStatus:<font></font>
    percentComplete: 16<font></font>
    rebalanceState: cluster_rebalance_ongoing<font></font>
rebalanceEnabled: True<font></font>
&lt;b&gt;admin@SpringpathController0VY9B6ERXT:~$&lt;/b&gt;</code></pre><br>
<img src="https://habrastorage.org/webt/sh/ry/qo/shryqousd3rdr6w7pwmdemcjpzk.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The operations froze for a couple of seconds and continued again, almost noticing the rebuild. </font><font style="vertical-align: inherit;">It is in a stable state when it is far from cluster overload. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Why is 2 hours Cisco not a problem, though competitors have fewer numbers? </font><font style="vertical-align: inherit;">Cisco strongly recommends using RF3 as a basic level of data protection for everything except machines that are not a pity. </font><font style="vertical-align: inherit;">You decided to install patches or do something with the host, turn it off. </font><font style="vertical-align: inherit;">And there is a chance that just at that moment another host will fail - and then in the case of RF2 everything will become a stake, and with RF3 there will be one active copy of the data. </font><font style="vertical-align: inherit;">And yes, indeed, it‚Äôs quite possible to survive 2 hours in an accident on RF2 until recovery to RF3 begins.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Break me completely!</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Breaking - so breaking. </font><font style="vertical-align: inherit;">Full load. </font><font style="vertical-align: inherit;">In this case, I created a test with a profile more or less resembling a real load (70% read, 20% random, 8k, 6d 128q). </font></font><br>
<br>
<img src="https://habrastorage.org/webt/4k/6l/rr/4k6lrrwh29qbybibwy2_gvq9y-e.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Guess where CVM turned off, and where did the rebuild begin? </font></font><br>
<br>
<img src="https://habrastorage.org/webt/0h/ur/am/0huramdwazcpyvzycht-hn1bwks.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In the situation with the rebuild, HyperFlex performed quite well, without causing a catastrophic drop in performance or a multiple increase in delays, even under load under the very tomatoes. </font><font style="vertical-align: inherit;">The only thing I would really like is dear Cisco, make the timeout all the same less than 2 hours by default.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">findings</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
To conclude, I recall the purpose of the testing: to investigate the Cisco HyperFlex system today, without looking at the history, to investigate its performance using synthetics and draw conclusions about its applicability to a real product. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Conclusion 1</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , on performance. The performance is very good, and you won‚Äôt give any other comments here. Since I had a system of the previous generation on the test, I can say exactly one thing - on HyperFlex All Flash you will run into capacity, into the processor, into memory, but not into disks. Except maybe 1% of super-loaded applications, but you need to conduct a conversation with them personally. Native RoW snapshots work. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Conclusion 2</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, by availability. The system after detecting a failure is quite good (without a performance drop at times) fulfills the restoration of the number of copies of data. There is a slight complaint in the 2-hour default timeout before starting recovery (if the host is lost), but given the highly recommended RF3, this is more nitpicking. Recovery after a disk failure begins immediately. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Conclusion 3</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, in price and comparison with competitors. </font><font style="vertical-align: inherit;">The price of the system can vary many times depending on the configuration for a specific project. </font><font style="vertical-align: inherit;">A huge share of the project cost will be licensed system and application software, which will work on top of the infrastructure platform. </font><font style="vertical-align: inherit;">Therefore, the only way to compare with competitors is to compare specific commercial offers that meet the technical requirements, specifically for your company for a specific project. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Final conclusion</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : the system is working, quite mature for use in the product for April 2020, if the vendor‚Äôs recommendations are read and applied, rather than smoking.</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../en497010/index.html">Investing in periods of market downturn: 3 strategies for behavior on the stock exchange</a></li>
<li><a href="../en497014/index.html">Who are you, QA engineer or tester?</a></li>
<li><a href="../en497016/index.html">April 13th Java Digest</a></li>
<li><a href="../en497020/index.html">DocuSign, a popular digital signature service, now supports the GlobalSign infrastructure</a></li>
<li><a href="../en497022/index.html">Updating corporate network access. New ExtremeSwitching X435 Gigabit Switches</a></li>
<li><a href="../en497028/index.html">Welcome to Robot Operating System MeetUp</a></li>
<li><a href="../en497030/index.html">Experience in implementing network factories based on EVPN VXLAN and Cisco ACI and a small comparison</a></li>
<li><a href="../en497032/index.html">Service business and crisis: success stories. How does thinking outside the box work for you?</a></li>
<li><a href="../en497034/index.html">What is the new nRF Connect SDK for Nordic? Evolution, revolution or alternative?</a></li>
<li><a href="../en497036/index.html">When to stop the video sequence recognition process?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>