<!doctype html>
<html class="no-js" lang="zh-CN">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🌝 👃 👩‍👩‍👧‍👧 带有Keras的递归神经网络（RNN） ✍🏾 🧝🏾 🤲🏻</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Tensorflow.org的《递归神经网络指南》的翻译。该材料讨论了Keras / Tensorflow 2.0的内置功能以实现快速网格划分，以及自定义图层和单元的可能性。还考虑了使用CuDNN内核的情况和局限性，这可以加快学习神经网络的过程。
 
 
 
 递归神经网络（RNN）是一类神经网络，...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>带有Keras的递归神经网络（RNN）</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/487808/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tensorflow.org的《递归神经网络指南》的翻译。</font><font style="vertical-align: inherit;">该材料讨论了Keras / Tensorflow 2.0的内置功能以实现快速网格划分，以及自定义图层和单元的可能性。</font><font style="vertical-align: inherit;">还考虑了使用CuDNN内核的情况和局限性，这可以加快学习神经网络的过程。</font></font><br>
 <br>
<img src="https://habrastorage.org/webt/xt/_q/nj/xt_qnjgfjengqoqd4gizkq4j_wk.png"><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
递归神经网络（RNN）是一类神经网络，可以很好地建模串行数据，例如时间序列或自然语言。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
如果示意性地表示，RNN层使用循环</font></font><code>for</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">遍历时间顺序的序列，同时以内部状态存储有关他已经看到的步骤的编码信息。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Keras RNN API的设计重点是：</font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">易于使用</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">内置层：</font></font><code>tf.keras.layers.RNN</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">，</font></font><code>tf.keras.layers.LSTM</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">，</font></font><code>tf.keras.layers.GRU</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">让您快速建立一个递归的模型，而无需进行复杂的配置设置。</font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">易于定制</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">：您还可以定义自己的RNN单元层（循环的内部）</font></font><code>for</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">）具有自定义行为，并将其与“ tf.keras.layers.RNN”的通用层（for循环本身）一起使用。</font><font style="vertical-align: inherit;">这将使您以最少的代码以灵活的方式快速原型化各种研究思路。</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">安装</font></font></h2><br>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> __future__ <span class="hljs-keyword">import</span> absolute_import, division, print_function, unicode_literals<font></font>
<font></font>
<span class="hljs-keyword">import</span> collections
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<font></font>
<font></font>
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<font></font>
<font></font>
<span class="hljs-keyword">from</span> tensorflow.keras <span class="hljs-keyword">import</span> layers</code></pre><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">建立一个简单的模型</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Keras具有三个内置的RNN层：</font></font><br>
<br>
<ol>
<li><code>tf.keras.layers.SimpleRNN</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">，是一个完全连接的RNN，其中前一个时间步的输出应传递到下一步。</font></font></li>
<li><code>tf.keras.layers.GRU</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">，在文章“ </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"><font style="vertical-align: inherit;">使用RNN编解码器研究统计机器翻译中的短语”中</font></a><font style="vertical-align: inherit;">首次提出</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"></font></a></li>
<li><code>tf.keras.layers.LSTM</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">，首先在文章</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">长期短期记忆中提出</font></font></a></li>
</ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
2015年初，Keras推出了第一个可重用的开源Python，LSTM和GRU实现。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
以下是</font></font><code>Sequential</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">模型</font><font style="vertical-align: inherit;">的示例，该</font><font style="vertical-align: inherit;">模型通过将每个整数嵌套在64维向量中，然后使用layer处理向量序列来处理整数序列</font></font><code>LSTM</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font></font><br>
<br>
<pre><code class="python hljs">model = tf.keras.Sequential()
<span class="hljs-comment">#   Embedding      1000, </span>
<span class="hljs-comment">#     64.</span>
model.add(layers.Embedding(input_dim=<span class="hljs-number">1000</span>, output_dim=<span class="hljs-number">64</span>))<font></font>
<font></font>
<span class="hljs-comment">#   LSTM  128  .</span>
model.add(layers.LSTM(<span class="hljs-number">128</span>))<font></font>
<font></font>
<span class="hljs-comment">#   Dense  10    softmax.</span>
model.add(layers.Dense(<span class="hljs-number">10</span>))<font></font>
<font></font>
model.summary()</code></pre><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">输出和状态</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
默认情况下，RNN层的输出每个元素包含一个向量。此向量是最后一个RNN单元的输出，其中包含有关整个输入序列的信息。此输出的尺寸</font></font><code>(batch_size, units)</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">，其中</font></font><code>units</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">对应于</font></font><code>units</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">传递给图层构造函数</font><font style="vertical-align: inherit;">的参数</font><font style="vertical-align: inherit;">。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
如果指定，则RNN层还可以返回每个元素的整个输出序列（每个步骤一个向量）</font></font><code>return_sequences=True</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。此输出的尺寸为</font></font><code>(batch_size, timesteps, units)</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font></font><br>
<br>
<pre><code class="python hljs">model = tf.keras.Sequential()<font></font>
model.add(layers.Embedding(input_dim=<span class="hljs-number">1000</span>, output_dim=<span class="hljs-number">64</span>))<font></font>
<font></font>
<span class="hljs-comment">#  GRU  3D   (batch_size, timesteps, 256)</span>
model.add(layers.GRU(<span class="hljs-number">256</span>, return_sequences=<span class="hljs-literal">True</span>))<font></font>
<font></font>
<span class="hljs-comment">#  SimpleRNN  2D   (batch_size, 128)</span>
model.add(layers.SimpleRNN(<span class="hljs-number">128</span>))<font></font>
<font></font>
model.add(layers.Dense(<span class="hljs-number">10</span>))<font></font>
<font></font>
model.summary()</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
另外，RNN层可以返回其最终内部状态。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
返回的状态可以稍后用于恢复RNN的执行或</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">初始化另一个RNN</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。此设置通常在编码器-解码器模型中按顺序使用，其中编码器的最终状态用于解码器的初始状态。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
为了使RNN图层返回其内部状态，请</font><font style="vertical-align: inherit;">在创建图层时</font><font style="vertical-align: inherit;">将参数设置</font></font><code>return_state</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">为value </font></font><code>True</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。请注意，有</font></font><code>LSTM</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2个状态张量，</font></font><code>GRU</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">只有一个。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
要调整图层的初始状态，只需使用附加参数调用该图层即可</font></font><code>initial_state</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
请注意，尺寸必须与图层元素的尺寸匹配，如以下示例所示。</font></font><br>
<br>
<pre><code class="python hljs">encoder_vocab = <span class="hljs-number">1000</span>
decoder_vocab = <span class="hljs-number">2000</span><font></font>
<font></font>
encoder_input = layers.Input(shape=(<span class="hljs-literal">None</span>, ))<font></font>
encoder_embedded = layers.Embedding(input_dim=encoder_vocab, output_dim=<span class="hljs-number">64</span>)(encoder_input)<font></font>
<font></font>
<span class="hljs-comment">#       </span><font></font>
output, state_h, state_c = layers.LSTM(<font></font>
    <span class="hljs-number">64</span>, return_state=<span class="hljs-literal">True</span>, name=<span class="hljs-string">'encoder'</span>)(encoder_embedded)<font></font>
encoder_state = [state_h, state_c]<font></font>
<font></font>
decoder_input = layers.Input(shape=(<span class="hljs-literal">None</span>, ))<font></font>
decoder_embedded = layers.Embedding(input_dim=decoder_vocab, output_dim=<span class="hljs-number">64</span>)(decoder_input)<font></font>
<font></font>
<span class="hljs-comment">#  2     LSTM    </span><font></font>
decoder_output = layers.LSTM(<font></font>
    <span class="hljs-number">64</span>, name=<span class="hljs-string">'decoder'</span>)(decoder_embedded, initial_state=encoder_state)<font></font>
output = layers.Dense(<span class="hljs-number">10</span>)(decoder_output)<font></font>
<font></font>
model = tf.keras.Model([encoder_input, decoder_input], output)<font></font>
model.summary()<font></font>
</code></pre><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">RNN层和RNN单元</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
除了内置的RNN层之外，RNN API还提供了单元级API。</font><font style="vertical-align: inherit;">与处理输入序列的整个包的RNN层不同，RNN单元仅处理一个时间步。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
该单元位于</font></font><code>for</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">RNN层</font><font style="vertical-align: inherit;">的循环内</font><font style="vertical-align: inherit;">。</font><font style="vertical-align: inherit;">用一层包裹一个单元格可以</font></font><code>tf.keras.layers.RNN</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">为您提供一层能够处理序列数据包的层，例如 </font></font><code>RNN(LSTMCell(10))</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
从数学上讲，</font></font><code>RNN(LSTMCell(10))</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">它的结果与相同</font></font><code>LSTM(10)</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font><font style="vertical-align: inherit;">实际上，在TF v1.x内部执行此层只是创建相应的RNN单元并将其包装在RNN层中。</font><font style="vertical-align: inherit;">但是，使用嵌入式层</font></font><code>GRU</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">并</font></font><code>LSTM</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">允许使用CuDNN，它可以为您提供更好的性能。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
有三个内置的RNN单元，每个单元对应于其自己的RNN层。</font></font><br>
<br>
<ul>
<li><code>tf.keras.layers.SimpleRNNCell</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">匹配图层</font></font><code>SimpleRNN</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font></font></li>
<li><code>tf.keras.layers.GRUCell</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">匹配图层</font></font><code>GRU</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font></font></li>
<li><code>tf.keras.layers.LSTMCell</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">匹配图层</font></font><code>LSTM</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
单元的抽象以及一个通用的类</font></font><code>tf.keras.layers.RNN</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使得为您的研究实现自定义RNN体系结构变得非常容易。</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">跨批次保存状态</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
在处理长序列（可能是无尽的）时，您可能需要使用</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">跨批状态</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">模式</font><font style="vertical-align: inherit;">。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
通常，RNN层的内部状态会随着每个新的数据包而重置（即，看到该层的每个示例都假定与过去无关）。该层将仅在处理此元素期间保持状态。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
但是，如果序列很长，将它们分解成较短的序列，然后依次将它们传输到RNN层而不重置层状态，将很有用。因此，一个层可以存储有关整个序列的信息，尽管一次只能看到一个子序列。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
您可以通过在构造函数中设置`stateful = True`来实现。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
如果您具有序列s = [t0，t1，... t1546，t1547]`，则可以将其拆分为：</font></font><br>
<br>
<pre><code class="python hljs">s1 = [t0, t1, ... t100]<font></font>
s2 = [t101, ... t201]<font></font>
...<font></font>
s16 = [t1501, ... t1547]</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
然后，您可以使用以下方法进行处理：</font></font><br>
<br>
<pre><code class="python hljs">lstm_layer = layers.LSTM(<span class="hljs-number">64</span>, stateful=<span class="hljs-literal">True</span>)
<span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> sub_sequences:<font></font>
  output = lstm_layer(s)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
当您想要清洁状态时，请使用</font></font><code>layer.reset_states()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font></font><br>
<blockquote><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">注意：</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">在这种情况下，假定</font></font><code>i</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">此程序包中的示例是</font></font><code>i</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">先前程序包</font><font style="vertical-align: inherit;">示例的延续</font><font style="vertical-align: inherit;">。</font><font style="vertical-align: inherit;">这意味着所有包都包含相同数量的元素（包大小）。</font><font style="vertical-align: inherit;">例如，如果软件包包含</font></font><code>[sequence_A_from_t0_to_t100, sequence_B_from_t0_to_t100]</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">，则下一个软件包应包含</font></font><code>[sequence_A_from_t101_to_t200, sequence_B_from_t101_to_t200]</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font></font></blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">这是一个完整的示例：</font></font><br>
<br>
<pre><code class="python hljs">paragraph1 = np.random.random((<span class="hljs-number">20</span>, <span class="hljs-number">10</span>, <span class="hljs-number">50</span>)).astype(np.float32)<font></font>
paragraph2 = np.random.random((<span class="hljs-number">20</span>, <span class="hljs-number">10</span>, <span class="hljs-number">50</span>)).astype(np.float32)<font></font>
paragraph3 = np.random.random((<span class="hljs-number">20</span>, <span class="hljs-number">10</span>, <span class="hljs-number">50</span>)).astype(np.float32)<font></font>
<font></font>
lstm_layer = layers.LSTM(<span class="hljs-number">64</span>, stateful=<span class="hljs-literal">True</span>)<font></font>
output = lstm_layer(paragraph1)<font></font>
output = lstm_layer(paragraph2)<font></font>
output = lstm_layer(paragraph3)<font></font>
<font></font>
<span class="hljs-comment"># reset_states()      initial_state.</span>
<span class="hljs-comment">#  initial_state   ,      .</span>
lstm_layer.reset_states()</code></pre><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">双向RNN</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
对于时间序列以外的序列（例如文本），如果RNN模型不仅从头到尾进行处理，反之亦然，那么经常会发生RNN模型工作得更好的情况。例如，要预测句子中的下一个单词，了解单词周围的上下文（而不仅仅是单词前面的单词）通常很有用。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Keras提供了一个用于创建此类双向RNN的简单API：包装器</font></font><code>tf.keras.layers.Bidirectional</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font></font><br>
<br>
<pre><code class="python hljs">model = tf.keras.Sequential()<font></font>
<font></font>
model.add(layers.Bidirectional(layers.LSTM(<span class="hljs-number">64</span>, return_sequences=<span class="hljs-literal">True</span>), <font></font>
                               input_shape=(<span class="hljs-number">5</span>, <span class="hljs-number">10</span>)))<font></font>
model.add(layers.Bidirectional(layers.LSTM(<span class="hljs-number">32</span>)))<font></font>
model.add(layers.Dense(<span class="hljs-number">10</span>))<font></font>
<font></font>
model.summary()</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
在引擎盖下，</font><font style="vertical-align: inherit;">将复制</font></font><code>Bidirectional</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">传输的RNN层，并将</font></font><code>go_backwards</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">新复制的层</font><font style="vertical-align: inherit;">的字段翻转</font><font style="vertical-align: inherit;">，从而将以相反的顺序处理输入数据。</font><font style="vertical-align: inherit;">默认情况下</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
，` </font></font><code>Bidirectional</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">RNN的输出将是前向层的输出与反向层的输出之和。</font><font style="vertical-align: inherit;">如果您需要其他合并行为，例如 </font><font style="vertical-align: inherit;">串联，请在“双向”包装构造函数中更改“ merge_mode”参数。</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">TensorFlow 2.0中的性能优化和CuDNN核心</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
在TensorFlow 2.0中，如果有图形处理器可用，则默认的CuDNN内核可以使用内置的LSTM和GRU层。</font><font style="vertical-align: inherit;">进行此更改后，以前的图层</font></font><code>keras.layers.CuDNNLSTM/CuDNNGRU</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">已过时，您可以构建模型而不必担心将在其上运行的设备。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
由于CuDNN内核是基于某些假设构建的，因此这意味着</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">如果您更改内置LSTM或GRU层的默认设置，则</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">该层</font><b><font style="vertical-align: inherit;">将无法使用CuDNN内核层</font></b><font style="vertical-align: inherit;">。</font><font style="vertical-align: inherit;">例如。</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">将功能</font></font><code>activation</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">从</font><font style="vertical-align: inherit;">更改为</font></font><code>tanh</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">其他。</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">将功能</font></font><code>recurrent_activation</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">从</font><font style="vertical-align: inherit;">更改为</font></font><code>sigmoid</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">其他。</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">用法</font></font><code>recurrent_dropout</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">&gt; 0。</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">将其设置</font></font><code>unroll</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">为True，这将导致LSTM / GRU将内部分解</font></font><code>tf.while_loop</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">为展开的循环</font></font><code>for</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">设置</font></font><code>use_bias</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">为False。</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">当输入数据不正确时使用遮罩（如果遮罩与正确严格对齐的数据相匹配，则仍可以使用CuDNN。这是最常见的情况）。</font></font></li>
</ul><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">尽可能使用CuDNN内核</font></font></h3><br>
<pre><code class="python hljs">batch_size = <span class="hljs-number">64</span>
<span class="hljs-comment">#    MNIST    (batch_size, 28, 28).</span>
<span class="hljs-comment">#     (28, 28) (   ).</span>
input_dim = <span class="hljs-number">28</span><font></font>
<font></font>
units = <span class="hljs-number">64</span>
output_size = <span class="hljs-number">10</span>  <span class="hljs-comment">#   0  9</span><font></font>
<font></font>
<span class="hljs-comment">#  RNN </span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">build_model</span>(<span class="hljs-params">allow_cudnn_kernel=True</span>):</span>
  <span class="hljs-comment"># CuDNN     ,     .</span>
  <span class="hljs-comment">#   `LSTM(units)`    CuDNN,</span>
  <span class="hljs-comment">#   RNN(LSTMCell(units))   non-CuDNN .</span>
  <span class="hljs-keyword">if</span> allow_cudnn_kernel:
    <span class="hljs-comment">#  LSTM      CuDNN.</span>
    lstm_layer = tf.keras.layers.LSTM(units, input_shape=(<span class="hljs-literal">None</span>, input_dim))
  <span class="hljs-keyword">else</span>:
    <span class="hljs-comment">#  LSTMCell  RNN    CuDNN.</span><font></font>
    lstm_layer = tf.keras.layers.RNN(<font></font>
        tf.keras.layers.LSTMCell(units),<font></font>
        input_shape=(<span class="hljs-literal">None</span>, input_dim))<font></font>
  model = tf.keras.models.Sequential([<font></font>
      lstm_layer,<font></font>
      tf.keras.layers.BatchNormalization(),<font></font>
      tf.keras.layers.Dense(output_size)]<font></font>
  )<font></font>
  <span class="hljs-keyword">return</span> model
</code></pre><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">加载MNIST数据集</font></font></h3><br>
<pre><code class="python hljs">mnist = tf.keras.datasets.mnist<font></font>
<font></font>
(x_train, y_train), (x_test, y_test) = mnist.load_data()<font></font>
x_train, x_test = x_train / <span class="hljs-number">255.0</span>, x_test / <span class="hljs-number">255.0</span>
sample, sample_label = x_train[<span class="hljs-number">0</span>], y_train[<span class="hljs-number">0</span>]</code></pre><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">创建模型的实例并进行编译</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
我们选择</font></font><code>sparse_categorical_crossentropy</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">了损失函数。</font><font style="vertical-align: inherit;">模型的输出具有一个维度</font></font><code>[batch_size, 10]</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font><font style="vertical-align: inherit;">模型的答案是一个整数向量，每个数字的范围是0到9。</font></font><br>
<br>
<pre><code class="python hljs">model = build_model(allow_cudnn_kernel=<span class="hljs-literal">True</span>)<font></font>
<font></font>
model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="hljs-literal">True</span>), <font></font>
              optimizer=<span class="hljs-string">'sgd'</span>,<font></font>
              metrics=[<span class="hljs-string">'accuracy'</span>])
</code></pre><br>
<pre><code class="python hljs">model.fit(x_train, y_train,<font></font>
          validation_data=(x_test, y_test),<font></font>
          batch_size=batch_size,<font></font>
          epochs=<span class="hljs-number">5</span>)</code></pre><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">在没有CuDNN核心的情况下建立新模型</font></font></h3><br>
<pre><code class="python hljs">slow_model = build_model(allow_cudnn_kernel=<span class="hljs-literal">False</span>)<font></font>
slow_model.set_weights(model.get_weights())<font></font>
slow_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="hljs-literal">True</span>), <font></font>
                   optimizer=<span class="hljs-string">'sgd'</span>, <font></font>
                   metrics=[<span class="hljs-string">'accuracy'</span>])<font></font>
slow_model.fit(x_train, y_train, <font></font>
               validation_data=(x_test, y_test), <font></font>
               batch_size=batch_size,<font></font>
               epochs=<span class="hljs-number">1</span>)  <span class="hljs-comment">#         .</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
如您所见，使用CuDNN构建的模型比使用常规TensorFlow核心的模型进行训练要快得多。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
具有CuDNN支持的相同模型可用于单处理器环境中的输出。</font><font style="vertical-align: inherit;">注释</font></font><code>tf.device</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">仅指示使用的设备。</font><font style="vertical-align: inherit;">如果没有GPU，该模型将默认在CPU上运行。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
您只需要担心正在使用的硬件。</font><font style="vertical-align: inherit;">那不是很酷吗？</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">with</span> tf.device(<span class="hljs-string">'CPU:0'</span>):<font></font>
  cpu_model = build_model(allow_cudnn_kernel=<span class="hljs-literal">True</span>)<font></font>
  cpu_model.set_weights(model.get_weights())<font></font>
  result = tf.argmax(cpu_model.predict_on_batch(tf.expand_dims(sample, <span class="hljs-number">0</span>)), axis=<span class="hljs-number">1</span>)<font></font>
  print(<span class="hljs-string">'Predicted result is: %s, target result is: %s'</span> % (result.numpy(), sample_label))<font></font>
  plt.imshow(sample, cmap=plt.get_cmap(<span class="hljs-string">'gray'</span>))
</code></pre><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">具有列表/字典输入或嵌套输入的RNN</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
嵌套结构使您可以一步一步包含更多信息。</font><font style="vertical-align: inherit;">例如，视频帧可能同时包含音频和视频输入。</font><font style="vertical-align: inherit;">在这种情况下，数据的维度可以是：</font></font><br>
<br>
<pre><code class="plaintext hljs">[batch, timestep, {\"video\": [height, width, channel], \"audio\": [frequency]}]</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
在另一个示例中，手写数据可以具有当前笔位置的x和y坐标以及压力信息。</font><font style="vertical-align: inherit;">因此数据可以表示如下：</font></font><br>
<br>
<pre><code class="plaintext hljs">[batch, timestep, {\"location\": [x, y], \"pressure\": [force]}]</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
以下代码构建了可与这种结构化输入一起使用的自定义RNN单元的示例。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">定义一个支持嵌套输入/输出的用户单元</font></font></h3><br>
<pre><code class="python hljs">NestedInput = collections.namedtuple(<span class="hljs-string">'NestedInput'</span>, [<span class="hljs-string">'feature1'</span>, <span class="hljs-string">'feature2'</span>])<font></font>
NestedState = collections.namedtuple(<span class="hljs-string">'NestedState'</span>, [<span class="hljs-string">'state1'</span>, <span class="hljs-string">'state2'</span>])<font></font>
<font></font>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">NestedCell</span>(<span class="hljs-params">tf.keras.layers.Layer</span>):</span><font></font>
<font></font>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, unit_1, unit_2, unit_3, **kwargs</span>):</span><font></font>
    self.unit_1 = unit_1<font></font>
    self.unit_2 = unit_2<font></font>
    self.unit_3 = unit_3<font></font>
    self.state_size = NestedState(state1=unit_1, <font></font>
                                  state2=tf.TensorShape([unit_2, unit_3]))<font></font>
    self.output_size = (unit_1, tf.TensorShape([unit_2, unit_3]))<font></font>
    super(NestedCell, self).__init__(**kwargs)<font></font>
<font></font>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">build</span>(<span class="hljs-params">self, input_shapes</span>):</span>
    <span class="hljs-comment"># #  input_shape  2 , [(batch, i1), (batch, i2, i3)]</span>
    input_1 = input_shapes.feature1[<span class="hljs-number">1</span>]<font></font>
    input_2, input_3 = input_shapes.feature2[<span class="hljs-number">1</span>:]<font></font>
<font></font>
    self.kernel_1 = self.add_weight(<font></font>
        shape=(input_1, self.unit_1), initializer=<span class="hljs-string">'uniform'</span>, name=<span class="hljs-string">'kernel_1'</span>)<font></font>
    self.kernel_2_3 = self.add_weight(<font></font>
        shape=(input_2, input_3, self.unit_2, self.unit_3),<font></font>
        initializer=<span class="hljs-string">'uniform'</span>,<font></font>
        name=<span class="hljs-string">'kernel_2_3'</span>)<font></font>
<font></font>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">call</span>(<span class="hljs-params">self, inputs, states</span>):</span>
    <span class="hljs-comment">#     [(batch, input_1), (batch, input_2, input_3)]</span>
    <span class="hljs-comment">#     [(batch, unit_1), (batch, unit_2, unit_3)]</span><font></font>
    input_1, input_2 = tf.nest.flatten(inputs)<font></font>
    s1, s2 = states<font></font>
<font></font>
    output_1 = tf.matmul(input_1, self.kernel_1)<font></font>
    output_2_3 = tf.einsum(<span class="hljs-string">'bij,ijkl-&gt;bkl'</span>, input_2, self.kernel_2_3)<font></font>
    state_1 = s1 + output_1<font></font>
    state_2_3 = s2 + output_2_3<font></font>
<font></font>
    output = [output_1, output_2_3]<font></font>
    new_states = NestedState(state1=state_1, state2=state_2_3)<font></font>
<font></font>
    <span class="hljs-keyword">return</span> output, new_states
</code></pre><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用嵌套的输入/输出构建RNN模型</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
让我们构建一个Keras模型，该模型使用</font></font><code>tf.keras.layers.RNN</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">我们刚刚定义</font><font style="vertical-align: inherit;">的图层</font><font style="vertical-align: inherit;">和自定义单元格。</font></font><br>
<br>
<pre><code class="python hljs">unit_1 = <span class="hljs-number">10</span>
unit_2 = <span class="hljs-number">20</span>
unit_3 = <span class="hljs-number">30</span><font></font>
<font></font>
input_1 = <span class="hljs-number">32</span>
input_2 = <span class="hljs-number">64</span>
input_3 = <span class="hljs-number">32</span>
batch_size = <span class="hljs-number">64</span>
num_batch = <span class="hljs-number">100</span>
timestep = <span class="hljs-number">50</span><font></font>
<font></font>
cell = NestedCell(unit_1, unit_2, unit_3)<font></font>
rnn = tf.keras.layers.RNN(cell)<font></font>
<font></font>
inp_1 = tf.keras.Input((<span class="hljs-literal">None</span>, input_1))<font></font>
inp_2 = tf.keras.Input((<span class="hljs-literal">None</span>, input_2, input_3))<font></font>
<font></font>
outputs = rnn(NestedInput(feature1=inp_1, feature2=inp_2))<font></font>
<font></font>
model = tf.keras.models.Model([inp_1, inp_2], outputs)<font></font>
<font></font>
model.compile(optimizer=<span class="hljs-string">'adam'</span>, loss=<span class="hljs-string">'mse'</span>, metrics=[<span class="hljs-string">'accuracy'</span>])unit_1 = <span class="hljs-number">10</span>
unit_2 = <span class="hljs-number">20</span>
unit_3 = <span class="hljs-number">30</span><font></font>
<font></font>
input_1 = <span class="hljs-number">32</span>
input_2 = <span class="hljs-number">64</span>
input_3 = <span class="hljs-number">32</span>
batch_size = <span class="hljs-number">64</span>
num_batch = <span class="hljs-number">100</span>
timestep = <span class="hljs-number">50</span><font></font>
<font></font>
cell = NestedCell(unit_1, unit_2, unit_3)<font></font>
rnn = tf.keras.layers.RNN(cell)<font></font>
<font></font>
inp_1 = tf.keras.Input((<span class="hljs-literal">None</span>, input_1))<font></font>
inp_2 = tf.keras.Input((<span class="hljs-literal">None</span>, input_2, input_3))<font></font>
<font></font>
outputs = rnn(NestedInput(feature1=inp_1, feature2=inp_2))<font></font>
<font></font>
model = tf.keras.models.Model([inp_1, inp_2], outputs)<font></font>
<font></font>
model.compile(optimizer=<span class="hljs-string">'adam'</span>, loss=<span class="hljs-string">'mse'</span>, metrics=[<span class="hljs-string">'accuracy'</span>])
</code></pre><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">在随机生成的数据上训练模型</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
由于我们没有适合该模型的数据集，因此我们将使用Numpy库生成的随机数据进行演示。</font></font><br>
<br>
<pre><code class="python hljs">input_1_data = np.random.random((batch_size * num_batch, timestep, input_1))<font></font>
input_2_data = np.random.random((batch_size * num_batch, timestep, input_2, input_3))<font></font>
target_1_data = np.random.random((batch_size * num_batch, unit_1))<font></font>
target_2_data = np.random.random((batch_size * num_batch, unit_2, unit_3))<font></font>
input_data = [input_1_data, input_2_data]<font></font>
target_data = [target_1_data, target_2_data]<font></font>
<font></font>
model.fit(input_data, target_data, batch_size=batch_size)<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
对于层，</font></font><code>tf.keras.layers.RNN</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">您只需要确定序列中单个步骤的数学逻辑，该层</font></font><code>tf.keras.layers.RNN</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">将为您处理序列的迭代。</font><font style="vertical-align: inherit;">这是快速新型RNN（例如LSTM变体）原型的强大方法。</font></font><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">经过验证后，翻译也将出现在Tensorflow.org上。</font><font style="vertical-align: inherit;">如果您想参与将Tensorflow.org网站的文档翻译成俄语，请以个人身份或评论联系。</font><font style="vertical-align: inherit;">任何更正和评论表示赞赏。</font></font></i></div>
      
    </div><p class="reference-to-source js-reference-to-source">Source: https://habr.com/ru/post/undefined/</p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN487798/index.html">我们如何从Oracle JDK和Java Web Start迁移到AdoptOpenJDK和OpenWebStart</a></li>
<li><a href="../zh-CN487800/index.html">为什么重要的是要告诉申请人面试中出了什么问题（以及正确的做法）</a></li>
<li><a href="../zh-CN487802/index.html">不间断APC Smart UPS，以及如何烹饪</a></li>
<li><a href="../zh-CN487804/index.html">Raiffeisenbank的成长团队聚会</a></li>
<li><a href="../zh-CN487806/index.html">创建一个小的Deno API</a></li>
<li><a href="../zh-CN487812/index.html">测试波兰LED光谱LED E27</a></li>
<li><a href="../zh-CN487814/index.html">速度和可靠性更高，价格更低。新的金士顿KC2000固态驱动器</a></li>
<li><a href="../zh-CN487822/index.html">AvitoTech巡回演出：下诺夫哥罗德的Android聚会</a></li>
<li><a href="../zh-CN487824/index.html">来自欧洲的Spectrum Led GU10 LED灯概述</a></li>
<li><a href="../zh-CN487826/index.html">波兰Spectrum Led E14的LED灯概述</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>