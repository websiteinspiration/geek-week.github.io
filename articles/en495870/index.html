<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚òùüèª üôçüèº ‚òòÔ∏è Tips & tricks working with Ceph in busy projects üòØ üßëüèø‚Äçü§ù‚Äçüßëüèø üßîüèæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Using Ceph as a network storage in different workload projects, we may encounter various tasks that at first glance do not seem simple or trivial. For...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Tips & tricks working with Ceph in busy projects</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/flant/blog/495870/"><img src="https://habrastorage.org/webt/ph/1x/bv/ph1xbvwvoynsjkdr3h_l_gxwoxe.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Using Ceph as a network storage in different workload projects, we may encounter various tasks that at first glance do not seem simple or trivial. </font><font style="vertical-align: inherit;">For instance:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> data migration from old Ceph to new with partial use of previous servers in the new cluster;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> solution of the problem of distribution of disk space in Ceph.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dealing with such tasks, we are faced with the need to correctly extract the OSD without data loss, which is especially true for large amounts of data. </font><font style="vertical-align: inherit;">This will be discussed in the article. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The methods described below are relevant for any version of Ceph. </font><font style="vertical-align: inherit;">In addition, the fact that a large amount of data can be stored in Ceph will be taken into account: to prevent data loss and other problems, some actions will be "split" into several others.</font></font><a name="habracut"></a><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">OSD Preface</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Since two of the three recipes in question are dedicated to OSD ( </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Object Storage Daemon</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ), before diving into the practical part, briefly explain what Ceph does and why it is so important. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
First of all, it should be said that the entire Ceph cluster consists of many OSDs. The more there are, the more free data volume in Ceph. From here it is easy to understand the </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">main function of OSD</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : it saves Ceph object data on file systems of all cluster nodes and provides network access to them (for reading, writing, and other requests). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
At the same level, replication parameters are set by copying objects between different OSDs. And here you can encounter various problems, the solution of which will be discussed later.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Case No. 1. </font><font style="vertical-align: inherit;">Safely retrieve OSD from Ceph cluster without data loss</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The need to remove the OSD may be caused by the withdrawal of the server from the cluster - for example, to replace it with another server - which happened with us, giving rise to the writing of the article. </font><font style="vertical-align: inherit;">Thus, the ultimate goal of manipulation is to extract all the OSDs and mones on a given server so that it can be stopped. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
For convenience and elimination of the situation where, in the process of executing commands, we make a mistake indicating the necessary OSD, we will set a separate variable, the value of which will be the number of the OSD to be deleted. </font><font style="vertical-align: inherit;">We will call it </font></font><code>${ID}</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- hereinafter, such a variable replaces the OSD number with which we work. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Let's look at the condition before starting work:</font></font><br>
<br>
<pre><code class="bash hljs">root@hv-1 ~ <span class="hljs-comment"># ceph osd tree</span><font></font>
ID CLASS WEIGHT  TYPE NAME      STATUS REWEIGHT PRI-AFF<font></font>
-1       0.46857 root default<font></font>
-3       0.15619      host hv-1<font></font>
-5       0.15619      host hv-2<font></font>
 1   ssd 0.15619      osd.1     up     1.00000  1.00000<font></font>
-7       0.15619      host hv-3<font></font>
 2   ssd 0.15619      osd.2     up     1.00000  1.00000</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
To initiate the removal of OSD, you will need to smoothly execute </font></font><code>reweight</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">on it to zero. Thus, we reduce the amount of data in the OSD by balancing with other OSDs. To do this, the following commands are executed:</font></font><br>
<br>
<pre><code class="bash hljs">ceph osd reweight osd.<span class="hljs-variable">${ID}</span> 0.98<font></font>
ceph osd reweight osd.<span class="hljs-variable">${ID}</span> 0.88<font></font>
ceph osd reweight osd.<span class="hljs-variable">${ID}</span> 0.78</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
... and so on to zero. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">UPDATED</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : The comments on the article told about the method with </font></font><code>norebalance</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+ </font></font><code>backfill</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. The solution is correct, but first of all you need to look at the situation, because </font></font><code>norebalance</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">we use it when we do not want any OSD to cause a network load. </font></font><code>osd_max_backfill</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">used in cases where it is necessary to limit the speed of rebalance. As a result, rebalancing will be slower and cause less network load. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Smooth balancing is necessary</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> so as not to lose data. This is especially true if the OSD contains a large amount of data. To make sure that </font></font><code>reweight</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">everything was successful </font><font style="vertical-align: inherit;">after executing the commands </font><font style="vertical-align: inherit;">, you can </font></font><code>ceph -s</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">either </font><font style="vertical-align: inherit;">execute it </font><font style="vertical-align: inherit;">or </font><font style="vertical-align: inherit;">run </font><font style="vertical-align: inherit;">it in a separate terminal window</font></font><code>ceph -w</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">in order to observe changes in real time. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
When the OSD is "empty", you can begin the standard operation to remove it. </font><font style="vertical-align: inherit;">To do this, put the desired OSD in the state </font></font><code>down</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">:</font></font><br>
<br>
<pre><code class="bash hljs">ceph osd down osd.<span class="hljs-variable">${ID}</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
‚ÄúPull‚Äù OSD from the cluster:</font></font><br>
<br>
<pre><code class="bash hljs">ceph osd out osd.<span class="hljs-variable">${ID}</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Stop the OSD service and unmount its partition in the FS:</font></font><br>
<br>
<pre><code class="bash hljs">systemctl stop ceph-osd@<span class="hljs-variable">${ID}</span>
umount /var/lib/ceph/osd/ceph-<span class="hljs-variable">${ID}</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Remove the OSD from the </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">CRUSH map</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> :</font></font><br>
<br>
<pre><code class="bash hljs">ceph osd crush remove osd.<span class="hljs-variable">${ID}</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Delete the OSD user:</font></font><br>
<br>
<pre><code class="bash hljs">ceph auth del osd.<span class="hljs-variable">${ID}</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
And finally, delete the OSD itself:</font></font><br>
<br>
<pre><code class="bash hljs">ceph osd rm osd.<span class="hljs-variable">${ID}</span></code></pre><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Note</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : if you are using Ceph Luminous or higher, the above steps to remove OSD can be reduced to two commands:</font></font><br>
<br>
<pre><code class="bash hljs">ceph osd out osd.<span class="hljs-variable">${ID}</span>
ceph osd purge osd.<span class="hljs-variable">${ID}</span>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
If, after performing the above steps, the command is executed </font></font><code>ceph osd tree</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, it should be seen that the server where the work was performed no longer has the OSD for which the operations above were performed:</font></font><br>
<br>
<pre><code class="bash hljs">root@hv-1 ~ <span class="hljs-comment"># ceph osd tree</span><font></font>
ID CLASS WEIGHT  TYPE NAME     STATUS REWEIGHT PRI-AFF<font></font>
-1       0.46857      root default<font></font>
-3       0.15619      host hv-1<font></font>
-5       0.15619      host hv-2<font></font>
-7       0.15619      host hv-3<font></font>
 2   ssd 0.15619      osd.2    up     1.00000  1.00000</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Along the way, we note that the state of the Ceph cluster will go into </font></font><code>HEALTH_WARN</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, and we will also see a decrease in the number of OSDs and the amount of available disk space. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Next, the steps that will be required if you want to completely stop the server and, accordingly, remove it from Ceph will be described. </font><font style="vertical-align: inherit;">In this case, it is important to remember that </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">before shutting down the server, you must extract all the OSDs</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> on this server. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
If there is no more OSD left on this server, then after removing them, you need to exclude the server from the OSD card by </font></font><code>hv-2</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">running the following command:</font></font><br>
<br>
<pre><code class="bash hljs">ceph osd crush rm hv-2</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We delete it </font></font><code>mon</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">from the server </font></font><code>hv-2</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">by running the command below on another server (i.e., in this case, on </font></font><code>hv-1</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">):</font></font><br>
<br>
<pre><code class="bash hljs">ceph-deploy mon destroy hv-2</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
After that, you can stop the server and proceed with the subsequent actions (its redeployment, etc.).</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Case No. 2. </font><font style="vertical-align: inherit;">Disk space allocation in an already created Ceph cluster</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The second story begins with a preface about PG ( </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Placement Groups</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ). </font><font style="vertical-align: inherit;">The main role of PG in Ceph is primarily in the aggregation of Ceph objects and further replication in OSD. </font><font style="vertical-align: inherit;">The formula with which you can calculate the required number of PGs can be found in the </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">corresponding section of the</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Ceph documentation. </font><font style="vertical-align: inherit;">In the same place, this issue was also analyzed with specific examples. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
So: one of the common problems during Ceph operation is an unbalanced amount of OSD and PG between pools in Ceph. </font><font style="vertical-align: inherit;">In general, a correctly selected value of PG is the key to reliable operation of the cluster, and then we will consider what can happen in the opposite case. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The difficulty in choosing the right amount of PG lies in two things:</font></font><br>
<br>
<ol>
<li>       PG,   , ,     chunk'.</li>
<li>   , ,   PG,      .</li>
</ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In practice, a more serious problem is obtained: data overflow in one of the OSDs. The reason for this is that Ceph, when calculating the available amount of data (you can find it out </font></font><code>MAX AVAIL</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">in the output of the command </font></font><code>ceph df</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">for each pool separately), relies on the amount of available data in OSD. If there is not enough space in at least one OSD, then writing more data will not work until the data is properly distributed between all OSDs. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
It is worth clarifying that these problems </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">are largely solved at the stage of configuration of the Ceph cluster</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . One tool you can use is </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ceph PGCalc</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . With its help, the necessary amount of PG is visually calculated. However, you can resort to it in a situation where the Ceph cluster is </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">already</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">not configured correctly. Here it‚Äôs worth clarifying that as part of the correction, you will most likely need to reduce the number of PGs, and this feature is not available in older versions of Ceph (it appeared only with the </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nautilus</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> version </font><font style="vertical-align: inherit;">). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
So, let's imagine the following picture: a cluster has a status </font></font><code>HEALTH_WARN</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">due to the end of a place in one of the OSDs. A mistake will testify to this </font></font><code>HEALTH_WARN: 1 near full osd</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Below is an algorithm for overcoming this situation. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
First of all, you need to distribute the available data between the rest of the OSD. We already performed a similar operation in the first case, when we ‚Äúdrained‚Äù the knot - with the only difference that now will need to be slightly reduced </font></font><code>reweight</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. For example, before 0.95:</font></font><br>
<br>
<pre><code class="bash hljs">ceph osd reweight osd.<span class="hljs-variable">${ID}</span> 0.95</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
This frees up disk space in the OSD and fixes a bug in ceph health. </font><font style="vertical-align: inherit;">However, as already mentioned, this problem mainly arises due to incorrect Ceph settings in the initial stages: it is very important to reconfigure so that it does not appear in the future. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In our particular case, everything rested on:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">too important </font></font><code>replication_count</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">in one of the pools,</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Too many PGs in one pool and too small in another. </font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We will use the already mentioned calculator. It clearly shows what needs to be entered and, in principle, there is nothing complicated. Having set the necessary parameters, we get the following recommendations: </font></font><br>
<br>
<i><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Note</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : if you configure the Ceph cluster from scratch, another useful function of the calculator will be the generation of commands that will create pools from scratch with the parameters specified in the table.</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
The last column, </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Suggested PG Count,</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> helps you </font><i><font style="vertical-align: inherit;">navigate</font></i><font style="vertical-align: inherit;"> . In our case, the second one is also useful, where the replication parameter is indicated, since we decided to change the replication factor.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
So, first you need to change the replication settings - this is worth doing first of all, since by reducing the multiplier we will free up disk space. </font><font style="vertical-align: inherit;">During the execution of the command, you will notice that the value of available disk space will increase:</font></font><br>
<br>
<pre><code class="bash hljs">ceph osd pool <span class="hljs-variable">$pool_name</span> <span class="hljs-built_in">set</span> <span class="hljs-variable">$replication_size</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
And after its completion - we change the values ‚Äã‚Äãof the parameters </font></font><code>pg_num</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">and </font></font><code>pgp_num</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">as follows:</font></font><br>
<br>
<pre><code class="bash hljs">ceph osd pool <span class="hljs-built_in">set</span> <span class="hljs-variable">$pool_name</span> pg_num <span class="hljs-variable">$pg_number</span>
ceph osd pool <span class="hljs-built_in">set</span> <span class="hljs-variable">$pool_name</span> pgp_num <span class="hljs-variable">$pg_number</span></code></pre><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Important</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : we must sequentially change the number of PGs in each pool and not change the values ‚Äã‚Äãin other pools until the warnings </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ÄúDegraded data redundancy‚Äù</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> and </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚Äún-number of pgs degraded‚Äù</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> disappear </font><font style="vertical-align: inherit;">. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
You can also verify that everything was successful, according to the conclusions of the </font></font><code>ceph health detail</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">and </font><font style="vertical-align: inherit;">commands </font></font><code>ceph -s</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Case No. 3. </font><font style="vertical-align: inherit;">Virtual Machine Migration from LVM to Ceph RBD</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In a situation when the project uses virtual machines installed on rented bare-metal servers, the question often arises of fault-tolerant storage. And it is also very desirable that there is enough space in this storage ... Another common situation: there is a virtual machine with local storage on the server and you need to expand the disk, but nowhere, because there is no free disk space left on the server.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The problem can be solved in different ways - for example, by migrating to another server (if there is one) or adding new disks to the server. But it is not always possible to do this, so migration from LVM to Ceph can be an excellent solution to this problem. By choosing this option, we also simplify the further process of migration between servers, since there will be no need to move local storage from one hypervisor to another. The only catch is that you will have to stop the VM for the duration of the work. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As the recipe given below, </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">an article from this blog was</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> taken </font><font style="vertical-align: inherit;">, the instructions of which were tested in action. By the way, the </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">method of unimpeded migration is also described there.</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">However, in our case, it was simply not needed, so we did not check it. </font><font style="vertical-align: inherit;">If this is critical for your project, we will be happy to know about the results in the comments. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Let's get down to the practical part. </font><font style="vertical-align: inherit;">In the example, we use virsh and, accordingly, libvirt. </font><font style="vertical-align: inherit;">First, make sure that the Ceph pool to which the data will be migrated is connected to libvirt:</font></font><br>
<br>
<pre><code class="bash hljs">virsh pool-dumpxml <span class="hljs-variable">$ceph_pool</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The pool description should contain Ceph connection data with authorization data. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The next step is to convert the LVM image to Ceph RBD. </font><font style="vertical-align: inherit;">The runtime depends primarily on the size of the image:</font></font><br>
<br>
<pre><code class="bash hljs">qemu-img convert -p -O rbd /dev/main/<span class="hljs-variable">$vm_image_name</span> rbd:<span class="hljs-variable">$ceph_pool</span>/<span class="hljs-variable">$vm_image_name</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
After the conversion, the LVM image will remain, which will be useful if you cannot migrate the VM to RBD and have to roll back the changes. </font><font style="vertical-align: inherit;">Also - for the ability to quickly roll back the changes - we will backup the configuration file of the virtual machine:</font></font><br>
<br>
<pre><code class="bash hljs">virsh dumpxml <span class="hljs-variable">$vm_name</span> &gt; <span class="hljs-variable">$vm_name</span>.xml<font></font>
cp <span class="hljs-variable">$vm_name</span>.xml <span class="hljs-variable">$vm_name_backup</span>.xml</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
... and edit the original ( </font></font><code>vm_name.xml</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">). </font><font style="vertical-align: inherit;">Find a block with a description of the disk (starts with a line </font></font><code>&lt;disk type='file' device='disk'&gt;</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">and ends with </font></font><code>&lt;/disk&gt;</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">) and bring it to the following form:</font></font><br>
<br>
<pre><code class="xml hljs"><span class="hljs-tag">&lt;<span class="hljs-name">disk</span> <span class="hljs-attr">type</span>=<span class="hljs-string">'network'</span> <span class="hljs-attr">device</span>=<span class="hljs-string">'disk'</span>&gt;</span>
<span class="hljs-tag">&lt;<span class="hljs-name">driver</span> <span class="hljs-attr">name</span>=<span class="hljs-string">'qemu'</span>/&gt;</span>
<span class="hljs-tag">&lt;<span class="hljs-name">auth</span> <span class="hljs-attr">username</span>=<span class="hljs-string">'libvirt'</span>&gt;</span>
  <span class="hljs-tag">&lt;<span class="hljs-name">secret</span> <span class="hljs-attr">type</span>=<span class="hljs-string">'ceph'</span> <span class="hljs-attr">uuid</span>=<span class="hljs-string">'sec-ret-uu-id'</span>/&gt;</span>
 <span class="hljs-tag">&lt;/<span class="hljs-name">auth</span>&gt;</span>
<span class="hljs-tag">&lt;<span class="hljs-name">source</span> <span class="hljs-attr">protocol</span>=<span class="hljs-string">'rbd'</span> <span class="hljs-attr">name</span>=<span class="hljs-string">'$ceph_pool/$vm_image_name&gt;
  &lt;host name='</span><span class="hljs-attr">10.0.0.1</span>' <span class="hljs-attr">port</span>=<span class="hljs-string">'6789'</span>/&gt;</span>
  <span class="hljs-tag">&lt;<span class="hljs-name">host</span> <span class="hljs-attr">name</span>=<span class="hljs-string">'10.0.0.2'</span> <span class="hljs-attr">port</span>=<span class="hljs-string">'6789'</span>/&gt;</span>
<span class="hljs-tag">&lt;/<span class="hljs-name">source</span>&gt;</span>
<span class="hljs-tag">&lt;<span class="hljs-name">target</span> <span class="hljs-attr">dev</span>=<span class="hljs-string">'vda'</span> <span class="hljs-attr">bus</span>=<span class="hljs-string">'virtio'</span>/&gt;</span> 
<span class="hljs-tag">&lt;<span class="hljs-name">alias</span> <span class="hljs-attr">name</span>=<span class="hljs-string">'virtio-disk0'</span>/&gt;</span>
<span class="hljs-tag">&lt;<span class="hljs-name">address</span> <span class="hljs-attr">type</span>=<span class="hljs-string">'pci'</span> <span class="hljs-attr">domain</span>=<span class="hljs-string">'0x0000'</span> <span class="hljs-attr">bus</span>=<span class="hljs-string">'0x00'</span> <span class="hljs-attr">slot</span>=<span class="hljs-string">'0x04'</span> <span class="hljs-attr">function</span>=<span class="hljs-string">'0x0'</span>/&gt;</span>
<span class="hljs-tag">&lt;/<span class="hljs-name">disk</span>&gt;</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Let's take a look at some of the details:</font></font><br>
<br>
<ol>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The protocol </font></font><code>source</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">indicates the address to the storage in Ceph RBD (this is the address indicating the name of the Ceph pool and RBD image, which was determined at the first stage).</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The block </font></font><code>secret</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">indicates the type </font></font><code>ceph</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, as well as the UUID of the secret for connecting to it. </font><font style="vertical-align: inherit;">Its uuid can be found with the command </font></font><code>virsh secret-list</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In the block </font></font><code>host</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">addresses to monitors Ceph are indicated.</font></font></li>
</ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
After editing the configuration file and completing the conversion of LVM to RBD, you can apply the modified configuration file and start the virtual machine:</font></font><br>
<br>
<pre><code class="bash hljs">virsh define <span class="hljs-variable">$vm_name</span>.xml<font></font>
virsh start <span class="hljs-variable">$vm_name</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
It's time to verify that the virtual machine started correctly: you can find out, for example, by connecting to it via SSH or through </font></font><code>virsh</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
If the virtual machine is working correctly and you did not find other problems, then you can delete the LVM image that is no longer in use:</font></font><br>
<br>
<pre><code class="bash hljs">lvremove main/<span class="hljs-variable">$vm_image_name</span></code></pre><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Conclusion</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We encountered all the described cases in practice - we hope that the instructions will help other administrators to solve similar problems. </font><font style="vertical-align: inherit;">If you have comments or other similar stories from Ceph operating experience, we will be glad to see them in the comments!</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PS</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Read also in our blog:</font></font><br>
<br>
<ul>
<li> ¬´<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=">    :   Rook  K8s</a>¬ª;</li>
<li> ¬´<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=">Rook   Rook ‚Äî    </a>¬ª;</li>
<li> ¬´<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=">Rook ‚Äî ‚Äû‚Äú    Kubernetes</a>¬ª;</li>
<li> ¬´<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=">    provisioning  Kubernetes   Ceph</a>¬ª.</li>
</ul></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../en495854/index.html">How to escape from Covid and from surveillance?</a></li>
<li><a href="../en495856/index.html">Intel DevCloud for oneAPI - a cloud service for developers who sit at home</a></li>
<li><a href="../en495858/index.html">Remote and self-isolation: space experience to help earthlings</a></li>
<li><a href="../en495862/index.html">Trusted Types - a new way to protect web application code from XSS attacks</a></li>
<li><a href="../en495864/index.html">Creating a design system for the game: a detailed analysis of the approach</a></li>
<li><a href="../en495880/index.html">The book ‚ÄúHead First. Learning Go ¬ª</a></li>
<li><a href="../en495882/index.html">The 9 best open source finds for March 2020</a></li>
<li><a href="../en495884/index.html">Time Series Prediction Using Recurrent Neural Networks</a></li>
<li><a href="../en495888/index.html">PyCon Russia has opened CFP for future speakers. Participation Forms and Expected Topics</a></li>
<li><a href="../en495890/index.html">Configuring a Nginx / LetsEncrypt Bundle in Docker Swarm</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>