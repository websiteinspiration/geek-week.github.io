<!doctype html>
<html class="no-js" lang="fr">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👩‍👧 👒 😀 Bases de ZFS: stockage et performances 🐰 🙆🏿 😡</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ce printemps, nous avons déjà discuté de quelques sujets d'introduction, tels que la façon de vérifier la vitesse de vos disques et ce qu'est le RAID ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Bases de ZFS: stockage et performances</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/504692/"><img src="https://habrastorage.org/getpro/habr/post_images/abf/883/e96/abf883e96b01dbc78420e0dc1a158460.jpg"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ce printemps, nous avons déjà discuté de quelques sujets d'introduction, tels </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">que la façon de vérifier la vitesse de vos disques</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> et </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ce qu'est le RAID</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Dans le second d'entre eux, nous avons même promis de continuer à étudier les performances de différentes topologies multi-disques dans ZFS. </font><font style="vertical-align: inherit;">Il s'agit du système de fichiers de nouvelle génération qui est implémenté partout: d' </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Apple</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> à </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ubuntu</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Eh bien, aujourd'hui est le meilleur jour pour découvrir ZFS, lecteurs curieux. </font><font style="vertical-align: inherit;">Sachez simplement que, selon une évaluation prudente du développeur d'OpenZFS Matt Arens, "c'est vraiment compliqué". </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Mais avant d'en arriver aux chiffres - et ils le feront, je le promets - pour toutes les variantes de configuration de vosmidiskovoy ZFS, vous devez parler de la </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">façon dont</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ZFS stocke les données sur le disque.</font></font><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zpool, vdev et appareil</font></font></h1><br>
<img src="https://habrastorage.org/getpro/habr/post_images/674/1c6/ab3/6741c6ab310f4e0edf2adf7e2ca4c6bb.png"><br>
<i><font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ce diagramme de pool complet comprend trois vdev auxiliaires, un pour chaque classe et quatre pour RAIDz2. </font></font></font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/b9f/82c/887/b9f82c88748c44d1f86cc412a053bf94.png"><br>
<i><font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Il n'y a généralement aucune raison de créer un pool de types et de tailles de </font></font></font></i><font style="vertical-align: inherit;"><i><font color="gray"><font style="vertical-align: inherit;">vdev </font></font></i><i><font color="gray"><font style="vertical-align: inherit;">inappropriés - mais si vous le souhaitez, rien ne vous empêche de le faire.</font></font></i></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Pour vraiment comprendre le système de fichiers ZFS , vous devez examiner attentivement sa structure réelle. </font><font style="vertical-align: inherit;">Premièrement, ZFS combine les niveaux traditionnels de gestion des volumes et le système de fichiers. </font><font style="vertical-align: inherit;">Deuxièmement, il utilise un mécanisme de copie transactionnelle lors de l'écriture. </font><font style="vertical-align: inherit;">Ces caractéristiques signifient que le système est structurellement très différent des systèmes de fichiers ordinaires et des matrices RAID. </font><font style="vertical-align: inherit;">Le premier ensemble de blocs de construction de base à comprendre: un pool de stockage (zpool), un périphérique virtuel (vdev) et un périphérique réel (périphérique).</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">zpool</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Le pool de stockage zpool est la structure ZFS la plus élevée. Chaque pool contient un ou plusieurs périphériques virtuels. À leur tour, chacun d'eux contient un ou plusieurs appareils réels (appareil). Les pools virtuels sont des blocs autonomes. Un ordinateur physique peut contenir deux ou plusieurs pools distincts, mais chacun est complètement indépendant des autres. Les pools ne peuvent pas partager de périphériques virtuels. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La redondance de ZFS se situe au niveau des périphériques virtuels, mais pas au niveau des pools. Au niveau du pool, il n'y a absolument aucune redondance - si un lecteur vdev ou un vdev spécial est perdu, alors le pool entier est perdu avec lui.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Les pools de stockage modernes peuvent survivre à la perte d'un cache ou d'un journal de périphérique virtuel - bien qu'ils puissent perdre une petite quantité de données sales s'ils perdent le journal vdev lors d'une panne de courant ou d'un crash système. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Il existe une idée fausse commune selon laquelle des «bandes de données» (bandes) de ZFS sont enregistrées sur l'ensemble du pool. Ce n'est pas vrai. Zpool n'est pas du tout un RAID0 amusant, c'est plutôt un </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">JBOD</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> amusant </font><font style="vertical-align: inherit;">avec un mécanisme de distribution modifiable complexe.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pour la plupart, les entrées sont réparties entre les périphériques virtuels disponibles en fonction de l'espace disponible, donc théoriquement, elles seront toutes remplies en même temps. Dans les versions ultérieures de ZFS, l'utilisation actuelle (l'élimination) de vdev est prise en compte - si un périphérique virtuel est nettement plus chargé que l'autre (par exemple, en raison de la charge de lecture), il sera temporairement ignoré pour l'écriture, malgré la présence du coefficient d'espace libre le plus élevé. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Un mécanisme de détection du recyclage intégré aux méthodes modernes de distribution des enregistrements ZFS peut réduire la latence et augmenter le débit pendant les périodes de charge inhabituellement élevée - mais ce n'est pas </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">carte blanche</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">mélange involontaire de disques durs lents et de disques SSD rapides dans un seul pool. </font><font style="vertical-align: inherit;">Une telle piscine inégale fonctionnera toujours à la vitesse de l'appareil le plus lent, c'est-à-dire comme s'il était entièrement composé de tels appareils.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">vdev</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Chaque pool de stockage se compose d'un ou plusieurs périphériques virtuels (périphérique virtuel, vdev). </font><font style="vertical-align: inherit;">À son tour, chaque vdev comprend un ou plusieurs appareils réels. </font><font style="vertical-align: inherit;">La plupart des périphériques virtuels sont utilisés pour stocker facilement des données, mais il existe plusieurs classes d'assistance vdev, notamment CACHE, LOG et SPECIAL. </font><font style="vertical-align: inherit;">Chacun de ces types de vdev peut avoir l'une des cinq topologies: mono-périphérique, RAIDz1, RAIDz2, RAIDz3 ou miroir.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
RAIDz1, RAIDz2 et RAIDz3 sont des variations spéciales de ce que les anciens appellent la parité double (diagonale) RAID. 1, 2 et 3 font référence au nombre de blocs de parité alloués pour chaque bande de données. Au lieu de disques séparés pour la parité, les périphériques RAIDz virtuels répartissent uniformément cette parité sur les disques. Une matrice RAIDz peut perdre autant de disques qu'elle a de blocs de parité; s'il en perd un autre, il échouera et emportera le pool de stockage avec lui.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dans les périphériques virtuels en miroir (miroir vdev), chaque bloc est stocké sur chaque périphérique dans vdev. Bien qu'il s'agisse des miroirs double largeur les plus courants, il peut y avoir un nombre arbitraire de périphériques dans le miroir - dans les grandes installations, les triples sont souvent utilisés pour augmenter les performances de lecture et la tolérance aux pannes. Le miroir vdev peut survivre à toute panne pendant qu'au moins un périphérique dans vdev continue de fonctionner. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Les vdev uniques sont intrinsèquement dangereux. Un tel périphérique virtuel ne survivra pas à une seule défaillance - et s'il est utilisé comme stockage ou un vdev spécial, sa défaillance entraînera la destruction de l'ensemble du pool. Soyez très, très prudent ici.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Les appliances virtuelles CACHE, LOG et SPECIAL peuvent être créées à l'aide de l'une des topologies ci-dessus - mais n'oubliez pas que la perte d'une appliance virtuelle SPECIAL signifie la perte d'un pool, donc une topologie excessive est fortement recommandée.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">dispositif</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
C'est probablement le terme le plus facile à comprendre dans ZFS - c'est littéralement un périphérique à accès aléatoire par blocs. N'oubliez pas que les périphériques virtuels sont constitués de périphériques individuels et que le pool est composé de périphériques virtuels. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Les disques - magnétiques ou à semi-conducteurs - sont les unités de blocs les plus courantes utilisées comme blocs de construction vdev. Cependant, tout périphérique avec une poignée dans / dev convient - vous pouvez donc utiliser des matrices RAID matérielles entières en tant que périphériques séparés. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Un fichier brut simple est l'un des périphériques de blocs alternatifs les plus importants à partir desquels vdev peut être construit. Pools de tests à partir de </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">fichiers épars</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">&nbsp;- Un moyen très pratique de vérifier les commandes du pool et de voir combien d'espace est disponible dans le pool ou le périphérique virtuel de cette topologie. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/5cf/aa5/62c/5cfaa562cb208b654af113f7535b8f57.png"><br>
<i><font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Vous pouvez créer un pool de test à partir de fichiers épars en quelques secondes - mais n'oubliez pas de supprimer le pool entier et ses composants plus tard.</font></font></font></i> <br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Supposons que vous souhaitiez mettre un serveur sur huit disques et prévoyez d'utiliser des disques de 10 To (~ 9300 Gio) - mais vous ne savez pas exactement La topologie correspond le mieux à vos besoins. Dans l'exemple ci-dessus, en quelques secondes, nous construisons un pool de test à partir de fichiers épars - et nous savons maintenant que RAIDz2 vdev à partir de huit disques de 10 To fournit 50 TiB de capacité utile.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Une autre classe spéciale d'appareils est SPARE (rechange). Les périphériques remplaçables à chaud, contrairement aux périphériques conventionnels, appartiennent à l'ensemble du pool, pas à un seul périphérique virtuel. Si certains vdev du pool échouent et qu'un périphérique de rechange est connecté au pool et accessible, il rejoindra automatiquement le vdev affecté. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Après la connexion au vdev affecté, le périphérique de rechange commence à recevoir des copies ou la reconstruction des données qui devraient se trouver sur le périphérique manquant. Dans le RAID traditionnel, cela s'appelle la reconstruction, tandis que dans ZFS, il est appelé «resilvering».</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Il est important de noter que les appareils de remplacement ne remplacent pas définitivement les appareils défectueux. </font><font style="vertical-align: inherit;">Il ne s'agit que d'un remplacement temporaire pour réduire le temps pendant lequel une dégradation de vdev est observée. </font><font style="vertical-align: inherit;">Une fois que l'administrateur a remplacé le périphérique vdev défectueux, la redondance est restaurée sur ce périphérique permanent et SPARE se déconnecte de vdev et retourne au travail en tant que pièce de rechange pour l'ensemble du pool.</font></font><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ensembles de données, blocs et secteurs</font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Le prochain ensemble de blocs de construction que vous devez comprendre lors de notre voyage à travers ZFS n'est pas tant le matériel, mais la façon dont les données sont organisées et stockées. </font><font style="vertical-align: inherit;">Nous sautons plusieurs niveaux ici - comme le métaslab - afin de ne pas empiler les détails tout en maintenant une compréhension de la structure globale.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Base de données</font></font></h3><br>
<img src="https://habrastorage.org/getpro/habr/post_images/bd3/c48/d9d/bd3c48d9dff6e0f493a5d90d1dca6d1d.png"><br>
<i><font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Lorsque nous créons un ensemble de données pour la première fois, il affiche tout l'espace de pool disponible. Ensuite, nous définissons le quota - et modifions le point de montage. La magie! </font></font></font></i> <br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/a18/3de/210/a183de210cdc57cd1421652201cbf2c3.png"><br>
<i><font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zvol n'est pour la plupart qu'un ensemble de données, dépourvu de sa couche de système de fichiers, que nous remplaçons ici par un</font></font></font></i> <br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
système de fichiers</font><i><font color="gray"><font style="vertical-align: inherit;"> ext4 tout à fait normal</font></font></i><font style="vertical-align: inherit;"> . L'ensemble de données ZFS est à peu près le même qu'un système de fichiers monté standard. Comme un système de fichiers classique, il semble à première vue être «juste un autre dossier». Mais aussi, comme les systèmes de fichiers montés conventionnels, chaque jeu de données ZFS a son propre ensemble de propriétés de base.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Tout d'abord, un ensemble de données peut avoir un quota attribué. S'il est installé</font></font><code>zfs set quota=100G poolname/datasetname</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, vous ne pouvez pas écrire dans le dossier monté</font></font><code>/poolname/datasetname</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">plus de 100 Gio.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Vous remarquez la présence - et l'absence - de barres obliques au début de chaque ligne? Chaque ensemble de données a sa propre place à la fois dans la hiérarchie ZFS et dans la hiérarchie de montage système. Il n'y a pas de barre oblique dans la hiérarchie ZFS - vous commencez avec le nom du pool, puis le chemin d'un ensemble de données au suivant. Par exemple, </font></font><code>pool/parent/child</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">pour un ensemble de données nommé </font></font><code>child</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">sous l'ensemble </font></font><code>parent</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">de </font><font style="vertical-align: inherit;">données parent </font><font style="vertical-align: inherit;">dans un pool avec un nom de création </font></font><code>pool</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Par défaut, le point de montage de l'ensemble de données sera équivalent à son nom dans la hiérarchie ZFS, avec une barre oblique au début - le pool avec le nom est </font></font><code>pool</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">monté en tant que </font></font><code>/pool</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, l'ensemble de données est </font></font><code>parent</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">monté dans </font></font><code>/pool/parent</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, et l' </font><font style="vertical-align: inherit;">ensemble de données </font><font style="vertical-align: inherit;">enfant </font><font style="vertical-align: inherit;">est monté </font></font><code>child</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">dans </font></font><code>/pool/parent/child</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Cependant, le point de montage système de l'ensemble de données peut être modifié. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Si nous indiquons</font></font><code>zfs set mountpoint=/lol pool/parent/child</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, puis l'ensemble de données est </font></font><code>pool/parent/child</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">monté dans le système en tant que </font></font><code>/lol</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En plus des ensembles de données, nous devons mentionner les volumes (zvols). </font><font style="vertical-align: inherit;">Un volume est à peu près similaire à un ensemble de données, sauf qu'il n'a en fait pas de système de fichiers - c'est juste un périphérique bloc. </font><font style="vertical-align: inherit;">Vous pouvez, par exemple, créer </font></font><code>zvol</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">avec un nom </font></font><code>mypool/myzvol</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, puis le formater avec le système de fichiers ext4, puis monter ce système de fichiers - vous avez maintenant le système de fichiers ext4, mais avec la prise en charge de toutes les fonctionnalités de sécurité ZFS! </font><font style="vertical-align: inherit;">Cela peut sembler idiot sur un ordinateur, mais cela a beaucoup plus de sens en tant que backend lors de l'exportation d'un périphérique iSCSI.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Blocs</font></font></h3><br>
<img src="https://habrastorage.org/getpro/habr/post_images/74b/4dd/d00/74b4ddd009e67db1b1b6c4467bcf6fa3.png"><br>
<i><font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Un fichier est représenté par un ou plusieurs blocs. Chaque bloc est stocké sur un appareil virtuel. La taille du bloc est généralement égale au paramètre </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">recordsize</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , mais peut être réduite à </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2 ^ ashift</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> s'il contient des métadonnées ou un petit fichier. </font></font></font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/8d1/7fd/ad2/8d17fdad2eda641c801e5e6a302f6e38.png"><br>
<i><font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nous </font><font style="vertical-align: inherit;">ne plaisantons </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">vraiment</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> pas sur les énormes dommages aux performances si vous installez trop petit ashift</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Dans le pool ZFS, toutes les données, y compris les métadonnées, sont stockées dans des blocs. La taille de bloc maximale pour chaque ensemble de données est définie dans la propriété</font></font><code>recordsize</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(taille d'enregistrement). La taille de l'enregistrement peut varier, mais cela ne changera pas la taille ou l'emplacement des blocs qui ont déjà été écrits dans l'ensemble de données - cela ne fonctionne que pour les nouveaux blocs au fur et à mesure qu'ils sont écrits.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Sauf indication contraire, la taille d'enregistrement actuelle est de 128 Ko par défaut. C'est une sorte de compromis difficile dans lequel les performances ne seront pas idéales, mais pas terribles dans la plupart des cas. </font></font><code>Recordsize</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">peut être réglé sur n'importe quelle valeur de 4K à 1M (avec des paramètres supplémentaires, </font></font><code>recordsize</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">vous pouvez définir encore plus, mais c'est rarement une bonne idée). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Tout bloc fait référence aux données d'un seul fichier - vous ne pouvez pas compresser deux fichiers différents en un seul bloc. Chaque fichier se compose d'un ou plusieurs blocs, selon la taille. Si la taille du fichier est inférieure à la taille de l'enregistrement, elle sera enregistrée dans un bloc plus petit - par exemple, un bloc avec un fichier de 2 Ko n'occupera qu'un seul secteur de 4 Ko sur le disque. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Si le fichier est suffisamment volumineux et nécessite plusieurs blocs, tous les enregistrements avec ce fichier auront une taille</font></font><code>recordsize</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">&nbsp;- y compris le dernier enregistrement, dont la partie principale peut s'avérer être de l' </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">espace inutilisé</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Les volumes Zvol n'ont pas de propriété </font></font><code>recordsize</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">&nbsp;- ils ont plutôt une propriété équivalente </font></font><code>volblocksize</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Les secteurs</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Le dernier élément de construction le plus fondamental est le secteur. Il s'agit de la plus petite unité physique pouvant être écrite ou lue sur l'unité de base. Pendant plusieurs décennies, la plupart des disques ont utilisé des secteurs de 512 octets. Récemment, la plupart des disques sont configurés pour 4 secteurs KiB, et dans certains - en particulier les SSD - 8 secteurs KiB ou même plus. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ZFS possède une propriété qui vous permet de définir manuellement la taille du secteur. Ceci est une propriété </font></font><code>ashift</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Il est quelque peu déroutant que ashift soit une puissance de deux. Par exemple, cela </font></font><code>ashift=9</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">signifie une taille de secteur de 2 ^ 9, ou 512 octets.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ZFS demande au système d'exploitation des informations détaillées sur chaque périphérique de bloc lorsqu'il est ajouté au nouveau vdev et théoriquement définit automatiquement ashift correctement sur la base de ces informations. Malheureusement, de nombreux disques mentent sur leur taille de secteur afin de maintenir la compatibilité avec Windows XP (qui n'a pas pu comprendre les disques avec d'autres tailles de secteur). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Cela signifie que l'administrateur ZFS est fortement conseillé de connaître la taille réelle du secteur de leurs appareils et d'installer manuellement</font></font><code>ashift</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Si un décalage trop petit est défini, le nombre d'opérations de lecture / écriture augmente astronomiquement. Donc, écrire des «secteurs» de 512 octets dans le vrai secteur de 4 Ko signifie écrire le premier «secteur», puis lire le secteur de 4 Ko, le changer avec le deuxième «secteur» de 512 octets, le réécrire dans le nouveau secteur de 4 Ko, etc. pour chaque entrée. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dans le monde réel, une telle pénalité bat les </font></font><code>ashift=13</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SSD </font><font style="vertical-align: inherit;">Samsung EVO, pour lesquels elle doit agir </font><font style="vertical-align: inherit;">, mais ces SSD reposent sur la taille de leur secteur, et par conséquent, elle est définie par défaut </font></font><code>ashift=9</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Si un administrateur système expérimenté ne modifie pas ce paramètre, ce SSD est </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">plus lent qu'un</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> disque dur magnétique classique. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A titre de comparaison, pour une taille trop grande</font></font><code>ashift</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">il n'y a pratiquement pas de pénalité. </font><font style="vertical-align: inherit;">Il n'y a pas de réelle diminution de la productivité et l'augmentation de l'espace inutilisé est infiniment petite (ou égale à zéro avec la compression activée). </font><font style="vertical-align: inherit;">Par conséquent, nous recommandons fortement que même les disques qui utilisent réellement des secteurs de 512 octets soient installés </font></font><code>ashift=12</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ou même </font></font><code>ashift=13</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">de regarder l'avenir avec confiance. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La propriété est </font></font><code>ashift</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">définie pour chaque périphérique virtuel vdev, et </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">non pour le pool</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , comme beaucoup le pensent par erreur - et ne change pas après l'installation. </font><font style="vertical-align: inherit;">Si vous avez accidentellement renversé </font></font><code>ashift</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">lors de l'ajout d'un nouveau vdev à la piscine, vous avez irrémédiablement contaminé cette piscine avec un appareil à faible performance et, en règle générale, il n'y a pas d'autre moyen que de détruire la piscine et de tout recommencer. </font><font style="vertical-align: inherit;">Même la suppression de vdev ne vous sauvera pas d'une configuration cassée</font></font><code>ashift</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">!</font></font><br>
<br>
<h3>   </h3><br>
<img src="https://habrastorage.org/getpro/habr/post_images/38b/a1e/4a8/38ba1e4a8fa0e255081ed8db259a302f.gif"><br>
<i><font color="gray">      &nbsp;—     ,   </font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/90d/4cb/a35/90d4cba35ffa5a3e44a7ca5f61d4491b.gif"><br>
<i><font color="gray">         ,     </font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/c8b/2af/ffb/c8b2afffbc46f63f6a7fe1167edf5dcb.gif"><br>
<i><font color="gray">  ,      ,   « »   « »,        </font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/4c1/e2b/818/4c1e2b818077cb07d651527e214363fe.gif"><br>
<i><font color="gray">     ,       —      ,     ,       </font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La copie sur écriture (CoW) est le fondement fondamental de ce qui rend ZFS si génial. Le concept de base est simple - si vous demandez au système de fichiers traditionnel de modifier le fichier, il fera exactement ce que vous avez demandé. Si vous demandez au système de fichiers avec copie pendant l'enregistrement de faire de même, il vous répondra «bien» - mais cela vous ment. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Au lieu de cela, le système de fichiers copie-écriture écrit la nouvelle version du bloc modifié, puis met à jour les métadonnées du fichier pour rompre le lien avec l'ancien bloc et associer le nouveau bloc que vous venez d'écrire.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Déconnecter l'ancienne unité et relier la nouvelle se fait en une seule opération, donc elle ne peut pas être interrompue - si vous réinitialisez l'alimentation après cela, vous avez une nouvelle version du fichier, et si vous réinitialisez l'alimentation plus tôt, alors vous avez l'ancienne version. </font><font style="vertical-align: inherit;">Dans tous les cas, il n'y aura pas de conflit dans le système de fichiers. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La copie lors de l'écriture dans ZFS s'effectue non seulement au niveau du système de fichiers, mais également au niveau de la gestion des disques. </font><font style="vertical-align: inherit;">Cela signifie que ZFS n'est pas soumis à un espace dans l'enregistrement (un </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">trou dans le RAID</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ) - un phénomène lorsque la bande n'a réussi qu'à enregistrer partiellement avant que le système ne plante, avec la baie endommagée après un redémarrage. </font><font style="vertical-align: inherit;">Ici, la bande est atomique, vdev est toujours cohérent et </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bob est votre oncle</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ZIL: Journal d'intention ZFS</font></font></h3><br>
<img src="https://habrastorage.org/getpro/habr/post_images/567/71c/73f/56771c73f9a28ebaed161e02313deadb.png"><br>
<i><font color="gray"> ZFS     &nbsp;—  ,      ZIL,            </font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/cec/7c5/437/cec7c5437087f6816f9cdea5f6829820.png"><br>
<i><font color="gray"> ,   ZIL,    .      </font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/075/65a/d6b/07565ad6b2f431db3e6bc20cd24a653b.png"><br>
<i><font color="gray">SLOG,   LOG-, —   &nbsp;— , ,  &nbsp;—&nbsp;vdev,  ZIL      </font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/927/0f7/539/9270f7539b759aa37896d41e04c4ec47.png"><br>
<i><font color="gray">      ZIL &nbsp;—    ZIL   SLOG,      </font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Il existe deux catégories principales d'opérations d'écriture: synchrones (sync) et asynchrones (async). Pour la plupart des charges de travail, la grande majorité des opérations d'écriture sont asynchrones - le système de fichiers vous permet de les agréger et de les livrer par lots, réduisant la fragmentation et augmentant considérablement le débit. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Les enregistrements synchrones sont une tout autre affaire. Lorsqu'une application demande une écriture synchrone, elle indique au système de fichiers: "Vous devez valider ceci dans la mémoire non volatile </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">dès maintenant</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , et d'ici là, je ne peux rien faire de plus." Par conséquent, les enregistrements synchrones doivent être immédiatement validés sur le disque - et si cela augmente la fragmentation ou réduit la bande passante, qu'il en soit ainsi.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ZFS traite les enregistrements synchrones différemment des systèmes de fichiers standard - au lieu de les télécharger immédiatement dans un stockage normal, ZFS les enregistre dans une zone de stockage spéciale appelée journal d'intention ZFS - Journal d'intention ZFS ou ZIL. L'astuce est que ces enregistrements </font><font style="vertical-align: inherit;">restent </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">également</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> en mémoire, étant agrégés avec des demandes d'écriture asynchrones régulières, pour être ensuite transférés dans le stockage en tant que TXG parfaitement normaux (groupes de transactions, groupes de transactions). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En fonctionnement normal, le ZIL est enregistré et jamais lu à nouveau. Lorsque, après quelques instants, les enregistrements de ZIL sont fixés dans le stockage principal dans le TXG ordinaire de la RAM, ils sont déconnectés de ZIL. La seule chose quand quelque chose est lu depuis ZIL est lors de l'importation du pool.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Si ZFS se bloque - le système d'exploitation se bloque ou des pannes de courant - lorsqu'il y a des données dans ZIL, ces données seront lues lors de la prochaine importation du pool (par exemple, lorsque le système d'urgence redémarre). Tout ce qui se trouve dans le ZIL sera lu, combiné en groupes TXG, engagé dans le stockage principal, puis déconnecté du ZIL pendant le processus d'importation. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
L'une des classes d'assistance vdev est appelée LOG ou SLOG, le périphérique LOG secondaire. Il a une tâche - fournir au pool un périphérique vdev séparé et, de préférence beaucoup plus rapide, avec une résistance à l'écriture très élevée, pour stocker ZIL, au lieu de stocker ZIL dans le stockage vdev principal. ZIL lui-même se comporte de la même manière quel que soit l'emplacement de stockage, mais si vdev avec LOG a des performances d'écriture très élevées, les écritures synchrones seront plus rapides.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
L'ajout de vdev avec LOG au pool </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ne peut pas</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> améliorer les performances d'écriture asynchrone - même si vous forcez toutes les écritures dans ZIL à l'aide </font></font><code>zfs set sync=always</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, elles seront toujours liées au référentiel principal dans TXG de la même manière et au même rythme que sans journal. </font><font style="vertical-align: inherit;">La seule amélioration directe des performances est le retard de l'enregistrement synchrone (car une vitesse d'enregistrement plus élevée accélère les opérations </font></font><code>sync</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Cependant, dans un environnement qui nécessite déjà un grand nombre d'écritures synchrones, vdev LOG peut indirectement accélérer les écritures asynchrones et les lectures non mises en cache. </font><font style="vertical-align: inherit;">Le téléchargement d'enregistrements ZIL vers un journal vdev séparé signifie moins de concurrence pour les IOPS dans le stockage principal, ce qui améliore dans une certaine mesure les performances de toutes les opérations de lecture et d'écriture.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Instantanés</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Le mécanisme de copie en écriture est également une base essentielle pour les instantanés ZFS atomiques et la réplication asynchrone incrémentielle. </font><font style="vertical-align: inherit;">Le système de fichiers actif a une arborescence de pointeurs qui marque tous les enregistrements avec les données actuelles - lorsque vous prenez un instantané, vous faites simplement une copie de cette arborescence de pointeurs. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Lorsqu'un enregistrement est écrasé dans le système de fichiers actif, ZFS écrit d'abord la nouvelle version du bloc dans l'espace inutilisé. </font><font style="vertical-align: inherit;">Il détache ensuite l'ancienne version du bloc du système de fichiers actuel. </font><font style="vertical-align: inherit;">Mais si un instantané fait référence à l'ancien bloc, il reste inchangé. </font><font style="vertical-align: inherit;">L'ancien bloc ne sera pas restauré en tant qu'espace libre jusqu'à ce que tous les instantanés liés à ce bloc soient détruits!</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La réplication</font></font></h3><br>
<img src="https://habrastorage.org/getpro/habr/post_images/e69/167/01d/e6916701d4aa3ff27bb42efc43be60da.png"><br>
<i><font color="gray">  Steam  2015   158&nbsp;   126&nbsp;927 .        rsync&nbsp;—  ZFS    « »  750% .</font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/25f/376/0ab/25f3760ab64d6647571b9c02804b39f0.png"><br>
<i><font color="gray">      40-     Windows 7&nbsp;—   .  ZFS   289  ,  rsync&nbsp;—  «»  161  ,    ,   rsync   --inplace.</font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/776/46b/a3a/77646ba3ac20eeb0933d7dc7d644296c.png"><br>
<i><font color="gray">    ,  rsync    .  1,9         &nbsp;—    ,   ZFS   1148  ,  rsync,    rsync --inplace</font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Une fois que vous comprenez le fonctionnement des instantanés, il est facile de saisir l'essence de la réplication. Puisqu'un instantané n'est qu'un arbre de pointeurs vers des enregistrements, il s'ensuit que si nous faisons un </font></font><code>zfs send</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">instantané, nous envoyons cet arbre et tous les enregistrements qui lui sont associés. Lorsque nous passons ce </font></font><code>zfs send</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">dans </font></font><code>zfs receive</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l'objet cible, il écrit deux le contenu réel du bloc et l'arbre des pointeurs qui font </font><font style="vertical-align: inherit;">référence à des </font><font style="vertical-align: inherit;">blocs à l'ensemble de données cible. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Tout devient encore plus intéressant dans la seconde </font></font><code>zfs send</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Nous avons maintenant deux systèmes, chacun contenant </font></font><code>poolname/datasetname@1</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, et vous prenez un nouveau cliché </font></font><code>poolname/datasetname@2</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Par conséquent, dans le pool source que vous avez </font></font><code>datasetname@1</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">et </font></font><code>datasetname@2</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, et dans le pool cible jusqu'à présent, seul le premier instantané </font></font><code>datasetname@1</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Puisque nous avons un instantané commun entre la source et la cible</font></font><code>datasetname@1</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, nous pouvons faire </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">incrémentiel</font></font></i> <code>zfs send</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> par-dessus. Lorsque nous disons au système </font></font><code>zfs send -i poolname/datasetname@1 poolname/datasetname@2</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, il compare deux arbres de pointeurs. Tous les pointeurs qui n'existent que dans </font></font><code>@2</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, évidemment, se réfèrent à de nouveaux blocs - nous avons donc besoin du contenu de ces blocs. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Sur un système distant, le traitement incrémentiel est </font></font><code>send</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">tout aussi simple. Tout d'abord, nous enregistrons toutes les nouvelles entrées incluses dans le flux </font></font><code>send</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, puis ajoutons des pointeurs à ces blocs. Voila, dans notre </font></font><code>@2</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">nouveau système! </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La réplication incrémentielle asynchrone ZFS est une énorme amélioration par rapport aux méthodes non instantanées antérieures comme rsync. Dans les deux cas, seules les données modifiées sont transmises - mais rsync doit d'abord </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">lire</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">du disque toutes les données des deux côtés pour vérifier le montant et le comparer. </font><font style="vertical-align: inherit;">En revanche, la réplication ZFS ne lit rien, sauf les arborescences de pointeurs - et les blocs qui ne sont pas représentés dans l'instantané général.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Compression en ligne</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Le mécanisme de copie sur écriture simplifie également le système de compression intégré. Dans un système de fichiers traditionnel, la compression est problématique - l'ancienne version et la nouvelle version des données modifiées se trouvent dans le même espace. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Si vous considérez un morceau de données au milieu d'un fichier qui commence sa vie comme un mégaoctet de zéros à partir de 0x00000000 et ainsi de suite - il est très facile de le compresser en un secteur sur le disque. Mais que se passe-t-il si nous remplaçons ce mégaoctet de zéros par un mégaoctet de données incompressibles comme le JPEG ou le bruit pseudo-aléatoire? Du coup, ce mégaoctet de données ne nécessitera pas un, mais 256 secteurs de 4 Ko, et à cet endroit sur le disque un seul secteur est réservé.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ZFS n'a pas un tel problème, car les enregistrements modifiés sont toujours écrits dans l'espace inutilisé - le bloc d'origine n'occupe qu'un seul secteur de 4 Ko, et un nouvel enregistrement prendra 256, mais ce n'est pas un problème - un fragment récemment modifié du «milieu» du fichier serait écrit dans l'espace inutilisé indépendamment du fait que sa taille ait changé ou non, donc pour ZFS, c'est une situation normale. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La compression ZFS intégrée est désactivée par défaut, et le système propose des algorithmes de plug-in - maintenant parmi eux sont LZ4, gzip (1-9), LZJB et ZLE.</font></font><br>
<br>
<ul>
<li><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">LZ4</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> est un algorithme de streaming qui offre une compression et une décompression extrêmement rapides et des gains de performances pour la plupart des cas d'utilisation - même sur des processeurs assez lents.</font></font><br>
</li>
<li><b>GZIP</b> —  ,       Unix-.        1-9,       CPU      9.       (   )  ,    &nbsp;   c CPU&nbsp;—    ,     .<br>
</li>
<li><b>LZJB</b> —    ZFS.       , LZ4     .<br>
</li>
<li><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ZLE</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - encodage de niveau zéro, encodage de niveau zéro. </font><font style="vertical-align: inherit;">Il ne touche pas du tout aux données normales, mais compresse de grandes séquences de zéros. </font><font style="vertical-align: inherit;">Utile pour les ensembles de données complètement incompressibles (par exemple, JPEG, MP4 ou d'autres formats déjà compressés), car il ignore les données incompressibles, mais compresse l'espace inutilisé dans les enregistrements résultants.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nous recommandons la compression LZ4 pour presque tous les cas d'utilisation; </font><font style="vertical-align: inherit;">La pénalité de performances pour rencontrer des données incompressibles est très faible et le </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">gain de</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> performances pour les données typiques est significatif. </font><font style="vertical-align: inherit;">Copier une image de machine virtuelle pour une nouvelle installation du système d'exploitation Windows (système d'exploitation fraîchement installé, aucune donnée à l'intérieur pour l'instant) avec </font></font><code>compression=lz4</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">passé 27% plus rapidement qu'avec </font></font><code>compression=none</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, dans </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ce test de 2015</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ARC - cache de remplacement adaptatif</font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ZFS est le seul système de fichiers moderne à notre connaissance qui utilise son propre mécanisme de mise en cache de lecture et ne s'appuie pas sur le cache de pages du système d'exploitation pour stocker des copies des blocs récemment lus dans la RAM. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bien que son propre cache ne soit pas sans problèmes - ZFS ne peut pas répondre aux nouvelles demandes d'allocation de mémoire aussi rapidement que le noyau, donc un nouvel appel d' </font></font><code>malloc()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">allocation de mémoire peut échouer s'il a besoin de RAM actuellement occupée par ARC. </font><font style="vertical-align: inherit;">Mais il y a de bonnes raisons d'utiliser votre propre cache, du moins pour l'instant.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Tous les systèmes d'exploitation modernes bien connus, y compris MacOS, Windows, Linux et BSD, utilisent l'algorithme LRU (le moins récemment utilisé) pour implémenter le cache de pages. Il s'agit d'un algorithme primitif qui soulève le bloc mis en cache «en haut de la file d'attente» après chaque lecture et pousse les blocs «en file d'attente» au besoin pour ajouter de nouveaux échecs de cache (blocs qui auraient dû être lus à partir du disque, pas à partir du cache). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Habituellement, l'algorithme fonctionne bien, mais sur les systèmes avec de grands ensembles de données de travail, LRU mène facilement à la destruction - évincant les blocs fréquemment nécessaires pour faire de la place pour les blocs qui ne seront plus jamais lus dans le cache. </font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ARC</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">&nbsp;- un algorithme beaucoup moins naïf, qui peut être considéré comme un cache "pondéré". Après chaque lecture du bloc mis en cache, il devient un peu «plus lourd» et il devient plus difficile à évincer - et même après avoir évincé le bloc est </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">suivi</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> pendant une certaine période de temps. Un bloc qui a été éliminé mais qui doit ensuite être relu dans le cache deviendra également «plus lourd».</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Le résultat final de tout cela est un cache avec un taux de réussite beaucoup plus élevé - le rapport entre les succès dans le cache (lus dans le cache) et les ratés (lus sur le disque). </font><font style="vertical-align: inherit;">Il s'agit de statistiques extrêmement importantes - non seulement le cache atteint lui-même des ordres de grandeur de service plus rapides, mais les échecs de cache peuvent également être traités plus rapidement, car plus il y a d'occurrences de cache, moins il y a de demandes de disque simultanées et moins le délai pour les autres manquants qui doivent être traités avec conduire.</font></font><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Conclusion</font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Après avoir étudié la sémantique de base de ZFS - comment fonctionne la copie pendant l'écriture, ainsi que les relations entre les pools de stockage, les périphériques virtuels, les blocs, les secteurs et les fichiers - nous sommes prêts à discuter des performances réelles avec des nombres réels. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dans la partie suivante, nous examinerons les performances réelles des pools avec vdev en miroir et RAIDz, en comparaison les uns avec les autres, ainsi qu'en comparaison avec les topologies RAID traditionnelles du noyau Linux, que nous avons examinées </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">précédemment</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Au début, nous voulions considérer uniquement les bases - les topologies ZFS elles-mêmes - mais après </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">cela,</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> nous serons prêts à parler d'un réglage et d'un réglage ZFS plus avancés, y compris l'utilisation de types de vdev auxiliaires tels que L2ARC, SLOG et allocation spéciale.</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr504680/index.html">Présentation de la bibliothèque NLP de SpaL</a></li>
<li><a href="../fr504682/index.html">Nostalgia Post: j2me, Gravity Defied, 64kb</a></li>
<li><a href="../fr504686/index.html">Comment dessiner un chat</a></li>
<li><a href="../fr504688/index.html">Les masques sont inutiles: critique scientifique de la politique sociale à KOVID-19</a></li>
<li><a href="../fr504690/index.html">L'histoire de la façon dont j'ai configuré Azure AD B2C sur React et React Native, partie 3 (didacticiel)</a></li>
<li><a href="../fr504694/index.html">Comment compiler un décorateur - C ++, Python et sa propre implémentation. Partie 1</a></li>
<li><a href="../fr504696/index.html">Nouvelles du monde d'OpenStreetMap n ° 513 (12.05.2020-18.05.2020)</a></li>
<li><a href="../fr504698/index.html">Intégration sur un site distant</a></li>
<li><a href="../fr504700/index.html">Tablette graphique soviétique "sketch"</a></li>
<li><a href="../fr504702/index.html">Les gens ne veulent pas connaître l'anglais</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>