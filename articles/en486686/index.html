<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üö® üò∫ üíáüèæ About implementing a deep learning library in Python üôÜüèº üôÖ üë©üèº‚Äçüé§</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Deep learning technologies have come a long way in a short period of time - from simple neural networks to fairly complex architectures. To support th...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>About implementing a deep learning library in Python</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ruvds/blog/486686/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Deep learning technologies have come a long way in a short period of time - from simple neural networks to fairly complex architectures. To support the rapid spread of these technologies, various libraries and deep learning platforms have been developed. One of the main goals of such libraries is to provide developers with simple interfaces to create and train neural network models. Such libraries allow their users to pay more attention to the tasks being solved, and not to the subtleties of model implementation. To do this, you may need to hide the implementation of basic mechanisms behind several levels of abstraction. And this, in turn, complicates the understanding of the basic principles on which deep learning libraries are based.</font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><img src="https://habrastorage.org/webt/bp/yi/sl/bpyislfb1o7e-qh7exvklh1oxuw.jpeg"></a><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The article, the translation of which we are publishing, is aimed at analyzing the features of the device of low-level building blocks of deep learning libraries. </font><font style="vertical-align: inherit;">First, we briefly talk about the essence of deep learning. </font><font style="vertical-align: inherit;">This will allow us to understand the functional requirements for the respective software. </font><font style="vertical-align: inherit;">Then we look at developing a simple but working deep learning library in Python using NumPy. </font><font style="vertical-align: inherit;">This library is capable of providing end-to-end training for simple neural network models. </font><font style="vertical-align: inherit;">Along the way, we'll talk about the various components of deep learning frameworks. </font><font style="vertical-align: inherit;">The library that we will be considering is quite small, less than 100 lines of code. </font><font style="vertical-align: inherit;">And this means that it will be quite simple to figure it out. </font><font style="vertical-align: inherit;">The full project code, which we will deal with, can be found </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">here</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<a name="habracut"></a><br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">General information</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Typically, deep learning libraries (such as TensorFlow and PyTorch) consist of the components shown in the following figure.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c49/b9f/396/c49b9f39652c0260a9e30ee4e5dea146.jpg"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Components of the deep learning framework</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Let's analyze these components.</font></font><br>
<br>
<h3><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ñç Operators</font></font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The concepts of ‚Äúoperator‚Äù and ‚Äúlayer‚Äù (layer) are usually used interchangeably. </font><font style="vertical-align: inherit;">These are the basic building blocks of any neural network. </font><font style="vertical-align: inherit;">Operators are vector functions that transform data. </font><font style="vertical-align: inherit;">Among the frequently used operators, one can distinguish such as linear and convolution layers, sub-sampling layers (pooling), semi-linear (ReLU) and sigmoid (sigmoid) activation functions.</font></font><br>
<br>
<h3><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ñçOptimizers (optimizers)</font></font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Optimizers are the foundation of deep learning libraries. </font><font style="vertical-align: inherit;">They describe methods for adjusting model parameters using certain criteria and taking into account the goal of optimization. </font><font style="vertical-align: inherit;">Among the well-known optimizers, SGD, RMSProp and Adam can be noted.</font></font><br>
<br>
<h3><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ñç Loss functions</font></font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Loss functions are analytic and differentiable mathematical expressions that are used as a substitute for the goal of optimization when solving a problem. </font><font style="vertical-align: inherit;">For example, the cross-entropy function and the piecewise linear function are usually used in classification problems.</font></font><br>
<br>
<h3><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ñç Initializers</font></font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Initializers provide initial values ‚Äã‚Äãfor model parameters. </font><font style="vertical-align: inherit;">It is these values ‚Äã‚Äãthat the parameters have at the beginning of training. </font><font style="vertical-align: inherit;">Initializers play an important role in the training of neural networks, since unsuccessful initial parameters may mean that the network will learn slowly, or may not learn at all. </font><font style="vertical-align: inherit;">There are many ways to initialize the weights of a neural network. </font><font style="vertical-align: inherit;">For example - you can assign them small random values ‚Äã‚Äãfrom the normal distribution. </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Here is a</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> page where you can learn about the different types of initializers.</font></font><br>
<br>
<h3><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ñç Regularizers</font></font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Regularizers are tools that avoid network retraining and help the network gain generalization. You can deal with retraining the network in explicit or implicit ways. Explicit methods involve structural limitations on weights. For example, minimizing their L1-Norm and L2-Norm, which, accordingly, makes the weight values ‚Äã‚Äãbetter dispersed and more evenly distributed. Implicit methods are represented by specialized operators that perform the transformation of intermediate representations. This is done either through explicit normalization, for example, using the packet normalization technique (BatchNorm), or by changing the network connectivity using DropOut and DropConnect algorithms.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The above components usually belong to the interface part of the library. </font><font style="vertical-align: inherit;">Here, by the ‚Äúinterface part‚Äù I mean the entities with which the user can interact. </font><font style="vertical-align: inherit;">They give him convenient tools for efficiently designing a neural network architecture. </font><font style="vertical-align: inherit;">If we talk about the internal mechanisms of libraries, they can provide support for automatic calculation of gradients of the loss function, taking into account various parameters of the model. </font><font style="vertical-align: inherit;">This technique is commonly called Automatic Differentiation (AD).</font></font><br>
<br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Automatic differentiation</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Each deep learning library provides the user with some automatic differentiation capabilities. This gives him the opportunity to focus on the description of the structure of the model (graph of calculations) and transfer the task of calculating the gradients to the AD module. Let‚Äôs take an example that will let us know how it all works. Suppose we want to calculate the partial derivatives of the following function with respect to its input variables X‚ÇÅ and X‚ÇÇ: </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Y = sin (x‚ÇÅ) + X‚ÇÅ * X‚ÇÇ </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The following figure, which I borrowed </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">from here</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , shows the graph of calculations and the calculation of derivatives using a chain rule.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/605/635/189/605635189c56a2f87927ec1a0c5b6318.png"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Computational graph and calculation of derivatives by a chain rule</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
What you see here is something like a ‚Äúreverse mode‚Äù of automatic differentiation. </font><font style="vertical-align: inherit;">The well-known error back propagation algorithm is a special case of the above algorithm for the case where the function located at the top is a loss function. </font><font style="vertical-align: inherit;">AD exploits the fact that any complex function consists of elementary arithmetic operations and elementary functions. </font><font style="vertical-align: inherit;">As a result, derivatives can be computed by applying a chain rule to these operations.</font></font><br>
<br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Implementation</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In the previous section, we examined the components necessary for creating a deep learning library designed for creating and end-to-end training of neural networks. In order not to complicate the example, I imitate the </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Caffe</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> library design pattern here </font><font style="vertical-align: inherit;">. Here we declare two abstract classes - </font></font><code>Function</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">and </font></font><code>Optimizer</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. In addition, there is a class </font></font><code>Tensor</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, which is a simple structure containing two multidimensional NumPy arrays. One of them is designed to store parameter values, the other - to store their gradients. All parameters in different layers (operators) will be of type </font></font><code>Tensor</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Before we go any further, take a look at the general outline of the library.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d05/98c/068/d0598c068139ecda1f2aacbd9ea5f068.jpg"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Library UML diagram</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
At the time of writing this material, this library contains an implementation of the linear layer, the ReLU activation function, the SoftMaxLoss layer, and the SGD optimizer. As a result, it turns out that the library can be used to train classification models consisting of fully connected layers and using a nonlinear activation function. Now let's look at some details about the abstract classes that we have.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
An abstract class</font></font><code>Function</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">provides an interface for operators. Here is his code:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword">class</span>&nbsp; <span class="hljs-title">Function</span>(<span class="hljs-params">object</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self</span>):</span>&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">raise</span> NotImplementedError<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backward</span>(<span class="hljs-params">self</span>):</span>&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">raise</span> NotImplementedError<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getParams</span>(<span class="hljs-params">self</span>):</span>&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span> []</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
All operators are implemented through the inheritance of an abstract class </font></font><code>Function</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Each operator must provide an implementation of the methods </font></font><code>forward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">and </font></font><code>backward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Operators may contain an implementation of an optional method </font></font><code>getParams()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">that returns their parameters (if any). The method </font></font><code>forward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">receives input data and returns the result of their transformation by the operator. In addition, he solves the internal problems necessary for calculating gradients. The method </font></font><code>backward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">accepts the partial derivatives of the loss function with respect to the outputs of the operator and implements the calculation of the partial derivatives of the loss function with respect to the input data of the operator and the parameters (if any). Note that the method</font></font><code>backward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, in essence, provides our library with the ability to perform automatic differentiation. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In order to deal with all this with a specific example, let's take a look at the implementation of the function </font></font><code>Linear</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Linear</span>(<span class="hljs-params">Function</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,in_nodes,out_nodes</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.weights = Tensor((in_nodes,out_nodes))<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.bias&nbsp; &nbsp; = Tensor((<span class="hljs-number">1</span>,out_nodes))<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.type = <span class="hljs-string">'linear'</span><font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self,x</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;output = np.dot(x,self.weights.data)+self.bias.data<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.input = x&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span> output<font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backward</span>(<span class="hljs-params">self,d_y</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.weights.grad += np.dot(self.input.T,d_y)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.bias.grad&nbsp; &nbsp; += np.sum(d_y,axis=<span class="hljs-number">0</span>,keepdims=<span class="hljs-literal">True</span>)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;grad_input &nbsp; &nbsp; &nbsp; &nbsp; = np.dot(d_y,self.weights.data.T)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span> grad_input<font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getParams</span>(<span class="hljs-params">self</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span> [self.weights,self.bias]</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The method </font></font><code>forward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">implements the transformation of the view </font></font><code>Y = X*W+b</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">and returns the result. In addition, it saves the input value </font></font><code>X</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, since it is needed to calculate the partial derivative of </font></font><code>dY</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">the loss function with respect to the output value </font></font><code>Y</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">in the method </font></font><code>backward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Method </font></font><code>backward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">receives the partial derivatives, calculated with respect to the input value </font></font><code>X</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">and the parameters </font></font><code>W</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">and </font></font><code>b</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Moreover, it returns the partial derivatives calculated with respect to the input value </font></font><code>X</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, which will be transferred to the previous layer. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
An abstract class </font></font><code>Optimizer</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">provides an interface for optimizers:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Optimizer</span>(<span class="hljs-params">object</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,parameters</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.parameters = parameters<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">step</span>(<span class="hljs-params">self</span>):</span>&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">raise</span> NotImplementedError<font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">zeroGrad</span>(<span class="hljs-params">self</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> self.parameters:<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;p.grad = <span class="hljs-number">0.</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
All optimizers are implemented by inheriting from the base class </font></font><code>Optimizer</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">A class describing a particular optimization should provide an implementation of the method </font></font><code>step()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">This method updates the model parameters using their partial derivatives calculated in relation to the optimized value of the loss function. </font><font style="vertical-align: inherit;">A link to various model parameters is provided in the function </font></font><code>__init__()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Please note that the universal functionality for resetting gradient values ‚Äã‚Äãis implemented in the base class itself. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Now, to better understand all this, consider a specific example - the implementation of the stochastic gradient descent (SGD) algorithm with support for adjusting the momentum and reducing weights:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">SGD</span>(<span class="hljs-params">Optimizer</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,parameters,lr=<span class="hljs-number">.001</span>,weight_decay=<span class="hljs-number">0.0</span>,momentum = <span class="hljs-number">.9</span></span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;super().__init__(parameters)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.lr &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = lr<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.weight_decay = weight_decay<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.momentum &nbsp; &nbsp; = momentum<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.velocity &nbsp; &nbsp; = []<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> parameters:<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.velocity.append(np.zeros_like(p.grad))<font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">step</span>(<span class="hljs-params">self</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> p,v <span class="hljs-keyword">in</span> zip(self.parameters,self.velocity):<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v = self.momentum*v+p.grad+self.weight_decay*p.data<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;p.data=p.data-self.lr*v</code></pre><br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The solution to the real problem</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Now we have everything necessary for training the (deep) neural network model using our library. </font><font style="vertical-align: inherit;">For this we need the following entities:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Model: calculation graph.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Data and target value: data for network training.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Loss function: substitute for optimization goal.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Optimizer: a mechanism for updating model parameters.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The following pseudo code describes a typical testing cycle:</font></font><br>
<br>
<pre><code class="python hljs">model <span class="hljs-comment"># </span>
data,target <span class="hljs-comment"># </span>
loss_fn <span class="hljs-comment"># </span>
optim <span class="hljs-comment">#,         </span>
Repeat:<span class="hljs-comment">#   ,    ,     </span>
&nbsp;&nbsp;&nbsp;optim.zeroGrad() <span class="hljs-comment">#    </span>
&nbsp;&nbsp;&nbsp;output = model.forward(data) <span class="hljs-comment">#   </span>
&nbsp;&nbsp;&nbsp;loss &nbsp; = loss_fn(output,target) <span class="hljs-comment"># </span>
&nbsp;&nbsp;&nbsp;grad &nbsp; = loss.backward() <span class="hljs-comment">#      </span>
&nbsp;&nbsp;&nbsp;model.backward(grad) <span class="hljs-comment">#    </span>
&nbsp;&nbsp;&nbsp;optim.step() <span class="hljs-comment">#  </span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Although this is not necessary in the deep learning library, it may be useful to include the above functionality in a separate class. </font><font style="vertical-align: inherit;">This will allow us not to repeat the same actions when learning new models (this idea corresponds to the philosophy of high-level abstractions of frameworks like </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Keras</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ). </font><font style="vertical-align: inherit;">In order to achieve this, declare a class </font></font><code>Model</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Model</span>():</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.computation_graph = []<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.parameters&nbsp; &nbsp; &nbsp; &nbsp; = []<font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">add</span>(<span class="hljs-params">self,layer</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.computation_graph.append(layer)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.parameters+=layer.getParams()<font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__innitializeNetwork</span>(<span class="hljs-params">self</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> self.computation_graph:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">if</span> f.type==<span class="hljs-string">'linear'</span>:<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;weights,bias = f.getParams()<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;weights.data = <span class="hljs-number">.01</span>*np.random.randn(weights.data.shape[<span class="hljs-number">0</span>],weights.data.shape[<span class="hljs-number">1</span>])<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bias.data&nbsp; &nbsp; = <span class="hljs-number">0.</span><font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fit</span>(<span class="hljs-params">self,data,target,batch_size,num_epochs,optimizer,loss_fn</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loss_history = []<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.__innitializeNetwork()<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;data_gen = DataGenerator(data,target,batch_size)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;itr = <span class="hljs-number">0</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(num_epochs):
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> X,Y <span class="hljs-keyword">in</span> data_gen:<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;optimizer.zeroGrad()<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> self.computation_graph: X=f.forward(X)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loss = loss_fn.forward(X,Y)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;grad = loss_fn.backward()<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> self.computation_graph[::<span class="hljs-number">-1</span>]: grad = f.backward(grad)&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loss_history+=[loss]<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(<span class="hljs-string">"Loss at epoch = {} and iteration = {}: {}"</span>.format(epoch,itr,loss_history[<span class="hljs-number">-1</span>]))<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;itr+=<span class="hljs-number">1</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;optimizer.step()<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span> loss_history<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span>(<span class="hljs-params">self,data</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;X = data<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> self.computation_graph: X = f.forward(X)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span> X</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
This class includes the following functionality:</font></font><br>
<br>
<ul>
<li>  :  <code>add()</code>   ,    .        <code>computation_graph</code>.</li>
<li> : ,   ,       ,    .</li>
<li> :    <code>fit()</code>       .       ,    .</li>
<li>  :  <code>predict()</code>   ,       ,   .</li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Since this class is not the basic building block of deep learning systems, I implemented it in a separate module </font></font><code>utilities.py</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Note that the method </font></font><code>fit()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">uses a class </font></font><code>DataGenerator</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">whose implementation is in the same module. </font><font style="vertical-align: inherit;">This class is just a wrapper for training data and generates mini-packages for each iteration of training.</font></font><br>
<br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Model training</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Now consider the last piece of code in which the neural network model is trained using the library described above. </font><font style="vertical-align: inherit;">I am going to train a multilayer network on data arranged in a spiral. </font><font style="vertical-align: inherit;">I was prompted by </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">this</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> publication. </font><font style="vertical-align: inherit;">Code for generating this data and for visualizing it can be found in the file </font></font><code>utilities.py</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/895/eb0/856/895eb085662368f9c1d171153a301fb0.png"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Data with three classes arranged in a spiral.&nbsp;</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
The previous figure shows the visualization of the data on which we will train the model. </font><font style="vertical-align: inherit;">This data is nonlinearly separable. </font><font style="vertical-align: inherit;">We can hope that a network with a hidden layer can correctly find nonlinear decision boundaries. </font><font style="vertical-align: inherit;">If you put together everything that we talked about, you get the following code fragment that allows you to train the model:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">import</span> dl_numpy <span class="hljs-keyword">as</span> DL
<span class="hljs-keyword">import</span> utilities<font></font>
<font></font>
batch_size&nbsp; &nbsp; &nbsp; &nbsp; = <span class="hljs-number">20</span>
num_epochs&nbsp; &nbsp; &nbsp; &nbsp; = <span class="hljs-number">200</span>
samples_per_class = <span class="hljs-number">100</span>
num_classes &nbsp; &nbsp; &nbsp; = <span class="hljs-number">3</span>
hidden_units&nbsp; &nbsp; &nbsp; = <span class="hljs-number">100</span><font></font>
data,target &nbsp; &nbsp; &nbsp; = utilities.genSpiralData(samples_per_class,num_classes)<font></font>
model &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = utilities.Model()<font></font>
model.add(DL.Linear(<span class="hljs-number">2</span>,hidden_units))<font></font>
model.add(DL.ReLU())<font></font>
model.add(DL.Linear(hidden_units,num_classes))<font></font>
optim &nbsp; = DL.SGD(model.parameters,lr=<span class="hljs-number">1.0</span>,weight_decay=<span class="hljs-number">0.001</span>,momentum=<span class="hljs-number">.9</span>)<font></font>
loss_fn = DL.SoftmaxWithLoss()<font></font>
model.fit(data,target,batch_size,num_epochs,optim,loss_fn)<font></font>
predicted_labels = np.argmax(model.predict(data),axis=<span class="hljs-number">1</span>)<font></font>
accuracy &nbsp; &nbsp; &nbsp; &nbsp; = np.sum(predicted_labels==target)/len(target)<font></font>
print(<span class="hljs-string">"Model Accuracy = {}"</span>.format(accuracy))<font></font>
utilities.plot2DDataWithDecisionBoundary(data,target,model)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The image below shows the same data and the decisive boundaries of the trained model.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/502/b0b/84f/502b0b84fee2c960c918494e8d63e33a.png"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Data and decision boundaries of the trained model</font></font></font></i><br>
 <br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Summary</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Given the increasing complexity of deep learning models, there is a tendency to increase the capabilities of the respective libraries and to increase the amount of code needed to implement these capabilities. But the most basic functionality of such libraries can still be implemented in a relatively compact form. Although the library we created can be used for end-to-end training of simple networks, it is still, in many ways, limited. We are talking about limitations in the field of capabilities that allow deep learning frameworks to be used in areas such as machine vision, speech and text recognition. This, of course, the possibilities of such frameworks are not limited. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
I believe that everyone can fork the </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">project</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, the code of which we examined here, and, as an exercise, introduce into it what they would like to see in it. </font><font style="vertical-align: inherit;">Here are some mechanisms you can try to implement yourself:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Operators: convolution, subsampling.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Optimizers: Adam, RMSProp.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Regulators: BatchNorm, DropOut.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
I hope this material allowed you to at least see from the corner of your eye what is happening in the bowels of libraries for deep learning. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dear readers! </font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">What deep learning libraries do you use?</font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><img src="https://habrastorage.org/files/1ba/550/d25/1ba550d25e8846ce8805de564da6aa63.png"></a></div>
      
    </div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/en486686/">https://habr.com/ru/post/en486686/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../en486676/index.html">Inevitability of FPGA penetration into data centers</a></li>
<li><a href="../en486678/index.html">Quartz in ASP.NET Core</a></li>
<li><a href="../en486680/index.html">ML, VR & Robots (and a bit of cloud)</a></li>
<li><a href="../en486682/index.html">Docker Compose: Simplify Using Makefile</a></li>
<li><a href="../en486684/index.html">My answer to those who believe that the value of TDD is exaggerated</a></li>
<li><a href="../en486688/index.html">Node.js, Tor, Puppeteer and Cheerio: anonymous web scraping</a></li>
<li><a href="../en486690/index.html">5 tips for writing quality arrow functions</a></li>
<li><a href="../en486692/index.html">Chrome console features you may never have used</a></li>
<li><a href="../en486694/index.html">News from the world of OpenStreetMap No. 496 (01/14/2020/20/01/2020)</a></li>
<li><a href="../en486702/index.html">Digital events in Moscow from February 3 to 9</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>