<!doctype html>
<html class="no-js" lang="ja">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👏🏽 👨🏿‍🔧 🏬 ニューラルネットワークとディープラーニング、第4章：ニューラルネットワークが任意の関数を計算できる視覚的証拠 🌺 🦔 🕴🏽</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="この章では、普遍性定理の簡単でほとんど視覚的な説明を提供します。この章の内容を理解するために、前のものを読む必要はありません。独立したエッセイとして構成されています。NSの最も基本的な理解があれば、説明を理解できるはずです。
 
 コンテンツ

- 1: 
- 2: 
- 3: .1: .2: ? ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>ニューラルネットワークとディープラーニング、第4章：ニューラルネットワークが任意の関数を計算できる視覚的証拠</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/461659/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">この章では、普遍性定理の簡単でほとんど視覚的な説明を提供します。</font><font style="vertical-align: inherit;">この章の内容を理解するために、前のものを読む必要はありません。</font><font style="vertical-align: inherit;">独立したエッセイとして構成されています。</font><font style="vertical-align: inherit;">NSの最も基本的な理解があれば、説明を理解できるはずです。</font></font><br>
<br>
<div class="spoiler"><b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">コンテンツ</font></font></b><div class="spoiler_text"><ul>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="> 1:      </a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="> 2:     </a></li>
<li> 3:<ul>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=">.1:    </a><br>
</li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=">.2:     ?</a><br>
</li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=">.3:    ?</a><br>
</li>
</ul></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="> 4:   ,      </a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="> 5:      ?</a></li>
<li> 6:<ul>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=">.1:  </a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=">.2:     </a></li>
</ul></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=">:       ?</a></li>
</ul></div></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ニューラルネットワークに関する最も驚くべき事実の1つは、あらゆる関数を計算できることです。</font><font style="vertical-align: inherit;">つまり、誰かがあなたにある種の複雑で曲がりくねった関数f（x）を与えたとしましょう：</font></font><br>
<br>
<img src="https://habrastorage.org/webt/yi/ot/sl/yiotslaplbwh6savahfpzfwdyum.png"><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
そして、この関数に関係なく、任意の入力xに対して値f（x）（またはそれに近い何らかの近似）がこのネットワークの出力になるというニューラルネットワークが保証されています。つまり</font></font><br>
<br>
<img src="https://habrastorage.org/webt/eb/ud/zc/ebudzc72xyiytdvkf4ulx-u7onq.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
、多くの変数の関数であっても機能しますf = f （x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、...、x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">m</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">）、そして多くの意味があります。</font><font style="vertical-align: inherit;">たとえば、次のネットワークは、m = 3入力、n = 2出力の関数を計算します。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/1g/01/i7/1g01i7vpnwo-mlm1r2brhd9al9m.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
この結果は、ニューラルネットワークに特定の普遍性があることを示しています。</font><font style="vertical-align: inherit;">計算する関数が何であれ、これを実行できるニューラルネットワークがあることはわかっています。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
さらに、ネットワークを着信ニューロンと発信ニューロンの間の単一層に制限したとしても、普遍性定理が成り立ちます-いわゆる1つの非表示のレイヤー。したがって、非常にシンプルなアーキテクチャのネットワークでさえ、非常に強力です。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
普遍性定理は、ニューラルネットワークを使用する人々によく知られています。しかし、そうではありますが、この事実を理解することはあまり普及していません。そして、これに関する説明のほとんどは、技術的に複雑すぎます。たとえば、</font><font style="vertical-align: inherit;">この結果を証明し</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">た最初の論文の1つは、</font></font></a><font style="vertical-align: inherit;"></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ハーン-バナッハの定理</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">リースの表現定理を使用しました。</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">そして少しフーリエ解析。</font><font style="vertical-align: inherit;">数学者であれば、この証拠を理解するのは簡単ですが、ほとんどの人にとってはそれほど簡単ではありません。</font><font style="vertical-align: inherit;">普遍性の基本的な理由はシンプルで美しいため、残念です。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
この章では、普遍性定理の簡単でほとんど視覚的な説明を提供します。</font><font style="vertical-align: inherit;">その根底にあるアイデアについて、段階的に説明します。</font><font style="vertical-align: inherit;">ニューラルネットワークが実際に関数を計算できる理由を理解します。</font><font style="vertical-align: inherit;">この結果の制限のいくつかを理解できます。</font><font style="vertical-align: inherit;">そして、結果が深いNSとどのように関連付けられているかを理解します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
この章の内容を理解するために、前のものを読む必要はありません。</font><font style="vertical-align: inherit;">独立したエッセイとして構成されています。</font><font style="vertical-align: inherit;">NSの最も基本的な理解があれば、説明を理解できるはずです。</font><font style="vertical-align: inherit;">しかし、知識のギャップを埋めるために、以前の資料へのリンクを提供することもあります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
普遍性の定理はコンピュータサイエンスでよく見られるため、驚くほどすごいことを忘れることもあります。ただし、覚えておく価値はあります。任意の関数を計算する機能は本当に素晴らしいです。想像できるほとんどすべてのプロセスを、関数の計算に減らすことができます。短い一節に基づいて楽曲の名前を見つけるタスクを考えてみましょう。これは関数計算と考えることができます。または、中国語のテキストを英語に翻訳するタスクを検討してください。そして、これは関数の計算と考えることができます（実際には、単一のテキストを翻訳するための多くの受け入れ可能なオプションがあるため、多くの関数）。または、mp4ファイルに基づいて、映画のプロットと演技の質の説明を生成するタスクを検討してください。これは、特定の関数の計算と見なすこともできます（ここでも注意事項は当てはまりますが、テキストを翻訳するためのオプションについて）。普遍性とは、原則として、NSはこれらすべてのタスク、および他の多くのタスクを実行できることを意味します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
もちろん、中国語から英語に翻訳できるNSがあることを知っているという事実だけから、そのようなネットワークを作成したり認識したりするための優れた技術があるとは限りません。</font><font style="vertical-align: inherit;">この制限は、ブールスキームなどのモデルの従来の普遍性定理にも適用されます。</font><font style="vertical-align: inherit;">しかし、この本ですでに見たように、NSには関数を学習するための強力なアルゴリズムがあります。</font><font style="vertical-align: inherit;">学習アルゴリズムと汎用性の組み合わせは魅力的な組み合わせです。</font><font style="vertical-align: inherit;">これまでのところ、本では、トレーニングアルゴリズムに焦点を当ててきました。</font><font style="vertical-align: inherit;">この章では、普遍性とその意味に焦点を当てます。</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2つのトリック</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
普遍性定理が真実である理由を説明する前に、「ニューラルネットワークは任意の関数を計算できる」という非公式ステートメントに含まれる2つのトリックについて触れておきたいと思います。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
まず、これは、ネットワークを使用して関数を正確に計算できるという意味ではありません。必要なだけの近似を得ることができます。隠れたニューロンの数を増やすことにより、近似を改善します。たとえば、以前に、3つの隠れたニューロンを使用して特定の関数f（x）を計算するネットワークを示しました。 3つのニューロンを使用するほとんどの関数では、低品質の近似しか取得できません。隠れたニューロンの数を増やすことにより（たとえば、最大5つまで）、通常は近似値</font></font><br>
<br>
<img src="https://habrastorage.org/webt/x2/nt/zw/x2ntzw4ykxb450nexszfsd-qz08.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
を改善できます。状況を改善するには、隠れたニューロンの数をさらに増やします。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
このステートメントを明確にするために、必要な精度ε&gt; 0で計算したい関数f（x）が与えられたとしましょう。十分な数の隠れニューロンを使用すると、出力g（x）が式| g（x）−f（x）| &lt;εを満たすxに対して常にNSを見つけることができるという保証があります。言い換えると、可能な入力値に対して望ましい精度で近似が達成されます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
2つ目の問題は、記述されたメソッドで近似できる関数が連続クラスに属していることです。関数が中断された場合、つまり、急激な急激なジャンプが発生した場合、一般的なケースでは、NSを使用して近似することはできません。そして、NSは入力データの連続関数を計算するため、これは驚くべきことではありません。ただし、実際に計算する必要がある関数が不連続であっても、近似は非常に連続的であることがよくあります。その場合は、NSを使用できます。実際には、この制限は通常重要ではありません。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
その結果、普遍性定理のより正確なステートメントは、1つの隠れ層を持つNSを使用して、任意の連続関数を任意の精度で近似できることです。</font><font style="vertical-align: inherit;">この章では、1つではなく2つの非表示レイヤーを使用して、この定理のやや厳密性の低いバージョンを証明します。</font><font style="vertical-align: inherit;">タスクでは、この説明を、わずかな変更を加えて、1つの非表示レイヤーのみを使用する証明に適用する方法を簡単に説明します。</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1つの入力値と1つの出力値を持つ汎用性</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
普遍性定理が真である理由を理解するために、1つの入力値と1つの出力値のみを持つ関数を近似するNSを作成する方法を理解することから始めます。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/yi/ot/sl/yiotslaplbwh6savahfpzfwdyum.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
これが普遍性問題の本質であることが</font><font style="vertical-align: inherit;">わかります</font><font style="vertical-align: inherit;">。この特別なケースを理解したら、多くの入出力値を持つ関数に拡張するのは非常に簡単です。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
fをカウントするためのネットワークを構築する方法を理解するために、2つの非表示ニューロンを含む単一の非表示層を含むネットワークと、1つの出力ニューロンを含む出力層から始め</font></font><br>
<br>
<img src="https://habrastorage.org/webt/b7/cz/ql/b7czqllzyyxbpzeq7gs3h6a2338.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ます。ネットワークコンポーネントの動作を想像するために、上部の非表示ニューロンに焦点を当てます。</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">元の記事の</font></a><font style="vertical-align: inherit;">チャート</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">マウスで「w」をクリックしてインタラクティブに重みを変更し、上部の隠れニューロンによって計算された関数がどのように変化するかをすぐに確認できます。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/mh/r0/dz/mhr0dzpmf_zop4a3bwi2of04qtu.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
本の前半で学習したように、隠れニューロンはσ（wx + b）を数えます。ここで、σ（z）≡1 /（1 + e </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">−z</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">）は</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">シグモイド</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">です。これまでのところ、この代数的形式をかなり頻繁に使用しています。ただし、普遍性を証明するには、この代数を完全に無視し、代わりにグラフ上の形状を操作して観察するほうがよいでしょう。これは、何が起こっているのかをよりよく感じるのに役立つだけでなく、シグモイド以外の他のアクティブ化機能に適用できる普遍性の証明にもなります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
厳密に言えば、私が選択した視覚的なアプローチは、伝統的に証拠とは見なされていません。しかし、私は視覚的なアプローチが従来の証明よりも最終結果の真実への洞察を提供すると信じています。そしてもちろん、そのような理解は証明の真の目的です。私が提案する証拠では、時々ギャップが出てきます。私は合理的ですが、必ずしも厳密ではない視覚的証拠を提供します。これが気になる場合は、これらのギャップを埋めるのがあなたの仕事だと考えてください。ただし、主な目的を見失ってはいけません。普遍性定理が真実である理由を理解することです。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
この証明を始めるには、元の図のオフセットbをクリックし、右にドラッグして拡大します。変位が増加すると、グラフは左に移動しますが、形状は変化しません。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
次に、それを左にドラッグしてオフセットを減らします。</font><font style="vertical-align: inherit;">グラフは形状を変えずに右に移動していることがわかります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
重量を2-3に減らします。</font><font style="vertical-align: inherit;">重量が減少するにつれて、曲線がまっすぐになることがわかります。</font><font style="vertical-align: inherit;">曲線がグラフからはみ出さないようにするには、オフセットを修正する必要がある場合があります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
最後に、ウェイトを100より大きい値に増やします。カーブはより急になり、最終的にステップに近づきます。</font><font style="vertical-align: inherit;">角度が点x = 0.3の領域になるようにオフセットを調整してみてください。</font><font style="vertical-align: inherit;">以下のビデオは何が起こるかを示しています：</font></font><br>
<br>
<div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">お使いのブラウザはHTML5ビデオをサポートしていません。</font></font><source src="http://neuralnetworksanddeeplearning.com/movies/create_step_function.mp4" type="video/mp4"></video></div></div></div><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
重みを大きくすることで分析を大幅に簡略化できるため、出力は本当にステップ関数の適切な近似になります。</font><font style="vertical-align: inherit;">以下では、重みw = 999の上位の隠れニューロンの出力を作成しました。</font><font style="vertical-align: inherit;">これは静止画像です：</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/94c/24e/8a8/94c24e8a8a262c06c102b97bef033e99.jpg"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ステップ関数の使用は、典型的なシグモイドよりも少し簡単です。その理由は、すべての隠れたニューロンからの寄与が出力層で加算されるためです。一連のステップ関数の合計を分析するのは簡単ですが、一連の曲線がシグモイドの形で追加されたときに何が起こるかについて話すのはより困難です。したがって、隠れたニューロンが段階的な関数を生成すると仮定する方がはるかに簡単です。より正確には、重みwを非常に大きな値に固定し、オフセットを介してステップの位置を割り当てることでこれを行います。もちろん、ステップ関数として出力を処理することは近似ですが、それは非常に優れており、これまでのところ、関数を真のステップ関数として扱います。後で、この近似からの逸脱の影響についての議論に戻ります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ステップのxの値は何ですか？つまり、ステップの位置は重量と変位にどのように依存しますか？</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
質問に答えるには、インタラクティブチャートで重みとオフセットを変更してみてください。ステップの位置がwとbにどのように依存するか理解できますか？少し練習することで、その位置がbに比例し、wに反比例することを理解できます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
実際、重みと変位を次の値に調整するとわかるように、ステップはs = −b / wにあります。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/ee/d9/zo/eed9zodaxp8ot33ip8y8j-sdcqg.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
単一のパラメーターs、つまりステップの位置で非表示のニューロンを記述すると、生活が大幅に簡略化されます。 s = −b / w。次のインタラクティブな図では、sを変更するだけです。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/uy/g6/9h/uyg69hokiufnmt7zwwmysjof5uc.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
上記のように、入力の重みwを非常に大きな値に特別に割り当てました。これは、ステップ関数が適切な近似になるように十分に大きい値です。また、バイアスb = −wsを選択することで、このようにパラメーター化されたニューロンを通常の形式に簡単に戻すことができます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
これまでは、優れた隠れたニューロンのみの出力に集中してきました。ネットワーク全体の動作を見てみましょう。非表示のニューロンが、ステップs </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">（上のニューロン）とs </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">（下のニューロン）</font><font style="vertical-align: inherit;">のパラメーターによって定義されるステップ関数を計算するとします</font><font style="vertical-align: inherit;">。それぞれの出力の重みはw </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">とw </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">です。ネットワークは次のとおりです。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/6u/ot/ns/6uotnsmlecwfh8iaqz1eb5p5tjo.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
右側に、重み付けされた出力グラフw </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> a </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> + w </font><sub><font style="vertical-align: inherit;">2</font></sub><font style="vertical-align: inherit;">をプロットします。</font></font><sub><font style="vertical-align: inherit;"></font></sub><font style="vertical-align: inherit;"></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2つの</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">隠された層。ここで、a </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">とa </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">は、それぞれ上部と下部の隠れニューロンの出力です。それらはしばしばニューロン活性化と呼ばれるため、「a」で示されます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ちなみに、ネットワーク全体の出力はσ（w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> a </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> + w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> a </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> + b）であり、bは出力ニューロンのバイアスです。これは明らかに、グラフを作成している非表示レイヤーの重み付き出力とは異なります。しかし、ここでは、隠れ層のバランスの取れた出力に集中し、後でそれがネットワーク全体の出力にどのように関連するかについて考えます。</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">元の記事の</font></a></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
インタラクティブダイアグラムの</font><font style="vertical-align: inherit;">ステップsを増減して</font><font style="vertical-align: inherit;">みてください</font><sub><font style="vertical-align: inherit;">1</font></sub></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"></font><sub><font style="vertical-align: inherit;"></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">上部の隠れたニューロン。これにより、非表示レイヤーの重み付き出力がどのように変化するかを確認してください。 s </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">がs </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">を超えると</font><font style="vertical-align: inherit;">どうなるかを理解することは特に役立ち</font><font style="vertical-align: inherit;">ます。これらの場合のグラフは、上部の隠れニューロンが最初にアクティブになる状況から下部の隠れニューロンが最初にアクティブになる状況に移ると、形状が変化することがわかります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
同様に、</font><font style="vertical-align: inherit;">下の隠れニューロン</font><font style="vertical-align: inherit;">のs </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ステップ</font><sub><font style="vertical-align: inherit;">を</font></sub><font style="vertical-align: inherit;">操作して、</font><font style="vertical-align: inherit;">隠れニューロンの全体的な出力がどのように変化するかを確認してください。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
出力の重みを減らしたり増やしたりしてみてください。これが、対応する隠れたニューロンからの寄与をどのようにスケーリングするかに注意してください。重みの1つが0に等しい場合はどうなりますか？</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
最後に、w </font><sub><font style="vertical-align: inherit;">1を</font></sub><font style="vertical-align: inherit;">設定してみてください。</font></font><sub><font style="vertical-align: inherit;"></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0.8であり、W </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> -0.8で。結果は、「突起」関数で、開始はs </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、終了はs </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、高さは0.8です。たとえば、重み付けされた出力は次のようになります。</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/84d/9be/e75/84d9bee755d8a57bbdb3aed20d07da78.jpg"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
もちろん、突起は任意の高さにスケーリングできます。高さを表す1つのパラメーターhを使用します。また、簡単にするために、「s </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = ...」および「w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = ...」</font><font style="vertical-align: inherit;">という表記を取り除き</font><font style="vertical-align: inherit;">ます。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/io/by/7l/ioby7lcd1whqowsw0ak9j1td16q.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
h値を増減して、突起の高さがどのように変化するかを確認してください。 hを負にしてみてください。ステップのポイントを変更して、突起の形状がどのように変化するかを確認してください。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ニューロンをグラフィックプリミティブとしてだけでなく、プログラマにとってより身近なユニットとしても使用していることがわかります-プログラミングにおけるif-then-else命令のようなもの：</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
if input&gt; = start of step：</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 add 1 to weighted output </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 else：</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 add 0バランスの取れた出口へ</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ほとんどの部分はグラフィック表記に固執します。ただし、場合によっては、if-then-elseビューに切り替えて、これらの用語で何が起こっているのかを考えることが役立つことがあります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
隠れたニューロンの2つの部分を同じネットワーク上で接着することにより、突出のトリックを使用できます。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/4w/4p/pz/4w4ppzryydmyz3f3dglgzcwisfm.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ここでは、隠れたニューロンの各ペアのh値を書き留めることで、重みを落としました。両方のh値で遊んでみて、グラフがどのように変化するかを確認してください。ステップのポイントを変更して、タブを移動します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
より一般的なケースでは、このアイデアを使用して、任意の高さの任意の数のピークを取得できます。特に、間隔[0,1]を多数の（N）サブ間隔に分割し、N組の隠れニューロンを使用して、任意の高さのピークを取得できます。これがN = 5でどのように機能するかを見てみましょう。これはすでにかなり多くのニューロンなので、私は少し狭い表現にしています。複雑な図で申し訳ありません-追加の抽象化の背後にある複雑さを隠すことができましたが、ニューラルネットワークのしくみをよりよく感じるには、複雑さに少し苦労する価値があるように思えます。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/do/2t/x-/do2tx-fp-h-w83rnboapp8w-o98.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ご覧のとおり、5つのペアの隠れたニューロンがあります。対応するペアのステップのポイントは0.1 / 5、次に1 / 5.2 / 5、最大4 / 5.5 / 5にあります。これらの値は固定されています-グラフ上に同じ幅の5つの突起が表示されます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ニューロンの各ペアには、値hが関連付けられています。ニューロン出力リンクには重みhと–hがあることに注意してください。図の元の記事では、h値をクリックして左右に移動できます。高さが変わると、スケジュールも変わります。出力の重みを変更して、最終的な関数を作成します！</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
図では、グラフをクリックして、ステップの高さを上下にドラッグできます。高さを変更すると、対応するhの高さがどのように変化するかがわかります。出力の重み+ hと–hはそれに応じて変化します。つまり、右側にグラフが表示されている関数を直接操作し、左側のhの値のこれらの変化を確認します。突起の1つでマウスボタンを押したままマウスを左または右にドラッグすると、突起が現在の高さに調整されます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
仕事を終わらせる時がきた。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
この章の冒頭で描いた関数を思い出してください。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/yi/ot/sl/yiotslaplbwh6savahfpzfwdyum.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
次に、そのことについては触れませんでしたが、実際には次のようになります。</font></font><br>
<br>
<p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display"><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mtable displaystyle=&quot;true&quot;><mlabeledtr><mtd id=&quot;mjx-eqn-113&quot;><mtext>(113)</mtext></mtd><mtd><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mn>0.2</mn><mo>+</mo><mn>0.4</mn><msup><mi>x</mi><mn>2</mn></msup><mo>+</mo><mn>0.3</mn><mi>x</mi><mi>sin</mi><mo>&amp;#x2061;</mo><mo stretchy=&quot;false&quot;>(</mo><mn>15</mn><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo><mo>+</mo><mn>0.05</mn><mi>cos</mi><mo>&amp;#x2061;</mo><mo stretchy=&quot;false&quot;>(</mo><mn>50</mn><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo></mtd></mlabeledtr></mtable></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="97.25ex" height="3.009ex" viewBox="0 -916.9 41871.4 1295.7" role="img" focusable="false" style="vertical-align: -0.88ex; max-width: 778px;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g transform="translate(39590,0)"><g id="mjx-eqn-113" transform="translate(0,-67)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhBU24J1nPCpJll9Cp_KirMgJtS5A#MJMAIN-28"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhBU24J1nPCpJll9Cp_KirMgJtS5A#MJMAIN-31" x="389" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhBU24J1nPCpJll9Cp_KirMgJtS5A#MJMAIN-31" x="890" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhBU24J1nPCpJll9Cp_KirMgJtS5A#MJMAIN-33" x="1390" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhBU24J1nPCpJll9Cp_KirMgJtS5A#MJMAIN-29" x="1891" y="0"></use></g></g><g transform="translate(10244,0)"><g transform="translate(-14,0)"><g transform="translate(0,-67)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhBU24J1nPCpJll9Cp_KirMgJtS5A#MJMATHI-66" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhBU24J1nPCpJll9Cp_KirMgJtS5A#MJMAIN-28" x="550" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhBU24J1nPCpJll9Cp_KirMgJtS5A#MJMATHI-78" x="940" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhBU24J1nPCpJll9Cp_KirMgJtS5A#MJMAIN-29" x="1512" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhBU24J1nPCpJll9Cp_KirMgJtS5A#MJMAIN-3D" x="2179" y="0"></use><g transform="translate(3236,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhBU24J1nPCpJll9Cp_KirMgJtS5A#MJMAIN-30"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhBU24J1nPCpJll9Cp_KirMgJtS5A#MJMAIN-2E" x="500" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhBU24J1nPCpJll9Cp_KirMgJtS5A#MJMAIN-32" x="779" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhBU24J1nPCpJll9Cp_KirMgJtS5A#MJMAIN-2B" x="4737" y="0"></use><g transform="translate(5738,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhBU24J1nPCpJll9Cp_KirMgJtS5A#MJMAIN-30"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhBU24J1nPCpJll9Cp_KirMgJtS5A#MJMAIN-2E" x="500" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhBU24J1nPCpJll9Cp_KirMgJtS5A#MJMAIN-34" x="779" y="0"></use></g><g transform="translate(7018,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhBU24J1nPCpJll9Cp_KirMgJtS5A#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhBU24J1nPCpJll9Cp_KirMgJtS5A#MJMAIN-32" x="809" y="583"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhBU24J1nPCpJll9Cp_KirMgJtS5A#MJMAIN-2B" x="8266" y="0"></use><g transform="translate(9267,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhBU24J1nPCpJll9Cp_KirMgJtS5A#MJMAIN-30"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhBU24J1nPCpJll9Cp_KirMgJtS5A#MJMAIN-2E" x="500" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhBU24J1nPCpJll9Cp_KirMgJtS5A#MJMAIN-33" x="779" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhBU24J1nPCpJll9Cp_KirMgJtS5A#MJMATHI-78" x="10546" y="0"></use><g transform="translate(11286,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhBU24J1nPCpJll9Cp_KirMgJtS5A#MJMAIN-73"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhBU24J1nPCpJll9Cp_KirMgJtS5A#MJMAIN-69" x="394" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhBU24J1nPCpJll9Cp_KirMgJtS5A#MJMAIN-6E" x="673" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhBU24J1nPCpJll9Cp_KirMgJtS5A#MJMAIN-28" x="12515" y="0"></use><g transform="translate(12905,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhBU24J1nPCpJll9Cp_KirMgJtS5A#MJMAIN-31"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhBU24J1nPCpJll9Cp_KirMgJtS5A#MJMAIN-35" x="500" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhBU24J1nPCpJll9Cp_KirMgJtS5A#MJMATHI-78" x="13906" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhBU24J1nPCpJll9Cp_KirMgJtS5A#MJMAIN-29" x="14478" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhBU24J1nPCpJll9Cp_KirMgJtS5A#MJMAIN-2B" x="15090" y="0"></use><g transform="translate(16090,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhBU24J1nPCpJll9Cp_KirMgJtS5A#MJMAIN-30"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhBU24J1nPCpJll9Cp_KirMgJtS5A#MJMAIN-2E" x="500" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhBU24J1nPCpJll9Cp_KirMgJtS5A#MJMAIN-30" x="779" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhBU24J1nPCpJll9Cp_KirMgJtS5A#MJMAIN-35" x="1279" y="0"></use></g><g transform="translate(18037,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhBU24J1nPCpJll9Cp_KirMgJtS5A#MJMAIN-63"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhBU24J1nPCpJll9Cp_KirMgJtS5A#MJMAIN-6F" x="444" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhBU24J1nPCpJll9Cp_KirMgJtS5A#MJMAIN-73" x="945" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhBU24J1nPCpJll9Cp_KirMgJtS5A#MJMAIN-28" x="19377" y="0"></use><g transform="translate(19766,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhBU24J1nPCpJll9Cp_KirMgJtS5A#MJMAIN-35"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhBU24J1nPCpJll9Cp_KirMgJtS5A#MJMAIN-30" x="500" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhBU24J1nPCpJll9Cp_KirMgJtS5A#MJMATHI-78" x="20767" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhBU24J1nPCpJll9Cp_KirMgJtS5A#MJMAIN-29" x="21340" y="0"></use></g></g></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable displaystyle="true"><mlabeledtr><mtd id="mjx-eqn-113"><mtext><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">（113）</font></font></mtext></mtd><mtd><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">f</font></font></mi><mo stretchy="false"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">（</font></font></mo><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">バツ</font></font></mi><mo stretchy="false"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">）</font></font></mo><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">=</font></font></mo><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0.2</font></font></mn><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+</font></font></mo><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0.4</font></font></mn><msup><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">バツ</font></font></mi><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></mn></msup><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+</font></font></mo><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0.3</font></font></mn><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">バツ</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">罪</font></font></mi><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">⁡</font></font></mo><mo stretchy="false"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">（</font></font></mo><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">15</font></font></mn><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">バツ</font></font></mi><mo stretchy="false"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">）</font></font></mo><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+</font></font></mo><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0.05</font></font></mn><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">cos</font></font></mi><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">⁡</font></font></mo><mo stretchy="false"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">（</font></font></mo><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">五十</font></font></mn><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">バツ</font></font></mi><mo stretchy="false"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">）</font></font></mo></mtd></mlabeledtr></mtable></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-1"> f(x) = 0.2+0.4 x^2+0.3x \sin(15 x) + 0.05 \cos(50 x) \tag{113} </script></p><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
これは0から1までのx値に対して構築され、y軸に沿った値は0から1まで変化します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
明らかに、この関数は自明ではあり</font><font style="vertical-align: inherit;">ません</font><font style="vertical-align: inherit;">。そして、ニューラルネットワークを使用してそれを計算する方法を理解する必要があります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
上記のニューラルネットワーク</font><sub><font style="vertical-align: inherit;">では</font></sub><font style="vertical-align: inherit;">、隠れたニューロンの出力の</font><font style="vertical-align: inherit;">重み付けされた組み合わせ∑ </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> a </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">を分析しました</font><font style="vertical-align: inherit;">。この値を大幅に制御する方法を知っています。ただし、前述のとおり、この値はネットワーク出力とは異なります。ネットワーク出力はσ（∑ </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> a </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> + b）です。ここで、bは出力ニューロンのバイアスです。ネットワーク出力を直接制御できますか？</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
解決策は、隠れ層の重み付き出力が式σで与えられるニューラルネットワークを開発することです。</font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">-1</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ⋅f（X）、σは</font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">-1</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> σの逆関数です。つまり、非表示層の重み付き出力を次のようにする必要があります。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/sk/bu/bw/skbubwnwkrrpukeblqe9a1qo8cw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
これが成功した場合、ネットワーク全体の出力はf（x）の適切な近似になります（出力ニューロンのオフセットを0に設定します）。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
次に、上に示した目的関数を近似するNSを開発します。何が起こっているのかをよりよく理解するには、この問題を2回解決することをお勧めします。</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">オリジナル記事</font></a><font style="vertical-align: inherit;">で初めて</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">グラフをクリックして、さまざまな棚の高さを直接調整します。目的関数を適切に近似するのは非常に簡単です。近似の程度は、目的関数とネットワークが計算する関数との差である平均偏差によって推定されます。あなたの仕事は、平均偏差を最小値にすることです。平均偏差が0.40を超えない場合、タスクは完了したと見なされます。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/jb/qh/j8/jbqhj8kul1dtc6o-nyrm0qxhh_0.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
成功したら、リセットボタンを押します。これにより、タブがランダムに変更されます。 2回目は、グラフに触れずに、図の左側のh値を変更して、平均偏差を0.40以下の値にしようとします。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
これで、ネットワークが関数f（x）を近似的に計算するために必要なすべての要素が見つかりました。近似はおおざっぱであることがわかりましたが、隠れたニューロンのペアの数を増やすだけで結果を簡単に改善できます。これにより、突起の数が増えます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
特に、NSで使用されるパラメーター化により、検出されたすべてのデータを標準ビューに戻すのは簡単です。これがどのように機能するかをすぐに思い出させてください。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
最初の層では、すべての重みが大きな定数値を持っています（たとえば、w = 1000）。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
隠れたニューロンの変位は、b = −wsによって計算されます。したがって、たとえば、2番目の隠れニューロンの場合、s = 0.2はb = −1000×0.2 = −200になります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
スケールの最後のレイヤーは、hの値によって決定されます。したがって、たとえば、最初のhに選択した値、h = -0.2は、2つの上位の隠れニューロンの出力の重みがそれぞれ-0.2と0.2であることを意味します。など、出力ウェイトのレイヤー全体に対して。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
最後に、出力ニューロンのオフセットは0 </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
です。それだけです。NSの完全な記述が得られました。これにより、初期目的関数が適切に計算されます。そして、隠れたニューロンの数を改善することにより、近似の品質を改善する方法を理解しています。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
さらに、元の目的関数f（x）= 0.2 + 0.4x </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+ 0.3sin（15x）+ 0.05cos（50x）は特別なものではありません。</font><font style="vertical-align: inherit;">同様の手順は、[0,1]から[0,1]までの任意の連続関数に使用できます。</font><font style="vertical-align: inherit;">実際、私たちは単層NSを使用して、関数のルックアップテーブルを作成します。</font><font style="vertical-align: inherit;">そして、この考えを基礎として、普遍性の一般化された証明を得ることができます。</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">多くのパラメータの機能</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
結果を入力変数のセットの場合に拡張します。複雑に聞こえますが、入力変数が2つだけの場合、必要なアイデアはすべてすでに理解されています。したがって、2つの入力変数の場合を考えます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ニューロンに2つの入力がある場合に何が起こるかを見てみましょう。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/k5/cm/a9/k5cma9i-bgfwxnp2ao9h1wiwfz0.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
入力xとyがあり、対応する重みw </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">とw </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">とニューロンのオフセットbがあります。ワットの重量を設定</font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">最初の1と0にして遊び、wは</font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、そして彼らは、ニューロンの出力にどのように影響するかを見るためにオフセットB：</font></font><br>
<br>
<img src="https://habrastorage.org/webt/bl/71/6p/bl716pdfanpkwighwk2dc6m20ey.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
あなたが見ることができるように、ワットで</font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = 0を、入力yはニューロンの出力には影響を与えません。すべてがxが唯一の入力であるかのように発生します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
これを踏まえ、w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">の重み</font><font style="vertical-align: inherit;">をw </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = 100 </font><font style="vertical-align: inherit;">に増やし、</font><font style="vertical-align: inherit;">w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">を0のままに</font><font style="vertical-align: inherit;">すると、どうなると思います</font><font style="vertical-align: inherit;">か？</font><font style="vertical-align: inherit;">これがすぐに分からない場合は、この問題について少し考えてください。</font><font style="vertical-align: inherit;">次に、何が起こるかを示す次のビデオを見てください。</font></font><br>
<br>
<div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">お使いのブラウザはHTML5ビデオをサポートしていません。</font></font><source src="http://neuralnetworksanddeeplearning.com/movies/step_3d.mp4" type="video/mp4"></video></div></div></div><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
以前と同様に、入力の重みが増加すると、出力はステップの形状に近づきます。違いは、ステップ関数が3次元に配置されていることです。以前と同様に、オフセットを変更することでステップの位置を移動できます。角度は、点sであろう</font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">X</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> B / W1 - ≡。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
パラメーターがステップの場所になるようにダイアグラムをやり直してみましょう</font></font><br>
<br>
<img src="https://habrastorage.org/webt/aw/qs/59/awqs59ahvnac-1i9piafzg2jbpi.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
。xの入力重みが非常に重要であると仮定します-私はw </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = 1000 </font><font style="vertical-align: inherit;">を使用</font><font style="vertical-align: inherit;">し、重みw </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = 0を使用しました。ニューロンの番号はステップの位置であり、その上のxはステップをx軸に沿って移動していることを示しています。当然、y軸に沿ってステップ関数を取得し、yの入力ウェイトを大きく（たとえば、w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = 1000）、xのウェイトを0、wに等しくすることは可能です。</font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = 0：</font></font><br>
<br>
<img src="https://habrastorage.org/webt/xg/zw/y0/xgzwy0jgsj5q1gl3oqgygzqv0nq.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ニューロンの数値はステップの位置を示し、その上のyはステップをy軸に沿って移動していることを示しています。 xとyの重みを直接指定することはできましたが、グラフを散らかすため、指定できませんでした。ただし、yマーカーは、yの重みが大きく、xの重みが0であること</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
を示していることに注意してください。3次元の突出関数を計算するために設計したステップ関数を使用できます。これを行うには、2つのニューロンを使用します。各ニューロンは、x軸に沿って階段関数を計算します。次に、これらのステップ関数を重みhおよび–hと組み合わせます。ここで、hは突起の希望の高さです。これはすべて、次の図で確認できます</font></font><br>
<br>
<img src="https://habrastorage.org/webt/5s/qn/wk/5sqnwkfmm7_uzs3jchov90ylyz8.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
。hの値を変更してみてください。ネットワークの重みとの関係をご覧ください。そして、彼女は右側の突起機能の高さをどのように変更したか。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
また、ステップのポイントを変更してみてください。その値は、上部の隠れたニューロンで0.30に設定されています。突起の形状がどのように変化するかを確認してください。下の隠れたニューロンに関連付けられた0.70ポイントを超えて移動するとどうなりますか？</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
x軸に沿って突出機能を構築する方法を学びました。当然、y軸に沿って2つのステップ関数を使用して、y軸に沿って突起関数を簡単に作成できます。入力yで大きな重みを作成し、入力xで重み0を設定することでこれを実行できることを思い出してください。そして、何が起こりますか：</font></font><br>
<br>
<img src="https://habrastorage.org/webt/ic/uu/fq/icuufqisf9gjv8zccnkg0f0bevc.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
以前のネットワークとほとんど同じに見えます！目に見える唯一の変化は、隠れたニューロンの小さなyマーカーです。それらは、xではなくyのステップ関数を生成することを思い出させます。そのため、入力yでの重みは非常に大きく、入力xでの重みはゼロであり、その逆はありません。以前と同様に、画像が乱雑にならないように、直接表示しないことにしました。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
高さhのx軸に沿って1つ、y軸に沿ってもう1つ、2つの突起関数を追加するとどうなるかを見てみましょう</font></font><br>
<br>
<img src="https://habrastorage.org/webt/7f/u7/fc/7fu7fcn8xnl5r4zffk3tpunuedg.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
。重み0の接続図を簡略化するため、省略しました。これまでは、隠れたニューロンに小さなxマーカーとyマーカーを残して、突起関数が計算される方向を思い出してきました。それらは入ってくる変数によって暗示されるので、後でそれらを拒否します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
パラメータhを変更してみてください。</font><font style="vertical-align: inherit;">ご覧のとおり、このため、出力の重みと、両方の突出関数の重みxとyが変化します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
私たちが作成したものは、「タワー関数」のようなものです。そのようなタワー関数</font></font><br>
<br>
<img src="https://habrastorage.org/webt/ad/le/ww/adlewwyzmc3zhrk-fm9a9yvx6zo.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
を作成できる場合、それらを使用して、さまざまな場所に異なる高さのタワーを追加するだけで、任意の関数を近似できます。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/u1/lv/xv/u1lvxvdmfi4xxsqgpjabiwofr2k.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
もちろん、まだ任意のタワー関数の作成には至っていません。</font><font style="vertical-align: inherit;">これまでのところ、高さhの高原を囲む高さ2hの中央塔のようなものを構築しました。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
しかし、タワー関数を作成できます。</font><font style="vertical-align: inherit;">以前に、ニューロンを使用してif-then-elseステートメントを実装する方法を示したことを思い出してください。</font></font><br>
<br>
<pre><code class="python hljs">    <span class="hljs-keyword">if</span>  &gt;= :<font></font>
         <span class="hljs-number">1</span> 
    <span class="hljs-keyword">else</span>:<font></font>
         <span class="hljs-number">0</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
それは1入力ニューロンでした。</font><font style="vertical-align: inherit;">そして、同様のアイデアを、隠れたニューロンの結合された出力に適用する必要があります。</font></font><br>
<br>
<pre><code class="python hljs">    <span class="hljs-keyword">if</span>     &gt;= :<font></font>
         <span class="hljs-number">1</span>
    <span class="hljs-keyword">else</span>:<font></font>
         <span class="hljs-number">0</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
正しいしきい値を選択すると（たとえば、3h / 2で高原の高さと中央塔の高さの間に圧迫されます）、高原をゼロに砕き、塔を1つだけ残すことができます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
これを行う方法を想像してみますか？次のネットワークを試してみてください。これで、隠れ層の重み付けされた出力だけでなく、ネットワーク全体の出力をプロットしています。これは、隠れ層からの重み付き出力にオフセット項を追加し、シグモイドを適用することを意味します。タワーを取得するためのhとbの値を見つけることができますか？この時点で行き詰まった場合は、次の2つのヒントを参考にしてください。（1）発信ニューロンがif-then-elseスタイルで正しい動作を表示するには、着信重み（すべてhまたは–h）を大きくする必要があります。 （2）bの値は、if-then-elseしきい値のスケールを決定します。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/ys/k-/1u/ysk-1uvu-jo68ikk5rqu274u7wc.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
デフォルトのパラメーターを使用すると、出力は前の図の平坦化されたバージョンに似ており、塔と高原があります。</font><font style="vertical-align: inherit;">目的の動作を得るには、hの値を増やす必要があります。</font><font style="vertical-align: inherit;">これにより、if-then-else動作のしきい値が得られます。</font><font style="vertical-align: inherit;">次に、しきい値を正しく設定するには、b≈−3h / 2を選択する必要があります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
h = 10の場合は次のようになります。</font></font><br>
<br>
<div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">お使いのブラウザはHTML5ビデオをサポートしていません。</font></font><source src="http://neuralnetworksanddeeplearning.com/movies/tower_construction.mp4" type="video/mp4"></video></div></div></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
hの値が比較的控えめな場合でも、素晴らしいタワー関数が得られます。そしてもちろん、hをさらに増やし、変位をレベルb = −3h / 2に保つことで、任意の美しい結果を得ることができます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
2つのネットワークを接着して、2つの異なるタワー機能をカウントしてみましょう。 2つのサブネットのそれぞれの役割を明確にするために、それらを別々の長方形に配置します。それぞれが上記の手法を使用してタワー関数を計算します。右側のグラフは、2番目の非表示レイヤーの重み付き出力、つまりタワー関数の重み付き組み合わせを示しています。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/8-/cl/ke/8-clkebo6vphf1-0_jgoaf3exts.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
特に、最後のレイヤーの重みを変更することで、出力タワーの高さを変更できることがわかります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
同じ考え方で、好きなだけ塔を計算することができます。私たちはそれらを任意に薄くて高くすることができます。その結果、2番目の隠れ層の重み付き出力が2つの変数の任意の望ましい関数に近似することを保証します。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/ig/0u/5z/ig0u5zbzifftdfq4ww4y9a4r7du.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
特に、2番目の隠れ層の重み付き出力をσ </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">−</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 1⋅fに</font><font style="vertical-align: inherit;">近似</font><font style="vertical-align: inherit;">させることにより、ネットワークの出力が望ましい関数fの適切な近似になることを保証します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
多くの変数の関数はどうですか？</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3の3</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">つの変数を使用してみましょう</font><font style="vertical-align: inherit;">。次のネットワークを使用して、タワー関数を4次元で計算できますか？</font></font><br>
<br>
<img src="https://habrastorage.org/webt/wg/ki/kn/wgkiknicnzeoaept0d-cemw0sd0.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ここで、x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">はネットワーク入力を示します。 s</font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、t </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1など</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> -ニューロンのステップポイント-つまり、最初のレイヤーのすべての重みが大きく、ステップポイントがs </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、t </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、s </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、...に</font><font style="vertical-align: inherit;">なるようにオフセットが割り当てられ</font><font style="vertical-align: inherit;">ます。2番目のレイヤーの重みは交互、+ h 、−h、ここでhは非常に大きな数です。出力オフセットは−5h / 2です。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ネットワーク機能は三つの条件の下で1に等しく計算：X </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1は、</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> S間にある</font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">とt </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、 x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">はs </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">とt </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2の間</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">です。 x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">はs </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">とt </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3の間にある</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。他のすべての場所でネットワークは0です。これはそのような塔で、1は入口スペースのごく一部であり、0はその他すべてです。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
多くのそのようなネットワークを接着すると、好きなだけタワーを取得し、3つの変数の任意の関数を近似できます。同じ考えがm次元でも機能します。出力オフセット（−m + 1/2）hのみが変更され、目的の値を適切に圧縮して、プラトーを削除します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
さて、これでNSを使用して多くの変数の実際の関数を概算する方法がわかりました。ベクトル関数f（x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、...、x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">m</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">）∈R </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">nは</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">どうですか？もちろん、そのような関数は、n個の実際の関数f1（x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、...、x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">m</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">）、f2（x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、...、x</font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">m</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">）など。</font><font style="vertical-align: inherit;">その後、すべてのネットワークを接着します。</font><font style="vertical-align: inherit;">したがって、それを理解するのは簡単です。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">仕事</font></font></h3><br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2つの隠れ層を持つニューラルネットワークを使用して任意の関数を近似する方法を見ました。</font><font style="vertical-align: inherit;">これが1つの非表示レイヤーで可能であることを証明できますか？</font><font style="vertical-align: inherit;">ヒント-2つの出力変数のみを操作してみてください。（a）ステップの関数をx軸またはy軸だけでなく、任意の方向にも取得できます。</font><font style="vertical-align: inherit;">（b）ステップ（a）の多くの構造を合計すると、長方形の塔ではなく、丸い塔の関数を近似することができます。</font><font style="vertical-align: inherit;">©丸い塔を使用すると、任意の関数を近似することが可能です。</font><font style="vertical-align: inherit;">ステップ©は、この章の少し下にある資料を使用すると簡単に実行できます。</font></font></li>
</ul><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">シグモイドニューロンを超えて</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
シグモイドニューロンのネットワークが任意の関数を計算できることを証明しました。シグモイドニューロンでは、入力x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、...が出力でσ（∑ </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> + b）に</font><font style="vertical-align: inherit;">変わります。</font><font style="vertical-align: inherit;">ここで、w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">は重み、bは変位、σはシグモイドです。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/0h/ut/93/0hut93wneejtjvxvxiwnfwpmo40.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
異なる活性化関数s（z）を使用して異なるタイプのニューロンを検討するとどうなるでしょうか。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/ua/0-/it/ua0-itpxz-uwkpnptxfsvszqabg.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
つまり、入力ニューロンにx </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、...重みw </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、...とバイアスb </font><font style="vertical-align: inherit;">があると仮定し</font><font style="vertical-align: inherit;">ます。の場合、出力はs（∑ </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> + b）になります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
シグモイドの場合と同様に、このアクティブ化関数を使用してステップを実行できます。図で（</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">元の記事で</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">）重みを上げて、たとえばw = 100にしてみてください。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/vz/-v/mu/vz-vmulc79w1g7xxom_btmnfyow.png"><br>
<br>
<img src="https://habrastorage.org/webt/nf/aq/iz/nfaqiznhl2klhebfc33iiy6htrg.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
シグモイドの場合と同様に、これによりアクティベーション関数が圧縮され、その結果、ステップ関数の非常に優れた近似になります。オフセットを変更してみてください。ステップの位置を任意に変更できることがわかります。したがって、必要な関数を計算するために、以前と同じトリックをすべて使用できます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
これが機能するために、s（z）にはどのようなプロパティが必要ですか？</font><font style="vertical-align: inherit;">s（z）はz→−∞およびz→∞として適切に定義されていると想定する必要があります。</font><font style="vertical-align: inherit;">これらの制限は、ステップ関数で受け入れられる2つの値です。</font><font style="vertical-align: inherit;">また、これらの制限は異なると想定する必要があります。</font><font style="vertical-align: inherit;">それらが変わらなければ、ステップは機能せず、単にフラットなスケジュールが存在するだけです！</font><font style="vertical-align: inherit;">しかし、活性化関数s（z）がこれらの特性を満たす場合、それに基づくニューロンは計算に広く適しています。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">タスク</font></font></h3><br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">この本の前半で、別のタイプのニューロン、つまり修正線形ニューロン、または修正線形ユニットReLUに出会いました。</font><font style="vertical-align: inherit;">このようなニューロンが普遍性に必要な条件を満たさない理由を説明してください。</font><font style="vertical-align: inherit;">ReLUがコンピューティングに広く適していることを示す多様性の証拠を見つけます。</font></font></li>
<li>,    ,    s(z)=z. ,       . ,        .</li>
</ul><br>
<h2>  </h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
当面は、ニューロンが正確なステップ関数を生成すると仮定しました。これは良い近似ですが、近似にすぎません。実際、次のグラフに示すように、失敗の狭いギャップがあり、関数の動作は段階的なものとは大きく異なります</font></font><br>
<br>
<img src="https://habrastorage.org/webt/mr/0t/ng/mr0tng4l1giob-gsuhyo_oh_vk0.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
。この失敗のギャップでは、普遍性に関する私の説明は機能しません。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
失敗はそれほど怖くない。十分に大きな入力重みを設定することにより、これらのギャップを任意に小さくすることができます。チャート上よりもはるかに小さく、目に見えないようにすることができます。したがって、この問題について心配する必要はないかもしれません。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
それにもかかわらず、私はそれを解決するいくつかの方法を持ちたいと思います。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
簡単に解決できることがわかりました。入力と出力が1つだけのNS関数を計算するためのこのソリューションを見てみましょう。同じアイデアが、多数の入力と出力の問題を解決するために機能します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
特に、ネットワークに関数fを計算させたいとしましょう。以前と同様に、ニューロンの隠れ層の重み付き出力がσ </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">−</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 1⋅f（x）に</font><font style="vertical-align: inherit;">なるようにネットワークを設計することでこれを試みます。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/sk/bu/bw/skbubwnwkrrpukeblqe9a1qo8cw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
上記の手法を使用してこれを行うと、隠れニューロンが一連の突出関数を生成するようになります：</font></font><br>
<br>
<img src="https://habrastorage.org/webt/71/uc/x_/71ucx_26mzx0_isj6dlob9wdacq.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Iもちろん、失敗の間隔のサイズを誇張して、見やすくしました。これらすべての突出関数を合計すると、σ </font><sup><font style="vertical-align: inherit;">-1の</font></sup><font style="vertical-align: inherit;">十分に良好な近似が得られることは明らかです。</font></font><sup><font style="vertical-align: inherit;"></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">failuref（x）は、故障の間隔を除いてどこにでもあります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ただし、今説明した近似を使用する代わりに、隠れたニューロンのセットを使用して、元の目的関数の半分の近似、つまりσ </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">−</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 1⋅f（x）/ 2 </font><font style="vertical-align: inherit;">を計算するとします</font><font style="vertical-align: inherit;">。もちろん、これは最後のグラフのスケーリングされたバージョンのように見えます。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/8-/0b/fv/8-0bfvrf5njiwum-w6d8edu4dro.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
別の隠しニューロンのセットにσ </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">−</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 1⋅f（x）/ 2 </font><font style="vertical-align: inherit;">への近似を計算させます</font><font style="vertical-align: inherit;">が、そのベースでは、突起は幅の半分だけシフトします。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/dj/cz/7n/djcz7nxhhm98yhiq94uluglvm-m.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
これで、σ−1⋅f（x）/ 2の2つの異なる近似が得られました。</font><font style="vertical-align: inherit;">これらの2つの近似を合計すると、σ−1⋅f（x）の一般的な近似が得られます。</font><font style="vertical-align: inherit;">この一般的な近似では、まだ小さな間隔で誤差があります。</font><font style="vertical-align: inherit;">しかし、問題は以前より少なくなります。なぜなら、最初の近似の失敗の区間に入る点は、2番目の近似の失敗の区間に落ちないからです。</font><font style="vertical-align: inherit;">したがって、これらの間隔の近似値は約2倍になります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
関数σ−1⋅f（x）/ Mのオーバーラップする近似の多数のMを追加することにより、状況を改善できます。</font><font style="vertical-align: inherit;">すべての故障間隔が十分に狭い場合、電流はそのうちの1つだけに流れます。</font><font style="vertical-align: inherit;">Mの十分な数の重複する近似を使用すると、結果は優れた一般的な近似になります。</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">結論</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ここで説明する普遍性の説明は、ニューラルネットワークを使用して関数を数える方法の実際的な説明とは言えません。この意味で、NANDロジックゲートの多様性の証明などに似ています。したがって、私は基本的にこのデザインを明確にし、詳細を最適化せずにわかりやすいように努めました。ただし、この設計を最適化しようとすることは、興味深く有益な練習になる可能性があります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
得られた結果を直接NSを作成するために使用することはできませんが、NSを使用する特定の関数の計算可能性の問題を取り除くため、これは重要です。そのような質問への答えは常に肯定的です。したがって、計算可能な関数があるかどうかを尋ねることは正しいですが、それを計算する正しい方法は何ですか。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
私たちのユニバーサルデザインは、任意の関数を計算するために2つの非表示レイヤーのみを使用します。すでに説明したように、単一の非表示レイヤーで同じ結果を得ることができます。これを考えると、深いネットワーク、つまり多数の隠れ層を持つネットワークがなぜ必要なのか疑問に思われるかもしれません。これらのネットワークを、1つの隠れ層を持つ浅いネットワークに置き換えることはできませんか？</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
原理的には可能ですが、ディープニューラルネットワークを使用することには実用的な理由があります。第1章で説明したように、ディープNSには階層構造があり、実際の問題の解決に役立つ階層的な知識を研究するのに適しています。具体的には、パターン認識などの問題を解決する場合、個々のピクセルだけでなく、境界線から単純な幾何学的形状、さらには複数のオブジェクトを含む複雑なシーンまで、ますます複雑な概念を理解するシステムを使用すると便利です。後の章で、深いNSは浅いものよりもそのような知識の階層の研究によりうまく対処できるという事実を支持する証拠を見るでしょう。要約すると、NSはあらゆる関数を計算できるという普遍性があります。経験的証拠は、深いNSが多くの現実世界の問題を解決するのに役立つ関数の研究により適していることを示唆しています。</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../ja461647/index.html">最高の開発者が必要とする5つの重要な欠落スキル</a></li>
<li><a href="../ja461649/index.html">どうやって世界を救うの</a></li>
<li><a href="../ja461653/index.html">ソフトウェア無線-それはどのように機能しますか？パート10</a></li>
<li><a href="../ja461655/index.html">先週のフロントエンドの世界からの新鮮な食材のダイジェストNo. 373（2019年7月22日〜28日）</a></li>
<li><a href="../ja461657/index.html">Red Hatの購入：ハイブリッドクラウドのリーダーシップを目指すBlue Giantの戦いに役立ちますか</a></li>
<li><a href="../ja461661/index.html">コンポーネントベースの開発ガイド</a></li>
<li><a href="../ja461663/index.html">LinuxがWindowsを導入した経緯</a></li>
<li><a href="../ja461665/index.html">Zen2。Ryzen 7 3700xの例におけるAM4プラットフォームの進化</a></li>
<li><a href="../ja461669/index.html">PHPダイジェストNo. 161（2019年7月15〜29日）</a></li>
<li><a href="../ja461673/index.html">初心者プログラマーのための8つのヒントまたは私のキャリアの回顧</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>