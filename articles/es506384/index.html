<!doctype html>
<html class="no-js" lang="es">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>   Red neuronal: entrenamiento sin maestro. M茅todo de gradiente de pol铆ticas   </title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Buenos d铆as, Habr
 Este art铆culo abre una serie de art铆culos sobre c贸mo entrenar redes neuronales sin un maestro. 
 (Aprendizaje de refuerzo para rede...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Red neuronal: entrenamiento sin maestro. M茅todo de gradiente de pol铆ticas</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/506384/"><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Buenos d铆as, Habr</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Este art铆culo abre una serie de art铆culos sobre c贸mo entrenar redes neuronales sin un maestro. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 (Aprendizaje de refuerzo para redes neuronales) </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En el ciclo planeo hacer tres art铆culos sobre teor铆a e implementaci贸n en el c贸digo de tres algoritmos de entrenamiento para redes neuronales sin un maestro. </font><font style="vertical-align: inherit;">El primer art铆culo ser谩 sobre Policy Gradient, el segundo sobre Q-learning, el tercer art铆culo ser谩 final seg煤n el m茅todo Actor-Critic. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Disfruta leyendo.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Art铆culo uno: Aprendizaje de gradiente de pol铆ticas sin maestro </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 (Gradiente de pol铆ticas para el aprendizaje de refuerzo)</font></font></h4><br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Introducci贸n</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Entre los algoritmos de aprendizaje autom谩tico, los algoritmos de aprendizaje autom谩tico ocupan un lugar especial donde el algoritmo aprende a resolver el problema por s铆 solo sin intervenci贸n humana, interactuando directamente con el entorno en el que aprende. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dichos algoritmos han recibido el nombre general: algoritmos de aprendizaje sin un maestro, para tales algoritmos, no es necesario recopilar bases de datos, no es necesario clasificarlas o marcarlas. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Es suficiente que el estudiante sin el maestro solo d茅 una respuesta hacia atr谩s a sus acciones o decisiones: fueron buenas o no.</font></font><br>
<a name="habracut"></a><br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cap铆tulo 1. Formaci贸n del profesorado</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Entonces, 驴qu茅 es? Aprender con o sin un maestro. Examinaremos esto con m谩s detalle con ejemplos del aprendizaje autom谩tico moderno y las tareas que resuelve. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La mayor铆a de los algoritmos modernos de aprendizaje autom谩tico para problemas de clasificaci贸n, regresi贸n y segmentaci贸n son esencialmente algoritmos de aprendizaje con un maestro en el que la persona es el maestro. Debido a que es la persona que marca los datos que le dice al algoritmo cu谩l deber铆a ser la respuesta correcta y, por lo tanto, el algoritmo intenta encontrar una soluci贸n para que la respuesta que el algoritmo da al resolver el problema coincida con la respuesta que la persona indic贸 para la tarea dada como la respuesta correcta. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Usando el ejemplo del problema de clasificaci贸n para el conjunto de datos Mnist, la respuesta correcta que la persona da al algoritmo es la etiqueta de la clase del d铆gito en el conjunto de entrenamiento.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/za/kq/b6/zakqb6ob1_xce07caywnurtsl7k.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En el conjunto de datos Mnist, para cada imagen que el algoritmo de la m谩quina tiene que aprender a clasificar, las personas preestablecen las etiquetas correctas a qu茅 clase pertenece esta imagen. En el proceso de aprendizaje, el algoritmo que predice la clase de imagen compara su clase obtenida para una imagen particular con la clase verdadera para la misma imagen y ajusta gradualmente sus par谩metros en el proceso de aprendizaje de modo que la clase predicha por el algoritmo tiende a corresponder a la clase especificada por la persona. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Por lo tanto, se puede resumir la siguiente idea: el algoritmo de aprendizaje con el maestro es cualquier algoritmo de aprendizaje autom谩tico, donde le damos al algoritmo c贸mo debe hacerse correctamente desde nuestro punto de vista.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Y no importa c贸mo proceder: indique a qu茅 clase se debe asignar esta imagen si es una tarea de clasificaci贸n, o dibuje los contornos de un objeto si es una tarea de segmentaci贸n o en qu茅 direcci贸n girar el volante del autom贸vil si el algoritmo es piloto autom谩tico, es importante que para cada situaci贸n espec铆fica Le indicamos expl铆citamente al algoritmo d贸nde est谩 la respuesta correcta, c贸mo hacerlo correctamente. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Esta es la clave para entender cu谩n fundamentalmente el algoritmo de aprendizaje con un maestro difiere del algoritmo de aprendizaje sin un maestro.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cap铆tulo 2. Aprendiendo sin un maestro</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Habiendo descubierto qu茅 es, ense帽ar con un maestro, ahora entenderemos qu茅 es, ense帽ar sin un maestro. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Como descubrimos en el 煤ltimo cap铆tulo, cuando ense帽amos con un maestro, para cada situaci贸n de ense帽anza, le damos al algoritmo una comprensi贸n de qu茅 respuesta es la correcta desde nuestro punto de vista, y luego vamos desde el opuesto: al aprender sin un maestro, para cada situaci贸n espec铆fica no le damos una respuesta al algoritmo estar谩n. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pero entonces surge la pregunta, si no le damos al algoritmo instrucciones expl铆citas sobre c贸mo proceder correctamente, 驴qu茅 aprender谩 el algoritmo? C贸mo aprender谩 el algoritmo sin saber d贸nde ajustar sus par谩metros internos para hacer lo correcto y finalmente resolver el problema como nos gustar铆a.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pensemos en este tema. Pero es importante para nosotros que el algoritmo resuelva el problema en su conjunto, y c贸mo exactamente actuar谩 en el proceso de resoluci贸n de este problema y de qu茅 manera ir谩 a resolverlo no nos concierne, lo entregaremos al algoritmo en s铆, esperamos solo el resultado final de 茅l. . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Por lo tanto, dejaremos que el algoritmo comprenda el resultado final, ya sea que haya resuelto bien nuestro problema o no. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Por lo tanto, resumiendo todo lo anterior, llegamos a la conclusi贸n de que llamamos a estos algoritmos de aprendizaje sin un maestro donde no hay una indicaci贸n expl铆cita para el algoritmo de c贸mo hacerlo, pero solo hay una evaluaci贸n general de todas sus acciones en el proceso de resoluci贸n del problema.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/qi/ci/nn/qicinnqzow8o5b7lacir_mrs4aa.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En el ejemplo de un juego en el que una raqueta intenta atrapar los cubos que caen desde arriba, no le decimos al algoritmo que controle la raqueta en qu茅 momento espec铆fico d贸nde mover la raqueta. </font><font style="vertical-align: inherit;">Le diremos al algoritmo solo el resultado de sus acciones: 驴atrap贸 un cubo con una raqueta o no? </font></font><br>
<br>
<img src="https://habrastorage.org/webt/-f/dp/df/-fdpdf5nbjpawt1mutegmxzw9iu.gif"><br>
<br>
<img src="https://habrastorage.org/webt/ve/jk/wz/vejkwz-fezjvbjcjydzmx7q4hk8.gif"><br>
<br>
<img src="https://habrastorage.org/webt/ws/gv/nb/wsgvnb34jwzwk--6iehpg3y3lqw.gif"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Esta es la esencia del aprendizaje sin un maestro. </font><font style="vertical-align: inherit;">El algoritmo en s铆 mismo debe aprender a decidir qu茅 hacer en cada caso espec铆fico en funci贸n de la evaluaci贸n final de la totalidad de todas sus acciones.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cap铆tulo 3. Agente, medio ambiente y recompensa</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Habiendo descubierto qu茅 es la capacitaci贸n sin un maestro, profundizaremos en los algoritmos que pueden aprender c贸mo resolver un problema sin nuestros consejos sobre c贸mo hacerlo correctamente. </font></font><br>
 <br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Es hora de presentarnos la terminolog铆a que usaremos en el futuro. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Llamaremos a un agente nuestro algoritmo que puede analizar el estado del entorno y realizar algunas acciones en 茅l. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El entorno es el mundo virtual en el que nuestro agente existe y, a trav茅s de sus acciones, puede cambiar su estado ... </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Recompensa: comentarios del entorno al agente como respuesta a sus acciones.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/23/dp/gw/23dpgwboqw80sv_1uj5_ubifms8.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El entorno en el que vive nuestro agente puede ser arbitrariamente complejo, es posible que el agente ni siquiera sepa c贸mo est谩 estructurado para tomar sus decisiones y realizar acciones. </font><font style="vertical-align: inherit;">Para el Agente, solo es importante la retroalimentaci贸n en forma de recompensa que recibe del medio ambiente. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Si consideramos con m谩s detalle el proceso de interacci贸n del agente con el entorno, entonces puede expresarse mediante el siguiente esquema </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ml/f5/eq/mlf5eqvivcvcwodu66mfyddfpdo.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
St - estado del entorno en el paso t </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
at - acci贸n del agente en el paso t </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
rt - recompensa en el paso t</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En cada momento de tiempo t, nuestro agente observa el estado del medio - St, realiza la acci贸n - en la cual recibe una recompensa - rt del medio, el campo del cual el medio pasa al estado St + 1, que nuestro agente observa, realiza la acci贸n - en + 1, para que recibe una recompensa del medio - rt + 1 y tales estados t, podemos tener un conjunto infinito - n.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cap铆tulo 4. Parametrizaci贸n de la tarea de aprender sin profesor</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para capacitar a un agente, necesitamos parametrizar de alguna manera la tarea de aprender sin un maestro, en otras palabras, para comprender qu茅 funciones vamos a optimizar. </font><font style="vertical-align: inherit;">En el aprendizaje por refuerzo, en lo que sigue llamaremos capacitaci贸n sin un maestro, hay tres funciones b谩sicas: </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
1) p (a | s) - funci贸n de pol铆tica </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La funci贸n de probabilidad de la 贸ptima de una acci贸n es a, dependiendo del estado del entorno. </font><font style="vertical-align: inherit;">Nos muestra c贸mo la acci贸n de a es 贸ptima bajo el estado del medio s. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
2) v (s) - funci贸n de valor </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La </font><font style="vertical-align: inherit;">funci贸n de valor de </font><font style="vertical-align: inherit;">estado es s. </font><font style="vertical-align: inherit;">Nos muestra cu谩nto es generalmente valioso el estado s para nosotros en t茅rminos de recompensas.3 </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
) Q (s, a) - Funci贸n Q</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Q es una funci贸n de estrategia 贸ptima. Nos permite, de acuerdo con esta estrategia 贸ptima, en los estados elegir la acci贸n 贸ptima para este estado. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Primero, consideramos la funci贸n funci贸n de pol铆tica, como la funci贸n de aprendizaje de refuerzo m谩s simple e intuitiva. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ya que vamos a resolver los problemas del aprendizaje por refuerzo a trav茅s de redes neuronales. Luego, esquem谩ticamente, podemos parametrizar una funci贸n de pol铆tica a trav茅s de una red neuronal de la siguiente manera.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/oa/fv/1x/oafv1xdki7zeejvxvy-tsgd5oeu.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Proporcionaremos estados a la entrada de la red neuronal y construiremos la salida de la red neuronal de modo que la capa de salida de la red neuronal sea la capa SoftMax, con el n煤mero de salidas igual al n煤mero de acciones posibles para el agente en nuestro entorno. </font><font style="vertical-align: inherit;">Por lo tanto, al pasar el estado s en la salida a trav茅s de las capas de la red neuronal, obtenemos la distribuci贸n de probabilidad de las acciones del agente en el estado s. </font><font style="vertical-align: inherit;">Lo que, de hecho, es lo que necesitamos para comenzar a entrenar nuestra red neuronal y mejorar iterativamente la funci贸n de pol铆tica, que ahora es esencialmente nuestra red neuronal, a trav茅s del algoritmo de error de propagaci贸n inversa.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cap铆tulo 5. Mejora de la funci贸n pol铆tica a trav茅s de la formaci贸n de redes neuronales</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para entrenar la red neuronal, utilizamos el m茅todo de descenso de gradiente. Dado que la 煤ltima capa de nuestra red neuronal es la capa SoftMax, su funci贸n de p茅rdida es: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/29/y9/u2/29y9u2wizl5w2yf5yeyw08nk1ic.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
donde: </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
1 - etiquetas verdaderas * log (etiquetas pronosticadas) </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
2 - la suma de todos los ejemplos </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Sin embargo, 驴c贸mo podemos entrenar una red neuronal si a煤n no tenemos las etiquetas correctas para las acciones de los agentes en los estados? S0-Sj? Y no los necesitamos, en lugar de las etiquetas correctas usaremos la recompensa que el agente recibi贸 del medio al realizar la acci贸n que la red neuronal predijo para 茅l.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/46/jp/or/46jporp-h9pj-bujcmytwhnfmze.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Tenemos todo el derecho de hacer esto porque, para Cross Entropy Loss, yj son etiquetas verdaderas para la clase correcta y son iguales a uno, y para Policy Function Loss, rj es la recompensa que el entorno acumul贸 para el agente por la acci贸n que realiz贸. </font><font style="vertical-align: inherit;">Es decir, rj sirve como un peso para los gradientes en la propagaci贸n hacia atr谩s del error cuando entrenamos la red neuronal. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Recibi贸 una recompensa positiva: significa que debe aumentar el peso de la red neuronal hacia donde se dirige el gradiente. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/2e/ge/bq/2egebqgdxjutx0tzxvwawunwffq.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Si la recompensa es negativa, entonces los pesos correspondientes en la red neuronal, de acuerdo con la direcci贸n del gradiente donde se dirige el error, reducimos.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cap铆tulo 6. Creaci贸n de un conjunto de datos para capacitaci贸n</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para capacitar a nuestro Agente, la red neuronal que utiliza el cl谩sico Machine Learning, a trav茅s del m茅todo de propagaci贸n inversa de errores, necesitamos ensamblar un conjunto de datos. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A partir de la exposici贸n del problema, est谩 claro que a la entrada de la red neuronal queremos presentar el estado del medio S: una imagen de un pozo con cubos que caen y una raqueta que los atrapa. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/4h/pv/hr/4hpvhry6ju1kcckfjj4vxu_7eps.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
e Y, que recopilaremos de acuerdo con el estado S, esta ser谩 la acci贸n prevista de la red neuronal, y la recompensa que el entorno acumul贸 para el agente por esta acci贸n: r</font></font><br>
<br>
<img src="https://habrastorage.org/webt/an/gh/wo/anghwor5xj9mqdbgnnjrjgvzbta.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pero despu茅s de todo, el mi茅rcoles, durante el transcurso del juego, el agente no puede otorgar una recompensa por cada acci贸n, por ejemplo, en nuestro caso, el agente recibir谩 una recompensa positiva solo cuando la raqueta atrape un dado que cae. Si la raqueta no atrapa el cubo y cae al fondo, el mi茅rcoles le cobrar谩 al agente una recompensa negativa. El resto del tiempo, independientemente de c贸mo el agente mueva la raqueta, hasta que el cubo golpee la raqueta o caiga al fondo, el mi茅rcoles le cobrar谩 al agente una recompensa igual a cero.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/7f/rh/5u/7frh5uda0syl7dbcutr8ez8fcq8.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Como se puede ver en la descripci贸n del proceso de nuestro juego, la recompensa positiva o negativa de un agente es extremadamente rara, pero b谩sicamente la recompensa independientemente de sus acciones es cero. C贸mo entrenar al Agente del C贸digo la mayor铆a de las veces no recibe una respuesta del entorno a sus acciones. Desde el punto de vista de que nuestro agente es una red neuronal y la recompensa del entorno es cero, entonces los gradientes para la propagaci贸n inversa del error a trav茅s de la red neuronal en la mayor铆a de los casos en el conjunto de datos ser谩n cero, los pesos de la red neuronal no tendr谩n ning煤n lugar para cambiar, lo que significa que nuestro agente no aprender谩 nada. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
驴C贸mo resolver el problema con cero recompensa en la mayor铆a del conjunto de datos que capacitar谩 al agente? </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Hay dos formas de salir de esta situaci贸n:</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
el primero es asignar todas las acciones del agente que tom贸 durante el tiempo en que el cubo cay贸 una y la misma recompensa final del episodio +1 o -1, dependiendo de si el agente atrap贸 el cubo o no. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Por lo tanto, consideraremos todas las acciones del Agente si atrap贸 el cubo correctamente y consolidaremos dicho comportamiento del agente durante el entrenamiento, asign谩ndoles una recompensa positiva. Si el Agente no atrap贸 el cubo, asignaremos una recompensa negativa a todas las acciones del Agente en el episodio y lo entrenaremos para evitar esa secuencia de acciones en el futuro. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
el segundo: el premio final con un cierto paso de descuento en orden descendente para aplicar a todas las acciones del agente en este episodio. En otras palabras, cuanto m谩s cerca est茅 la acci贸n del agente de la final, m谩s cerca de +1 o -1 recompensa por esta acci贸n.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Al introducir una recompensa con tal descuento para una acci贸n en orden decreciente a medida que la acci贸n se aleja del final del episodio, le permitimos al Agente comprender que las 煤ltimas acciones que realiz贸 son m谩s importantes para el resultado del episodio del Juego que las acciones que realiz贸 al principio. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Por lo general, la recompensa con descuento se calcula mediante la f贸rmula: la recompensa final del episodio se multiplica por el coeficiente de descuento en la potencia del n煤mero de paso menos uno para todas las acciones del agente en el episodio (por el momento en que cay贸 el cubo). </font></font><br>
<br>
<img src="https://habrastorage.org/webt/4r/cf/w6/4rcfw67kmlniyfhczs_p2v2kb-c.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Gamma: coeficiente de descuento (disminuci贸n de la recompensa). Siempre est谩 en el rango de 0 a 1. Por lo general, la gama se toma en la regi贸n de 0,95. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Despu茅s de decidir qu茅 datos recopilamos en DataSet, ejecuta el simulador de entorno y juega un juego con varios episodios varias veces seguidas, recopila datos sobre:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">estado del medio ambiente, </font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> acciones tomadas por el agente </font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La recompensa que recibi贸 el Agente.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para facilitar la comprensi贸n, llamemos a la ca铆da de un cubo a trav茅s del pozo: un episodio del juego, tambi茅n asumimos que el juego en s铆 consistir谩 en varios episodios. </font><font style="vertical-align: inherit;">Esto significa que en un juego tiraremos varios cubos a su vez en el pozo, y la raqueta intentar谩 atraparlos. </font><font style="vertical-align: inherit;">Por cada dado capturado, el agente recibir谩 +1 punto, por cada dado que cay贸 al fondo y la raqueta no lo atrap贸, el agente recibir谩 -1 punto.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cap铆tulo 7. La estructura interna del agente y el entorno.</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Entorno: dado que tenemos un entorno en el que el agente debe existir, consiste esencialmente en una matriz de pozo dentro de la cual los cubos caen a una velocidad de una l铆nea por reloj, y el agente va en una direcci贸n tambi茅n a una celda. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Escribiremos un simulador del entorno que puede dejar caer un cubo desde la l铆nea superior en una columna arbitraria en un momento aleatorio, sabe c贸mo recibir un comando de un agente para mover la raqueta una celda en una de las direcciones, despu茅s de lo cual verifica si el cubo ca铆do fue atrapado por la raqueta o si cay贸 fondo del pozo. Dependiendo de esto, el simulador devuelve al agente la recompensa que recibi贸 por su acci贸n.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Agente: el elemento principal es nuestra red neuronal, capaz de devolver las probabilidades de todas las acciones para un estado dado del entorno como el estado del entorno dedicado a 茅l en la entrada. </font><font style="vertical-align: inherit;">A partir de la probabilidad de acciones recibidas de la red neuronal, el agente selecciona lo mejor, lo env铆a al entorno, recibe comentarios del entorno en forma de una recompensa del entorno. </font><font style="vertical-align: inherit;">Adem谩s, el Agente debe tener un algoritmo interno basado en el cual podr谩 aprender a maximizar la recompensa recibida del entorno.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cap铆tulo 8. Entrenamiento de agentes</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para capacitar al Agente, necesitamos acumular estad铆sticas de acuerdo con los datos de nuestro simulador y las acciones tomadas por el agente. </font><font style="vertical-align: inherit;">Recopilaremos datos estad铆sticos para la capacitaci贸n en triples de valores: el estado del medio ambiente, la acci贸n del agente y la recompensa por esta acci贸n.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/yo/qq/od/yoqqodgrfzatgri5dljps6uqnvw.png"><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Un poco de c贸digo sobre c贸mo recopilar estad铆sticas</font></font></h4><br>
<img src="https://habrastorage.org/webt/ph/hr/6x/phhr6xb35udm3hyc3mr9rwv5yno.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ponemos cada uno de estos tres valores en un b煤fer de memoria especial, donde los almacenamos todo el tiempo mientras simulamos el juego y acumulamos estad铆sticas sobre 茅l.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">C贸digo para organizar un b煤fer de memoria:</font></font></h4><br>
<img src="https://habrastorage.org/webt/u2/55/ty/u255ty2pzaoeagitk_osyxzxirg.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Despu茅s de haber realizado una serie de juegos de nuestro Agente con el mi茅rcoles y haber acumulado estad铆sticas, podemos proceder a la capacitaci贸n del Agente. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para hacer esto, obtenemos un lote de nuestros datos en forma de triples de valores del b煤fer de memoria con estad铆sticas acumuladas, desempaqu茅telo y convi茅rtalo en tensores Pytorch. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/jo/se/6t/jose6tkxmmtzsttgrnc-q0xkrf0.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 El lote de estado ambiental en forma de tensor de Pytorch se aplica a la entrada en la red neuronal del agente, obtenemos la distribuci贸n de probabilidad para cada movimiento del agente para cada estado del entorno en el partido, logaritmo estas probabilidades, multiplicamos la recompensa recibida por el agente por movimiento por el logaritmo de estas probabilidades, luego tomamos el promedio sobre los productos y hacer que esto signifique negativo: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/hx/om/3i/hxom3ikgq9n13pb6brfzsfsbmkw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Etapa 1 </font></font><br>
<br>
<img src="https://habrastorage.org/webt/4j/gh/8l/4jgh8layvkcwg0dhrylxuwul2ri.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Etapa 2</font></font><br>
<br>
<img src="https://habrastorage.org/webt/pn/ac/pj/pnacpjuzovqe4-zql7lxlarerly.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Despu茅s de obtener el valor de la funci贸n P茅rdida, hacemos un paso inverso a trav茅s de la red neuronal para obtener gradientes y hacer que el paso sea un optimizador para ajustar los pesos. </font><font style="vertical-align: inherit;">Sobre esto, se est谩 descargando el ciclo de capacitaci贸n de nuestro agente. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dado que despu茅s de que el optimizador cambi贸 los pesos de la red neuronal, nuestros datos en las estad铆sticas recopiladas ya no son relevantes, porque una red neuronal con pesos cambiados dar谩 probabilidades completamente diferentes para acciones en las mismas condiciones ambientales y el entrenamiento del agente volver谩 a fallar. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Por lo tanto, borramos nuestro b煤fer con memoria, volvemos a perder una cierta cantidad de juegos para recopilar estad铆sticas y reiniciamos el proceso de entrenamiento del agente. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Este es el ciclo de aprendizaje cuando se aprende sin un maestro utilizando el m茅todo de Gradiente de Pol铆ticas.</font></font><br>
<br>
<ol>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Acumulaci贸n de estad铆sticas</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Entrenamiento de agente</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Reiniciar las estad铆sticas</font></font></li>
</ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 Repetimos el proceso de aprendizaje tantas veces hasta que nuestro Agente aprenda a recibir del sistema la recompensa que m谩s nos convenga.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cap铆tulo 9. Experimentos con Agente y Medio Ambiente</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Comencemos una serie de experimentos para entrenar a nuestro Agente. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para los experimentos, seleccionamos los siguientes par谩metros ambientales: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/xi/ym/2_/xiym2_kitmamvf52hhuff6tkpqs.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para el Agente, elegimos - construimos una red neuronal - ser谩 convolucional (a medida que trabajamos con la imagen), tendr谩 9 salidas (1-derecha, 2-izquierda, 3-arriba, 4-abajo, en la salida) 5-derecha-arriba, 6-izquierda-arriba, 7-derecha-abajo, 8-izquierda-abajo, 9-no hacer nada) y SoftMax para obtener la probabilidad de cada acci贸n.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Arquitectura de red neuronal</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Primer tama帽o de imagen de la neurona de la capa Conv2d 32 1 * 32 * 16 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Tama帽o de imagen de la capa MaxPool2d 32 * 16 * 8 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Segundo tama帽o de la imagen de la neurona capa 32 de Conv2d 32 * 16 * 8 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Tama帽o de la imagen de la capa MaxPool2d 32 * 8 * 4 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Flatten - enderece la imagen a un tama帽o de 1024 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Lineal capa para 1024 neuronas Capa de </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
abandono (0.25) Capa </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
lineal para 512 neuronas </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Capa lineal para 256 neuronas </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Capa lineal para 9 neuronas y SoftMax</font></font><br>
<br>
<img src="https://habrastorage.org/webt/2d/tz/ne/2dtzneqr2lsxdkz-yvu9xonh0uw.png"><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">C贸digo de creaci贸n de red neuronal Pytorch</font></font></h4><br>
<img src="https://habrastorage.org/webt/wq/mx/yw/wqmxyw67mbjtmnpnbj86zry8y4a.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Uno por uno, lanzaremos tres ciclos de capacitaci贸n de Agentes en el Medio Ambiente, cuyos par谩metros se discutieron anteriormente:</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Experimento No. 1 - El agente aprendi贸 a resolver el problema en 13,600 ciclos de juego.</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Estado inicial del Agente. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ws/gv/nb/wsgvnb34jwzwk--6iehpg3y3lqw.gif"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Programa </font><font style="vertical-align: inherit;">de </font><font style="vertical-align: inherit;">entrenamiento </font><font style="vertical-align: inherit;">del Agente. </font><font style="vertical-align: inherit;">El </font></font><br>
<br>
<img src="https://habrastorage.org/webt/jt/kd/3q/jtkd3qmdl7xixi-bwpkdp-tk0uy.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
estado </font><font style="vertical-align: inherit;">entrenado </font><font style="vertical-align: inherit;">del Agente.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/4a/v5/9l/4av59lvev4oget2gotgg-xsfy2w.gif"><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Experimento 2: el agente aprendi贸 a resolver el problema en 8250 ciclos del juego.</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Estado inicial del Agente. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ve/jk/wz/vejkwz-fezjvbjcjydzmx7q4hk8.gif"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Programa </font><font style="vertical-align: inherit;">de </font><font style="vertical-align: inherit;">entrenamiento </font><font style="vertical-align: inherit;">del Agente. </font><font style="vertical-align: inherit;">El </font></font><br>
<br>
<img src="https://habrastorage.org/webt/9e/zc/1x/9ezc1x4gfv7bilgb8sx-r3iemgy.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
estado </font><font style="vertical-align: inherit;">entrenado </font><font style="vertical-align: inherit;">del Agente.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/l9/r7/bn/l9r7bnstfbaqszje4npotjrze4o.gif"><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Experimento No. 3: el agente aprendi贸 a resolver el problema en 19800 ciclos de juego</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Estado inicial del Agente. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/hz/wm/ss/hzwmssjogsxwzv0wrik-1nb8loc.gif"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Programa </font><font style="vertical-align: inherit;">de </font><font style="vertical-align: inherit;">entrenamiento </font><font style="vertical-align: inherit;">del Agente. </font><font style="vertical-align: inherit;">El </font></font><br>
<br>
<img src="https://habrastorage.org/webt/sh/xv/zd/shxvzdpumy7fnoctkb6bqnhie2o.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
estado </font><font style="vertical-align: inherit;">entrenado </font><font style="vertical-align: inherit;">del Agente.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/2h/ga/yh/2hgayhvtgypaq1fvu-i54d4l4ok.gif"><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cap铆tulo 10. Conclusiones</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Mirando los cuadros, se puede decir que la capacitaci贸n del Agente es, por supuesto, lenta. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El agente inicialmente ha estado buscando al menos alguna pol铆tica razonable para sus acciones durante mucho tiempo para comenzar a recibir una recompensa positiva. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En este momento, en la primera etapa del cronograma, la recompensa por el juego crece lentamente, luego, de repente, el Agente encuentra una buena opci贸n para sus movimientos, y la recompensa recibida por 茅l para el juego aumenta abruptamente y sube, y luego, acerc谩ndose a la recompensa m谩xima, el agente vuelve a ir. un aumento lento en la eficiencia cuando mejora su pol铆tica de movimientos que ya aprendi贸, pero se esfuerza, como cualquier algoritmo codicioso, por tomar su recompensa por completo.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Tambi茅n me gustar铆a se帽alar la gran necesidad de c谩lculos para capacitar al Agente utilizando el m茅todo de Gradiente de Pol铆tica, porque El momento principal en que funciona el algoritmo es la recopilaci贸n de estad铆sticas sobre los movimientos del Agente y no su entrenamiento. Despu茅s de haber recopilado estad铆sticas sobre los movimientos de toda la matriz, usamos solo un lote de datos para entrenar al Agente, y descartamos todos los dem谩s datos que ya no son adecuados para el entrenamiento. Y nuevamente recopilamos nuevos datos. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Todav铆a puede experimentar mucho con este algoritmo y entorno: cambiando la profundidad y el ancho del pozo, aumentando o disminuyendo el n煤mero de dados que caen durante el juego, haciendo estos dados de diferentes colores. Observar lo que este efecto tendr谩 sobre la efectividad y la velocidad de entrenamiento del Agente.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Adem谩s, un campo extenso para los experimentos son los par谩metros de la red neuronal, en esencia de los cuales estamos entrenando a nuestro Agente, puede cambiar capas, n煤cleos de convoluci贸n, habilitar y ajustar la regularizaci贸n. </font><font style="vertical-align: inherit;">S铆, y se puede intentar mucho m谩s para aumentar la efectividad de la capacitaci贸n del Agente. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Por lo tanto, despu茅s de haber lanzado experimentos pr谩cticos con la ense帽anza sin un maestro utilizando el m茅todo de Gradiente de Pol铆ticas, est谩bamos convencidos de que ense帽ar sin un maestro tiene un lugar para estar, y realmente funciona. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El agente se entren贸 independientemente para maximizar su recompensa en el juego. </font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Enlace a GitHub con c贸digo adaptado para trabajar en una computadora port谩til Google Colab</font></font></a></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es506358/index.html">3 opciones para el comportamiento de la corte cuando reciben casos de bloqueos fiscales</a></li>
<li><a href="../es506362/index.html">C贸mo hicimos un baile en l铆nea</a></li>
<li><a href="../es506370/index.html">Compare el rendimiento de la restricci贸n de verificaci贸n y la clave externa en SQL Server</a></li>
<li><a href="../es506372/index.html">El instinto de reproducci贸n se encuentra en la inteligencia artificial.</a></li>
<li><a href="../es506380/index.html">Software gratis o dom茅stico. Matr铆cula est谩ndar o gratuita</a></li>
<li><a href="../es506386/index.html">C贸mo abandonar la escuela y transferir a un ni帽o a un lugar remoto</a></li>
<li><a href="../es506392/index.html">Notificaci贸n de Roskomnadzor sobre el procesamiento de datos personales en 2020</a></li>
<li><a href="../es506394/index.html">Impresora 3D Anycubic Mega X: una gran impresora a un precio modesto</a></li>
<li><a href="../es506396/index.html">Elon Musk: Lidar es una p茅rdida de tiempo. Todos los que conf铆an en lidar est谩n condenados "</a></li>
<li><a href="../es506398/index.html">Joel Spolsky: "No solo usabilidad"</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>