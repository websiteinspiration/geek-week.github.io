<!doctype html>
<html class="no-js" lang="es">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üèÖ üê£ ü¶ë Red neuronal: entrenamiento sin maestro. M√©todo de gradiente de pol√≠ticas ü•î üëäüèæ ü¶ï</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Buenos d√≠as, Habr
 Este art√≠culo abre una serie de art√≠culos sobre c√≥mo entrenar redes neuronales sin un maestro. 
 (Aprendizaje de refuerzo para rede...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Red neuronal: entrenamiento sin maestro. M√©todo de gradiente de pol√≠ticas</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/506384/"><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Buenos d√≠as, Habr</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Este art√≠culo abre una serie de art√≠culos sobre c√≥mo entrenar redes neuronales sin un maestro. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 (Aprendizaje de refuerzo para redes neuronales) </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En el ciclo planeo hacer tres art√≠culos sobre teor√≠a e implementaci√≥n en el c√≥digo de tres algoritmos de entrenamiento para redes neuronales sin un maestro. </font><font style="vertical-align: inherit;">El primer art√≠culo ser√° sobre Policy Gradient, el segundo sobre Q-learning, el tercer art√≠culo ser√° final seg√∫n el m√©todo Actor-Critic. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Disfruta leyendo.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Art√≠culo uno: Aprendizaje de gradiente de pol√≠ticas sin maestro </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 (Gradiente de pol√≠ticas para el aprendizaje de refuerzo)</font></font></h4><br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Introducci√≥n</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Entre los algoritmos de aprendizaje autom√°tico, los algoritmos de aprendizaje autom√°tico ocupan un lugar especial donde el algoritmo aprende a resolver el problema por s√≠ solo sin intervenci√≥n humana, interactuando directamente con el entorno en el que aprende. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dichos algoritmos han recibido el nombre general: algoritmos de aprendizaje sin un maestro, para tales algoritmos, no es necesario recopilar bases de datos, no es necesario clasificarlas o marcarlas. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Es suficiente que el estudiante sin el maestro solo d√© una respuesta hacia atr√°s a sus acciones o decisiones: fueron buenas o no.</font></font><br>
<a name="habracut"></a><br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cap√≠tulo 1. Formaci√≥n del profesorado</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Entonces, ¬øqu√© es? Aprender con o sin un maestro. Examinaremos esto con m√°s detalle con ejemplos del aprendizaje autom√°tico moderno y las tareas que resuelve. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La mayor√≠a de los algoritmos modernos de aprendizaje autom√°tico para problemas de clasificaci√≥n, regresi√≥n y segmentaci√≥n son esencialmente algoritmos de aprendizaje con un maestro en el que la persona es el maestro. Debido a que es la persona que marca los datos que le dice al algoritmo cu√°l deber√≠a ser la respuesta correcta y, por lo tanto, el algoritmo intenta encontrar una soluci√≥n para que la respuesta que el algoritmo da al resolver el problema coincida con la respuesta que la persona indic√≥ para la tarea dada como la respuesta correcta. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Usando el ejemplo del problema de clasificaci√≥n para el conjunto de datos Mnist, la respuesta correcta que la persona da al algoritmo es la etiqueta de la clase del d√≠gito en el conjunto de entrenamiento.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/za/kq/b6/zakqb6ob1_xce07caywnurtsl7k.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En el conjunto de datos Mnist, para cada imagen que el algoritmo de la m√°quina tiene que aprender a clasificar, las personas preestablecen las etiquetas correctas a qu√© clase pertenece esta imagen. En el proceso de aprendizaje, el algoritmo que predice la clase de imagen compara su clase obtenida para una imagen particular con la clase verdadera para la misma imagen y ajusta gradualmente sus par√°metros en el proceso de aprendizaje de modo que la clase predicha por el algoritmo tiende a corresponder a la clase especificada por la persona. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Por lo tanto, se puede resumir la siguiente idea: el algoritmo de aprendizaje con el maestro es cualquier algoritmo de aprendizaje autom√°tico, donde le damos al algoritmo c√≥mo debe hacerse correctamente desde nuestro punto de vista.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Y no importa c√≥mo proceder: indique a qu√© clase se debe asignar esta imagen si es una tarea de clasificaci√≥n, o dibuje los contornos de un objeto si es una tarea de segmentaci√≥n o en qu√© direcci√≥n girar el volante del autom√≥vil si el algoritmo es piloto autom√°tico, es importante que para cada situaci√≥n espec√≠fica Le indicamos expl√≠citamente al algoritmo d√≥nde est√° la respuesta correcta, c√≥mo hacerlo correctamente. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Esta es la clave para entender cu√°n fundamentalmente el algoritmo de aprendizaje con un maestro difiere del algoritmo de aprendizaje sin un maestro.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cap√≠tulo 2. Aprendiendo sin un maestro</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Habiendo descubierto qu√© es, ense√±ar con un maestro, ahora entenderemos qu√© es, ense√±ar sin un maestro. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Como descubrimos en el √∫ltimo cap√≠tulo, cuando ense√±amos con un maestro, para cada situaci√≥n de ense√±anza, le damos al algoritmo una comprensi√≥n de qu√© respuesta es la correcta desde nuestro punto de vista, y luego vamos desde el opuesto: al aprender sin un maestro, para cada situaci√≥n espec√≠fica no le damos una respuesta al algoritmo estar√°n. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pero entonces surge la pregunta, si no le damos al algoritmo instrucciones expl√≠citas sobre c√≥mo proceder correctamente, ¬øqu√© aprender√° el algoritmo? C√≥mo aprender√° el algoritmo sin saber d√≥nde ajustar sus par√°metros internos para hacer lo correcto y finalmente resolver el problema como nos gustar√≠a.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pensemos en este tema. Pero es importante para nosotros que el algoritmo resuelva el problema en su conjunto, y c√≥mo exactamente actuar√° en el proceso de resoluci√≥n de este problema y de qu√© manera ir√° a resolverlo no nos concierne, lo entregaremos al algoritmo en s√≠, esperamos solo el resultado final de √©l. . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Por lo tanto, dejaremos que el algoritmo comprenda el resultado final, ya sea que haya resuelto bien nuestro problema o no. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Por lo tanto, resumiendo todo lo anterior, llegamos a la conclusi√≥n de que llamamos a estos algoritmos de aprendizaje sin un maestro donde no hay una indicaci√≥n expl√≠cita para el algoritmo de c√≥mo hacerlo, pero solo hay una evaluaci√≥n general de todas sus acciones en el proceso de resoluci√≥n del problema.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/qi/ci/nn/qicinnqzow8o5b7lacir_mrs4aa.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En el ejemplo de un juego en el que una raqueta intenta atrapar los cubos que caen desde arriba, no le decimos al algoritmo que controle la raqueta en qu√© momento espec√≠fico d√≥nde mover la raqueta. </font><font style="vertical-align: inherit;">Le diremos al algoritmo solo el resultado de sus acciones: ¬øatrap√≥ un cubo con una raqueta o no? </font></font><br>
<br>
<img src="https://habrastorage.org/webt/-f/dp/df/-fdpdf5nbjpawt1mutegmxzw9iu.gif"><br>
<br>
<img src="https://habrastorage.org/webt/ve/jk/wz/vejkwz-fezjvbjcjydzmx7q4hk8.gif"><br>
<br>
<img src="https://habrastorage.org/webt/ws/gv/nb/wsgvnb34jwzwk--6iehpg3y3lqw.gif"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Esta es la esencia del aprendizaje sin un maestro. </font><font style="vertical-align: inherit;">El algoritmo en s√≠ mismo debe aprender a decidir qu√© hacer en cada caso espec√≠fico en funci√≥n de la evaluaci√≥n final de la totalidad de todas sus acciones.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cap√≠tulo 3. Agente, medio ambiente y recompensa</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Habiendo descubierto qu√© es la capacitaci√≥n sin un maestro, profundizaremos en los algoritmos que pueden aprender c√≥mo resolver un problema sin nuestros consejos sobre c√≥mo hacerlo correctamente. </font></font><br>
 <br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Es hora de presentarnos la terminolog√≠a que usaremos en el futuro. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Llamaremos a un agente nuestro algoritmo que puede analizar el estado del entorno y realizar algunas acciones en √©l. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El entorno es el mundo virtual en el que nuestro agente existe y, a trav√©s de sus acciones, puede cambiar su estado ... </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Recompensa: comentarios del entorno al agente como respuesta a sus acciones.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/23/dp/gw/23dpgwboqw80sv_1uj5_ubifms8.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El entorno en el que vive nuestro agente puede ser arbitrariamente complejo, es posible que el agente ni siquiera sepa c√≥mo est√° estructurado para tomar sus decisiones y realizar acciones. </font><font style="vertical-align: inherit;">Para el Agente, solo es importante la retroalimentaci√≥n en forma de recompensa que recibe del medio ambiente. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Si consideramos con m√°s detalle el proceso de interacci√≥n del agente con el entorno, entonces puede expresarse mediante el siguiente esquema </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ml/f5/eq/mlf5eqvivcvcwodu66mfyddfpdo.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
St - estado del entorno en el paso t </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
at - acci√≥n del agente en el paso t </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
rt - recompensa en el paso t</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En cada momento de tiempo t, nuestro agente observa el estado del medio - St, realiza la acci√≥n - en la cual recibe una recompensa - rt del medio, el campo del cual el medio pasa al estado St + 1, que nuestro agente observa, realiza la acci√≥n - en + 1, para que recibe una recompensa del medio - rt + 1 y tales estados t, podemos tener un conjunto infinito - n.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cap√≠tulo 4. Parametrizaci√≥n de la tarea de aprender sin profesor</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para capacitar a un agente, necesitamos parametrizar de alguna manera la tarea de aprender sin un maestro, en otras palabras, para comprender qu√© funciones vamos a optimizar. </font><font style="vertical-align: inherit;">En el aprendizaje por refuerzo, en lo que sigue llamaremos capacitaci√≥n sin un maestro, hay tres funciones b√°sicas: </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
1) p (a | s) - funci√≥n de pol√≠tica </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La funci√≥n de probabilidad de la √≥ptima de una acci√≥n es a, dependiendo del estado del entorno. </font><font style="vertical-align: inherit;">Nos muestra c√≥mo la acci√≥n de a es √≥ptima bajo el estado del medio s. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
2) v (s) - funci√≥n de valor </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La </font><font style="vertical-align: inherit;">funci√≥n de valor de </font><font style="vertical-align: inherit;">estado es s. </font><font style="vertical-align: inherit;">Nos muestra cu√°nto es generalmente valioso el estado s para nosotros en t√©rminos de recompensas.3 </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
) Q (s, a) - Funci√≥n Q</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Q es una funci√≥n de estrategia √≥ptima. Nos permite, de acuerdo con esta estrategia √≥ptima, en los estados elegir la acci√≥n √≥ptima para este estado. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Primero, consideramos la funci√≥n funci√≥n de pol√≠tica, como la funci√≥n de aprendizaje de refuerzo m√°s simple e intuitiva. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ya que vamos a resolver los problemas del aprendizaje por refuerzo a trav√©s de redes neuronales. Luego, esquem√°ticamente, podemos parametrizar una funci√≥n de pol√≠tica a trav√©s de una red neuronal de la siguiente manera.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/oa/fv/1x/oafv1xdki7zeejvxvy-tsgd5oeu.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Proporcionaremos estados a la entrada de la red neuronal y construiremos la salida de la red neuronal de modo que la capa de salida de la red neuronal sea la capa SoftMax, con el n√∫mero de salidas igual al n√∫mero de acciones posibles para el agente en nuestro entorno. </font><font style="vertical-align: inherit;">Por lo tanto, al pasar el estado s en la salida a trav√©s de las capas de la red neuronal, obtenemos la distribuci√≥n de probabilidad de las acciones del agente en el estado s. </font><font style="vertical-align: inherit;">Lo que, de hecho, es lo que necesitamos para comenzar a entrenar nuestra red neuronal y mejorar iterativamente la funci√≥n de pol√≠tica, que ahora es esencialmente nuestra red neuronal, a trav√©s del algoritmo de error de propagaci√≥n inversa.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cap√≠tulo 5. Mejora de la funci√≥n pol√≠tica a trav√©s de la formaci√≥n de redes neuronales</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para entrenar la red neuronal, utilizamos el m√©todo de descenso de gradiente. Dado que la √∫ltima capa de nuestra red neuronal es la capa SoftMax, su funci√≥n de p√©rdida es: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/29/y9/u2/29y9u2wizl5w2yf5yeyw08nk1ic.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
donde: </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
1 - etiquetas verdaderas * log (etiquetas pronosticadas) </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
2 - la suma de todos los ejemplos </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Sin embargo, ¬øc√≥mo podemos entrenar una red neuronal si a√∫n no tenemos las etiquetas correctas para las acciones de los agentes en los estados? S0-Sj? Y no los necesitamos, en lugar de las etiquetas correctas usaremos la recompensa que el agente recibi√≥ del medio al realizar la acci√≥n que la red neuronal predijo para √©l.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/46/jp/or/46jporp-h9pj-bujcmytwhnfmze.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Tenemos todo el derecho de hacer esto porque, para Cross Entropy Loss, yj son etiquetas verdaderas para la clase correcta y son iguales a uno, y para Policy Function Loss, rj es la recompensa que el entorno acumul√≥ para el agente por la acci√≥n que realiz√≥. </font><font style="vertical-align: inherit;">Es decir, rj sirve como un peso para los gradientes en la propagaci√≥n hacia atr√°s del error cuando entrenamos la red neuronal. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Recibi√≥ una recompensa positiva: significa que debe aumentar el peso de la red neuronal hacia donde se dirige el gradiente. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/2e/ge/bq/2egebqgdxjutx0tzxvwawunwffq.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Si la recompensa es negativa, entonces los pesos correspondientes en la red neuronal, de acuerdo con la direcci√≥n del gradiente donde se dirige el error, reducimos.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cap√≠tulo 6. Creaci√≥n de un conjunto de datos para capacitaci√≥n</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para capacitar a nuestro Agente, la red neuronal que utiliza el cl√°sico Machine Learning, a trav√©s del m√©todo de propagaci√≥n inversa de errores, necesitamos ensamblar un conjunto de datos. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A partir de la exposici√≥n del problema, est√° claro que a la entrada de la red neuronal queremos presentar el estado del medio S: una imagen de un pozo con cubos que caen y una raqueta que los atrapa. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/4h/pv/hr/4hpvhry6ju1kcckfjj4vxu_7eps.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
e Y, que recopilaremos de acuerdo con el estado S, esta ser√° la acci√≥n prevista de la red neuronal, y la recompensa que el entorno acumul√≥ para el agente por esta acci√≥n: r</font></font><br>
<br>
<img src="https://habrastorage.org/webt/an/gh/wo/anghwor5xj9mqdbgnnjrjgvzbta.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pero despu√©s de todo, el mi√©rcoles, durante el transcurso del juego, el agente no puede otorgar una recompensa por cada acci√≥n, por ejemplo, en nuestro caso, el agente recibir√° una recompensa positiva solo cuando la raqueta atrape un dado que cae. Si la raqueta no atrapa el cubo y cae al fondo, el mi√©rcoles le cobrar√° al agente una recompensa negativa. El resto del tiempo, independientemente de c√≥mo el agente mueva la raqueta, hasta que el cubo golpee la raqueta o caiga al fondo, el mi√©rcoles le cobrar√° al agente una recompensa igual a cero.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/7f/rh/5u/7frh5uda0syl7dbcutr8ez8fcq8.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Como se puede ver en la descripci√≥n del proceso de nuestro juego, la recompensa positiva o negativa de un agente es extremadamente rara, pero b√°sicamente la recompensa independientemente de sus acciones es cero. C√≥mo entrenar al Agente del C√≥digo la mayor√≠a de las veces no recibe una respuesta del entorno a sus acciones. Desde el punto de vista de que nuestro agente es una red neuronal y la recompensa del entorno es cero, entonces los gradientes para la propagaci√≥n inversa del error a trav√©s de la red neuronal en la mayor√≠a de los casos en el conjunto de datos ser√°n cero, los pesos de la red neuronal no tendr√°n ning√∫n lugar para cambiar, lo que significa que nuestro agente no aprender√° nada. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
¬øC√≥mo resolver el problema con cero recompensa en la mayor√≠a del conjunto de datos que capacitar√° al agente? </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Hay dos formas de salir de esta situaci√≥n:</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
el primero es asignar todas las acciones del agente que tom√≥ durante el tiempo en que el cubo cay√≥ una y la misma recompensa final del episodio +1 o -1, dependiendo de si el agente atrap√≥ el cubo o no. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Por lo tanto, consideraremos todas las acciones del Agente si atrap√≥ el cubo correctamente y consolidaremos dicho comportamiento del agente durante el entrenamiento, asign√°ndoles una recompensa positiva. Si el Agente no atrap√≥ el cubo, asignaremos una recompensa negativa a todas las acciones del Agente en el episodio y lo entrenaremos para evitar esa secuencia de acciones en el futuro. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
el segundo: el premio final con un cierto paso de descuento en orden descendente para aplicar a todas las acciones del agente en este episodio. En otras palabras, cuanto m√°s cerca est√© la acci√≥n del agente de la final, m√°s cerca de +1 o -1 recompensa por esta acci√≥n.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Al introducir una recompensa con tal descuento para una acci√≥n en orden decreciente a medida que la acci√≥n se aleja del final del episodio, le permitimos al Agente comprender que las √∫ltimas acciones que realiz√≥ son m√°s importantes para el resultado del episodio del Juego que las acciones que realiz√≥ al principio. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Por lo general, la recompensa con descuento se calcula mediante la f√≥rmula: la recompensa final del episodio se multiplica por el coeficiente de descuento en la potencia del n√∫mero de paso menos uno para todas las acciones del agente en el episodio (por el momento en que cay√≥ el cubo). </font></font><br>
<br>
<img src="https://habrastorage.org/webt/4r/cf/w6/4rcfw67kmlniyfhczs_p2v2kb-c.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Gamma: coeficiente de descuento (disminuci√≥n de la recompensa). Siempre est√° en el rango de 0 a 1. Por lo general, la gama se toma en la regi√≥n de 0,95. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Despu√©s de decidir qu√© datos recopilamos en DataSet, ejecuta el simulador de entorno y juega un juego con varios episodios varias veces seguidas, recopila datos sobre:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">estado del medio ambiente, </font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> acciones tomadas por el agente </font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La recompensa que recibi√≥ el Agente.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para facilitar la comprensi√≥n, llamemos a la ca√≠da de un cubo a trav√©s del pozo: un episodio del juego, tambi√©n asumimos que el juego en s√≠ consistir√° en varios episodios. </font><font style="vertical-align: inherit;">Esto significa que en un juego tiraremos varios cubos a su vez en el pozo, y la raqueta intentar√° atraparlos. </font><font style="vertical-align: inherit;">Por cada dado capturado, el agente recibir√° +1 punto, por cada dado que cay√≥ al fondo y la raqueta no lo atrap√≥, el agente recibir√° -1 punto.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cap√≠tulo 7. La estructura interna del agente y el entorno.</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Entorno: dado que tenemos un entorno en el que el agente debe existir, consiste esencialmente en una matriz de pozo dentro de la cual los cubos caen a una velocidad de una l√≠nea por reloj, y el agente va en una direcci√≥n tambi√©n a una celda. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Escribiremos un simulador del entorno que puede dejar caer un cubo desde la l√≠nea superior en una columna arbitraria en un momento aleatorio, sabe c√≥mo recibir un comando de un agente para mover la raqueta una celda en una de las direcciones, despu√©s de lo cual verifica si el cubo ca√≠do fue atrapado por la raqueta o si cay√≥ fondo del pozo. Dependiendo de esto, el simulador devuelve al agente la recompensa que recibi√≥ por su acci√≥n.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Agente: el elemento principal es nuestra red neuronal, capaz de devolver las probabilidades de todas las acciones para un estado dado del entorno como el estado del entorno dedicado a √©l en la entrada. </font><font style="vertical-align: inherit;">A partir de la probabilidad de acciones recibidas de la red neuronal, el agente selecciona lo mejor, lo env√≠a al entorno, recibe comentarios del entorno en forma de una recompensa del entorno. </font><font style="vertical-align: inherit;">Adem√°s, el Agente debe tener un algoritmo interno basado en el cual podr√° aprender a maximizar la recompensa recibida del entorno.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cap√≠tulo 8. Entrenamiento de agentes</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para capacitar al Agente, necesitamos acumular estad√≠sticas de acuerdo con los datos de nuestro simulador y las acciones tomadas por el agente. </font><font style="vertical-align: inherit;">Recopilaremos datos estad√≠sticos para la capacitaci√≥n en triples de valores: el estado del medio ambiente, la acci√≥n del agente y la recompensa por esta acci√≥n.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/yo/qq/od/yoqqodgrfzatgri5dljps6uqnvw.png"><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Un poco de c√≥digo sobre c√≥mo recopilar estad√≠sticas</font></font></h4><br>
<img src="https://habrastorage.org/webt/ph/hr/6x/phhr6xb35udm3hyc3mr9rwv5yno.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ponemos cada uno de estos tres valores en un b√∫fer de memoria especial, donde los almacenamos todo el tiempo mientras simulamos el juego y acumulamos estad√≠sticas sobre √©l.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">C√≥digo para organizar un b√∫fer de memoria:</font></font></h4><br>
<img src="https://habrastorage.org/webt/u2/55/ty/u255ty2pzaoeagitk_osyxzxirg.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Despu√©s de haber realizado una serie de juegos de nuestro Agente con el mi√©rcoles y haber acumulado estad√≠sticas, podemos proceder a la capacitaci√≥n del Agente. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para hacer esto, obtenemos un lote de nuestros datos en forma de triples de valores del b√∫fer de memoria con estad√≠sticas acumuladas, desempaqu√©telo y convi√©rtalo en tensores Pytorch. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/jo/se/6t/jose6tkxmmtzsttgrnc-q0xkrf0.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 El lote de estado ambiental en forma de tensor de Pytorch se aplica a la entrada en la red neuronal del agente, obtenemos la distribuci√≥n de probabilidad para cada movimiento del agente para cada estado del entorno en el partido, logaritmo estas probabilidades, multiplicamos la recompensa recibida por el agente por movimiento por el logaritmo de estas probabilidades, luego tomamos el promedio sobre los productos y hacer que esto signifique negativo: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/hx/om/3i/hxom3ikgq9n13pb6brfzsfsbmkw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Etapa 1 </font></font><br>
<br>
<img src="https://habrastorage.org/webt/4j/gh/8l/4jgh8layvkcwg0dhrylxuwul2ri.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Etapa 2</font></font><br>
<br>
<img src="https://habrastorage.org/webt/pn/ac/pj/pnacpjuzovqe4-zql7lxlarerly.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Despu√©s de obtener el valor de la funci√≥n P√©rdida, hacemos un paso inverso a trav√©s de la red neuronal para obtener gradientes y hacer que el paso sea un optimizador para ajustar los pesos. </font><font style="vertical-align: inherit;">Sobre esto, se est√° descargando el ciclo de capacitaci√≥n de nuestro agente. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dado que despu√©s de que el optimizador cambi√≥ los pesos de la red neuronal, nuestros datos en las estad√≠sticas recopiladas ya no son relevantes, porque una red neuronal con pesos cambiados dar√° probabilidades completamente diferentes para acciones en las mismas condiciones ambientales y el entrenamiento del agente volver√° a fallar. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Por lo tanto, borramos nuestro b√∫fer con memoria, volvemos a perder una cierta cantidad de juegos para recopilar estad√≠sticas y reiniciamos el proceso de entrenamiento del agente. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Este es el ciclo de aprendizaje cuando se aprende sin un maestro utilizando el m√©todo de Gradiente de Pol√≠ticas.</font></font><br>
<br>
<ol>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Acumulaci√≥n de estad√≠sticas</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Entrenamiento de agente</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Reiniciar las estad√≠sticas</font></font></li>
</ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 Repetimos el proceso de aprendizaje tantas veces hasta que nuestro Agente aprenda a recibir del sistema la recompensa que m√°s nos convenga.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cap√≠tulo 9. Experimentos con Agente y Medio Ambiente</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Comencemos una serie de experimentos para entrenar a nuestro Agente. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para los experimentos, seleccionamos los siguientes par√°metros ambientales: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/xi/ym/2_/xiym2_kitmamvf52hhuff6tkpqs.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para el Agente, elegimos - construimos una red neuronal - ser√° convolucional (a medida que trabajamos con la imagen), tendr√° 9 salidas (1-derecha, 2-izquierda, 3-arriba, 4-abajo, en la salida) 5-derecha-arriba, 6-izquierda-arriba, 7-derecha-abajo, 8-izquierda-abajo, 9-no hacer nada) y SoftMax para obtener la probabilidad de cada acci√≥n.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Arquitectura de red neuronal</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Primer tama√±o de imagen de la neurona de la capa Conv2d 32 1 * 32 * 16 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Tama√±o de imagen de la capa MaxPool2d 32 * 16 * 8 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Segundo tama√±o de la imagen de la neurona capa 32 de Conv2d 32 * 16 * 8 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Tama√±o de la imagen de la capa MaxPool2d 32 * 8 * 4 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Flatten - enderece la imagen a un tama√±o de 1024 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Lineal capa para 1024 neuronas Capa de </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
abandono (0.25) Capa </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
lineal para 512 neuronas </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Capa lineal para 256 neuronas </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Capa lineal para 9 neuronas y SoftMax</font></font><br>
<br>
<img src="https://habrastorage.org/webt/2d/tz/ne/2dtzneqr2lsxdkz-yvu9xonh0uw.png"><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">C√≥digo de creaci√≥n de red neuronal Pytorch</font></font></h4><br>
<img src="https://habrastorage.org/webt/wq/mx/yw/wqmxyw67mbjtmnpnbj86zry8y4a.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Uno por uno, lanzaremos tres ciclos de capacitaci√≥n de Agentes en el Medio Ambiente, cuyos par√°metros se discutieron anteriormente:</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Experimento No. 1 - El agente aprendi√≥ a resolver el problema en 13,600 ciclos de juego.</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Estado inicial del Agente. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ws/gv/nb/wsgvnb34jwzwk--6iehpg3y3lqw.gif"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Programa </font><font style="vertical-align: inherit;">de </font><font style="vertical-align: inherit;">entrenamiento </font><font style="vertical-align: inherit;">del Agente. </font><font style="vertical-align: inherit;">El </font></font><br>
<br>
<img src="https://habrastorage.org/webt/jt/kd/3q/jtkd3qmdl7xixi-bwpkdp-tk0uy.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
estado </font><font style="vertical-align: inherit;">entrenado </font><font style="vertical-align: inherit;">del Agente.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/4a/v5/9l/4av59lvev4oget2gotgg-xsfy2w.gif"><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Experimento 2: el agente aprendi√≥ a resolver el problema en 8250 ciclos del juego.</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Estado inicial del Agente. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ve/jk/wz/vejkwz-fezjvbjcjydzmx7q4hk8.gif"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Programa </font><font style="vertical-align: inherit;">de </font><font style="vertical-align: inherit;">entrenamiento </font><font style="vertical-align: inherit;">del Agente. </font><font style="vertical-align: inherit;">El </font></font><br>
<br>
<img src="https://habrastorage.org/webt/9e/zc/1x/9ezc1x4gfv7bilgb8sx-r3iemgy.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
estado </font><font style="vertical-align: inherit;">entrenado </font><font style="vertical-align: inherit;">del Agente.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/l9/r7/bn/l9r7bnstfbaqszje4npotjrze4o.gif"><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Experimento No. 3: el agente aprendi√≥ a resolver el problema en 19800 ciclos de juego</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Estado inicial del Agente. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/hz/wm/ss/hzwmssjogsxwzv0wrik-1nb8loc.gif"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Programa </font><font style="vertical-align: inherit;">de </font><font style="vertical-align: inherit;">entrenamiento </font><font style="vertical-align: inherit;">del Agente. </font><font style="vertical-align: inherit;">El </font></font><br>
<br>
<img src="https://habrastorage.org/webt/sh/xv/zd/shxvzdpumy7fnoctkb6bqnhie2o.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
estado </font><font style="vertical-align: inherit;">entrenado </font><font style="vertical-align: inherit;">del Agente.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/2h/ga/yh/2hgayhvtgypaq1fvu-i54d4l4ok.gif"><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cap√≠tulo 10. Conclusiones</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Mirando los cuadros, se puede decir que la capacitaci√≥n del Agente es, por supuesto, lenta. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El agente inicialmente ha estado buscando al menos alguna pol√≠tica razonable para sus acciones durante mucho tiempo para comenzar a recibir una recompensa positiva. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En este momento, en la primera etapa del cronograma, la recompensa por el juego crece lentamente, luego, de repente, el Agente encuentra una buena opci√≥n para sus movimientos, y la recompensa recibida por √©l para el juego aumenta abruptamente y sube, y luego, acerc√°ndose a la recompensa m√°xima, el agente vuelve a ir. un aumento lento en la eficiencia cuando mejora su pol√≠tica de movimientos que ya aprendi√≥, pero se esfuerza, como cualquier algoritmo codicioso, por tomar su recompensa por completo.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Tambi√©n me gustar√≠a se√±alar la gran necesidad de c√°lculos para capacitar al Agente utilizando el m√©todo de Gradiente de Pol√≠tica, porque El momento principal en que funciona el algoritmo es la recopilaci√≥n de estad√≠sticas sobre los movimientos del Agente y no su entrenamiento. Despu√©s de haber recopilado estad√≠sticas sobre los movimientos de toda la matriz, usamos solo un lote de datos para entrenar al Agente, y descartamos todos los dem√°s datos que ya no son adecuados para el entrenamiento. Y nuevamente recopilamos nuevos datos. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Todav√≠a puede experimentar mucho con este algoritmo y entorno: cambiando la profundidad y el ancho del pozo, aumentando o disminuyendo el n√∫mero de dados que caen durante el juego, haciendo estos dados de diferentes colores. Observar lo que este efecto tendr√° sobre la efectividad y la velocidad de entrenamiento del Agente.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Adem√°s, un campo extenso para los experimentos son los par√°metros de la red neuronal, en esencia de los cuales estamos entrenando a nuestro Agente, puede cambiar capas, n√∫cleos de convoluci√≥n, habilitar y ajustar la regularizaci√≥n. </font><font style="vertical-align: inherit;">S√≠, y se puede intentar mucho m√°s para aumentar la efectividad de la capacitaci√≥n del Agente. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Por lo tanto, despu√©s de haber lanzado experimentos pr√°cticos con la ense√±anza sin un maestro utilizando el m√©todo de Gradiente de Pol√≠ticas, est√°bamos convencidos de que ense√±ar sin un maestro tiene un lugar para estar, y realmente funciona. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El agente se entren√≥ independientemente para maximizar su recompensa en el juego. </font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Enlace a GitHub con c√≥digo adaptado para trabajar en una computadora port√°til Google Colab</font></font></a></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es506358/index.html">3 opciones para el comportamiento de la corte cuando reciben casos de bloqueos fiscales</a></li>
<li><a href="../es506362/index.html">C√≥mo hicimos un baile en l√≠nea</a></li>
<li><a href="../es506370/index.html">Compare el rendimiento de la restricci√≥n de verificaci√≥n y la clave externa en SQL Server</a></li>
<li><a href="../es506372/index.html">El instinto de reproducci√≥n se encuentra en la inteligencia artificial.</a></li>
<li><a href="../es506380/index.html">Software gratis o dom√©stico. Matr√≠cula est√°ndar o gratuita</a></li>
<li><a href="../es506386/index.html">C√≥mo abandonar la escuela y transferir a un ni√±o a un lugar remoto</a></li>
<li><a href="../es506392/index.html">Notificaci√≥n de Roskomnadzor sobre el procesamiento de datos personales en 2020</a></li>
<li><a href="../es506394/index.html">Impresora 3D Anycubic Mega X: una gran impresora a un precio modesto</a></li>
<li><a href="../es506396/index.html">Elon Musk: ‚ÄúLidar es una p√©rdida de tiempo. Todos los que conf√≠an en lidar est√°n condenados "</a></li>
<li><a href="../es506398/index.html">Joel Spolsky: "No solo usabilidad"</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>