<!doctype html>
<html class="no-js" lang="pt">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üö≠ üí® üè† Rede neural - treinamento sem professor. M√©todo do Gradiente de Pol√≠tica üôÉ üë¥üèº üë®üèª‚Äç‚úàÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bom dia, Habr
 Este artigo abre uma s√©rie de artigos sobre como treinar redes neurais sem um professor. 
 (Aprendizado por refor√ßo para redes de neur√¥...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Rede neural - treinamento sem professor. M√©todo do Gradiente de Pol√≠tica</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/506384/"><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bom dia, Habr</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Este artigo abre uma s√©rie de artigos sobre como treinar redes neurais sem um professor. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 (Aprendizado por refor√ßo para redes de neur√¥nios) </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
No ciclo, planejo elaborar tr√™s artigos sobre a teoria e a implementa√ß√£o no c√≥digo de tr√™s algoritmos de treinamento para redes neurais sem professor. </font><font style="vertical-align: inherit;">O primeiro artigo ser√° sobre Policy Gradient, o segundo sobre Q-learning, o terceiro artigo ser√° final, de acordo com o m√©todo Ator-Critic. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Gostar de ler.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Artigo Um - Aprendizado com Gradiente de Pol√≠tica Sem Professor </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 (Gradiente de Pol√≠tica para Aprendizado por Refor√ßo)</font></font></h4><br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Introdu√ß√£o</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Entre os algoritmos de aprendizado de m√°quina, um lugar especial √© ocupado por algoritmos de aprendizado de m√°quina, onde o algoritmo aprende a resolver o problema sozinho, sem interven√ß√£o humana, interagindo diretamente com o ambiente em que ele aprende. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Esses algoritmos receberam um nome comum - algoritmos de aprendizado sem professor, para esses algoritmos, voc√™ n√£o precisa coletar bancos de dados, n√£o precisa classific√°-los ou marc√°-los. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Basta que o aluno sem o professor d√™ apenas uma resposta √†s suas a√ß√µes ou decis√µes - elas foram boas ou n√£o.</font></font><br>
<a name="habracut"></a><br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cap√≠tulo 1. Forma√ß√£o de Professores</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ent√£o, o que √© isso - aprender com ou sem um professor. Examinaremos mais detalhadamente isso com exemplos do aprendizado de m√°quina moderno e das tarefas que ele resolve. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A maioria dos algoritmos modernos de aprendizado de m√°quina para problemas de classifica√ß√£o, regress√£o e segmenta√ß√£o s√£o essencialmente algoritmos de treinamento com um professor em que a pr√≥pria pessoa √© o professor. Como √© a pessoa que est√° marcando os dados que informa ao algoritmo qual deve ser a resposta correta e, assim, o algoritmo tenta encontrar uma solu√ß√£o para que a resposta que o algoritmo fornece ao resolver o problema corresponda √† resposta que a pessoa indicou para a tarefa especificada como a resposta correta. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Usando o exemplo do problema de classifica√ß√£o para o conjunto de dados Mnist, a resposta correta que a pessoa fornece ao algoritmo √© o r√≥tulo da classe do d√≠gito no conjunto de treinamento.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/za/kq/b6/zakqb6ob1_xce07caywnurtsl7k.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
No conjunto de dados Mnist, para cada imagem que o algoritmo da m√°quina precisa aprender a classificar, as pessoas pr√©-configuram os r√≥tulos corretos √† qual classe essa imagem pertence. No processo de aprendizagem, o algoritmo que prev√™ a classe de imagem compara sua classe recebida para uma imagem espec√≠fica com a classe verdadeira para a mesma imagem e ajusta gradualmente seus par√¢metros no processo de aprendizagem para que a classe prevista pelo algoritmo tenda a corresponder √† classe especificada pela pessoa. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Assim, a id√©ia a seguir pode ser generalizada - o algoritmo de aprendizado com o professor √© qualquer algoritmo de aprendizado de m√°quina, onde damos ao algoritmo como ele precisa ser feito em nossa opini√£o.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
E n√£o importa como proceder - indique a que classe essa imagem deve ser atribu√≠da se for uma tarefa de classifica√ß√£o ou desenhe os contornos de um objeto se for uma tarefa de segmenta√ß√£o ou em qual dire√ß√£o girar o volante do carro se o algoritmo for piloto autom√°tico, √© importante que, para cada situa√ß√£o espec√≠fica, Indicamos explicitamente ao algoritmo onde est√° a resposta correta, como faz√™-lo corretamente. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Essa √© a chave para entender como fundamentalmente o algoritmo de aprendizado com um professor difere do algoritmo de aprendizado sem um professor.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cap√≠tulo 2. Aprendendo sem um Professor</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Tendo descoberto o que √© - ensinar com um professor, agora vamos entender o que √© - ensinar sem um professor. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Como descobrimos no cap√≠tulo anterior, ao ensinar com um professor, para cada situa√ß√£o de ensino, damos ao algoritmo um entendimento de qual resposta √© correta do nosso ponto de vista e depois partimos do oposto - aprendendo sem professor, para cada situa√ß√£o espec√≠fica que n√£o damos essa resposta ao algoritmo ser√°. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Mas ent√£o surge a pergunta: se n√£o dermos ao algoritmo instru√ß√µes expl√≠citas sobre como proceder corretamente, o que o algoritmo aprender√°? Como o algoritmo ser√° treinado sem saber onde ajustar seus par√¢metros internos para fazer a coisa certa e, finalmente, resolver o problema como gostar√≠amos.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Vamos pensar sobre este t√≥pico. Mas √© importante para n√≥s que o algoritmo resolva o problema como um todo, e como exatamente ele ir√° agir no processo de solu√ß√£o desse problema e para que lado ele ir√° resolv√™-lo n√£o nos interessa, vamos entreg√°-lo ao pr√≥prio algoritmo, esperamos apenas o resultado final dele . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Portanto, deixaremos o algoritmo entender o resultado final, resolvendo nosso problema bem ou n√£o. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Assim, resumindo tudo isso, chegamos √† conclus√£o de que chamamos esses algoritmos de aprendizagem sem um professor, onde n√£o h√° indica√ß√£o expl√≠cita para o algoritmo de como faz√™-lo, mas h√° apenas uma avalia√ß√£o geral de todas as suas a√ß√µes no processo de resolu√ß√£o do problema.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/qi/ci/nn/qicinnqzow8o5b7lacir_mrs4aa.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
No exemplo de um jogo em que uma raquete tenta pegar os cubos que caem de cima, n√£o dizemos ao algoritmo para controlar a raquete em que momento espec√≠fico para onde mover a raquete. </font><font style="vertical-align: inherit;">Vamos contar ao algoritmo apenas o resultado de suas a√ß√µes - ele pegou um cubo com uma raquete ou n√£o. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/-f/dp/df/-fdpdf5nbjpawt1mutegmxzw9iu.gif"><br>
<br>
<img src="https://habrastorage.org/webt/ve/jk/wz/vejkwz-fezjvbjcjydzmx7q4hk8.gif"><br>
<br>
<img src="https://habrastorage.org/webt/ws/gv/nb/wsgvnb34jwzwk--6iehpg3y3lqw.gif"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Essa √© a ess√™ncia do aprendizado sem um professor. </font><font style="vertical-align: inherit;">O pr√≥prio algoritmo deve aprender a decidir o que fazer em cada caso espec√≠fico, com base na avalia√ß√£o final da totalidade de todas as suas a√ß√µes.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cap√≠tulo 3. Agente, Ambiente e Recompensa</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Depois de descobrir o que √© o treinamento sem um professor, vamos nos aprofundar nos algoritmos que podem aprender a resolver um problema sem nossas dicas sobre como faz√™-lo corretamente. </font></font><br>
 <br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
√â hora de nos apresentar a terminologia que usaremos no futuro. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Vamos chamar um agente de nosso algoritmo que pode analisar o estado do ambiente e executar algumas a√ß√µes nele. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O ambiente √© o mundo virtual em que nosso agente existe e, por meio de suas a√ß√µes, pode mudar de estado ... </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Recompensa - feedback do ambiente para o agente como resposta √†s suas a√ß√µes.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/23/dp/gw/23dpgwboqw80sv_1uj5_ubifms8.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O ambiente em que nosso agente vive pode ser arbitrariamente complexo, o agente pode nem saber como est√° estruturado para tomar suas decis√µes e executar a√ß√µes. </font><font style="vertical-align: inherit;">Para o agente, apenas o feedback na forma de uma recompensa que ele recebe do ambiente √© importante. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Se considerarmos mais detalhadamente o processo de intera√ß√£o entre o agente e o ambiente, ele pode ser expresso pelo seguinte esquema </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ml/f5/eq/mlf5eqvivcvcwodu66mfyddfpdo.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
St - estado do ambiente na etapa t </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
na a√ß√£o do agente na etapa t </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
rt - recompensa na etapa t</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A cada momento t, nosso agente observa o estado do meio - St, executa a a√ß√£o - em, pela qual recebe uma recompensa - rt do meio, cujo campo o meio vai para o estado St + 1, que nosso agente observa, executa a a√ß√£o - em + 1, por que recebe uma recompensa do meio - rt + 1 e tais estados t, podemos ter um conjunto infinito - n.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cap√≠tulo 4. Parametriza√ß√£o da tarefa de aprender sem professor</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para treinar um agente, precisamos, de alguma forma, parametrizar a tarefa de aprender sem um professor, ou seja, para entender quais fun√ß√µes vamos otimizar. </font><font style="vertical-align: inherit;">No aprendizado por refor√ßo - a seguir, chamaremos treinamento sem professor, existem tr√™s fun√ß√µes b√°sicas: </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
1) p (a | s) - fun√ß√£o de pol√≠tica </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A fun√ß√£o de probabilidade da otimiza√ß√£o de uma a√ß√£o √© a, dependendo do estado do ambiente -s. </font><font style="vertical-align: inherit;">Ele nos mostra como a a√ß√£o de a √© ideal sob o estado do meio s. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
2) v (s) - fun√ß√£o de valor </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A </font><font style="vertical-align: inherit;">fun√ß√£o de valor de </font><font style="vertical-align: inherit;">estado √© s. </font><font style="vertical-align: inherit;">Mostra-nos quanto o estado s √© geralmente valioso para n√≥s em termos de recompensas </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
3) Q (s, a) - fun√ß√£o Q</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Q √© uma fun√ß√£o estrat√©gica ideal. Ele nos permite, de acordo com essa estrat√©gia √≥tima, nos estados - escolher a a√ß√£o ideal para esse estado - a </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Primeiro, consideramos a fun√ß√£o - fun√ß√£o pol√≠tica, como a mais simples e intuitiva para entender a fun√ß√£o de aprendizado por refor√ßo. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Como vamos resolver os problemas do aprendizado por refor√ßo atrav√©s de redes neurais. Em seguida, esquematicamente, podemos parametrizar uma fun√ß√£o de pol√≠tica por meio de uma rede neural da seguinte maneira.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/oa/fv/1x/oafv1xdki7zeejvxvy-tsgd5oeu.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Forneceremos estados - s √† entrada da rede neural e construiremos a sa√≠da da rede neural de forma que a camada de sa√≠da da rede neural seja a camada SoftMax, com o n√∫mero de sa√≠das igual ao n√∫mero de a√ß√µes poss√≠veis para o agente em nosso ambiente. </font><font style="vertical-align: inherit;">Assim, passando os estados s na sa√≠da pelas camadas da rede neural, obtemos a distribui√ß√£o de probabilidade para as a√ß√µes do agente nos estados s. </font><font style="vertical-align: inherit;">O que, de fato, √© o que precisamos iniciar com o algoritmo de erro de retropropaga√ß√£o para treinar nossa rede neural e melhorar iterativamente a fun√ß√£o de pol√≠tica, que agora √© essencialmente nossa rede neural.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cap√≠tulo 5. Melhorando a Fun√ß√£o Pol√≠tica Atrav√©s do Treinamento em Redes Neurais</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para treinar a rede neural, usamos o m√©todo de descida de gradiente. Como a √∫ltima camada de nossa rede neural √© a camada SoftMax, sua fun√ß√£o Loss √©: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/29/y9/u2/29y9u2wizl5w2yf5yeyw08nk1ic.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
where: </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
1 - r√≥tulos verdadeiros * log (r√≥tulos previstos) </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
2 - a soma de todos os exemplos </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
No entanto, como podemos treinar uma rede neural se ainda n√£o temos os r√≥tulos corretos para a√ß√µes de agentes nos estados S0-Sj? E n√£o precisamos deles, em vez dos r√≥tulos corretos, usaremos a recompensa que o agente recebeu do meio executando a a√ß√£o que a rede neural previu para ele.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/46/jp/or/46jporp-h9pj-bujcmytwhnfmze.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Temos todo o direito de fazer isso porque, para Perda de entropia cruzada, yj s√£o r√≥tulos verdadeiros da classe correta e s√£o iguais a um, e para Perda de fun√ß√£o de pol√≠tica, rj √© a recompensa que o ambiente acumulou no agente pela a√ß√£o que ele executou. </font><font style="vertical-align: inherit;">Ou seja, rj serve como um peso para os gradientes na propaga√ß√£o posterior do erro quando treinamos a rede neural. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Recebeu uma recompensa positiva - significa que voc√™ precisa aumentar o peso da rede neural para onde o gradiente √© direcionado. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/2e/ge/bq/2egebqgdxjutx0tzxvwawunwffq.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Se a recompensa for negativa, reduziremos os pesos correspondentes na rede neural, de acordo com a dire√ß√£o do gradiente para onde o erro √© direcionado.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cap√≠tulo 6. Construindo um conjunto de dados para treinamento</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para treinar nosso Agente - a rede neural usando o cl√°ssico Machine Learning - atrav√©s do m√©todo de propaga√ß√£o reversa de erros, precisamos montar um conjunto de dados. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A partir da declara√ß√£o do problema, fica claro que, na entrada da rede neural, queremos enviar o estado do meio S - a imagem do po√ßo com cubos caindo e uma raquete que os captura. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/4h/pv/hr/4hpvhry6ju1kcckfjj4vxu_7eps.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
e Y - que coletaremos de acordo com o estado S, essa ser√° a a√ß√£o prevista da rede neural - e a recompensa que o ambiente acumulou para o agente por essa a√ß√£o - r</font></font><br>
<br>
<img src="https://habrastorage.org/webt/an/gh/wo/anghwor5xj9mqdbgnnjrjgvzbta.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Mas, afinal, quarta-feira, durante o decorrer do jogo, o agente n√£o pode recompensar cada a√ß√£o. Por exemplo, no nosso caso, o agente receber√° uma recompensa positiva somente quando a raquete pegar um dado em queda. Se a raquete n√£o pegar o cubo e cair no fundo, quarta-feira cobrar√° ao agente uma recompensa negativa. O resto do tempo, independentemente de como o agente move a raquete, at√© o cubo atingir a raquete ou cair no fundo, quarta-feira cobrar√° ao agente uma recompensa igual a zero.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/7f/rh/5u/7frh5uda0syl7dbcutr8ez8fcq8.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Como pode ser visto na descri√ß√£o do processo do nosso jogo, uma recompensa positiva ou negativa em um agente √© extremamente rara, mas basicamente uma recompensa, independentemente de suas a√ß√µes, √© zero. Como treinar o Code Agent na maioria das vezes, ele n√£o recebe uma resposta do ambiente para suas a√ß√µes. Do ponto de vista de que nosso agente √© uma rede neural e a recompensa do ambiente √© zero, os gradientes da propaga√ß√£o reversa do erro atrav√©s da rede neural na maioria dos casos no conjunto de dados ser√£o zero, os pesos da rede neural n√£o ter√£o onde mudar, o que significa que nosso agente n√£o aprender√° nada. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Como resolver o problema com recompensa zero na maior parte do conjunto de dados que treinar√° o agente? </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Existem duas maneiras de sair dessa situa√ß√£o:</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
a primeira √© atribuir todas as a√ß√µes do agente que ele executou durante o tempo em que o cubo caiu uma e a mesma recompensa final do epis√≥dio +1 ou -1, dependendo se o agente pegou o cubo ou n√£o. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Assim, consideraremos todas as a√ß√µes do agente se ele pegou o cubo correto e consolidar√° esse comportamento do agente durante o treinamento, atribuindo-lhes uma recompensa positiva. Se o agente n√£o capturar o cubo, atribuiremos uma recompensa negativa a todas as a√ß√µes do agente no epis√≥dio e o treinaremos para evitar essa sequ√™ncia de a√ß√µes no futuro. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
o segundo - o pr√™mio final com uma determinada etapa de desconto em ordem decrescente a ser aplicada a todas as a√ß√µes do agente neste epis√≥dio. Em outras palavras, quanto mais pr√≥xima a a√ß√£o do agente da final, mais pr√≥xima de +1 ou -1 de recompensa por essa a√ß√£o.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ao introduzir uma recompensa com desconto por uma a√ß√£o em ordem decrescente √† medida que a a√ß√£o se afasta do final do epis√≥dio, deixamos o Agente entender que as √∫ltimas a√ß√µes que ele realizou s√£o mais importantes para o resultado do epis√≥dio do Jogo do que as a√ß√µes que ele realizou no in√≠cio. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Geralmente, a recompensa com desconto √© calculada pela f√≥rmula - A recompensa final do epis√≥dio √© multiplicada pelo coeficiente de desconto no poder do n√∫mero da etapa menos um para todas as a√ß√µes do agente no epis√≥dio (durante o tempo em que o cubo caiu). </font></font><br>
<br>
<img src="https://habrastorage.org/webt/4r/cf/w6/4rcfw67kmlniyfhczs_p2v2kb-c.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Gamma - coeficiente de desconto (diminui√ß√£o da recompensa). Ele est√° sempre no intervalo de 0 a 1. Normalmente, a gama √© calculada na regi√£o de 0,95. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ap√≥s decidirmos quais dados coletamos no DataSet, execute o simulador de ambiente e jogue um jogo com v√°rios epis√≥dios v√°rias vezes seguidas, colete dados sobre:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">estado do ambiente, </font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> a√ß√µes executadas pelo agente </font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">a recompensa que o agente recebeu.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para facilitar a compreens√£o, vamos chamar a queda de um cubo atrav√©s do po√ßo - um epis√≥dio do jogo, tamb√©m assumimos que o jogo em si consistir√° em v√°rios epis√≥dios. </font><font style="vertical-align: inherit;">Isso significa que, em um jogo, jogaremos v√°rios dados no po√ßo e a raquete tentar√° peg√°-los. </font><font style="vertical-align: inherit;">Para cada dado capturado, o agente receber√° +1 ponto, para cada dado que caiu no fundo e a raquete n√£o o pegou, o agente receber√° -1 ponto.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cap√≠tulo 7. A Estrutura Interna do Agente e Ambiente</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ambiente - como temos um ambiente no qual o agente deve existir, ele consiste essencialmente de uma matriz de um po√ßo dentro do qual os cubos caem a uma velocidade de uma linha por rel√≥gio, e o agente segue em uma dire√ß√£o tamb√©m uma c√©lula. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Escreveremos um simulador do ambiente que pode soltar um cubo da linha superior em uma coluna arbitr√°ria em um momento aleat√≥rio, sabe como receber um comando de um agente para mover a raquete uma c√©lula em uma das dire√ß√µes, ap√≥s o que verifica se o cubo em queda foi capturado pela raquete ou se caiu. fundo do po√ßo. Dependendo disso, o simulador retorna ao agente a recompensa que ele recebeu por sua a√ß√£o.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Agente - cujo principal elemento √© a nossa rede neural, capaz de retornar as probabilidades de todas as a√ß√µes para um dado estado do ambiente como o estado do ambiente dedicado a ele na entrada. </font><font style="vertical-align: inherit;">Pela probabilidade de a√ß√µes recebidas da rede neural, o agente seleciona o melhor, envia para o ambiente, recebe feedback do ambiente na forma de uma recompensa do ambiente. </font><font style="vertical-align: inherit;">Al√©m disso, o agente deve ter um algoritmo interno com base no qual ele poder√° aprender a maximizar a recompensa recebida do ambiente.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cap√≠tulo 8. Treinamento de Agente</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para treinar o Agente, precisamos acumular estat√≠sticas de acordo com os dados do nosso simulador e as a√ß√µes executadas pelo agente. </font><font style="vertical-align: inherit;">Coletaremos dados estat√≠sticos para treinamento em triplos valores - o estado do ambiente, a a√ß√£o do agente e a recompensa por essa a√ß√£o.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/yo/qq/od/yoqqodgrfzatgri5dljps6uqnvw.png"><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Um pouco de c√≥digo sobre como coletar estat√≠sticas</font></font></h4><br>
<img src="https://habrastorage.org/webt/ph/hr/6x/phhr6xb35udm3hyc3mr9rwv5yno.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Colocamos cada um desses tr√™s valores em um buffer de mem√≥ria especial, onde os armazenamos o tempo todo enquanto simulamos o jogo e acumulamos estat√≠sticas sobre ele.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">C√≥digo para organizar um buffer de mem√≥ria:</font></font></h4><br>
<img src="https://habrastorage.org/webt/u2/55/ty/u255ty2pzaoeagitk_osyxzxirg.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Tendo conduzido uma s√©rie de jogos do nosso Agente na quarta-feira e tendo acumulado estat√≠sticas, podemos prosseguir com o treinamento do Agente. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para fazer isso, obtemos um lote de nossos dados na forma de triplos de valores do buffer de mem√≥ria com estat√≠sticas acumuladas, descompacte-o e converta-o em tensores Pytorch. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/jo/se/6t/jose6tkxmmtzsttgrnc-q0xkrf0.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 O lote do estado ambiental na forma do tensor Pytorch √© aplicado √† entrada na rede neural do agente, obtemos a distribui√ß√£o de probabilidade para cada movimento do agente para cada estado do ambiente na partida, logaritmo essas probabilidades, multiplicamos a recompensa recebida pelo agente por movimento pelo logaritmo dessas probabilidades e, em seguida, medimos a m√©dia dos produtos e tornar isso negativo: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/hx/om/3i/hxom3ikgq9n13pb6brfzsfsbmkw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Fase 1 </font></font><br>
<br>
<img src="https://habrastorage.org/webt/4j/gh/8l/4jgh8layvkcwg0dhrylxuwul2ri.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Fase 2</font></font><br>
<br>
<img src="https://habrastorage.org/webt/pn/ac/pj/pnacpjuzovqe4-zql7lxlarerly.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Depois de obtermos o valor da fun√ß√£o Perda, fazemos uma passagem reversa pela rede neural para obter gradientes e fazemos da etapa um otimizador para ajustar os pesos. </font><font style="vertical-align: inherit;">Nisso, o ciclo de treinamento do nosso agente est√° sendo baixado. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Como depois que o otimizador alterou os pesos da rede neural, nossos dados nas estat√≠sticas coletadas n√£o s√£o mais relevantes, porque uma rede neural com pesos alterados fornecer√° probabilidades completamente diferentes para a√ß√µes nas mesmas condi√ß√µes ambientais e o treinamento de agentes voltar√° a dar errado. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Portanto, limpamos nosso buffer com mem√≥ria, perdemos novamente um certo n√∫mero de jogos para coletar estat√≠sticas e reiniciamos o processo de treinamento do agente. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Esse √© o ciclo de aprendizado ao aprender sem um professor usando o m√©todo Gradiente de pol√≠tica.</font></font><br>
<br>
<ol>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Acumula√ß√£o de estat√≠sticas</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Treinamento de agentes</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Limpar estat√≠sticas</font></font></li>
</ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 Repetimos o processo de aprendizado v√°rias vezes at√© que nosso agente aprenda a receber a recompensa que mais nos conv√©m do sistema.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cap√≠tulo 9. Experi√™ncias com Agente e Ambiente</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Vamos come√ßar uma s√©rie de experimentos para treinar nosso agente. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para experimentos, selecionamos os seguintes par√¢metros ambientais: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/xi/ym/2_/xiym2_kitmamvf52hhuff6tkpqs.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para o Agente, escolhemos - constru√≠mos uma rede neural - ser√° convolucional (conforme trabalhamos com a imagem), ter√° 9 sa√≠das (1 √† direita, 2 √† esquerda, 3 para cima, 4 para baixo, na sa√≠da) 5-direita-direita, 6-esquerda-esquerda, 7-direita-baixo, 8-esquerda-baixo, 9-nada) e SoftMax para obter a probabilidade de cada a√ß√£o.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Arquitetura de rede neural</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Primeira imagem do neur√¥nio da camada Conv2d 32 tamanho 1 * 32 * 16 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Tamanho da imagem da camada MaxPool2d 32 * 16 * 8 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Segunda imagem do neur√¥nio da camada Conv2d 32 tamanho da imagem 32 * 16 * 8 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Tamanho de imagem da camada Max2Pool2d 32 * 8 * 4 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Achatar - endireitar a imagem para um tamanho de 1024 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Linear camada para 1024 neur√¥nios camada </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dropout (0,25) camada </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
linear para 512 neur√¥nios </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
camada linear para 256 neur√¥nios </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
camada linear para 9 neur√¥nios e SoftMax</font></font><br>
<br>
<img src="https://habrastorage.org/webt/2d/tz/ne/2dtzneqr2lsxdkz-yvu9xonh0uw.png"><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">C√≥digo de cria√ß√£o de rede neural Pytorch</font></font></h4><br>
<img src="https://habrastorage.org/webt/wq/mx/yw/wqmxyw67mbjtmnpnbj86zry8y4a.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Um por um, lan√ßaremos tr√™s ciclos de treinamento de agentes no ambiente, cujos par√¢metros foram discutidos acima:</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Experi√™ncia n¬∫ 1 - O agente aprendeu a resolver o problema em 13.600 ciclos de jogo</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Estado inicial do agente </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ws/gv/nb/wsgvnb34jwzwk--6iehpg3y3lqw.gif"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Programa </font><font style="vertical-align: inherit;">de </font><font style="vertical-align: inherit;">treinamento </font><font style="vertical-align: inherit;">do agente </font><font style="vertical-align: inherit;">O </font></font><br>
<br>
<img src="https://habrastorage.org/webt/jt/kd/3q/jtkd3qmdl7xixi-bwpkdp-tk0uy.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
estado </font><font style="vertical-align: inherit;">treinado </font><font style="vertical-align: inherit;">do agente.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/4a/v5/9l/4av59lvev4oget2gotgg-xsfy2w.gif"><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Experi√™ncia n¬∫ 2 - O agente aprendeu a resolver o problema em 8250 ciclos do jogo</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Estado inicial do agente </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ve/jk/wz/vejkwz-fezjvbjcjydzmx7q4hk8.gif"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Programa </font><font style="vertical-align: inherit;">de </font><font style="vertical-align: inherit;">treinamento </font><font style="vertical-align: inherit;">do agente </font><font style="vertical-align: inherit;">O </font></font><br>
<br>
<img src="https://habrastorage.org/webt/9e/zc/1x/9ezc1x4gfv7bilgb8sx-r3iemgy.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
estado </font><font style="vertical-align: inherit;">treinado </font><font style="vertical-align: inherit;">do agente.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/l9/r7/bn/l9r7bnstfbaqszje4npotjrze4o.gif"><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Experi√™ncia n¬∫ 3 - O agente aprendeu a resolver o problema em 19800 ciclos de jogo</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Estado inicial do agente </font></font><br>
<br>
<img src="https://habrastorage.org/webt/hz/wm/ss/hzwmssjogsxwzv0wrik-1nb8loc.gif"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Programa </font><font style="vertical-align: inherit;">de </font><font style="vertical-align: inherit;">treinamento </font><font style="vertical-align: inherit;">do agente </font><font style="vertical-align: inherit;">O </font></font><br>
<br>
<img src="https://habrastorage.org/webt/sh/xv/zd/shxvzdpumy7fnoctkb6bqnhie2o.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
estado </font><font style="vertical-align: inherit;">treinado </font><font style="vertical-align: inherit;">do agente.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/2h/ga/yh/2hgayhvtgypaq1fvu-i54d4l4ok.gif"><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cap√≠tulo 10. Conclus√µes</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Observando os gr√°ficos, pode-se dizer que o treinamento do Agente √© obviamente lento. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O agente inicialmente procura h√° pelo menos alguma pol√≠tica razo√°vel para suas a√ß√µes por um longo tempo para come√ßar a receber uma recompensa positiva. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nesse momento, no primeiro est√°gio do cronograma, a recompensa pelo jogo cresce lentamente; ent√£o, de repente, o Agente encontra uma boa op√ß√£o para seus movimentos, e a recompensa recebida por ele pelo jogo aumenta acentuadamente e sobe, e, aproximando-se da recompensa m√°xima, o agente volta novamente. um lento aumento de efici√™ncia quando ele aprimora sua pol√≠tica de movimentos que ele j√° aprendeu, mas se esfor√ßando, como qualquer algoritmo ganancioso, para receber sua recompensa completamente.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Tamb√©m gostaria de observar a grande necessidade de c√°lculos para treinar o agente usando o m√©todo Gradiente de pol√≠tica, porque O principal momento em que o algoritmo funciona √© a coleta de estat√≠sticas sobre os movimentos do agente e n√£o seu treinamento. Depois de coletar estat√≠sticas sobre movimentos de toda a matriz, usamos apenas um lote de dados para treinar o Agente e descartamos todos os outros dados como j√° inadequados para o treinamento. E, novamente, coletamos novos dados. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Voc√™ ainda pode experimentar muito com esse algoritmo e ambiente - alterando a profundidade e a largura do po√ßo, aumentando ou diminuindo o n√∫mero de dados que caem durante o jogo, produzindo dados de cores diferentes. Observar o que esse efeito ter√° sobre a efic√°cia e a velocidade do treinamento do Agente.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Al√©m disso, um campo extenso para experimentos s√£o os par√¢metros da rede neural, em ess√™ncia dos quais estamos treinando nosso Agente, voc√™ pode alterar camadas, n√∫cleos de convolu√ß√£o, ativar e ajustar a regulariza√ß√£o. </font><font style="vertical-align: inherit;">Sim, e muito mais voc√™ pode tentar aumentar a efic√°cia do treinamento do agente. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Assim, tendo lan√ßado experimentos pr√°ticos com o ensino sem professor, usando o m√©todo Gradiente de Pol√≠tica, est√°vamos convencidos de que o ensino sem professor tem um lugar para estar, e realmente funciona. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O agente treinou independentemente para maximizar sua recompensa no jogo. </font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Link para GitHub com c√≥digo adaptado para trabalhar em um laptop Google Colab</font></font></a></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt506358/index.html">3 op√ß√µes de comportamento no tribunal quando recebem casos de bloqueios do Minist√©rio P√∫blico</a></li>
<li><a href="../pt506362/index.html">Como fizemos uma bola de dan√ßa online</a></li>
<li><a href="../pt506370/index.html">Compare o desempenho da verifica√ß√£o de restri√ß√£o e chave estrangeira no SQL Server</a></li>
<li><a href="../pt506372/index.html">O instinto de reprodu√ß√£o √© encontrado na intelig√™ncia artificial</a></li>
<li><a href="../pt506380/index.html">Software gratuito ou dom√©stico. Aulas padr√£o ou gratuitas</a></li>
<li><a href="../pt506386/index.html">Como abandonar a escola e transferir uma crian√ßa para um local remoto</a></li>
<li><a href="../pt506392/index.html">Notifica√ß√£o de Roskomnadzor sobre o tratamento de dados pessoais em 2020</a></li>
<li><a href="../pt506394/index.html">Impressora Anycubic Mega X 3D: uma √≥tima impressora a um pre√ßo modesto</a></li>
<li><a href="../pt506396/index.html">Elon Musk: ‚ÄúLidar √© uma perda de tempo. Todos que confiam no lidar est√£o condenados ‚Äù</a></li>
<li><a href="../pt506398/index.html">Joel Spolsky: ‚ÄúN√£o est√° sozinho na usabilidade‚Äù</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>