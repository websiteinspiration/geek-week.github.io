<!doctype html>
<html class="no-js" lang="es">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üèñÔ∏è ‚ÑπÔ∏è ü§í Bosque aleatorio, el m√©todo de componentes principales y optimizaci√≥n de hiperpar√°metros: un ejemplo de resoluci√≥n del problema de clasificaci√≥n en Python üê≥ üëèüèª üë©‚Äçüë©‚Äçüëß</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Los especialistas en an√°lisis y procesamiento de datos tienen muchas herramientas para crear modelos de clasificaci√≥n. Uno de los m√©todos m√°s populare...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Bosque aleatorio, el m√©todo de componentes principales y optimizaci√≥n de hiperpar√°metros: un ejemplo de resoluci√≥n del problema de clasificaci√≥n en Python</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ruvds/blog/488342/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Los especialistas en an√°lisis y procesamiento de datos tienen muchas herramientas para crear modelos de clasificaci√≥n. </font><font style="vertical-align: inherit;">Uno de los m√©todos m√°s populares y confiables para desarrollar tales modelos es usar el algoritmo Random Forest (RF). </font><font style="vertical-align: inherit;">Para intentar mejorar el rendimiento de un modelo construido utilizando el algoritmo de </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">RF</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , puede utilizar la optimizaci√≥n del hiperpar√°metro del modelo ( </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ajuste de hiperpar√°metro</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , HT). </font><font style="vertical-align: inherit;">
Adem√°s, existe un enfoque generalizado seg√∫n el cual los datos, antes de ser transferidos al modelo, se procesan utilizando el </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;">An√°lisis de componentes principales</font></a></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><img src="https://habrastorage.org/webt/tt/m5/h7/ttm5h7jbbx2u2wuc1var1azxwew.jpeg"></a><br>
<br><font style="vertical-align: inherit;"></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, PCA). </font><font style="vertical-align: inherit;">¬øPero vale la pena usarlo? </font><font style="vertical-align: inherit;">¬øNo es el prop√≥sito principal del algoritmo de RF ayudar al analista a interpretar la importancia de los rasgos?</font></font><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
S√≠, el uso del algoritmo PCA puede conducir a una ligera complicaci√≥n de la interpretaci√≥n de cada "caracter√≠stica" en el an√°lisis de la "importancia de las caracter√≠sticas" del modelo de RF. Sin embargo, el algoritmo PCA reduce la dimensi√≥n del espacio de caracter√≠sticas, lo que puede conducir a una disminuci√≥n en la cantidad de caracter√≠sticas que el modelo RF debe procesar. Tenga en cuenta que el volumen de c√°lculos es una de las principales desventajas del algoritmo de bosque aleatorio (es decir, puede llevar mucho tiempo completar el modelo). La aplicaci√≥n del algoritmo PCA puede ser una parte muy importante del modelado, especialmente en los casos en que funcionan con cientos o incluso miles de caracter√≠sticas. Como resultado, si lo m√°s importante es simplemente crear el modelo m√°s efectivo, y al mismo tiempo puede sacrificar la precisi√≥n de determinar la importancia de los atributos, entonces el PCA puede valer la pena intentarlo.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ahora al grano. </font><font style="vertical-align: inherit;">Vamos a trabajar con un </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">conjunto de datos sobre el c√°ncer de mama</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;">Scikit-learn "c√°ncer de mama"</font></a><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Crearemos tres modelos y compararemos su efectividad. </font><font style="vertical-align: inherit;">A saber, estamos hablando de los siguientes modelos:</font></font><br>
<br>
<ol>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El modelo b√°sico basado en el algoritmo de RF (abreviaremos este modelo de RF).</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El mismo modelo que el n√∫mero 1, pero en el que se aplica una reducci√≥n en la dimensi√≥n del espacio de caracter√≠sticas utilizando el m√©todo del componente principal (RF + PCA).</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El mismo modelo que el No. 2, pero construido utilizando la optimizaci√≥n de hiperpar√°metros (RF + PCA + HT).</font></font></li>
</ol><br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1. Importar datos</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para comenzar, cargue los datos y cree un marco de datos Pandas. </font><font style="vertical-align: inherit;">Como utilizamos un conjunto de datos de "juguete" previamente borrado de Scikit-learn, entonces ya podemos comenzar el proceso de modelado. </font><font style="vertical-align: inherit;">Pero incluso cuando use dichos datos, se recomienda que siempre comience a trabajar realizando un an√°lisis preliminar de los datos utilizando los siguientes comandos aplicados al marco de datos ( </font></font><code>df</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">):</font></font><br>
<br>
<ul>
<li><code>df.head()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - echar un vistazo al nuevo marco de datos y ver si se ve como se esperaba.</font></font></li>
<li><code>df.info()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- para conocer las caracter√≠sticas de los tipos de datos y el contenido de las columnas. </font><font style="vertical-align: inherit;">Puede ser necesario realizar una conversi√≥n de tipo de datos antes de continuar.</font></font></li>
<li><code>df.isna()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- para asegurarse de que no hay valores en los datos </font></font><code>NaN</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Los valores correspondientes, si los hay, pueden necesitar ser procesados ‚Äã‚Äãde alguna manera, o, si es necesario, puede ser necesario eliminar filas enteras del marco de datos.</font></font></li>
<li><code>df.describe()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - para encontrar los valores m√≠nimos, m√°ximos y promedio de los indicadores en las columnas, para encontrar los indicadores del cuadrado medio y la desviaci√≥n probable en las columnas.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En nuestro conjunto de datos, una columna </font></font><code>cancer</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(c√°ncer) es la variable objetivo cuyo valor queremos predecir usando el modelo. </font></font><code>0</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">significa "sin enfermedad". </font></font><code>1</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- "la presencia de la enfermedad".</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_breast_cancer<font></font>
columns = [<span class="hljs-string">'mean radius'</span>, <span class="hljs-string">'mean texture'</span>, <span class="hljs-string">'mean perimeter'</span>, <span class="hljs-string">'mean area'</span>, <span class="hljs-string">'mean smoothness'</span>, <span class="hljs-string">'mean compactness'</span>, <span class="hljs-string">'mean concavity'</span>, <span class="hljs-string">'mean concave points'</span>, <span class="hljs-string">'mean symmetry'</span>, <span class="hljs-string">'mean fractal dimension'</span>, <span class="hljs-string">'radius error'</span>, <span class="hljs-string">'texture error'</span>, <span class="hljs-string">'perimeter error'</span>, <span class="hljs-string">'area error'</span>, <span class="hljs-string">'smoothness error'</span>, <span class="hljs-string">'compactness error'</span>, <span class="hljs-string">'concavity error'</span>, <span class="hljs-string">'concave points error'</span>, <span class="hljs-string">'symmetry error'</span>, <span class="hljs-string">'fractal dimension error'</span>, <span class="hljs-string">'worst radius'</span>, <span class="hljs-string">'worst texture'</span>, <span class="hljs-string">'worst perimeter'</span>, <span class="hljs-string">'worst area'</span>, <span class="hljs-string">'worst smoothness'</span>, <span class="hljs-string">'worst compactness'</span>, <span class="hljs-string">'worst concavity'</span>, <span class="hljs-string">'worst concave points'</span>, <span class="hljs-string">'worst symmetry'</span>, <span class="hljs-string">'worst fractal dimension'</span>]<font></font>
dataset = load_breast_cancer()<font></font>
data = pd.DataFrame(dataset[<span class="hljs-string">'data'</span>], columns=columns)<font></font>
data[<span class="hljs-string">'cancer'</span>] = dataset[<span class="hljs-string">'target'</span>]<font></font>
display(data.head())<font></font>
display(data.info())<font></font>
display(data.isna().sum())<font></font>
display(data.describe())</code></pre><br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/bce/fb8/b60/bcefb8b60462b7658b40e1e56f7744ab.png"></div><br>
<i><font color="#999999">      .       .  , cancer,   ,    . 0  ¬´ ¬ª. 1 ‚Äî ¬´ ¬ª</font></i><br>
 <br>
<h2><font color="#3AC1EF">2.        </font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ahora divida los datos usando la funci√≥n Scikit-learn </font></font><code>train_test_split</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Queremos darle al modelo tantos datos de entrenamiento como sea posible. Sin embargo, necesitamos tener suficientes datos a nuestra disposici√≥n para probar el modelo. En general, podemos decir que, a medida que aumenta el n√∫mero de filas en el conjunto de datos, tambi√©n lo hace la cantidad de datos que se pueden considerar educativos. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Por ejemplo, si hay millones de l√≠neas, puede dividir el conjunto resaltando el 90% de las l√≠neas para datos de entrenamiento y el 10% para datos de prueba. Pero el conjunto de datos de prueba contiene solo 569 filas. Y esto no es tanto para entrenar y probar el modelo. Como resultado, para ser justos en relaci√≥n con los datos de capacitaci√≥n y verificaci√≥n, dividiremos el conjunto en dos partes iguales: 50% de datos de capacitaci√≥n y 50% de datos de verificaci√≥n. Instalamos</font></font><code>stratify=y</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> para garantizar que tanto el conjunto de datos de entrenamiento como el de prueba tengan la misma proporci√≥n de 0 y 1 que el conjunto de datos original.</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<font></font>
X = data.drop(<span class="hljs-string">'cancer'</span>, axis=<span class="hljs-number">1</span>)&nbsp;&nbsp;<font></font>
y = data[<span class="hljs-string">'cancer'</span>]&nbsp;<font></font>
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="hljs-number">0.50</span>, random_state = <span class="hljs-number">2020</span>, stratify=y)</code></pre><br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3. Escalado de datos</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Antes de proceder al modelado, debe "centrar" y "estandarizar" los datos </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">escalando</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">El escalado se realiza debido al hecho de que diferentes cantidades se expresan en diferentes unidades. </font><font style="vertical-align: inherit;">Este procedimiento le permite organizar una "lucha justa" entre los signos para determinar su importancia. </font><font style="vertical-align: inherit;">Adem√°s, convertimos </font></font><code>y_train</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">del tipo de datos Pandas </font></font><code>Series</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">a la matriz NumPy para que luego el modelo pueda trabajar con los objetivos correspondientes.</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler<font></font>
ss = StandardScaler()<font></font>
X_train_scaled = ss.fit_transform(X_train)<font></font>
X_test_scaled = ss.transform(X_test)<font></font>
y_train = np.array(y_train)</code></pre><br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">4. Entrenamiento del modelo b√°sico (modelo No. 1, RF)</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ahora cree el modelo n√∫mero 1. </font><font style="vertical-align: inherit;">En √©l, recordamos que solo se utiliza el algoritmo Random Forest. </font><font style="vertical-align: inherit;">Utiliza todas las funciones y se configura con los valores predeterminados (los detalles sobre estas configuraciones se pueden encontrar en la documentaci√≥n de </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">sklearn.ensemble.RandomForestClassifier</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ). </font><font style="vertical-align: inherit;">Inicializa el modelo. </font><font style="vertical-align: inherit;">Despu√©s de eso, la entrenaremos en datos escalados. </font><font style="vertical-align: inherit;">La precisi√≥n del modelo se puede medir en los datos de entrenamiento:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier
<span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> recall_score<font></font>
rfc = RandomForestClassifier()<font></font>
rfc.fit(X_train_scaled, y_train)<font></font>
display(rfc.score(X_train_scaled, y_train))<font></font>
<span class="hljs-comment"># 1.0</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Si estamos interesados ‚Äã‚Äãen saber qu√© rasgos son los m√°s importantes para el modelo de RF en la predicci√≥n del c√°ncer de mama, podemos visualizar y cuantificar los √≠ndices de gravedad de los rasgos haciendo referencia al atributo </font></font><code>feature_importances_</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">:</font></font><br>
<br>
<pre><code class="python hljs">feats = {}
<span class="hljs-keyword">for</span> feature, importance <span class="hljs-keyword">in</span> zip(data.columns, rfc_1.feature_importances_):<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;feats[feature] = importance<font></font>
importances = pd.DataFrame.from_dict(feats, orient=<span class="hljs-string">'index'</span>).rename(columns={<span class="hljs-number">0</span>: <span class="hljs-string">'Gini-Importance'</span>})<font></font>
importances = importances.sort_values(by=<span class="hljs-string">'Gini-Importance'</span>, ascending=<span class="hljs-literal">False</span>)<font></font>
importances = importances.reset_index()<font></font>
importances = importances.rename(columns={<span class="hljs-string">'index'</span>: <span class="hljs-string">'Features'</span>})<font></font>
sns.set(font_scale = <span class="hljs-number">5</span>)<font></font>
sns.set(style=<span class="hljs-string">"whitegrid"</span>, color_codes=<span class="hljs-literal">True</span>, font_scale = <span class="hljs-number">1.7</span>)<font></font>
fig, ax = plt.subplots()<font></font>
fig.set_size_inches(<span class="hljs-number">30</span>,<span class="hljs-number">15</span>)<font></font>
sns.barplot(x=importances[<span class="hljs-string">'Gini-Importance'</span>], y=importances[<span class="hljs-string">'Features'</span>], data=importances, color=<span class="hljs-string">'skyblue'</span>)<font></font>
plt.xlabel(<span class="hljs-string">'Importance'</span>, fontsize=<span class="hljs-number">25</span>, weight = <span class="hljs-string">'bold'</span>)<font></font>
plt.ylabel(<span class="hljs-string">'Features'</span>, fontsize=<span class="hljs-number">25</span>, weight = <span class="hljs-string">'bold'</span>)<font></font>
plt.title(<span class="hljs-string">'Feature Importance'</span>, fontsize=<span class="hljs-number">25</span>, weight = <span class="hljs-string">'bold'</span>)<font></font>
display(plt.show())<font></font>
display(importances)</code></pre><br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a02/a8f/cd2/a02a8fcd28f87af338f364a70faeca3e.jpg"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Visualizaci√≥n de la "importancia" de los signos.</font></font></font></i><br>
 <br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/830/176/e1f/830176e1fc9ce63bfedf2d727619253b.png"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Indicadores de significancia</font></font></font></i><br>
<br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">5. El m√©todo de los componentes principales.</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ahora preguntemos c√≥mo podemos mejorar el modelo b√°sico de RF. Utilizando la t√©cnica de reducir la dimensi√≥n del espacio de caracter√≠sticas, es posible presentar el conjunto de datos inicial a trav√©s de menos variables y, al mismo tiempo, reducir la cantidad de recursos computacionales necesarios para garantizar el funcionamiento del modelo. Con el PCA, puede estudiar la varianza de muestra acumulativa de estas caracter√≠sticas para comprender qu√© caracter√≠sticas explican la mayor parte de la varianza en los datos. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Inicializamos el objeto PCA ( </font></font><code>pca_test</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">), indicando el n√∫mero de componentes (caracter√≠sticas) que deben considerarse. Establecemos este indicador en 30 para ver la varianza explicada de todos los componentes generados antes de decidir cu√°ntos componentes necesitamos. Luego transferimos a los </font></font><code>pca_test</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">datos escalados</font></font><code>X_train</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">utilizando el m√©todo </font></font><code>pca_test.fit()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Despu√©s de eso visualizamos los datos.</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns
<span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> PCA<font></font>
pca_test = PCA(n_components=<span class="hljs-number">30</span>)<font></font>
pca_test.fit(X_train_scaled)<font></font>
sns.set(style=<span class="hljs-string">'whitegrid'</span>)<font></font>
plt.plot(np.cumsum(pca_test.explained_variance_ratio_))<font></font>
plt.xlabel(<span class="hljs-string">'number of components'</span>)<font></font>
plt.ylabel(<span class="hljs-string">'cumulative explained variance'</span>)<font></font>
plt.axvline(linewidth=<span class="hljs-number">4</span>, color=<span class="hljs-string">'r'</span>, linestyle = <span class="hljs-string">'--'</span>, x=<span class="hljs-number">10</span>, ymin=<span class="hljs-number">0</span>, ymax=<span class="hljs-number">1</span>)<font></font>
display(plt.show())<font></font>
evr = pca_test.explained_variance_ratio_<font></font>
cvr = np.cumsum(pca_test.explained_variance_ratio_)<font></font>
pca_df = pd.DataFrame()<font></font>
pca_df[<span class="hljs-string">'Cumulative Variance Ratio'</span>] = cvr<font></font>
pca_df[<span class="hljs-string">'Explained Variance Ratio'</span>] = evr<font></font>
display(pca_df.head(<span class="hljs-number">10</span>))</code></pre><br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6eb/65f/acc/6eb65facc6c8b05f1e910d3b2b676d5e.jpg"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Despu√©s de que el n√∫mero de componentes utilizados excede de 10, el aumento en su n√∫mero no aumenta en gran medida la varianza explicada</font></font></font></i><br>
 <br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f12/e3c/915/f12e3c915d761e1d4623051dac74cd8d.png"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Este marco de datos contiene indicadores tales como la relaci√≥n de varianza acumulada (tama√±o acumulado de la varianza explicada de los datos) y la raz√≥n de varianza explicada (contribuci√≥n de cada componente al volumen total de la varianza explicada).</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Si observa el marco de datos anterior, resulta que usar el PCA para pasar de 30 variables a 10 a componentes permite explicar el 95% de la dispersi√≥n de datos. Los otros 20 componentes representan menos del 5% de la variaci√≥n, lo que significa que podemos rechazarlos. Siguiendo esta l√≥gica, utilizamos el PCA para reducir el n√∫mero de componentes de 30 a 10 para</font></font><code>X_train</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">y</font></font><code>X_test</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Escribimos estos conjuntos de datos de "dimensi√≥n reducida" creados artificialmente en</font></font><code>X_train_scaled_pca</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">y dentro</font></font><code>X_test_scaled_pca</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font><br>
<br>
<pre><code class="python hljs">pca = PCA(n_components=<span class="hljs-number">10</span>)<font></font>
pca.fit(X_train_scaled)<font></font>
X_train_scaled_pca = pca.transform(X_train_scaled)<font></font>
X_test_scaled_pca = pca.transform(X_test_scaled)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Cada componente es una combinaci√≥n lineal de variables de origen con los correspondientes "pesos". </font><font style="vertical-align: inherit;">Podemos ver estos "pesos" para cada componente creando un marco de datos.</font></font><br>
<br>
<pre><code class="python hljs">pca_dims = []
<span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>, len(pca_df)):<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;pca_dims.append(<span class="hljs-string">'PCA Component {}'</span>.format(x))<font></font>
pca_test_df = pd.DataFrame(pca_test.components_, columns=columns, index=pca_dims)<font></font>
pca_test_df.head(<span class="hljs-number">10</span>).T</code></pre><br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/086/a28/ae4/086a28ae45e9048811cf813d4868902e.png"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Marco de datos de informaci√≥n de componentes</font></font></font></i><br>
 <br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">6. Capacitaci√≥n del modelo b√°sico de RF despu√©s de aplicar el m√©todo de componentes principales a los datos (modelo No. 2, RF + PCA)</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ahora podemos pasar a otro modelo de datos b√°sicos de RF </font></font><code>X_train_scaled_pca</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">y </font></font><code>y_train</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">y podemos averiguar acerca de si hay una mejora en la precisi√≥n de las predicciones emitidas por el modelo.</font></font><br>
<br>
<pre><code class="python hljs">rfc = RandomForestClassifier()<font></font>
rfc.fit(X_train_scaled_pca, y_train)<font></font>
display(rfc.score(X_train_scaled_pca, y_train))<font></font>
<span class="hljs-comment"># 1.0</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Los modelos se comparan a continuaci√≥n.</font></font><br>
<br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">7. Optimizaci√≥n de hiperpar√°metros. </font><font style="vertical-align: inherit;">Ronda 1: RandomizedSearchCV</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Despu√©s de procesar los datos utilizando el m√©todo del componente principal, puede intentar utilizar la </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">optimizaci√≥n de los hiperpar√°metros del</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> modelo para mejorar la calidad de las predicciones producidas por el modelo de RF. Los hiperpar√°metros se pueden considerar como algo as√≠ como "configuraciones" del modelo. Las configuraciones que son perfectas para un conjunto de datos no funcionar√°n para otro, es por eso que necesita optimizarlas. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Puede comenzar con el algoritmo RandomizedSearchCV, que le permite explorar m√°s o menos una amplia gama de valores. Las descripciones de todos los hiperpar√°metros para los modelos de RF se pueden encontrar </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">aqu√≠</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En el curso del trabajo, generamos una entidad </font></font><code>param_dist</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">que contiene, para cada hiperpar√°metro, un rango de valores que necesitan ser probados. A continuaci√≥n, inicializamos el objeto.</font></font><code>rs</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">usando la funci√≥n </font></font><code>RandomizedSearchCV()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, pas√°ndole el modelo RF </font></font><code>param_dist</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, el n√∫mero de iteraciones y el n√∫mero de </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">validaciones cruzadas</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> que deben realizarse. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El hiperpar√°metro le </font></font><code>verbose</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">permite controlar la cantidad de informaci√≥n que muestra el modelo durante su funcionamiento (como la salida de informaci√≥n durante el entrenamiento del modelo). </font><font style="vertical-align: inherit;">El hiperpar√°metro le </font></font><code>n_jobs</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">permite especificar cu√°ntos n√∫cleos de procesador necesita usar para garantizar el funcionamiento del modelo. </font><font style="vertical-align: inherit;">Establecerlo </font></font><code>n_jobs</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">en un valor </font></font><code>-1</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">conducir√° a un modelo m√°s r√°pido, ya que usar√° todos los n√∫cleos de procesador. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nos dedicaremos a la selecci√≥n de los siguientes hiperpar√°metros:</font></font><br>
<br>
<ul>
<li><code>n_estimators</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - el n√∫mero de "√°rboles" en el "bosque aleatorio".</font></font></li>
<li><code>max_features</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - la cantidad de funciones para seleccionar la divisi√≥n.</font></font></li>
<li><code>max_depth</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - Profundidad m√°xima de los √°rboles.</font></font></li>
<li><code>min_samples_split</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - el n√∫mero m√≠nimo de objetos necesarios para que un nodo de √°rbol se divida.</font></font></li>
<li><code>min_samples_leaf</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - El n√∫mero m√≠nimo de objetos en las hojas.</font></font></li>
<li><code>bootstrap</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - Se utiliza para construir √°rboles de submuestras con retorno.</font></font></li>
</ul><br>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> RandomizedSearchCV<font></font>
n_estimators = [int(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> np.linspace(start = <span class="hljs-number">100</span>, stop = <span class="hljs-number">1000</span>, num = <span class="hljs-number">10</span>)]<font></font>
max_features = [<span class="hljs-string">'log2'</span>, <span class="hljs-string">'sqrt'</span>]<font></font>
max_depth = [int(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> np.linspace(start = <span class="hljs-number">1</span>, stop = <span class="hljs-number">15</span>, num = <span class="hljs-number">15</span>)]<font></font>
min_samples_split = [int(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> np.linspace(start = <span class="hljs-number">2</span>, stop = <span class="hljs-number">50</span>, num = <span class="hljs-number">10</span>)]<font></font>
min_samples_leaf = [int(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> np.linspace(start = <span class="hljs-number">2</span>, stop = <span class="hljs-number">50</span>, num = <span class="hljs-number">10</span>)]<font></font>
bootstrap = [<span class="hljs-literal">True</span>, <span class="hljs-literal">False</span>]<font></font>
param_dist = {<span class="hljs-string">'n_estimators'</span>: n_estimators,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'max_features'</span>: max_features,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'max_depth'</span>: max_depth,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'min_samples_split'</span>: min_samples_split,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'min_samples_leaf'</span>: min_samples_leaf,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'bootstrap'</span>: bootstrap}<font></font>
rs = RandomizedSearchCV(rfc_2,&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;param_dist,&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;n_iter = <span class="hljs-number">100</span>,&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;cv = <span class="hljs-number">3</span>,&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;verbose = <span class="hljs-number">1</span>,&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;n_jobs=<span class="hljs-number">-1</span>,&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;random_state=<span class="hljs-number">0</span>)<font></font>
rs.fit(X_train_scaled_pca, y_train)<font></font>
rs.best_params_<font></font>
<span class="hljs-comment"># {'n_estimators': 700,</span>
<span class="hljs-comment"># 'min_samples_split': 2,</span>
<span class="hljs-comment"># 'min_samples_leaf': 2,</span>
<span class="hljs-comment"># 'max_features': 'log2',</span>
<span class="hljs-comment"># 'max_depth': 11,</span>
<span class="hljs-comment"># 'bootstrap': True}</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Con los valores de los par√°metros </font></font><code>n_iter = 100</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">y </font></font><code>cv = 3</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, creamos 300 modelos de RF, eligiendo aleatoriamente combinaciones de los hiperpar√°metros presentados anteriormente. </font><font style="vertical-align: inherit;">Podemos consultar el atributo </font></font><code>best_params_ </code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">para obtener informaci√≥n sobre un conjunto de par√°metros que le permite crear el mejor modelo. </font><font style="vertical-align: inherit;">Pero en esta etapa, esto puede no darnos los datos m√°s interesantes sobre los rangos de par√°metros que vale la pena explorar en la pr√≥xima ronda de optimizaci√≥n. </font><font style="vertical-align: inherit;">Para descubrir en qu√© rango de valores vale la pena continuar buscando, podemos obtener f√°cilmente un marco de datos que contiene los resultados del algoritmo RandomizedSearchCV.</font></font><br>
<br>
<pre><code class="python hljs">rs_df = pd.DataFrame(rs.cv_results_).sort_values(<span class="hljs-string">'rank_test_score'</span>).reset_index(drop=<span class="hljs-literal">True</span>)<font></font>
rs_df = rs_df.drop([<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'mean_fit_time'</span>,&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'std_fit_time'</span>,&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'mean_score_time'</span>,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'std_score_time'</span>,&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'params'</span>,&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'split0_test_score'</span>,&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'split1_test_score'</span>,&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'split2_test_score'</span>,&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'std_test_score'</span>],<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;axis=<span class="hljs-number">1</span>)<font></font>
rs_df.head(<span class="hljs-number">10</span>)</code></pre><br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/617/b8c/20b/617b8c20b787acc3c76c23d9235b4b5a.png"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Resultados del algoritmo RandomizedSearchCV</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Ahora crearemos gr√°ficos de barras en los que, en el eje X, se encuentran los valores de hiperpar√°metro, y en el eje Y son los valores promedio que muestran los modelos. </font><font style="vertical-align: inherit;">Esto permitir√° comprender qu√© valores de hiperpar√°metros, en promedio, muestran su mejor rendimiento.</font></font><br>
<br>
<pre><code class="python hljs">fig, axs = plt.subplots(ncols=<span class="hljs-number">3</span>, nrows=<span class="hljs-number">2</span>)<font></font>
sns.set(style=<span class="hljs-string">"whitegrid"</span>, color_codes=<span class="hljs-literal">True</span>, font_scale = <span class="hljs-number">2</span>)<font></font>
fig.set_size_inches(<span class="hljs-number">30</span>,<span class="hljs-number">25</span>)<font></font>
sns.barplot(x=<span class="hljs-string">'param_n_estimators'</span>, y=<span class="hljs-string">'mean_test_score'</span>, data=rs_df, ax=axs[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>], color=<span class="hljs-string">'lightgrey'</span>)<font></font>
axs[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>].set_ylim([<span class="hljs-number">.83</span>,<span class="hljs-number">.93</span>])axs[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>].set_title(label = <span class="hljs-string">'n_estimators'</span>, size=<span class="hljs-number">30</span>, weight=<span class="hljs-string">'bold'</span>)<font></font>
sns.barplot(x=<span class="hljs-string">'param_min_samples_split'</span>, y=<span class="hljs-string">'mean_test_score'</span>, data=rs_df, ax=axs[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>], color=<span class="hljs-string">'coral'</span>)<font></font>
axs[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>].set_ylim([<span class="hljs-number">.85</span>,<span class="hljs-number">.93</span>])axs[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>].set_title(label = <span class="hljs-string">'min_samples_split'</span>, size=<span class="hljs-number">30</span>, weight=<span class="hljs-string">'bold'</span>)<font></font>
sns.barplot(x=<span class="hljs-string">'param_min_samples_leaf'</span>, y=<span class="hljs-string">'mean_test_score'</span>, data=rs_df, ax=axs[<span class="hljs-number">0</span>,<span class="hljs-number">2</span>], color=<span class="hljs-string">'lightgreen'</span>)<font></font>
axs[<span class="hljs-number">0</span>,<span class="hljs-number">2</span>].set_ylim([<span class="hljs-number">.80</span>,<span class="hljs-number">.93</span>])axs[<span class="hljs-number">0</span>,<span class="hljs-number">2</span>].set_title(label = <span class="hljs-string">'min_samples_leaf'</span>, size=<span class="hljs-number">30</span>, weight=<span class="hljs-string">'bold'</span>)<font></font>
sns.barplot(x=<span class="hljs-string">'param_max_features'</span>, y=<span class="hljs-string">'mean_test_score'</span>, data=rs_df, ax=axs[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>], color=<span class="hljs-string">'wheat'</span>)<font></font>
axs[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>].set_ylim([<span class="hljs-number">.88</span>,<span class="hljs-number">.92</span>])axs[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>].set_title(label = <span class="hljs-string">'max_features'</span>, size=<span class="hljs-number">30</span>, weight=<span class="hljs-string">'bold'</span>)<font></font>
sns.barplot(x=<span class="hljs-string">'param_max_depth'</span>, y=<span class="hljs-string">'mean_test_score'</span>, data=rs_df, ax=axs[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>], color=<span class="hljs-string">'lightpink'</span>)<font></font>
axs[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>].set_ylim([<span class="hljs-number">.80</span>,<span class="hljs-number">.93</span>])axs[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>].set_title(label = <span class="hljs-string">'max_depth'</span>, size=<span class="hljs-number">30</span>, weight=<span class="hljs-string">'bold'</span>)<font></font>
sns.barplot(x=<span class="hljs-string">'param_bootstrap'</span>,y=<span class="hljs-string">'mean_test_score'</span>, data=rs_df, ax=axs[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>], color=<span class="hljs-string">'skyblue'</span>)<font></font>
axs[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>].set_ylim([<span class="hljs-number">.88</span>,<span class="hljs-number">.92</span>])<font></font>
axs[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>].set_title(label = <span class="hljs-string">'bootstrap'</span>, size=<span class="hljs-number">30</span>, weight=<span class="hljs-string">'bold'</span>)<font></font>
plt.show()</code></pre><br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/418/311/ba6/418311ba6c38bfcebbf152af810d6b58.jpg"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">An√°lisis de los valores de hiperpar√°metros</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Si analizamos los gr√°ficos anteriores, podemos notar algunas cosas interesantes que hablan de c√≥mo, en promedio, cada valor de un hiperpar√°metro afecta al modelo.</font></font><br>
<br>
<ul>
<li><code>n_estimators</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">: los valores de 300, 500, 700, aparentemente, muestran los mejores resultados promedio.</font></font></li>
<li><code>min_samples_split</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">: Los valores peque√±os como 2 y 7 parecen mostrar los mejores resultados. </font><font style="vertical-align: inherit;">El valor 23 tambi√©n se ve bien. Puede examinar varios valores de este hiperpar√°metro en exceso de 2, as√≠ como varios valores de aproximadamente 23.</font></font></li>
<li><code>min_samples_leaf</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">: Existe la sensaci√≥n de que los valores peque√±os de este hiperpar√°metro dan mejores resultados. </font><font style="vertical-align: inherit;">Esto significa que podemos experimentar valores entre 2 y 7.</font></font></li>
<li><code>max_features</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">: la opci√≥n </font></font><code>sqrt</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">da el resultado promedio m√°s alto.</font></font></li>
<li><code>max_depth</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">: no existe una relaci√≥n clara entre el valor del hiperpar√°metro y el resultado del modelo, pero existe la sensaci√≥n de que los valores 2, 3, 7, 11, 15 se ven bien.</font></font></li>
<li><code>bootstrap</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">: el valor </font></font><code>False</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">muestra el mejor resultado promedio.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ahora, utilizando estos hallazgos, podemos pasar a la segunda ronda de optimizaci√≥n de hiperpar√°metros. </font><font style="vertical-align: inherit;">Esto reducir√° el rango de valores que nos interesan.</font></font><br>
<br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">8. Optimizaci√≥n de hiperpar√°metros. </font><font style="vertical-align: inherit;">Ronda 2: GridSearchCV (preparaci√≥n final de par√°metros para el modelo No. 3, RF + PCA + HT)</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Despu√©s de aplicar el algoritmo RandomizedSearchCV, utilizaremos el algoritmo GridSearchCV para realizar una b√∫squeda m√°s precisa de la mejor combinaci√≥n de hiperpar√°metros. Aqu√≠ se investigan los mismos hiperpar√°metros, pero ahora estamos aplicando una b√∫squeda m√°s "exhaustiva" de su mejor combinaci√≥n. Usando el algoritmo GridSearchCV, se examina cada combinaci√≥n de hiperpar√°metros. Esto requiere muchos m√°s recursos computacionales que usar el algoritmo RandomizedSearchCV cuando establecemos independientemente el n√∫mero de iteraciones de b√∫squeda. Por ejemplo, investigar 10 valores para cada uno de los 6 hiperpar√°metros con validaci√≥n cruzada en 3 bloques requerir√° 10‚Å∂ x 3, o 3,000,000 de sesiones de entrenamiento modelo. Es por eso que usamos el algoritmo GridSearchCV despu√©s de, despu√©s de aplicar RandomizedSearchCV, redujimos los rangos de los valores de los par√°metros estudiados.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Entonces, usando lo que descubrimos con la ayuda de RandomizedSearchCV, examinamos los valores de los hiperpar√°metros que se han mostrado mejor:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> GridSearchCV<font></font>
n_estimators = [<span class="hljs-number">300</span>,<span class="hljs-number">500</span>,<span class="hljs-number">700</span>]<font></font>
max_features = [<span class="hljs-string">'sqrt'</span>]<font></font>
max_depth = [<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">7</span>,<span class="hljs-number">11</span>,<span class="hljs-number">15</span>]<font></font>
min_samples_split = [<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">22</span>,<span class="hljs-number">23</span>,<span class="hljs-number">24</span>]<font></font>
min_samples_leaf = [<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>,<span class="hljs-number">7</span>]<font></font>
bootstrap = [<span class="hljs-literal">False</span>]<font></font>
param_grid = {<span class="hljs-string">'n_estimators'</span>: n_estimators,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'max_features'</span>: max_features,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'max_depth'</span>: max_depth,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'min_samples_split'</span>: min_samples_split,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'min_samples_leaf'</span>: min_samples_leaf,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'bootstrap'</span>: bootstrap}<font></font>
gs = GridSearchCV(rfc_2, param_grid, cv = <span class="hljs-number">3</span>, verbose = <span class="hljs-number">1</span>, n_jobs=<span class="hljs-number">-1</span>)<font></font>
gs.fit(X_train_scaled_pca, y_train)<font></font>
rfc_3 = gs.best_estimator_<font></font>
gs.best_params_<font></font>
<span class="hljs-comment"># {'bootstrap': False,</span>
<span class="hljs-comment"># 'max_depth': 7,</span>
<span class="hljs-comment"># 'max_features': 'sqrt',</span>
<span class="hljs-comment"># 'min_samples_leaf': 3,</span>
<span class="hljs-comment"># 'min_samples_split': 2,</span>
<span class="hljs-comment"># 'n_estimators': 500}</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Aqu√≠ aplicamos la validaci√≥n cruzada en 3 bloques para 540 (3 x 1 x 5 x 6 x 6 x 1) sesiones de entrenamiento modelo, lo que da 1620 sesiones de entrenamiento modelo. </font><font style="vertical-align: inherit;">Y ahora, despu√©s de usar RandomizedSearchCV y GridSearchCV, podemos recurrir al atributo </font></font><code>best_params_</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">para averiguar qu√© valores de hiperpar√°metros permiten que el modelo funcione mejor con el conjunto de datos en estudio (estos valores se pueden ver en la parte inferior del bloque de c√≥digo anterior) . </font><font style="vertical-align: inherit;">Estos par√°metros se utilizan para crear el modelo n√∫mero 3.</font></font><br>
<br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">9. Evaluaci√≥n de la calidad de los modelos en los datos de verificaci√≥n.</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ahora puede evaluar los modelos creados en los datos de verificaci√≥n. </font><font style="vertical-align: inherit;">Es decir, estamos hablando de esos tres modelos descritos al comienzo del material. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Mira estos modelos:</font></font><br>
<br>
<pre><code class="python hljs">y_pred = rfc.predict(X_test_scaled)<font></font>
y_pred_pca = rfc.predict(X_test_scaled_pca)<font></font>
y_pred_gs = gs.best_estimator_.predict(X_test_scaled_pca)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Cree matrices de error para los modelos y descubra qu√© tan bien cada uno de ellos puede predecir el c√°ncer de seno:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> confusion_matrix<font></font>
conf_matrix_baseline = pd.DataFrame(confusion_matrix(y_test, y_pred), index = [<span class="hljs-string">'actual 0'</span>, <span class="hljs-string">'actual 1'</span>], columns = [<span class="hljs-string">'predicted 0'</span>, <span class="hljs-string">'predicted 1'</span>])<font></font>
conf_matrix_baseline_pca = pd.DataFrame(confusion_matrix(y_test, y_pred_pca), index = [<span class="hljs-string">'actual 0'</span>, <span class="hljs-string">'actual 1'</span>], columns = [<span class="hljs-string">'predicted 0'</span>, <span class="hljs-string">'predicted 1'</span>])<font></font>
conf_matrix_tuned_pca = pd.DataFrame(confusion_matrix(y_test, y_pred_gs), index = [<span class="hljs-string">'actual 0'</span>, <span class="hljs-string">'actual 1'</span>], columns = [<span class="hljs-string">'predicted 0'</span>, <span class="hljs-string">'predicted 1'</span>])<font></font>
display(conf_matrix_baseline)<font></font>
display(<span class="hljs-string">'Baseline Random Forest recall score'</span>, recall_score(y_test, y_pred))<font></font>
display(conf_matrix_baseline_pca)<font></font>
display(<span class="hljs-string">'Baseline Random Forest With PCA recall score'</span>, recall_score(y_test, y_pred_pca))<font></font>
display(conf_matrix_tuned_pca)<font></font>
display(<span class="hljs-string">'Hyperparameter Tuned Random Forest With PCA Reduced Dimensionality recall score'</span>, recall_score(y_test, y_pred_gs))</code></pre><br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f48/a9e/92f/f48a9e92fd5fdca613d6073e00bae2c6.png"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Resultados del trabajo de los tres modelos</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Aqu√≠ se eval√∫a la m√©trica "integridad" (recuerdo). </font><font style="vertical-align: inherit;">El hecho es que estamos lidiando con un diagn√≥stico de c√°ncer. </font><font style="vertical-align: inherit;">Por lo tanto, estamos extremadamente interesados ‚Äã‚Äãen minimizar los pron√≥sticos falsos negativos emitidos por los modelos. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ante esto, podemos concluir que el modelo b√°sico de RF dio los mejores resultados. </font><font style="vertical-align: inherit;">Su tasa de completitud fue del 94,97%. </font><font style="vertical-align: inherit;">En el conjunto de datos de la prueba, hubo un registro de 179 pacientes con c√°ncer. </font><font style="vertical-align: inherit;">El modelo encontr√≥ 170 de ellos.</font></font><br>
<br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Resumen</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Este estudio proporciona una observaci√≥n importante. </font><font style="vertical-align: inherit;">A veces, el modelo RF, que utiliza el m√©todo del componente principal y la optimizaci√≥n a gran escala de hiperpar√°metros, puede no funcionar tan bien como el modelo m√°s com√∫n con configuraciones est√°ndar. </font><font style="vertical-align: inherit;">Pero esta no es una raz√≥n para limitarse solo a los modelos m√°s simples. </font><font style="vertical-align: inherit;">Sin probar diferentes modelos, es imposible decir cu√°l mostrar√° el mejor resultado. </font><font style="vertical-align: inherit;">Y en el caso de los modelos que se utilizan para predecir la presencia de c√°ncer en los pacientes, podemos decir que cuanto mejor sea el modelo, m√°s vidas se pueden salvar. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">¬°Queridos lectores! </font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">¬øQu√© tareas resuelves usando m√©todos de aprendizaje autom√°tico?</font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><img src="https://habrastorage.org/files/1ba/550/d25/1ba550d25e8846ce8805de564da6aa63.png"></a></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es488330/index.html">Ingl√©s con George Karlin: analizamos el ingenioso stand-up sobre unidades fraseol√≥gicas</a></li>
<li><a href="../es488332/index.html">Cero, uno, dos, Freddy te recoger√°</a></li>
<li><a href="../es488336/index.html">Consejos para usar el algoritmo de colapso de la funci√≥n de onda</a></li>
<li><a href="../es488338/index.html">Pasant√≠as de Google: Zurich, Londres y Silicon Valley</a></li>
<li><a href="../es488340/index.html">Profesi√≥n: Desarrollador de backend</a></li>
<li><a href="../es488346/index.html">Instalaci√≥n de herramientas o con SCIP y GLPK en un entorno virtual Python 3.7 en Linux</a></li>
<li><a href="../es488348/index.html">Seminario web "Diez desaf√≠os √°giles principales y formas de superarlos en una hora" 17 de febrero a las 20:00 hora de Mosc√∫</a></li>
<li><a href="../es488352/index.html">Comparaci√≥n de costos de VDI: en las instalaciones versus la nube p√∫blica</a></li>
<li><a href="../es488356/index.html">Capacitaci√≥n en la Universidad T√©cnica Mar√≠tima Estatal de San Petersburgo para productos Dassault Syst√®mes</a></li>
<li><a href="../es488360/index.html">Big Data Mitos y Cultura Digital</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>