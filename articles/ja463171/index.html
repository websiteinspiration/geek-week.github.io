<!doctype html>
<html class="no-js" lang="ja">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🏂🏿 😼 🍑 ニューラルネットワークとディープラーニング：オンラインチュートリアル、第6章、パート1：ディープラーニング 👩🏻‍💻 🤙🏼 🤦🏽</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="コンテンツ

- 1: 
- 2: 
- 3: .1: .2: ? .3: ? 
- .1: 
- .2: ? 
- .3: ? 
- 4: , 
- 5: ?
- 6: .1: .2: 
- .1: 
- .2: 
- : ?
 前の章では、ディープニューラルネットワーク（GNS）は、浅いものより...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>ニューラルネットワークとディープラーニング：オンラインチュートリアル、第6章、パート1：ディープラーニング</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/463171/"><div class="spoiler"><b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">コンテンツ</font></font></b><div class="spoiler_text"><ul>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="> 1:      </a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="> 2:     </a></li>
<li> 3:<ul>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=">.1:    </a><br>
</li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=">.2:     ?</a><br>
</li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=">.3:    ?</a><br>
</li>
</ul></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="> 4:   ,      </a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="> 5:      ?</a></li>
<li> 6:<ul>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=">.1:  </a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=">.2:     </a></li>
</ul></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=">:       ?</a></li>
</ul></div></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
前の章では、ディープニューラルネットワーク（GNS）は、浅いものよりもトレーニングが難しいことが多いことを学びました。そして、これは悪いことです。STSをトレーニングできれば、タスクを実行するのにはるかに優れていると私たちが信じるあらゆる理由があるからです。しかし、前の章からのニュースはがっかりするものの、私たちを止めるものではありません。この章では、ディープネットワークをトレーニングして実践するために使用できるテクニックを開発します。また、状況をより広く見て、画像認識、音声、およびその他のアプリケーションでのGNSの使用における最近の進歩について簡単に理解します。また、ニューラルネットワークとAIがどのような将来を期待できるかを表面的に考えます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
これは長い章になるので、目次を少し見てみましょう。そのセクションは強く相互接続されていないため、ニューラルネットワークに関するいくつかの基本的な概念がある場合は、より興味のあるセクションから始めることができます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
この章の主要部分は、最も一般的なタイプのディープネットワークの1つであるディープコンボリューションネットワーク（GSS）の概要です。 MNISTデータセットから手書きの数字を分類する問題を解決するために、コードなどを含む畳み込みネットワークの使用の詳細な例を使用します。</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/839/d0b/543/839d0b54370af70f06b3f097897de457.png"><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
本書の前半でこの問題を解決するために使用した、浅いネットワークを持つ畳み込みネットワークのレビューを開始します。いくつかの段階で、より強力なネットワークを作成します。途中で、畳み込み、プーリング、GPUを使用して浅いネットワークで行ったものと比較してトレーニング量を大幅に増やす、トレーニングデータをアルゴリズムで拡張する（オーバーフィッティングを減らす）、ドロップアウトテクノロジーを使用して、多くの強力なテクノロジーについて学びます。 （再トレーニングを減らすためにも）、ネットワークのアンサンブルの使用など。その結果、ほぼ人間レベルの能力を持つシステムになります。 10,000のMNIST検証イメージ（トレーニング中にシステムが認識しなかったもの）のうち、9967を正しく認識できます。これらのイメージの一部を次に示します正しく認識されませんでした。右上には正しいオプションがあります。私たちのプログラムが示したものは、右下隅に示されています。</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/b6e/2d7/69a/b6e2d769a802b1ae5f249932789f2dff.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
それらの多くは、人間に分類することが困難です。たとえば、上の行の3桁目を考えてみましょう。正式版の「8」よりも「9」のように思えます。私たちのネットワークもそれが「9」であることを決定しました。少なくとも、そのようなエラーは完全に理解でき、おそらく承認さえできます。ニューラルネットワーク（特に、畳み込み）が最近達成した驚くべき進歩の概要を示して、画像認識についての説明を締めくくります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
この章の残りの部分では、より広範で詳細度の低い視点からのディープラーニングについて説明します。他のNSモデル、特に再発性NSと長期短期記憶の単位について簡単に検討し、これらのモデルを使用して音声認識や自然言語処理などの問題を解決する方法を検討します。 NSとCivil Defenseの将来について、意図主導型のユーザーインターフェースなどのアイデアからAIでのディープラーニングの役割までを取り上げます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
この章は、本の前の章の内容に基づいており、逆伝播、正則化、ソフトマックスなどのアイデアを使用および統合しています。ただし、この章を読むために、これまでのすべての章の内容を詳しく説明する必要はありません。ただし、</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">第1章</font></a><font style="vertical-align: inherit;">を読むのに支障はありません。</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、およびNAの基本について学びます。</font><font style="vertical-align: inherit;">第2章から第5章の概念を使用する場合、必要に応じて資料への必要なリンクを提供します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
この章では説明していません。</font><font style="vertical-align: inherit;">これは、NSと連携するための最新のクールなライブラリに関するトレーニング資料ではありません。</font><font style="vertical-align: inherit;">最先端の研究からの問題を解決するために、何十ものレイヤーでSTSをトレーニングするつもりはありません。</font><font style="vertical-align: inherit;">GNSの基礎となるいくつかの基本原則を理解し、それらをMNISTタスクのシンプルで理解しやすいコンテキストに適用しようとします。</font><font style="vertical-align: inherit;">言い換えれば、この章では、地域の最前線に移動することはありません。</font><font style="vertical-align: inherit;">この章と前の章の目的は、基本に集中し、幅広い現代作品を理解できるようにすることです。</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">畳み込みニューラルネットワークの概要</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
前の章では、手書きの数字の画像を認識するのはかなり良いことをニューラルネットワークに教え</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/839/d0b/543/839d0b54370af70f06b3f097897de457.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ました。これは、隣接するレイヤーが互いに完全に接続されているネットワークを使用して行いました。つまり、ネットワークの各ニューロンは隣接するレイヤーの各ニューロンに関連付けられていました</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/248/73a/b05/24873ab052991e684b9ff0650c11a1c4.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
。特に、画像の各ピクセルの強度を入力レイヤーの対応するニューロンの値としてエンコードしました。サイズが28x28ピクセルの画像の場合、これはネットワークに784（= 28×28）の入力ニューロンがあることを意味します。次に、ネットワークの重みとオフセットをトレーニングして、出力でネットワーク（そのような希望があった）が入力画像（0、1、2、...、8、または9）を正しく識別できるようにしました。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
初期のネットワークは非常にうまく機能しています。MNIST手書き数字からのトレーニングおよび検証データを使用して、98％を超える分類精度を達成しました。しかし、この状況を今評価すると、完全に接続されたレイヤーを持つネットワークを使用して画像を分類するのは奇妙に思えます。実際、そのようなネットワークは画像の空間構造を考慮に入れていません。たとえば、互いに遠くにあるピクセルだけでなく、隣接するピクセルにもまったく同じように適用されます。そのような空間構造の概念についての結論は、トレーニングデータの研究に基づいて行われるべきであると想定されています。しかし、ネットワーク構造をゼロから開始する代わりに、アーキテクチャを使用するとどうなるでしょうか。空間構造を利用しようとしていますか？このセクションでは、畳み込みニューラルネットワーク（SNA）について説明します。それらは特別なアーキテクチャを使用しており、特に画像の分類に適しています。このようなアーキテクチャーの使用により、SNAはより速く学習します。これは、画像を分類するための優れた階層型ネットワークのトレーニングに役立ちます。今日、画像認識のほとんどの場合、ディープSNAまたはそれらに近いバリアントが使用されています。今日、画像認識のほとんどの場合、ディープSNAまたはそれらに近いバリアントが使用されています。今日、画像認識のほとんどの場合、ディープSNAまたはそれらに近いバリアントが使用されています。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
SNAの起源は1970年代にさかのぼります。しかし、彼らの現代的な配布を開始した最初の作品は1998年の作品「</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ドキュメントを認識するための勾配学習</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">」でした。 Lekunは興味深い</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">発言をしました</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SNAで使用されている用語について：「たたみ込みネットワークなどのモデルと神経生物学の関係は表面的なものです。</font><font style="vertical-align: inherit;">したがって、私はそれらを畳み込みニューラルネットワークではなく、畳み込みネットワークと呼びます。したがって、ニューロンではなくノード要素と呼びます。</font><font style="vertical-align: inherit;">しかし、これにも関わらず、SNAは逆伝播、勾配降下、正則化、非線形活性化関数など、すでに研究したNSの世界からの多くのアイデアを使用しています。</font><font style="vertical-align: inherit;">したがって、一般的に受け入れられている契約に従い、それらを一種のNAと見なします。</font><font style="vertical-align: inherit;">私は、それらをネットワークとニューラルネットワークの両方、およびそれらのノード（ニューロンと要素の両方）と呼びます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
SNAは、3つの基本的な考え方を使用します。それは、局所受容野、総重み、およびプーリングです。</font><font style="vertical-align: inherit;">これらのアイデアを順に見ていきましょう。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">地域の受容野</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
完全に接続されたネットワーク層では、入力層はニューロンの垂直線で示されます。 SNAでは、28x28の次元を持つニューロンの正方形として入力層を表す方が便利です。その値は、28x28の画像のピクセルの強度に対応します。</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/da3/848/9d0/da38489d04325743131546e76f99396d.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
通常は、着信ピクセルを非表示のニューロンの層に関連付けます。ただし、すべてのピクセルをすべての隠れたニューロンに関連付けるわけではありません。着信画像の小さな局所領域でコミュニケーションを編成します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
より正確には、最初の非表示層の各ニューロンは、入ってくるニューロンの小さな部分、たとえば25個の入ってくるピクセルに対応する5x5の領域に関連付けられます。したがって、隠れたニューロンの場合、接続は次のようになります。</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/cf9/71d/5dc/cf971d5dc7106f1c56832c8416d7847a.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
入力画像のこの部分は、この隠れたニューロンのローカル受容野と呼ばれます。これは、着信ピクセルを見る小さなウィンドウです。各結合は、それ自体の重みを学習します。また、隠れたニューロンは一般的な変位を研究します。この特定のニューロンは、その特定の局所受容野を分析することを学んでいると想定できます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
次に、ローカルの受容野を入力画像全体に移動します。各ローカル受容野には、最初の非表示層に独自の非表示ニューロンがあります。より具体的な例として、左上隅のローカル受容野から始めましょう：</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/c41/5cd/64d/c415cd64dfc93b81b89395ae360026c1.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ローカル受容野を1ピクセル右（1ニューロン）に移動して、2番目の非表示ニューロンに関連付けます。</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/db1/285/a7a/db1285a7a7009e0210e97253061054f3.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
それで、最初の隠れ層を構築します。入力画像が28x28でローカルの受容野が5x5の場合、非表示層には24x24のニューロンがあることに注意してください。これは、ローカルの受容野を23ニューロンだけ右（または下）に移動できるため、入力画像の右（または下）側に出会うためです。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
この例では、局所受容野は一度に1ピクセル移動します。</font><font style="vertical-align: inherit;">ただし、異なるステップサイズが使用される場合もあります。</font><font style="vertical-align: inherit;">たとえば、ローカル受容野を2ピクセル横にシフトできます。この場合、ステップ2のサイズについて説明できます。この章では、主にステップ1を使用しますが、異なるサイズのステップで実験が行われることもあります。 。</font><font style="vertical-align: inherit;">他のハイパーパラメーターと同様に、ステップサイズを試すことができます。</font><font style="vertical-align: inherit;">ローカル受容野のサイズを変更することもできますが、通常、ローカル受容野のサイズが大きいほど、28x28ピクセルよりも大幅に大きい画像に適しています。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">総重量とオフセット</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
それぞれの隠れたニューロンには、その局所受容野に関連するオフセットと5x5の重みがあると述べました。</font><font style="vertical-align: inherit;">しかし、24x24のすべての隠しニューロンに同じ重みと変位を使用することについては触れませんでした。</font><font style="vertical-align: inherit;">つまり、隠れたニューロンj、kの場合、出力は次のようになります。</font></font><br>
<br>
<p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display"><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mtable displaystyle=&quot;true&quot;><mlabeledtr><mtd id=&quot;mjx-eqn-125&quot;><mtext>(125)</mtext></mtd><mtd><mi>&amp;#x03C3;</mi><mrow><mo>(</mo><mrow><mi>b</mi><mo>+</mo><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>l</mi><mo>=</mo><mn>0</mn></mrow><mn>4</mn></munderover><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>m</mi><mo>=</mo><mn>0</mn></mrow><mn>4</mn></munderover><msub><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>l</mi><mo>,</mo><mi>m</mi></mrow></msub><msub><mi>a</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>j</mi><mo>+</mo><mi>l</mi><mo>,</mo><mi>k</mi><mo>+</mo><mi>m</mi></mrow></msub></mrow><mo>)</mo></mrow></mtd></mlabeledtr></mtable></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="97.25ex" height="7.384ex" viewBox="0 -1831.9 41871.4 3179.3" role="img" focusable="false" style="vertical-align: -3.13ex; max-width: 778px;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g transform="translate(39590,0)"><g id="mjx-eqn-125"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhi1Txql4KFVyEtF323rNon-dwUGZg#MJMAIN-28"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhi1Txql4KFVyEtF323rNon-dwUGZg#MJMAIN-31" x="389" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhi1Txql4KFVyEtF323rNon-dwUGZg#MJMAIN-32" x="890" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhi1Txql4KFVyEtF323rNon-dwUGZg#MJMAIN-35" x="1390" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhi1Txql4KFVyEtF323rNon-dwUGZg#MJMAIN-29" x="1891" y="0"></use></g></g><g transform="translate(14836,0)"><g transform="translate(-14,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhi1Txql4KFVyEtF323rNon-dwUGZg#MJMATHI-3C3" x="0" y="0"></use><g transform="translate(739,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhi1Txql4KFVyEtF323rNon-dwUGZg#MJSZ4-28"></use><g transform="translate(792,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhi1Txql4KFVyEtF323rNon-dwUGZg#MJMATHI-62" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhi1Txql4KFVyEtF323rNon-dwUGZg#MJMAIN-2B" x="651" y="0"></use><g transform="translate(1652,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhi1Txql4KFVyEtF323rNon-dwUGZg#MJSZ2-2211" x="0" y="0"></use><g transform="translate(164,-1110)"><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhi1Txql4KFVyEtF323rNon-dwUGZg#MJMATHI-6C" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhi1Txql4KFVyEtF323rNon-dwUGZg#MJMAIN-3D" x="298" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhi1Txql4KFVyEtF323rNon-dwUGZg#MJMAIN-30" x="1077" y="0"></use></g><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhi1Txql4KFVyEtF323rNon-dwUGZg#MJMAIN-34" x="771" y="1627"></use></g><g transform="translate(3263,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhi1Txql4KFVyEtF323rNon-dwUGZg#MJSZ2-2211" x="40" y="0"></use><g transform="translate(0,-1090)"><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhi1Txql4KFVyEtF323rNon-dwUGZg#MJMATHI-6D" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhi1Txql4KFVyEtF323rNon-dwUGZg#MJMAIN-3D" x="878" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhi1Txql4KFVyEtF323rNon-dwUGZg#MJMAIN-30" x="1657" y="0"></use></g><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhi1Txql4KFVyEtF323rNon-dwUGZg#MJMAIN-34" x="828" y="1627"></use></g><g transform="translate(4955,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhi1Txql4KFVyEtF323rNon-dwUGZg#MJMATHI-77" x="0" y="0"></use><g transform="translate(716,-150)"><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhi1Txql4KFVyEtF323rNon-dwUGZg#MJMATHI-6C" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhi1Txql4KFVyEtF323rNon-dwUGZg#MJMAIN-2C" x="298" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhi1Txql4KFVyEtF323rNon-dwUGZg#MJMATHI-6D" x="577" y="0"></use></g></g><g transform="translate(6801,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhi1Txql4KFVyEtF323rNon-dwUGZg#MJMATHI-61" x="0" y="0"></use><g transform="translate(529,-150)"><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhi1Txql4KFVyEtF323rNon-dwUGZg#MJMATHI-6A" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhi1Txql4KFVyEtF323rNon-dwUGZg#MJMAIN-2B" x="412" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhi1Txql4KFVyEtF323rNon-dwUGZg#MJMATHI-6C" x="1191" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhi1Txql4KFVyEtF323rNon-dwUGZg#MJMAIN-2C" x="1489" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhi1Txql4KFVyEtF323rNon-dwUGZg#MJMATHI-6B" x="1768" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhi1Txql4KFVyEtF323rNon-dwUGZg#MJMAIN-2B" x="2289" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhi1Txql4KFVyEtF323rNon-dwUGZg#MJMATHI-6D" x="3068" y="0"></use></g></g></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhi1Txql4KFVyEtF323rNon-dwUGZg#MJSZ4-29" x="11014" y="0"></use></g></g></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable displaystyle="true"><mlabeledtr><mtd id="mjx-eqn-125"><mtext><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">（125）</font></font></mtext></mtd><mtd><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">σ</font></font></mi><mrow><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">（</font></font></mo><mrow><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">b</font></font></mi><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+</font></font></mo><munderover><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Σ</font></font></mo><mrow class="MJX-TeXAtom-ORD"><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l</font></font></mi><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">=</font></font></mo><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0</font></font></mn></mrow><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">4</font></font></mn></munderover><munderover><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Σ</font></font></mo><mrow class="MJX-TeXAtom-ORD"><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">メートル</font></font></mi><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">=</font></font></mo><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0</font></font></mn></mrow><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">4</font></font></mn></munderover><msub><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w</font></font></mi><mrow class="MJX-TeXAtom-ORD"><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l</font></font></mi><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、</font></font></mo><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">メートル</font></font></mi></mrow></msub><msub><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">a</font></font></mi><mrow class="MJX-TeXAtom-ORD"><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></mi><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+</font></font></mo><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l</font></font></mi><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、</font></font></mo><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">k</font></font></mi><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+</font></font></mo><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">メートル</font></font></mi></mrow></msub></mrow><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">）</font></font></mo></mrow></mtd></mlabeledtr></mtable></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-1"> \sigma\left(b + \sum_{l=0}^4 \sum_{m=0}^4 w_{l,m} a_{j+l, k+m} \right) \tag{125} </script></p><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ここで、σは活性化関数であり、おそらく前の章のシグモイドです。</font><font style="vertical-align: inherit;">bは合計オフセット値です。</font><font style="vertical-align: inherit;">w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l、m-</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">合計ウェイト5x5の配列。</font><font style="vertical-align: inherit;">そして最後に、a </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x、y</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">は位置x、yでの入力アクティブ化を示します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
つまり、最初の非表示層のすべてのニューロンは、画像の異なる部分にある同じ標識を検出します。非表示のニューロンによって検出された兆候は、ニューロンの活性化につながる特定の入力シーケンスです。おそらく、画像のエッジまたは何らかのフォームです。これが理にかなっている理由を理解するために、私たちの重みと変位が、隠れたニューロンが特定の局所受容野の垂直面などを認識できるようなものであると仮定します。この機能は、画像の他の場所で役立つ可能性があります。したがって、画像領域全体で同じ特徴検出器を使用すると便利です。より抽象的には、SNAは画像の並進不変性によく適合しています。たとえば、猫の画像を少し横に動かしても、猫の画像のままです。真実、MNISTディジット分類問題からの画像はすべて中央に配置され、サイズが正規化されます。したがって、MNISTはランダムな画像よりも並進不変性が少なくなります。それでも、顔や角度などの機能は、入力画像の表面全体に役立つ可能性があります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
このため、着信レイヤーと非表示レイヤーのマッピングを機能マップと呼ぶことがあります。特性マップを定義する重みは、総重みと呼ばれます。そして、特徴マップを定義するバイアスは一般的なバイアスです。総重量と変位がカーネルまたはフィルターを決定するとよく言われます。しかし、文学では人々がこれらの用語をわずかに異なる理由で使用することがあるので、私は専門用語に深く入りません。より具体的な例をいくつか見てみましょう。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
私が説明するネットワーク構造は、1つの種の局所的な属性のみを認識することができます。画像を認識するには、より多くの機能マップが必要です。したがって、完成した畳み込み層は、いくつかの異なる機能マップで構成されます。</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/f84/5df/a57/f845dfa572668e27590c5bd1c057f849.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
この例は、3つの機能マップを示しています。各カードは、5x5の合計ウェイトのセットと1つの共通オフセットによって決定されます。その結果、このようなネットワークは3つの異なるタイプの標識を認識でき、各標識は画像の任意の部分で見つけることができます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
わかりやすくするために、3つのフィーチャーカードを描画しました。実際には、SNAはより多くの（場合によってはさらに多くの）フィーチャー・マップを使用できます。初期のSNSの1つであるLeNet-5は、MNISTディジットを認識するために、それぞれが5x5の受信フィールドに関連付けられた6つのフィーチャカードを使用していました。したがって、上記の例はLeNet-5とよく似ています。独自にさらに開発する例では、20枚と40枚のフィーチャカードを含むたたみ込み層を使用します。検討する兆候を簡単に見てみましょう。</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/fad/a16/6b7/fada166b767b58edbff262944ee6488b.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
これらの20個の画像は、20個の異なる属性マップ（フィルター、またはカーネル）に対応しています。各カードは、ローカルの受容野の5x5の重みに対応する5x5の画像で表されます。白いピクセルは低い（通常はより負の）重みを意味し、機能マップは対応するピクセルに対してあまり反応しません。暗いピクセルはより多くの重みを意味し、機能マップは対応するピクセルにより多く反応します。大まかに言えば、これらの画像は、たたみ込み層が応答する兆候を示しています。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
これらの属性マップからどのような結論を導き出すことができますか？ここの空間構造は、明らかに、ランダムな方法では表示されませんでした-多くの標識は、明確な明るい領域と暗い領域を示しています。これは、私たちのネットワークが実際に空間構造に関連する何かを学んでいることを示唆しています。しかし、これに加えて、これらの兆候が何であるかを理解することはかなり困難です。たとえば、</font><font style="vertical-align: inherit;">パターン認識への多くの従来のアプローチで使用されていた</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ガボールフィルター</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">などは明らかに研究していません</font><font style="vertical-align: inherit;">。実際、SNAがどの兆候を研究しているかを正確に理解するために、現在多くの作業が行われています。興味をお持ちの方は、</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2013年</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">から開始することをお勧めし</font><font style="vertical-align: inherit;">ます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
一般的な重みとオフセットの大きな利点は、これによりSNAで使用可能なパラメーターの数が大幅に減少することです。各フィーチャマップには、5×5 = 25の合計ウェイトと1つの共通オフセットが必要です。したがって、機能マップごとに26個のパラメーターが必要です。 20個の特徴マップがある場合、畳み込み層を定義する20×26 = 520パラメータがあります。比較のために、28×28 = 784の入力ニューロンと比較的控えめな30の隠れニューロンを持つ完全に接続された第1層があると仮定します。以前の多くの例でこのスキームを使用しました。 784×30の重み、30のオフセット、合計23,550のパラメーターになります。つまり、完全に接続されたレイヤーには、畳み込みレイヤーよりも40倍以上多くのパラメーターがあります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
もちろん、これらの2つのモデルは根本的に異なるため、パラメーターの数を直接比較することはできません。しかし、直観的には、たたみ込み並進不変性を使用すると、完全に接続されたモデルに匹敵する効率を達成するために必要なパラメーターの数が減少するようです。そして、これは畳み込みモデルのトレーニングを加速し、最終的には畳み込み層を使用してより深いネットワークを作成するのに役立ちます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ちなみに、「たたみ込み」という名前は、式（125）の演算に由来し、</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">たたみ込み</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">と呼ばれることもあり</font><font style="vertical-align: inherit;">ます。より正確には、人々はこの方程式をa </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> =σ（b + w ∗ a </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">）と</font><font style="vertical-align: inherit;">書くことがあります</font><font style="vertical-align: inherit;">。ここで、a </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">は1つの属性マップの出力アクティベーションのセット、a</font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">は入力アクティベーションのセットで、*は畳み込み演算と呼ばれます。</font><font style="vertical-align: inherit;">畳み込みの数学について深く掘り下げることはしないので、この関係について特に心配する必要はありません。</font><font style="vertical-align: inherit;">しかし、名前がどこから来たのかを知るだけの価値があります。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">プーリング層</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
SNAで説明されているたたみ込み層に加えて、プーリング層もあります。</font><font style="vertical-align: inherit;">通常、畳み込みの直後に使用されます。</font><font style="vertical-align: inherit;">彼らは、たたみ込み層の出力からの情報を単純化することに専念しています。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ここでは、「機能マップ」という語句を、畳み込み層によって計算された関数の意味ではなく、隠れ層ニューロンの出力のアクティブ化を示すために使用します。</font><font style="vertical-align: inherit;">このような用語の自由な使用は、研究文献によく見られます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
プーリングレイヤーは各畳み込みレイヤーフィーチャマップの出力を受け入れ、圧縮されたフィーチャマップを準備します。たとえば、プーリングレイヤーの各要素は、たとえば、前のレイヤーの2x2ニューロンのセクションを要約できます。ケーススタディ：1つの一般的なプーリング手順は、最大プーリングとして知られています。 max-poolingでは、図に示すように、プーリング要素は単に2x2セクションから最大のアクティブ化</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/dd0/f6b/86c/dd0f6b86c374504de4ae58056a0f7008.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
を提供します。たたみ込み層ニューロンの出力は24x24の値を提供するため、プーリングの後、12x12ニューロンを取得します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
上記のように、畳み込み層は通常、単一の特徴マップ以上のものを意味します。各フィーチャマップに個別に最大プーリングを適用します。したがって、3つのフィーチャカードがある場合、コンボリューションレイヤーと最大プーリングレイヤーの組み合わせは次のようになります。</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/a95/68f/8d1/a9568f8d10dd7dced2f682fe259aed48.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Max-pullingは、画像の任意の場所に特定の標識があるかどうかを尋ねるネットワークの方法として想像できます。そして、その正確な位置に関する情報を破棄します。標識が見つかると、その正確な位置は、他の標識と比較したおおよその位置ほど重要ではなくなります。利点は、プールによって取得されるフィーチャの数がはるかに少ないことです。これにより、次のレイヤーで必要なパラメーターの数を減らすことができます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
最大プーリングは、唯一のプーリング技術ではありません。</font><font style="vertical-align: inherit;">別の一般的なアプローチは、L2プーリングとして知られています。</font><font style="vertical-align: inherit;">その中で、ニューロンの2x2領域の最大活性化をとる代わりに、2x2領域の活性化の二乗の合計の平方根をとります。</font><font style="vertical-align: inherit;">アプローチの詳細は異なりますが、直感的にはmax-poolingに似ています。L2プーリングは、畳み込み層からの情報を圧縮する方法です。</font><font style="vertical-align: inherit;">実際には、両方のテクノロジーがよく使用されます。</font><font style="vertical-align: inherit;">時々人々は他のタイプのプーリングを使用します。</font><font style="vertical-align: inherit;">ネットワークの品質を最適化するのに苦労している場合は、サポートデータを使用して、プルへのいくつかの異なるアプローチを比較し、最適なアプローチを選択できます。</font><font style="vertical-align: inherit;">ただし、このような詳細な最適化については心配しません。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">まとめ</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
これで、すべての情報をまとめて、完全なSNAを取得できます。これは、最近レビューしたアーキテクチャに似ていますが、MNIST桁の可能な10個の値（「0」、「1」、「2」、..）に対応する10個の出力ニューロンの追加レイヤーがあります：</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/b2d/ba1/8ee/b2dba18ee40b3f642fb9f4e9cbda772b.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ネットワークは、使用される28x28入力ニューロンから始まりますMNIST画像のピクセル強度をエンコードします。その後、ローカルの受容野5x5および3の特徴マップを使用する畳み込み層が続きます。結果は、標識の3x24x24の隠れたニューロンの層です。次のステップは、3つのフィーチャマップのそれぞれの2x2領域に適用される最大プーリングレイヤーです。結果は、3x12x12の隠れた特性ニューロンのレイヤーです。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ネットワークの最後の接続層は完全に接続されています。つまり、最大プーリング層の各ニューロンを10個の出力ニューロンのそれぞれに接続します。このような完全に接続されたアーキテクチャを以前に使用しました。上の図では、簡単にするために、すべてのリンクを表示せずに1つの矢印を使用していることに注意してください。あなたはそれらすべてを簡単に想像することができます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
この畳み込みアーキテクチャは、以前使用していたものとは大きく異なります。ただし、全体像は似ています。多くの単純な要素で構成されるネットワークであり、その動作は重みとオフセットによって決まります。目標は同じです。トレーニングデータを使用してネットワークを重みとオフセットでトレーニングし、ネットワークが着信番号を適切に分類できるようにします。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
特に、前の章と同様に、確率的勾配降下法と逆伝播を使用してネットワークをトレーニングします。</font><font style="vertical-align: inherit;">手順は以前とほとんど同じです。</font><font style="vertical-align: inherit;">ただし、逆伝播手順にいくつかの変更を加える必要があります。</font><font style="vertical-align: inherit;">実際のところ、バックプロパゲーションの派生物は、完全に接続されたレイヤーを持つネットワークを対象としています。</font><font style="vertical-align: inherit;">幸いなことに、畳み込み層と最大プーリング層の微分を変更するのは非常に簡単です。</font><font style="vertical-align: inherit;">詳細を知りたい方は、以下の問題を解決してみてください。</font><font style="vertical-align: inherit;">バックプロパゲーションを区別することの初期の問題を完全に理解していない限り、時間がかかることを警告します。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">仕事</font></font></h3><br>
<ul>
<li>    .            (BP1)-(BP4). ,     ,  -     ,     .      ?</li>
</ul><br>
<h2>    </h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
SNAの背後にあるアイデアについて話し合いました。いくつかのSNAを実装し、それらをMNIST桁分類問題に適用することにより、実際にどのように機能するかを見てみましょう。前の章で作成したnetwork.pyおよびnetwork2.pyプログラムの改良版であるnetwork3.pyプログラムを使用します。 network3.pyプログラムは、</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Misha Denil</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">と</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">Chris Olah</font></a><font style="vertical-align: inherit;">から</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">の例外</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">の</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">実装</font></a><font style="vertical-align: inherit;">から、</font><font style="vertical-align: inherit;">Theanoライブラリドキュメント（特に</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">LeNet-5 </font></a><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">実装</font></a><font style="vertical-align: inherit;">）の</font><font style="vertical-align: inherit;">アイデアを使用し</font><font style="vertical-align: inherit;">ます。プログラムコードはGitHubで入手できます。次のセクションでは、network3.pyプログラムのコードを調べ、このセクションでは、SNAを作成するためのライブラリーとして使用します。</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
network.pyおよびnetwork2.pyプログラムは、Numpyマトリックスライブラリを使用してPythonで作成されました。彼らは第一原理に基づいて働き、逆伝播、確率的勾配降下法などの最も詳細な詳細に達しました。しかし、今、これらの詳細を理解したら、network3.pyにTheano機械学習ライブラリを使用します（</font><font style="vertical-align: inherit;">その説明付き</font><font style="vertical-align: inherit;">の</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">科学研究</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">を</font><font style="vertical-align: inherit;">参照してください</font><font style="vertical-align: inherit;">）。 Theanoも人気NS用のライブラリの基本で</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pylearn2</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">と</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Keras</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">だけでなく、</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">カフェ</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">や</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">トーチ</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Theanoを使用すると、すべてのカードが自動的にカウントされるため、SNAでのバックプロパゲーションの実装が容易になります。 Theanoは、以前のコード（理解を容易にするために作成されたもので、高速作業用ではありません）よりも著しく高速であるため、より複雑なネットワークのトレーニングに使用するのが妥当です。特に、Theanoの優れた機能の1つは、可能であれば、CPUとGPUの両方でコードを実行することです。 GPUで実行すると、速度が大幅に向上し、より複雑なネットワークのトレーニングに役立ちます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
本と並行して作業するには、システムにTheanoをインストールする必要があります。これを行うに</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">は、プロジェクトのホームページの</font></a><font style="vertical-align: inherit;">指示に従って</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ください</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。例を書いて公開した時点では、Theano 0.7が利用可能でした。 Mac OS X YosemiteでGPUを使用せずにいくつかの実験を行いました。 NVIDIA GPUを搭載したUbuntu 14.04の一部。そして、いくつかはそこにあります。 network3.pyを開始するには、コードのGPUフラグをTrueまたはFalseに設定します。さらに、</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">次の手順</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">は、GPUでTheanoを実行するのに役立ちます</font><font style="vertical-align: inherit;">。オンラインでトレーニング資料を見つけるのも簡単です。独自のGPUがない場合は、</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">Amazon Web Services EC2 G2に</font></a><font style="vertical-align: inherit;">目を向けることができます</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。しかし、GPUを使用しても、コードはすぐには機能しません。多くの実験は数分から数時間になります。単一のCPUで最も複雑なものは、数日間実行されます。前の章と同様に、実験を開始し、定期的に動作を確認しながら読み続けることをお勧めします。 GPUを使用せずに、最も複雑な実験の場合は、トレーニング期間の数を減らすことをお勧めします。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
比較のための基本的な結果を得るために、100の隠れたニューロンを含む1つの隠れた層を持つ浅いアーキテクチャから始めましょう。 60年代を学習し、学習速度η= 0.1、ミニパッケージのサイズ10を使用し、正則化せずに学習します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
このセクションでは、特定の数のトレーニング時代を設定します。これは、学習プロセスをわかりやすくするために行います。実際には、早期停止を使用して確認セットの精度を追跡し、確認の精度が向上していないと確信したときにトレーニングを停止すると便利です。</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> network3
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> network3 <span class="hljs-keyword">import</span> Network
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> network3 <span class="hljs-keyword">import</span> ConvPoolLayer, FullyConnectedLayer, SoftmaxLayer
<span class="hljs-meta">&gt;&gt;&gt; </span>training_data, validation_data, test_data = network3.load_data_shared()
<span class="hljs-meta">&gt;&gt;&gt; </span>mini_batch_size = <span class="hljs-number">10</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>net = Network([<font></font>
        FullyConnectedLayer(n_in=<span class="hljs-number">784</span>, n_out=<span class="hljs-number">100</span>),<font></font>
        SoftmaxLayer(n_in=<span class="hljs-number">100</span>, n_out=<span class="hljs-number">10</span>)], mini_batch_size)
<span class="hljs-meta">&gt;&gt;&gt; </span>net.SGD(training_data, <span class="hljs-number">60</span>, mini_batch_size, <span class="hljs-number">0.1</span>, <font></font>
            validation_data, test_data)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
最高の分類精度は97.80％でした。これは、validation_dataからのデータの最高の分類精度を得たトレーニング時代から推定された分類精度test_dataです。裏付けとなる証拠を使用して精度評価に関する決定を行うと、再トレーニングを回避するのに役立ちます。その後、そうします。ネットワークの重みとオフセットはランダムに初期化されるため、結果は若干異なる場合があります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
97.80％の精度は、同様のネットワークアーキテクチャとトレーニングハイパーパラメータを使用して、第3章で取得した98.04％の精度にかなり近いです。特に、どちらの例でも、100個の隠れたニューロンを含む1つの隠れた層を持つ浅いネットワークを使用しています。どちらのネットワークも60の時代を学習し、ミニパケットサイズは10、学習率はη= 0.1です。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ただし、以前のネットワークには2つの違いがありました。最初に、再トレーニングの影響を減らすために、正規化を実行しました。現在のネットワークを正則化すると精度は向上しますが、それほど改善されないため、ここではそれについては考えません。第二に、初期のネットワークの最後の層は、シグモイドアクティベーションとクロスエントロピーコスト関数を使用しましたが、現在のネットワークは、ソフトマックスの最後の層と、コスト関数として対数尤度関数を使用しています。第3章で説明したように、これは大きな変更ではありません。いくつかの深い理由から、私は一方からもう一方に切り替えませんでした。主に、ソフトマックスと対数尤度関数が画像を分類するために現代のネットワークでより頻繁に使用されているためです。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
より深いネットワークアーキテクチャを使用して結果を改善できますか？</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
まず、ネットワークの最初にたたみ込み層を挿入します。ローカル受容フィールド5x5、ステップ長1および20フィーチャーカードを使用します。また、2x2プーリングウィンドウを使用して、機能を組み合わせた最大プーリングレイヤーを挿入します。したがって、全体的なネットワークアーキテクチャは、前のセクションで説明したものと似ていますが、完全に接続されたレイヤーが追加されています</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/7ca/178/8d2/7ca1788d2206313b37a6f8896086b582.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
。このアーキテクチャでは、畳み込みレイヤーとプーリングレイヤーは、受信トレーニング画像に含まれるローカル空間構造でトレーニングされ、最後の完全に接続されたレイヤーはすでにトレーニングされています画像周辺のグローバル情報を統合することにより、より抽象的なレベル。これは、SNAで一般的に使用されるスキームです。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
そのようなネットワークをトレーニングして、それがどのように動作するかを見てみましょう。</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-meta">&gt;&gt;&gt; </span>net = Network([<font></font>
        ConvPoolLayer(image_shape=(mini_batch_size, <span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>), <font></font>
                      filter_shape=(<span class="hljs-number">20</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>), <font></font>
                      poolsize=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)),<font></font>
        FullyConnectedLayer(n_in=<span class="hljs-number">20</span>*<span class="hljs-number">12</span>*<span class="hljs-number">12</span>, n_out=<span class="hljs-number">100</span>),<font></font>
        SoftmaxLayer(n_in=<span class="hljs-number">100</span>, n_out=<span class="hljs-number">10</span>)], mini_batch_size)
<span class="hljs-meta">&gt;&gt;&gt; </span>net.SGD(training_data, <span class="hljs-number">60</span>, mini_batch_size, <span class="hljs-number">0.1</span>, <font></font>
            validation_data, test_data)   </code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
精度は98.78％で、これまでのどの結果よりも大幅に高くなっています。</font><font style="vertical-align: inherit;">エラーを3分の1以上減らしました-優れた結果です。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ネットワーク構造につ​​いて説明すると、畳み込み層とプーリング層を1つの層と見なしました。</font><font style="vertical-align: inherit;">それらを個別のレイヤー、または単一のレイヤーと見なす-好みの問題。</font><font style="vertical-align: inherit;">コードがよりコンパクトであるため、network3.pyはそれらを1つのレイヤーと見なします。</font><font style="vertical-align: inherit;">ただし、レイヤーを個別に設定できるようにnetwork3.pyを変更するのは簡単です。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">運動</font></font></h3><br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">完全に接続されたレイヤーを下げて、畳み込み/プールレイヤーとソフトマックスレイヤーのみを使用すると、どのような分類精度が得られますか？</font><font style="vertical-align: inherit;">完全に接続されたレイヤーを含めることは役立ちますか？</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
結果を98.78％改善できますか？</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
2番目の畳み込み/プーリング層を挿入してみましょう。既存の畳み込み/プーリングと完全に接続された非表示レイヤーの間に挿入します。ローカルの5x5受容フィールドと2x2セクションのプールを再び使用します。以前とほぼ同じハイパーパラメータでネットワークをトレーニングするとどうなるか見てみましょう。</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-meta">&gt;&gt;&gt; </span>net = Network([<font></font>
        ConvPoolLayer(image_shape=(mini_batch_size, <span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>), <font></font>
                      filter_shape=(<span class="hljs-number">20</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>), <font></font>
                      poolsize=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)),<font></font>
        ConvPoolLayer(image_shape=(mini_batch_size, <span class="hljs-number">20</span>, <span class="hljs-number">12</span>, <span class="hljs-number">12</span>), <font></font>
                      filter_shape=(<span class="hljs-number">40</span>, <span class="hljs-number">20</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>), <font></font>
                      poolsize=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)),<font></font>
        FullyConnectedLayer(n_in=<span class="hljs-number">40</span>*<span class="hljs-number">4</span>*<span class="hljs-number">4</span>, n_out=<span class="hljs-number">100</span>),<font></font>
        SoftmaxLayer(n_in=<span class="hljs-number">100</span>, n_out=<span class="hljs-number">10</span>)], mini_batch_size)
<span class="hljs-meta">&gt;&gt;&gt; </span>net.SGD(training_data, <span class="hljs-number">60</span>, mini_batch_size, <span class="hljs-number">0.1</span>, <font></font>
            validation_data, test_data)      </code></pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
繰り返しになりますが、改善が行われました。精度は99.06％になりました。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
現時点では、2つの自然な疑問が生じます。まず、2番目の畳み込み/プーリングレイヤーを使用するとはどういう意味ですか？ 2番目のコンボリューション/プーリングレイヤーでは、「12x12」の画像が入力されると想定できます。その「ピクセル」は、元の入力画像に特定のローカライズされた機能の存在（または不在）を表します。つまり、元の入力画像の特定のバージョンがこのレイヤーの入力に入ると想定できます。これはより抽象的で簡潔なバージョンになりますが、それでも十分な空間構造があるため、2番目の畳み込み/プルレイヤーを使用して処理することは理にかなっています。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
快適な見方ですが、2つ目の質問があります。前のレイヤーからの出力では、20の個別のCPが取得されるため、入力データの20x12x12グループが2番目の畳み込み/プーリングレイヤーに送られます。最初の畳み込み/プールレイヤーの場合とは異なり、畳み込み/プールレイヤーには20の個別の画像があり、1つの画像ではないことがわかりました。 2番目の畳み込み/プールレイヤーのニューロンは、これらの着信画像の多数にどのように応答する必要がありますか？実際、この層の各ニューロンは、ローカルの受容野に入るすべての20x5x5ニューロンに基づいてトレーニングすることができます。あまり正式でない言語では、2番目の畳み込み/プールレイヤーの特徴検出器は、最初のレイヤーのすべての特徴にアクセスできますが、特定のローカル受容フィールド内にのみアクセスできます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ちなみに、このような問題は、画像がカラーの場合、第1層で発生したと考えられます。</font><font style="vertical-align: inherit;">この場合、元の画像の赤、緑、青のチャネルに対応する、ピクセルごとに3つの入力属性があります。</font><font style="vertical-align: inherit;">そして、標識検出器にすべての色情報へのアクセスを許可しますが、その地域の受容野の枠組み内のみです。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">仕事</font></font></h3><br>
<ul>
<li>      .             ,   tanh,  ,       ,  .      ,         .        tanh    .     -      (   activation_fn=tanh    ConvPoolLayer  FullyConnectedLayer).     ,     ,    20 ,   60.    ?  ,    60- ?            ,   60- .       ,  ,        ,      .  ,   ?           – ,       (,  σ(z)=(1+tanh(z/2))/2)?  -     , ,     . ,    .     -      ,      , , ,   .   ,           ,          .</li>
</ul><br>
<h3>   </h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
私たちが現在開発しているネットワーク</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">は、1998年</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">の</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">実りある研究で</font></a><font style="vertical-align: inherit;">使用されたネットワークオプションの1つであり、</font><font style="vertical-align: inherit;">MNISTのタスクであるLeNet-5と呼ばれるネットワークが最初に提示されました。これは、問題と直観の理解を深めるために、さらなる実験の良い基礎となります。特に、結果を改善する方法を求めてネットワークを変更する方法はたくさんあります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
まず、ニューロンを変更して、シグモイド活動化関数を使用する代わりに、直線化された線形要素（ReLU）を使用できるようにします。つまり、f（z）≡max（0、z）の形式の活性化関数を使用します。 60時代のネットワークを、η= 0.03の速度でトレーニングします。また、正則化パラメーターλ= 0.1でL2正則化を使用する方が少し便利であることもわかりました。</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> network3 <span class="hljs-keyword">import</span> ReLU
<span class="hljs-meta">&gt;&gt;&gt; </span>net = Network([<font></font>
        ConvPoolLayer(image_shape=(mini_batch_size, <span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>), <font></font>
                      filter_shape=(<span class="hljs-number">20</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>), <font></font>
                      poolsize=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), <font></font>
                      activation_fn=ReLU),<font></font>
        ConvPoolLayer(image_shape=(mini_batch_size, <span class="hljs-number">20</span>, <span class="hljs-number">12</span>, <span class="hljs-number">12</span>), <font></font>
                      filter_shape=(<span class="hljs-number">40</span>, <span class="hljs-number">20</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>), <font></font>
                      poolsize=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), <font></font>
                      activation_fn=ReLU),<font></font>
        FullyConnectedLayer(n_in=<span class="hljs-number">40</span>*<span class="hljs-number">4</span>*<span class="hljs-number">4</span>, n_out=<span class="hljs-number">100</span>, activation_fn=ReLU),<font></font>
        SoftmaxLayer(n_in=<span class="hljs-number">100</span>, n_out=<span class="hljs-number">10</span>)], mini_batch_size)
<span class="hljs-meta">&gt;&gt;&gt; </span>net.SGD(training_data, <span class="hljs-number">60</span>, mini_batch_size, <span class="hljs-number">0.03</span>, <font></font>
            validation_data, test_data, lmbda=<span class="hljs-number">0.1</span>)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
分類精度は99.23％でした。シグモイド結果よりもわずかな改善（99.06％）。ただし、すべての実験で、ReLUに基づくネットワークは、シグモイド活性化関数に基づくネットワークよりも優れており、不変性はありません。明らかに、この問題を解決するためにReLUに切り替えることには本当の利点があります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ReLUアクティベーション機能がシグモイドまたは双曲線正接よりも優れている理由は何ですか？現時点では、特にわかりません。シグモイドニューロンとは対照的に、関数max（0、z）は大きなzでは飽和しないと通常言われています。これは、ReLUニューロンが学習を続けるのに役立ちます。私は主張しませんが、この正当化は包括的であるとは言えません。それは単なる観察結果です（</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">第2章で</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">飽和について説明したことを思い出します</font><font style="vertical-align: inherit;">）。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ReLUはここ数年で積極的に使用され始めました。</font><font style="vertical-align: inherit;">それらは経験的な理由で採用されました：一部の人々は、しばしば単に直感やヒューリスティックな議論に基づいてReLUを試しました。</font><font style="vertical-align: inherit;">彼らは良い結果を得て、練習は広がりました。</font><font style="vertical-align: inherit;">理想的な世界では、どのアプリケーションにどのアクティベーション機能が最適かを示す理論があります。</font><font style="vertical-align: inherit;">しかし、今のところ、そのような状況に至るにはまだ長い道のりがあります。</font><font style="vertical-align: inherit;">さらに適切なアクティベーション機能を選択することでネットワークのさらなる改善が得られたとしても、私はまったく驚かないでしょう。</font><font style="vertical-align: inherit;">また、活性化機能の良い理論が今後数十年で開発されることを期待しています。</font><font style="vertical-align: inherit;">しかし、今日では、十分に研究されていない経験則と経験則に頼らなければなりません。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">トレーニングデータの拡張</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
結果の改善に役立つ可能性のあるもう1つの方法は、トレーニングデータをアルゴリズムで拡張することです。トレーニングデータを拡張する最も簡単な方法は、各トレーニング画像を1ピクセルずつ上下左右にシフトすることです。これは、</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">expand_mnist.py</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">プログラムを実行することで実行できます</font><font style="vertical-align: inherit;">。</font></font><br>
<br>
<pre><code class="bash hljs">$ python expand_mnist.py</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
プログラムの開始により、MNISTの50,000トレーニングイメージが250,000トレーニングイメージの拡張セットに変わります。次に、これらのトレーニング画像を使用してネットワークをトレーニングできます。 ReLUでは以前と同じネットワークを使用します。私の最初の実験では、トレーニング時代の数を減らしました-トレーニングデータが5倍あるので、それは理にかなっています。ただし、データセットを拡張すると、再トレーニングの効果が大幅に減少しました。したがって、いくつかの実験を行った後、私は60の時代の数に戻りました。いずれにせよ、訓練しましょう：</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-meta">&gt;&gt;&gt; </span>expanded_training_data, _, _ = network3.load_data_shared(
        <span class="hljs-string">"../data/mnist_expanded.pkl.gz"</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>net = Network([<font></font>
        ConvPoolLayer(image_shape=(mini_batch_size, <span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>), <font></font>
                      filter_shape=(<span class="hljs-number">20</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>), <font></font>
                      poolsize=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), <font></font>
                      activation_fn=ReLU),<font></font>
        ConvPoolLayer(image_shape=(mini_batch_size, <span class="hljs-number">20</span>, <span class="hljs-number">12</span>, <span class="hljs-number">12</span>), <font></font>
                      filter_shape=(<span class="hljs-number">40</span>, <span class="hljs-number">20</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>), <font></font>
                      poolsize=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), <font></font>
                      activation_fn=ReLU),<font></font>
        FullyConnectedLayer(n_in=<span class="hljs-number">40</span>*<span class="hljs-number">4</span>*<span class="hljs-number">4</span>, n_out=<span class="hljs-number">100</span>, activation_fn=ReLU),<font></font>
        SoftmaxLayer(n_in=<span class="hljs-number">100</span>, n_out=<span class="hljs-number">10</span>)], mini_batch_size)
<span class="hljs-meta">&gt;&gt;&gt; </span>net.SGD(expanded_training_data, <span class="hljs-number">60</span>, mini_batch_size, <span class="hljs-number">0.03</span>, <font></font>
            validation_data, test_data, lmbda=<span class="hljs-number">0.1</span>)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
高度なトレーニングデータを使用して、99.37％の精度を得ました。このようなほとんど取るに足らない変更により、分類精度が大幅に向上します。また、前述したように、アルゴリズムによるデータ拡張をさらに開発することができます。ちょうど思い出させるために：2003年に</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、シマード、シュタインクラウス、プラット</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ネットワークの精度が99.6％に向上しました。</font><font style="vertical-align: inherit;">彼らのネットワークは私たちのネットワークに似ていて、2つの畳み込み/プールレイヤーを使用し、100ニューロンの完全に接続されたレイヤーがそれに続きました。</font><font style="vertical-align: inherit;">アーキテクチャの詳細はさまざまで、たとえばReLUを利用する機会がありませんでしたが、作業の質を向上させるための鍵はトレーニングデータの拡張でした。</font><font style="vertical-align: inherit;">彼らは、MNISTトレーニング画像を回転、転送、歪ませることでこれを実現しました。</font><font style="vertical-align: inherit;">彼らはまた、「弾性変形」プロセスを開発し、書き込み中の腕の筋肉のランダムな振動をエミュレートしました。</font><font style="vertical-align: inherit;">これらすべてのプロセスを組み合わせることにより、トレーニングデータベースの有効量が大幅に増加し、これにより99.6％の精度が達成されました。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">仕事</font></font></h3><br>
<ul>
<li>     ,        .     ,     ,      .    ,       ?</li>
</ul><br>
<br>
<h3>   </h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
状況を改善することは可能ですか？</font><font style="vertical-align: inherit;">可能性の1つは、まったく同じ手順を使用することですが、同時に完全に接続されたレイヤーのサイズを大きくすることです。</font><font style="vertical-align: inherit;">私はプログラムを300ニューロンと1000ニューロンで開始し、それぞれ99.46％と99.43％の結果を得ました。</font><font style="vertical-align: inherit;">これは興味深いですが、以前の結果（99.37％）よりも特に説得力があります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
完全に接続されたレイヤーを追加するのはどうですか？</font><font style="vertical-align: inherit;">100個のニューロンの2つの非表示の完全に接続された層ができるように、完全に接続された層を追加してみましょう。</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-meta">&gt;&gt;&gt; </span>net = Network([<font></font>
        ConvPoolLayer(image_shape=(mini_batch_size, <span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>), <font></font>
                      filter_shape=(<span class="hljs-number">20</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>), <font></font>
                      poolsize=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), <font></font>
                      activation_fn=ReLU),<font></font>
        ConvPoolLayer(image_shape=(mini_batch_size, <span class="hljs-number">20</span>, <span class="hljs-number">12</span>, <span class="hljs-number">12</span>), <font></font>
                      filter_shape=(<span class="hljs-number">40</span>, <span class="hljs-number">20</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>), <font></font>
                      poolsize=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), <font></font>
                      activation_fn=ReLU),<font></font>
        FullyConnectedLayer(n_in=<span class="hljs-number">40</span>*<span class="hljs-number">4</span>*<span class="hljs-number">4</span>, n_out=<span class="hljs-number">100</span>, activation_fn=ReLU),<font></font>
        FullyConnectedLayer(n_in=<span class="hljs-number">100</span>, n_out=<span class="hljs-number">100</span>, activation_fn=ReLU),<font></font>
        SoftmaxLayer(n_in=<span class="hljs-number">100</span>, n_out=<span class="hljs-number">10</span>)], mini_batch_size)
<span class="hljs-meta">&gt;&gt;&gt; </span>net.SGD(expanded_training_data, <span class="hljs-number">60</span>, mini_batch_size, <span class="hljs-number">0.03</span>, <font></font>
            validation_data, test_data, lmbda=<span class="hljs-number">0.1</span>)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
これにより、検証精度99.43％を達成しました。</font><font style="vertical-align: inherit;">拡張されたネットワークは、パフォーマンスを大幅に改善しませんでした。</font><font style="vertical-align: inherit;">300と100ニューロンの完全に接続されたレイヤーで同様の実験を行った後、99.48％と99.47％の精度が得られました。</font><font style="vertical-align: inherit;">インスピレーションを与えるが、本当の勝利のようではない。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
何が起こっていますか？拡張または追加の完全に接続されたレイヤーがMNIST問題の解決に役立たない可能性はありますか？または、ネットワークはより良い成果を上げることができますが、私たちはそれを間違った方向に開発していますか？たとえば、再トレーニングを減らすために、より厳密な正則化を使用できます。可能性の1つは、第3章で説明したドロップアウト手法です。例外の基本的な考え方は、ネットワークのトレーニング時に個々のアクティベーションをランダムに削除することであることを思い出してください。その結果、モデルは個々の証拠の損失に対してより耐性が高くなるため、トレーニングデータのいくつかの小さな非標準機能に依存する可能性が低くなります。最後に完全に接続されたレイヤーに例外を適用してみましょう：</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-meta">&gt;&gt;&gt; </span>net = Network([<font></font>
        ConvPoolLayer(image_shape=(mini_batch_size, <span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>), <font></font>
                      filter_shape=(<span class="hljs-number">20</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>), <font></font>
                      poolsize=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), <font></font>
                      activation_fn=ReLU),<font></font>
        ConvPoolLayer(image_shape=(mini_batch_size, <span class="hljs-number">20</span>, <span class="hljs-number">12</span>, <span class="hljs-number">12</span>), <font></font>
                      filter_shape=(<span class="hljs-number">40</span>, <span class="hljs-number">20</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>), <font></font>
                      poolsize=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), <font></font>
                      activation_fn=ReLU),<font></font>
        FullyConnectedLayer(<font></font>
            n_in=<span class="hljs-number">40</span>*<span class="hljs-number">4</span>*<span class="hljs-number">4</span>, n_out=<span class="hljs-number">1000</span>, activation_fn=ReLU, p_dropout=<span class="hljs-number">0.5</span>),<font></font>
        FullyConnectedLayer(<font></font>
            n_in=<span class="hljs-number">1000</span>, n_out=<span class="hljs-number">1000</span>, activation_fn=ReLU, p_dropout=<span class="hljs-number">0.5</span>),<font></font>
        SoftmaxLayer(n_in=<span class="hljs-number">1000</span>, n_out=<span class="hljs-number">10</span>, p_dropout=<span class="hljs-number">0.5</span>)], <font></font>
        mini_batch_size)<font></font>
<span class="hljs-meta">&gt;&gt;&gt; </span>net.SGD(expanded_training_data, <span class="hljs-number">40</span>, mini_batch_size, <span class="hljs-number">0.03</span>, <font></font>
            validation_data, test_data)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
このアプローチを使用すると、99.60％の精度が達成されます。これは、以前の精度よりもはるかに優れており、特に基本的な評価である100個の隠れたニューロンを持つネットワークで、99.37％の精度が得られます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ここで注目すべき2つの変更点があります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
最初に、トレーニングの時代を40に減らしました。例外は再トレーニングを減らし、私たちはより速く学びます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
次に、完全に接続された非表示層には、以前のように100ではなく1000のニューロンが含まれます。</font><font style="vertical-align: inherit;">もちろん、例外は実際にはトレーニング中に多くのニューロンを排除するので、何らかの拡張が期待できます。</font><font style="vertical-align: inherit;">実際、私は300と1000のニューロンを使って実験を行い、1000のニューロンの場合に少し良い確認を得ました。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Network Ensembleの使用</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
効率を改善する簡単な方法は、いくつかのニューラルネットワークを作成し、より良い分類に投票させることです。たとえば、上記のレシピを使用して5つの異なるNSをトレーニングし、それぞれが99.6％に近い精度を達成したとします。すべてのネットワークで同様の精度が示されますが、ランダムな初期化が異なるため、エラーが異なる場合があります。 5 NAの投票の場合、それらの一般的な分類は、どのネットワークの分類よりも個別に優れていると想定するのが妥当です。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
それは本当であるには余りにも良いように聞こえますが、そのようなアンサンブルを組み立てることは、国会と他のMO技術の両方にとって一般的なトリックです。そして、実際には効率が向上します。精度は99.67％です。つまり、ネットワークアンサンブルでは、33を除くすべての10,000枚の確認画像を正しく分類しています。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
残りのエラーを以下に示します。右上隅のラベルはMNISTデータに基づく正しい分類であり、右下隅はネットワークのアンサンブルによって受信されたラベル</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/b6e/2d7/69a/b6e2d769a802b1ae5f249932789f2dff.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
です。画像に</font><font style="vertical-align: inherit;">精通する</font><font style="vertical-align: inherit;">ことは価値があります。最初の2桁の6と5は、アンサンブルの実際の間違いです。しかし、彼らは理解することができます、そのような間違いは人によってなされる可能性があります。この6は0と非常によく似ており、5は3と非常によく似ています。3番目の画像（おそらく8）は、実際には9のように見えます。私はネットワークのアンサンブルに賛成です。一方、4番目の画像6は、ネットワークによって実際に誤って分類されています。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
等。</font><font style="vertical-align: inherit;">ほとんどの場合、ネットワークソリューションはもっともらしいようであり、場合によっては、ユーザーが書いたよりも図をより適切に分類しました。</font><font style="vertical-align: inherit;">全体として、私たちのネットワークは非常に効果的です。特に、ここでは提示していない9967の画像を正しく分類したことを思い出した場合はなおさらです。</font><font style="vertical-align: inherit;">このコンテキストでは、いくつかの明らかなエラーを理解できます。</font><font style="vertical-align: inherit;">慎重な人でも時々誤解されます。</font><font style="vertical-align: inherit;">したがって、私は非常に正確で系統的な人からのみより良い結果を期待できます。</font><font style="vertical-align: inherit;">私たちのネットワークは人間のパフォーマンスに近づいています。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">完全に接続されたレイヤーにのみ例外を適用した理由</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
上記のコードをよく見ると、完全に接続されたネットワーク層にのみ例外が適用されており、たたみ込み層には適用されていないことがわかります。</font><font style="vertical-align: inherit;">原則として、畳み込み層にも同様の手順を適用できます。</font><font style="vertical-align: inherit;">しかし、これは必要ありません。たたみ込み層には、再トレーニングに対する大きな組み込み抵抗があります。</font><font style="vertical-align: inherit;">これは、重みの合計により、畳み込みフィルターが画像全体を一度に学習するためです。</font><font style="vertical-align: inherit;">その結果、トレーニングデータの一部の局所的な歪みをつまずく可能性が低くなります。</font><font style="vertical-align: inherit;">したがって、例外など、他の正則化関数を適用する必要は特にありません。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">先へ</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
MNIST問題の解決効率をさらに向上させることができます。 Rodrigo Benensonは</font><font style="vertical-align: inherit;">、長年の進歩を示し、仕事へのリンクを提供</font><font style="vertical-align: inherit;">する</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">有益なタブレット</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">を作成しました。多くの作品は、私たちが使用したのと同じ方法でGSSを使用しています。あなたの仕事をかき回すと、あなたは多くの興味深いテクニックを見つけるでしょう、そしてあなたはそれらのいくつかを実装したいと思うかもしれません。この場合、すぐにトレーニングできるシンプルなネットワークから実装を始めるのが賢明です。これは、何が起こっているのかをすばやく理解し始めるのに役立ちます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ほとんどの場合、私は最近の仕事を復習しようとはしません。しかし、私は一つの例外に抵抗することはできません。</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">2010年に</font></a><font style="vertical-align: inherit;">約1 </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">作品</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。私は彼女のシンプルさが好きです。ネットワークは多層であり、完全に接続されたレイヤーのみを使用します（畳み込みなし）。彼らの最も成功したネットワークには、それぞれ2500、2000、1500、1000、500のニューロンを含む隠れ層があります。彼らは同様のアイデアを使用してトレーニングデータを拡張しました。しかし、これに加えて、畳み込み層の欠如など、さらにいくつかのトリックを適用しました。これは、適切な忍耐力と適切なコンピューター機能の可用性を備えた1980年代に（MNISTセットが存在する場合）教えられた、最も単純なバニラネットワークでした。彼らは99.65％の分類精度を達成しました。これは、私たちのものとほぼ一致します。彼らの仕事の主なものは、非常に大規模で深いネットワークの使用と、学習を加速するためのGPUの使用です。これにより、彼らは多くの時代を学ぶことができました。彼らはまた、長いトレーニング間隔を利用して、学習速度を10から徐々に下げました</font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">-3</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">から10 </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">-6</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font><font style="vertical-align: inherit;">彼らのようなアーキテクチャで同様の結果を達成しようとすることは、興味深い演習です。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">なぜ私たちは学ぶのですか？</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
前の章では、深い多層NSの学習に基本的な障害を見ました。</font><font style="vertical-align: inherit;">特に、グラデーションが非常に不安定になることがわかりました。出力レイヤーから前のレイヤーに移動すると、グラデーションが消える（グラデーションが消える問題）か、爆発的に成長する（爆発的なグラデーション成長の問題）傾向があります。</font><font style="vertical-align: inherit;">勾配はトレーニングに使用する信号であるため、問題が発生します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
どのようにしてそれらを回避することができましたか？</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
答えは、当然、これは避けられませんでした。代わりに、これにも関わらず、作業を続けることができるいくつかのことを行いました。特に：（1）畳み込み層を使用すると、それらに含まれるパラメーターの数が大幅に減少し、学習問題が大幅に促進されます。 （2）より効率的な正則化手法（除外レイヤーと畳み込みレイヤー）の使用。 （3）シグモイドニューロンの代わりにReLUを使用して学習を加速する-経験的に最大3〜5回。 （4）GPUの使用と経時的に学習する機能。特に、最近の実験では、標準のMNISTトレーニングデータの5倍のデータセットを使用して40の時代を調査しました。本の前半で、主に標準的なトレーニングデータを使用して30の時代を研究しました。要因（3）と（4）を組み合わせると、このような効果が得られます。まるで以前より30倍長く勉強したようです。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
あなたはおそらく「それだけですか？」と言うでしょう。ディープニューラルネットワークのトレーニングに必要なのはそれだけですか？そして、それで大騒ぎは何のために火をつかんだのですか？」</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
もちろん、他のアイデアを使用しました。十分な大きさのデータセット（再トレーニングを回避するため）。正しいコスト関数（学習の減速を回避するため）;重みの適切な初期化（ニューロンの飽和による学習の速度低下を回避するためにも）;トレーニングデータセットのアルゴリズム拡張。これらのアイデアやその他のアイデアについては前の章で説明しましたが、通常、この章では小さなメモを付けてそれらを再利用する機会がありました。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
すべての兆候から、これはかなり単純なアイデアのセットです。シンプルですが、複雑な場所で使用すると多くのことが可能です。ディープラーニングの開始は非常に簡単でした。</font></font><br>
<br>
<h3>      ?</h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
畳み込み/プーリングレイヤーを1つと考えると、最終的なアーキテクチャーには4つの非表示レイヤーがあります。そのようなネットワークは深いタイトルに値しますか？当然、4つの隠れ層は、以前に調査した浅いネットワークよりもはるかに多くなります。ほとんどのネットワークには1つの隠れ層があり、場合によっては2つあります。一方、現代の高度なネットワークには、数十の隠れ層がある場合があります。時々、ネットワークが深いほど良い、そして十分な数の隠しレイヤーを使用しないと、本当に深層学習をしないと思っている人に会いました。特にそのようなアプローチはディープラーニングの定義を瞬間的な結果に依存する手順に変えるため、私はそうは思いません。この領域での真のブレークスルーは、1つまたは2つの非表示レイヤーを持つネットワークを超える実用性のアイデアでした。2000年代半ばに普及しました。これは本当に画期的なことであり、より表現力豊かなモデルで研究分野を切り開きました。まあ、特定の数の層は基本的な関心事ではありません。ディープネットワークの使用は、分類精度の向上など、他の目標を達成するためのツールです。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">手続き上の問題</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
このセクションでは、1つの隠れ層を持つ浅いネットワークから多層畳み込みネットワークにスムーズに切り替えました。</font><font style="vertical-align: inherit;">すべてがとても簡単に見えました！</font><font style="vertical-align: inherit;">変更を加えて改善しました。</font><font style="vertical-align: inherit;">実験を始めたら、通常はすべてがスムーズに進まないことを保証します。</font><font style="vertical-align: inherit;">失敗したものを含め、多くの実験を省略して、くしを使ったストーリーを紹介しました。</font><font style="vertical-align: inherit;">このくし形の物語があなたが基本的な考えをよりよく理解するのに役立つことを願っています。</font><font style="vertical-align: inherit;">しかし、彼は不完全な印象を伝えるリスクがあります。</font><font style="vertical-align: inherit;">うまく機能するネットワークを取得するには、多くの試行錯誤が必要であり、欲求不満が散在しています。</font><font style="vertical-align: inherit;">実際には、膨大な数の実験が期待できます。</font><font style="vertical-align: inherit;">プロセスを高速化するために、ネットワークハイパーパラメーターの選択に関する第3章の情報と、そこで言及されている追加の資料が役立ちます。</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">畳み込みネットワークのコード</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
では、network3.pyプログラムのコードを見てみましょう。構造的には、第3章で開発したnetwork2.pyに似ていますが、Theanoライブラリを使用しているため、詳細は異なります。前に調べたレイヤーと同様に、FullyConnectedLayerクラスから始めましょう。</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">FullyConnectedLayer</span>(<span class="hljs-params">object</span>):</span><font></font>
<font></font>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, n_in, n_out, activation_fn=sigmoid, p_dropout=<span class="hljs-number">0.0</span></span>):</span><font></font>
        self.n_in = n_in<font></font>
        self.n_out = n_out<font></font>
        self.activation_fn = activation_fn<font></font>
        self.p_dropout = p_dropout<font></font>
        <span class="hljs-comment"># Initialize weights and biases</span><font></font>
        self.w = theano.shared(<font></font>
            np.asarray(<font></font>
                np.random.normal(<font></font>
                    loc=<span class="hljs-number">0.0</span>, scale=np.sqrt(<span class="hljs-number">1.0</span>/n_out), size=(n_in, n_out)),<font></font>
                dtype=theano.config.floatX),<font></font>
            name=<span class="hljs-string">'w'</span>, borrow=<span class="hljs-literal">True</span>)<font></font>
        self.b = theano.shared(<font></font>
            np.asarray(np.random.normal(loc=<span class="hljs-number">0.0</span>, scale=<span class="hljs-number">1.0</span>, size=(n_out,)),<font></font>
                       dtype=theano.config.floatX),<font></font>
            name=<span class="hljs-string">'b'</span>, borrow=<span class="hljs-literal">True</span>)<font></font>
        self.params = [self.w, self.b]<font></font>
<font></font>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">set_inpt</span>(<span class="hljs-params">self, inpt, inpt_dropout, mini_batch_size</span>):</span><font></font>
        self.inpt = inpt.reshape((mini_batch_size, self.n_in))<font></font>
        self.output = self.activation_fn(<font></font>
            (<span class="hljs-number">1</span>-self.p_dropout)*T.dot(self.inpt, self.w) + self.b)<font></font>
        self.y_out = T.argmax(self.output, axis=<span class="hljs-number">1</span>)<font></font>
        self.inpt_dropout = dropout_layer(<font></font>
            inpt_dropout.reshape((mini_batch_size, self.n_in)), self.p_dropout)<font></font>
        self.output_dropout = self.activation_fn(<font></font>
            T.dot(self.inpt_dropout, self.w) + self.b)<font></font>
<font></font>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">accuracy</span>(<span class="hljs-params">self, y</span>):</span>
        <span class="hljs-string">"Return the accuracy for the mini-batch."</span>
        <span class="hljs-keyword">return</span> T.mean(T.eq(y, self.y_out))</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
__init__メソッドの大部分はそれ自体で説明できますが、コードを明確にするのに役立ついくつかの注意事項があります。いつものように、適切な標準偏差を持つ通常のランダム値を使用して、重みとオフセットをランダムに初期化します。これらの線は少しわかりにくいように見えます。ただし、奇妙なコードのほとんどは、Theanoライブラリーが共有変数と呼ぶものに重みとオフセットをロードしています。これにより、GPUで変数を処理できるようになります（可能な場合）。この質問については詳しく説明しません。興味があれば、Theanoのドキュメントを読んでください。また、この重みとオフセットの初期化は、シグモイドアクティベーション関数用であることに注意してください。理想的には、双曲線正接やReLUなどの関数では、重みとオフセットを異なる方法で初期化します。この問題については、今後のタスクで説明します。__init__メソッドは、ステートメントself.params = [self.w、self.b]で終わります。これは、レイヤーに関連付けられているすべての学習パラメーターをまとめるのに便利な方法です。 Network.SGDは後でparams属性を使用して、トレーニングできるNetworkクラスインスタンスの変数を見つけます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
set_inptメソッドは、入力データをレイヤーに転送し、対応する出力を計算するために使用されます。 inputは組み込みのpython関数であるため、私はinputではなくinptを記述します。これを使用すると、プログラムの動作が予測できなくなり、エラーの診断が困難になる可能性があります。実際、入力は、self.inptとself.inpt_dropoutの2つの方法で渡されます。これは、トレーニング中に例外を使用したい場合があるためです。次に、self.p_dropoutニューロンの一部を削除する必要があります。これは、set_inptメソッドの最後から2行目のdropout_layer関数が行うことです。したがって、トレーニング中にself.inpt_dropoutとself.output_dropoutが使用され、検証データとテストデータの精度を評価するなど、他のすべての目的でself.inptとself.outputが使用されます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ConvPoolLayerおよびSoftmaxLayerのクラス定義は、FullyConnectedLayerに似ています。コードも引用しないほど似ています。興味があれば、プログラムの完全なコードをこの章の後半で学習できます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
いくつかの異なる詳細について言及する価値があります。明らかに、ConvPoolLayerとSoftmaxLayerでは、レイヤーのタイプに適した方法で出力アクティベーションを計算します。幸いなことに、Theanoは簡単に実行でき、畳み込み、最大プーリング、およびソフトマックス関数を計算するための組み込み演算を備えています。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Softmaxレイヤーで重みとオフセットを初期化する方法はそれほど明白ではありません。これについては説明しませんでした。シグモイドウェイトレイヤーの場合、適切にパラメーター化された正規ランダム分布を初期化する必要があることを述べました。しかし、このヒューリスティックな議論はシグモイドニューロンに適用されました（マイナーな修正を加えて、タンニューロンに適用されました）。ただし、この議論がソフトマックスレイヤーに適用される特別な理由はありません。したがって、この初期化を再度演繹的に適用する理由はありません。代わりに、すべての重みとオフセットを0に初期化します。このオプションは自発的ですが、実際にはかなりうまく機能します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
したがって、レイヤーのすべてのクラスを調査しました。 Networkクラスはどうですか？まず、__ init__メソッドを見てみましょう。</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Network</span>(<span class="hljs-params">object</span>):</span><font></font>
    <font></font>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, layers, mini_batch_size</span>):</span>
        <span class="hljs-string">"""   layers,   ,   
        mini_batch_size       
         

        """</span><font></font>
        self.layers = layers<font></font>
        self.mini_batch_size = mini_batch_size<font></font>
        self.params = [param <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> self.layers <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> layer.params]<font></font>
        self.x = T.matrix(<span class="hljs-string">"x"</span>)  <font></font>
        self.y = T.ivector(<span class="hljs-string">"y"</span>)<font></font>
        init_layer = self.layers[<span class="hljs-number">0</span>]<font></font>
        init_layer.set_inpt(self.x, self.x, self.mini_batch_size)<font></font>
        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> xrange(<span class="hljs-number">1</span>, len(self.layers)):<font></font>
            prev_layer, layer  = self.layers[j<span class="hljs-number">-1</span>], self.layers[j]<font></font>
            layer.set_inpt(<font></font>
                prev_layer.output, prev_layer.output_dropout, self.mini_batch_size)<font></font>
        self.output = self.layers[<span class="hljs-number">-1</span>].output<font></font>
        self.output_dropout = self.layers[<span class="hljs-number">-1</span>].output_dropout</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
コードの大部分はそれ自体が物語っています。 self.params = [param for layer in ...]という行は、各レイヤーのすべてのパラメーターを1つのリストに収集します。以前に提案したように、Network.SGDメソッドはself.paramsを使用して、ネットワークが学習できるパラメーターを特定します。 self.x = T.matrix（ "x"）およびself.y = T.ivector（ "y"）の行は、Theano xおよびyシンボリック変数を定義します。これらは、ネットワークの入力と目的の出力を表します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
これはTheanoの使用に関するチュートリアルではないため、シンボリック変数の意味については</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">説明し</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ません（</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">ドキュメントを</font></a><font style="vertical-align: inherit;">参照してください。</font><font style="vertical-align: inherit;">また、</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">チュートリアルの</font></a><font style="vertical-align: inherit;"> 1つも</font><font style="vertical-align: inherit;">参照し</font><font style="vertical-align: inherit;">て</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ください）</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">）大まかに言えば、それらは特定の変数ではなく、数学変数を表します。これらを使用すると、加算、減算、乗算、関数の適用など、多くの通常の操作を実行できます。 Theanoは、そのようなシンボリック変数の操作、たたみ込み、最大引きなどの多くの可能性を提供します。ただし、主なものは、非常に一般的な形式の逆伝播アルゴリズムを使用して記号を迅速に区別できる可能性です。これは、確率的勾配降下法を幅広いネットワークアーキテクチャに適用する場合に非常に役立ちます。特に、次のコード行はネットワークのシンボリック出力を定義しています。入力を最初のレイヤーに割り当てることから始めます。</font></font><br>
<br>
<pre><code class="python hljs">        init_layer.set_inpt(self.x, self.x, self.mini_batch_size)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
入力データは一度に1つのミニパケットで送信されるため、そのサイズがそこに示されます。 self.x入力を2回渡します。実際には、ネットワークを2つの異なる方法で（例外ありまたはなしで）使用できます。 forループは、シンボリック変数self.xをネットワークレイヤーに伝播します。これにより、ネットワークの出力を象徴的に表す最終的な属性outputおよびoutput_dropoutを定義できます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Networkの初期化を扱ったので、SGDメソッドによるトレーニングを見てみましょう。コードは長く見えますが、その構造はかなり単純です。説明はコードに従います。</font></font><br>
<br>
<pre><code class="python hljs">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">SGD</span>(<span class="hljs-params">self, training_data, epochs, mini_batch_size, eta, 
            validation_data, test_data, lmbda=<span class="hljs-number">0.0</span></span>):</span>
        <span class="hljs-string">"""    -   
        ."""</span><font></font>
        training_x, training_y = training_data<font></font>
        validation_x, validation_y = validation_data<font></font>
        test_x, test_y = test_data<font></font>
<font></font>
        <span class="hljs-comment">#   -  ,   </span><font></font>
        num_training_batches = size(training_data)/mini_batch_size<font></font>
        num_validation_batches = size(validation_data)/mini_batch_size<font></font>
        num_test_batches = size(test_data)/mini_batch_size<font></font>
<font></font>
        <span class="hljs-comment">#    ,    </span>
        l2_norm_squared = sum([(layer.w**<span class="hljs-number">2</span>).sum() <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> self.layers])<font></font>
        cost = self.layers[<span class="hljs-number">-1</span>].cost(self)+\
               <span class="hljs-number">0.5</span>*lmbda*l2_norm_squared/num_training_batches<font></font>
        grads = T.grad(cost, self.params)<font></font>
        updates = [(param, param-eta*grad) <font></font>
                   <span class="hljs-keyword">for</span> param, grad <span class="hljs-keyword">in</span> zip(self.params, grads)]<font></font>
<font></font>
        <span class="hljs-comment">#     -   </span>
        <span class="hljs-comment">#      -.</span>
        i = T.lscalar() <span class="hljs-comment"># mini-batch index</span><font></font>
        train_mb = theano.function(<font></font>
            [i], cost, updates=updates,<font></font>
            givens={<font></font>
                self.x:<font></font>
                training_x[i*self.mini_batch_size: (i+<span class="hljs-number">1</span>)*self.mini_batch_size],<font></font>
                self.y: <font></font>
                training_y[i*self.mini_batch_size: (i+<span class="hljs-number">1</span>)*self.mini_batch_size]<font></font>
            })<font></font>
        validate_mb_accuracy = theano.function(<font></font>
            [i], self.layers[<span class="hljs-number">-1</span>].accuracy(self.y),<font></font>
            givens={<font></font>
                self.x: <font></font>
                validation_x[i*self.mini_batch_size: (i+<span class="hljs-number">1</span>)*self.mini_batch_size],<font></font>
                self.y: <font></font>
                validation_y[i*self.mini_batch_size: (i+<span class="hljs-number">1</span>)*self.mini_batch_size]<font></font>
            })<font></font>
        test_mb_accuracy = theano.function(<font></font>
            [i], self.layers[<span class="hljs-number">-1</span>].accuracy(self.y),<font></font>
            givens={<font></font>
                self.x: <font></font>
                test_x[i*self.mini_batch_size: (i+<span class="hljs-number">1</span>)*self.mini_batch_size],<font></font>
                self.y: <font></font>
                test_y[i*self.mini_batch_size: (i+<span class="hljs-number">1</span>)*self.mini_batch_size]<font></font>
            })<font></font>
        self.test_mb_predictions = theano.function(<font></font>
            [i], self.layers[<span class="hljs-number">-1</span>].y_out,<font></font>
            givens={<font></font>
                self.x: <font></font>
                test_x[i*self.mini_batch_size: (i+<span class="hljs-number">1</span>)*self.mini_batch_size]<font></font>
            })<font></font>
        <span class="hljs-comment">#   </span>
        best_validation_accuracy = <span class="hljs-number">0.0</span>
        <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> xrange(epochs):
            <span class="hljs-keyword">for</span> minibatch_index <span class="hljs-keyword">in</span> xrange(num_training_batches):<font></font>
                iteration = num_training_batches*epoch+minibatch_index<font></font>
                <span class="hljs-keyword">if</span> iteration <font></font>
                    print(<span class="hljs-string">"Training mini-batch number {0}"</span>.format(iteration))<font></font>
                cost_ij = train_mb(minibatch_index)<font></font>
                <span class="hljs-keyword">if</span> (iteration+<span class="hljs-number">1</span>) <font></font>
                    validation_accuracy = np.mean(<font></font>
                        [validate_mb_accuracy(j) <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> xrange(num_validation_batches)])<font></font>
                    print(<span class="hljs-string">"Epoch {0}: validation accuracy {1:.2
                        epoch, validation_accuracy))
                    if validation_accuracy &gt;= best_validation_accuracy:
                        print("</span>This <span class="hljs-keyword">is</span> the best validation accuracy to date.<span class="hljs-string">")
                        best_validation_accuracy = validation_accuracy
                        best_iteration = iteration
                        if test_data:
                            test_accuracy = np.mean(
                                [test_mb_accuracy(j) for j in xrange(num_test_batches)])
                            print('The corresponding test accuracy is {0:.2
                                test_accuracy))
        print("</span>Finished training network.<span class="hljs-string">")
        print("</span>Best validation accuracy of {<span class="hljs-number">0</span>:<span class="hljs-number">.2</span><font></font>
            best_validation_accuracy, best_iteration))<font></font>
        print(<span class="hljs-string">"Corresponding test accuracy of {0:.2</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
最初の行は明確で、データセットをコンポーネントxとyに分割し、各データセットで使用されるミニパケットの数を計算します。次の行はより興味深いものであり、Theanoライブラリを使用することがなぜ興味深いのかを示しています。ここでそれらを引用します：</font></font><br>
<br>
<pre><code class="python hljs">        <span class="hljs-comment">#    ,    </span>
        l2_norm_squared = sum([(layer.w**<span class="hljs-number">2</span>).sum() <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> self.layers])<font></font>
        cost = self.layers[<span class="hljs-number">-1</span>].cost(self)+\
               <span class="hljs-number">0.5</span>*lmbda*l2_norm_squared/num_training_batches<font></font>
        grads = T.grad(cost, self.params)<font></font>
        updates = [(param, param-eta*grad) <font></font>
                   <span class="hljs-keyword">for</span> param, grad <span class="hljs-keyword">in</span> zip(self.params, grads)]</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
これらの行では、対数尤度関数に基づいて正則化されたコスト関数を記号的に定義し、勾配関数の対応する導関数と、対応するパラメーターの更新を計算します。 Theanoを使用すると、これらすべてを数行で実行できます。隠されている唯一のことは、コストの計算には、出力層のコストメソッドの呼び出しが含まれることです。このコードはnetwork3.pyの他の場所にあります。しかし、それは短くてシンプルです。これらすべての定義により、すべてがtrain_mb関数を定義する準備ができました。これは、更新を使用してミニパケットインデックスによってネットワークパラメーターを更新するTheanoシンボリック関数です。同様に、validate_mb_accuracyおよびtest_mb_accuracy関数は、確認または検証データの特定のミニパケットのネットワーク精度を計算します。これらの関数を平均して、検証および検証データセット全体の精度を計算できます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
SGDメソッドの残りの部分はそれ自体が意味をなしています-訓練データのミニパケットでネットワークを訓練し、確認と検証の正確さを計算するだけで、エポックを連続して繰り返します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
これで、年の最も重要な部分を理解しましたnetwork3.py。</font><font style="vertical-align: inherit;">プログラム全体を簡単に見ていきましょう。</font><font style="vertical-align: inherit;">すべてを詳しく研究する必要はありませんが、あなたはトップを越えて、そしておそらく、いくつかの特に好きなパッセージを掘り下げるのが好きかもしれません。</font><font style="vertical-align: inherit;">しかし、もちろん、プログラムを理解する最良の方法は、プログラムを変更し、何か新しいものを追加し、あなたの意見では改善できる部分をリファクタリングすることです。</font><font style="vertical-align: inherit;">コードの後に​​、ここで何ができるかについてのいくつかの最初の提案を含むいくつかのタスクを示します。</font><font style="vertical-align: inherit;">これがコードです。</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-string">"""network3.py
~~~~~~~~~~~~~~

    Theano      .

    (, , -, softmax)    (,  , ReLU;   ).

   CPU     ,  network.py  network2.py. ,    ,      GPU,    .

    Theano,       network.py  network2.py.  ,       .  , API   network2.py.       ,  ,     .   ,     ,    .

     Theano   (http://deeplearning.net/tutorial/lenet.html ),       (https://github.com/mdenil/dropout )      (http://colah.github.io ).

  Theano 0.6  0.7,       .

"""</span><font></font>
<font></font>
<span class="hljs-comment">#### </span>
<span class="hljs-comment"># </span>
<span class="hljs-keyword">import</span> cPickle
<span class="hljs-keyword">import</span> gzip<font></font>
<font></font>
<span class="hljs-comment">#  </span>
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> theano
<span class="hljs-keyword">import</span> theano.tensor <span class="hljs-keyword">as</span> T
<span class="hljs-keyword">from</span> theano.tensor.nnet <span class="hljs-keyword">import</span> conv
<span class="hljs-keyword">from</span> theano.tensor.nnet <span class="hljs-keyword">import</span> softmax
<span class="hljs-keyword">from</span> theano.tensor <span class="hljs-keyword">import</span> shared_randomstreams
<span class="hljs-keyword">from</span> theano.tensor.signal <span class="hljs-keyword">import</span> downsample<font></font>
<font></font>
<span class="hljs-comment">#   </span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">linear</span>(<span class="hljs-params">z</span>):</span> <span class="hljs-keyword">return</span> z
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">ReLU</span>(<span class="hljs-params">z</span>):</span> <span class="hljs-keyword">return</span> T.maximum(<span class="hljs-number">0.0</span>, z)
<span class="hljs-keyword">from</span> theano.tensor.nnet <span class="hljs-keyword">import</span> sigmoid
<span class="hljs-keyword">from</span> theano.tensor <span class="hljs-keyword">import</span> tanh<font></font>
<font></font>
<font></font>
<span class="hljs-comment">#### </span>
GPU = <span class="hljs-literal">True</span>
<span class="hljs-keyword">if</span> GPU:
    <span class="hljs-keyword">print</span> <span class="hljs-string">"Trying to run under a GPU.  If this is not desired, then modify "</span>+\
        <span class="hljs-string">"network3.py\nto set the GPU flag to False."</span>
    <span class="hljs-keyword">try</span>: theano.config.device = <span class="hljs-string">'gpu'</span>
    <span class="hljs-keyword">except</span>: <span class="hljs-keyword">pass</span> <span class="hljs-comment"># it's already set</span>
    theano.config.floatX = <span class="hljs-string">'float32'</span>
<span class="hljs-keyword">else</span>:
    <span class="hljs-keyword">print</span> <span class="hljs-string">"Running with a CPU.  If this is not desired, then the modify "</span>+\
        <span class="hljs-string">"network3.py to set\nthe GPU flag to True."</span><font></font>
<font></font>
<span class="hljs-comment">####   MNIST</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">load_data_shared</span>(<span class="hljs-params">filename=<span class="hljs-string">"../data/mnist.pkl.gz"</span></span>):</span>
    f = gzip.open(filename, <span class="hljs-string">'rb'</span>)<font></font>
    training_data, validation_data, test_data = cPickle.load(f)<font></font>
    f.close()<font></font>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">shared</span>(<span class="hljs-params">data</span>):</span>
        <span class="hljs-string">"""    .   Theano    GPU,   .

        """</span><font></font>
        shared_x = theano.shared(<font></font>
            np.asarray(data[<span class="hljs-number">0</span>], dtype=theano.config.floatX), borrow=<span class="hljs-literal">True</span>)<font></font>
        shared_y = theano.shared(<font></font>
            np.asarray(data[<span class="hljs-number">1</span>], dtype=theano.config.floatX), borrow=<span class="hljs-literal">True</span>)
        <span class="hljs-keyword">return</span> shared_x, T.cast(shared_y, <span class="hljs-string">"int32"</span>)
    <span class="hljs-keyword">return</span> [shared(training_data), shared(validation_data), shared(test_data)]<font></font>
<font></font>
<span class="hljs-comment">####       </span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Network</span>(<span class="hljs-params">object</span>):</span><font></font>
<font></font>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, layers, mini_batch_size</span>):</span>
        <span class="hljs-string">"""   layers,   ,   
        mini_batch_size       
         .

        """</span><font></font>
        self.layers = layers<font></font>
        self.mini_batch_size = mini_batch_size<font></font>
        self.params = [param <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> self.layers <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> layer.params]<font></font>
        self.x = T.matrix(<span class="hljs-string">"x"</span>)<font></font>
        self.y = T.ivector(<span class="hljs-string">"y"</span>)<font></font>
        init_layer = self.layers[<span class="hljs-number">0</span>]<font></font>
        init_layer.set_inpt(self.x, self.x, self.mini_batch_size)<font></font>
        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> xrange(<span class="hljs-number">1</span>, len(self.layers)):<font></font>
            prev_layer, layer  = self.layers[j<span class="hljs-number">-1</span>], self.layers[j]<font></font>
            layer.set_inpt(<font></font>
                prev_layer.output, prev_layer.output_dropout, self.mini_batch_size)<font></font>
        self.output = self.layers[<span class="hljs-number">-1</span>].output<font></font>
        self.output_dropout = self.layers[<span class="hljs-number">-1</span>].output_dropout<font></font>
<font></font>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">SGD</span>(<span class="hljs-params">self, training_data, epochs, mini_batch_size, eta,
            validation_data, test_data, lmbda=<span class="hljs-number">0.0</span></span>):</span>
        <span class="hljs-string">"""    -   
        ."""</span><font></font>
        training_x, training_y = training_data<font></font>
        validation_x, validation_y = validation_data<font></font>
        test_x, test_y = test_data<font></font>
<font></font>
        <span class="hljs-comment">#   -  ,   </span><font></font>
        num_training_batches = size(training_data)/mini_batch_size<font></font>
        num_validation_batches = size(validation_data)/mini_batch_size<font></font>
        num_test_batches = size(test_data)/mini_batch_size<font></font>
<font></font>
        <span class="hljs-comment">#    ,    </span>
        l2_norm_squared = sum([(layer.w**<span class="hljs-number">2</span>).sum() <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> self.layers])<font></font>
        cost = self.layers[<span class="hljs-number">-1</span>].cost(self)+\
               <span class="hljs-number">0.5</span>*lmbda*l2_norm_squared/num_training_batches<font></font>
        grads = T.grad(cost, self.params)<font></font>
        updates = [(param, param-eta*grad)<font></font>
                   <span class="hljs-keyword">for</span> param, grad <span class="hljs-keyword">in</span> zip(self.params, grads)]<font></font>
<font></font>
        <span class="hljs-comment">#     -   </span>
        <span class="hljs-comment">#      -.</span>
        i = T.lscalar() <span class="hljs-comment"># mini-batch index</span><font></font>
        train_mb = theano.function(<font></font>
            [i], cost, updates=updates,<font></font>
            givens={<font></font>
                self.x:<font></font>
                training_x[i*self.mini_batch_size: (i+<span class="hljs-number">1</span>)*self.mini_batch_size],<font></font>
                self.y:<font></font>
                training_y[i*self.mini_batch_size: (i+<span class="hljs-number">1</span>)*self.mini_batch_size]<font></font>
            })<font></font>
        validate_mb_accuracy = theano.function(<font></font>
            [i], self.layers[<span class="hljs-number">-1</span>].accuracy(self.y),<font></font>
            givens={<font></font>
                self.x:<font></font>
                validation_x[i*self.mini_batch_size: (i+<span class="hljs-number">1</span>)*self.mini_batch_size],<font></font>
                self.y:<font></font>
                validation_y[i*self.mini_batch_size: (i+<span class="hljs-number">1</span>)*self.mini_batch_size]<font></font>
            })<font></font>
        test_mb_accuracy = theano.function(<font></font>
            [i], self.layers[<span class="hljs-number">-1</span>].accuracy(self.y),<font></font>
            givens={<font></font>
                self.x:<font></font>
                test_x[i*self.mini_batch_size: (i+<span class="hljs-number">1</span>)*self.mini_batch_size],<font></font>
                self.y:<font></font>
                test_y[i*self.mini_batch_size: (i+<span class="hljs-number">1</span>)*self.mini_batch_size]<font></font>
            })<font></font>
        self.test_mb_predictions = theano.function(<font></font>
            [i], self.layers[<span class="hljs-number">-1</span>].y_out,<font></font>
            givens={<font></font>
                self.x:<font></font>
                test_x[i*self.mini_batch_size: (i+<span class="hljs-number">1</span>)*self.mini_batch_size]<font></font>
            })<font></font>
        <span class="hljs-comment">#   </span>
        best_validation_accuracy = <span class="hljs-number">0.0</span>
        <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> xrange(epochs):
            <span class="hljs-keyword">for</span> minibatch_index <span class="hljs-keyword">in</span> xrange(num_training_batches):<font></font>
                iteration = num_training_batches*epoch+minibatch_index<font></font>
                <span class="hljs-keyword">if</span> iteration % <span class="hljs-number">1000</span> == <span class="hljs-number">0</span>:<font></font>
                    print(<span class="hljs-string">"Training mini-batch number {0}"</span>.format(iteration))<font></font>
                cost_ij = train_mb(minibatch_index)<font></font>
                <span class="hljs-keyword">if</span> (iteration+<span class="hljs-number">1</span>) % num_training_batches == <span class="hljs-number">0</span>:<font></font>
                    validation_accuracy = np.mean(<font></font>
                        [validate_mb_accuracy(j) <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> xrange(num_validation_batches)])<font></font>
                    print(<span class="hljs-string">"Epoch {0}: validation accuracy {1:.2%}"</span>.format(<font></font>
                        epoch, validation_accuracy))<font></font>
                    <span class="hljs-keyword">if</span> validation_accuracy &gt;= best_validation_accuracy:<font></font>
                        print(<span class="hljs-string">"This is the best validation accuracy to date."</span>)<font></font>
                        best_validation_accuracy = validation_accuracy<font></font>
                        best_iteration = iteration<font></font>
                        <span class="hljs-keyword">if</span> test_data:<font></font>
                            test_accuracy = np.mean(<font></font>
                                [test_mb_accuracy(j) <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> xrange(num_test_batches)])<font></font>
                            print(<span class="hljs-string">'The corresponding test accuracy is {0:.2%}'</span>.format(<font></font>
                                test_accuracy))<font></font>
        print(<span class="hljs-string">"Finished training network."</span>)<font></font>
        print(<span class="hljs-string">"Best validation accuracy of {0:.2%} obtained at iteration {1}"</span>.format(<font></font>
            best_validation_accuracy, best_iteration))<font></font>
        print(<span class="hljs-string">"Corresponding test accuracy of {0:.2%}"</span>.format(test_accuracy))<font></font>
<font></font>
<span class="hljs-comment">####   </span><font></font>
<font></font>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ConvPoolLayer</span>(<span class="hljs-params">object</span>):</span>
    <span class="hljs-string">"""     - .   
        ,         , 
       ,   .

    """</span><font></font>
<font></font>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, filter_shape, image_shape, poolsize=(<span class="hljs-params"><span class="hljs-number">2</span>, <span class="hljs-number">2</span></span>),
                 activation_fn=sigmoid</span>):</span>
        <span class="hljs-string">"""`filter_shape` -   4,   ,    ,     .

        `image_shape` -   4,   -,    ,    .

        `poolsize` -   2,    y  x.

        """</span><font></font>
        self.filter_shape = filter_shape<font></font>
        self.image_shape = image_shape<font></font>
        self.poolsize = poolsize<font></font>
        self.activation_fn=activation_fn<font></font>
        <span class="hljs-comment"># initialize weights and biases</span>
        n_out = (filter_shape[<span class="hljs-number">0</span>]*np.prod(filter_shape[<span class="hljs-number">2</span>:])/np.prod(poolsize))<font></font>
        self.w = theano.shared(<font></font>
            np.asarray(<font></font>
                np.random.normal(loc=<span class="hljs-number">0</span>, scale=np.sqrt(<span class="hljs-number">1.0</span>/n_out), size=filter_shape),<font></font>
                dtype=theano.config.floatX),<font></font>
            borrow=<span class="hljs-literal">True</span>)<font></font>
        self.b = theano.shared(<font></font>
            np.asarray(<font></font>
                np.random.normal(loc=<span class="hljs-number">0</span>, scale=<span class="hljs-number">1.0</span>, size=(filter_shape[<span class="hljs-number">0</span>],)),<font></font>
                dtype=theano.config.floatX),<font></font>
            borrow=<span class="hljs-literal">True</span>)<font></font>
        self.params = [self.w, self.b]<font></font>
<font></font>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">set_inpt</span>(<span class="hljs-params">self, inpt, inpt_dropout, mini_batch_size</span>):</span><font></font>
        self.inpt = inpt.reshape(self.image_shape)<font></font>
        conv_out = conv.conv2d(<font></font>
            input=self.inpt, filters=self.w, filter_shape=self.filter_shape,<font></font>
            image_shape=self.image_shape)<font></font>
        pooled_out = downsample.max_pool_2d(<font></font>
            input=conv_out, ds=self.poolsize, ignore_border=<span class="hljs-literal">True</span>)<font></font>
        self.output = self.activation_fn(<font></font>
            pooled_out + self.b.dimshuffle(<span class="hljs-string">'x'</span>, <span class="hljs-number">0</span>, <span class="hljs-string">'x'</span>, <span class="hljs-string">'x'</span>))<font></font>
        self.output_dropout = self.output <span class="hljs-comment"># no dropout in the convolutional layers</span><font></font>
<font></font>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">FullyConnectedLayer</span>(<span class="hljs-params">object</span>):</span><font></font>
<font></font>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, n_in, n_out, activation_fn=sigmoid, p_dropout=<span class="hljs-number">0.0</span></span>):</span><font></font>
        self.n_in = n_in<font></font>
        self.n_out = n_out<font></font>
        self.activation_fn = activation_fn<font></font>
        self.p_dropout = p_dropout<font></font>
        <span class="hljs-comment"># Initialize weights and biases</span><font></font>
        self.w = theano.shared(<font></font>
            np.asarray(<font></font>
                np.random.normal(<font></font>
                    loc=<span class="hljs-number">0.0</span>, scale=np.sqrt(<span class="hljs-number">1.0</span>/n_out), size=(n_in, n_out)),<font></font>
                dtype=theano.config.floatX),<font></font>
            name=<span class="hljs-string">'w'</span>, borrow=<span class="hljs-literal">True</span>)<font></font>
        self.b = theano.shared(<font></font>
            np.asarray(np.random.normal(loc=<span class="hljs-number">0.0</span>, scale=<span class="hljs-number">1.0</span>, size=(n_out,)),<font></font>
                       dtype=theano.config.floatX),<font></font>
            name=<span class="hljs-string">'b'</span>, borrow=<span class="hljs-literal">True</span>)<font></font>
        self.params = [self.w, self.b]<font></font>
<font></font>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">set_inpt</span>(<span class="hljs-params">self, inpt, inpt_dropout, mini_batch_size</span>):</span><font></font>
        self.inpt = inpt.reshape((mini_batch_size, self.n_in))<font></font>
        self.output = self.activation_fn(<font></font>
            (<span class="hljs-number">1</span>-self.p_dropout)*T.dot(self.inpt, self.w) + self.b)<font></font>
        self.y_out = T.argmax(self.output, axis=<span class="hljs-number">1</span>)<font></font>
        self.inpt_dropout = dropout_layer(<font></font>
            inpt_dropout.reshape((mini_batch_size, self.n_in)), self.p_dropout)<font></font>
        self.output_dropout = self.activation_fn(<font></font>
            T.dot(self.inpt_dropout, self.w) + self.b)<font></font>
<font></font>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">accuracy</span>(<span class="hljs-params">self, y</span>):</span>
        <span class="hljs-string">"Return the accuracy for the mini-batch."</span>
        <span class="hljs-keyword">return</span> T.mean(T.eq(y, self.y_out))<font></font>
<font></font>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">SoftmaxLayer</span>(<span class="hljs-params">object</span>):</span><font></font>
<font></font>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, n_in, n_out, p_dropout=<span class="hljs-number">0.0</span></span>):</span><font></font>
        self.n_in = n_in<font></font>
        self.n_out = n_out<font></font>
        self.p_dropout = p_dropout<font></font>
        <span class="hljs-comment">#    </span><font></font>
        self.w = theano.shared(<font></font>
            np.zeros((n_in, n_out), dtype=theano.config.floatX),<font></font>
            name=<span class="hljs-string">'w'</span>, borrow=<span class="hljs-literal">True</span>)<font></font>
        self.b = theano.shared(<font></font>
            np.zeros((n_out,), dtype=theano.config.floatX),<font></font>
            name=<span class="hljs-string">'b'</span>, borrow=<span class="hljs-literal">True</span>)<font></font>
        self.params = [self.w, self.b]<font></font>
<font></font>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">set_inpt</span>(<span class="hljs-params">self, inpt, inpt_dropout, mini_batch_size</span>):</span><font></font>
        self.inpt = inpt.reshape((mini_batch_size, self.n_in))<font></font>
        self.output = softmax((<span class="hljs-number">1</span>-self.p_dropout)*T.dot(self.inpt, self.w) + self.b)<font></font>
        self.y_out = T.argmax(self.output, axis=<span class="hljs-number">1</span>)<font></font>
        self.inpt_dropout = dropout_layer(<font></font>
            inpt_dropout.reshape((mini_batch_size, self.n_in)), self.p_dropout)<font></font>
        self.output_dropout = softmax(T.dot(self.inpt_dropout, self.w) + self.b)<font></font>
<font></font>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">cost</span>(<span class="hljs-params">self, net</span>):</span>
        <span class="hljs-string">"   ."</span>
        <span class="hljs-keyword">return</span> -T.mean(T.log(self.output_dropout)[T.arange(net.y.shape[<span class="hljs-number">0</span>]), net.y])<font></font>
<font></font>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">accuracy</span>(<span class="hljs-params">self, y</span>):</span>
        <span class="hljs-string">"  -."</span>
        <span class="hljs-keyword">return</span> T.mean(T.eq(y, self.y_out))<font></font>
<font></font>
<font></font>
<span class="hljs-comment">#### </span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">size</span>(<span class="hljs-params">data</span>):</span>
    <span class="hljs-string">"    `data`."</span>
    <span class="hljs-keyword">return</span> data[<span class="hljs-number">0</span>].get_value(borrow=<span class="hljs-literal">True</span>).shape[<span class="hljs-number">0</span>]<font></font>
<font></font>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">dropout_layer</span>(<span class="hljs-params">layer, p_dropout</span>):</span><font></font>
    srng = shared_randomstreams.RandomStreams(<font></font>
        np.random.RandomState(<span class="hljs-number">0</span>).randint(<span class="hljs-number">999999</span>))<font></font>
    mask = srng.binomial(n=<span class="hljs-number">1</span>, p=<span class="hljs-number">1</span>-p_dropout, size=layer.shape)
    <span class="hljs-keyword">return</span> layer*T.cast(mask, theano.config.floatX)</code></pre><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">タスク</font></font></h3><br>
<ul>
<li>    SGD       .            ,  .  network3.py ,      .</li>
<li>  Network ,       .</li>
<li> SGD ,       η      (   , , ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=">  </a>).</li>
<li>             ,   .  network3.py,      . ,         ,      .    .</li>
<li>     .</li>
<li>   –     .    ,    ,  ,   ?  .</li>
<li>   ReLU    ,     ( -) .       .  ,    ReLU ( ). ,        c&gt;0     c<sup> L−1</sup> ,  L –  .  ,     softmax?         ReLU?       ? ,    ,       .        ,   ReLU.</li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">シグモイドニューロンに関連する不安定な勾配の問題の分析。</font><font style="vertical-align: inherit;">ネットワークがReLUで構成されている場合、分析はどのように変化しますか？</font><font style="vertical-align: inherit;">不安定な勾配の問題に悩まされないように、そのようなネットワークを変更する良い方法を考え出すことができますか？</font><font style="vertical-align: inherit;">注：「良い」という言葉は、ある程度の調査を意味します。</font><font style="vertical-align: inherit;">そのような変更を行う方法を思いつくのは簡単ですが、本当に優れたテクノロジーを見つけるのに十分な調査を行っていません。</font></font></li>
</ul></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../ja463157/index.html">I2CとSPIディスプレイを接続するESP32-CAMビデオストリーミングサーバー</a></li>
<li><a href="../ja463159/index.html">セキュリティ、数字、メール、そして広告について</a></li>
<li><a href="../ja463165/index.html">電報がDPIを取り消してロックする-偽のTLS</a></li>
<li><a href="../ja463167/index.html">トレーニングVRプロジェクトの開発を開始するために必要な資料</a></li>
<li><a href="../ja463169/index.html">オープンソース補聴器-仕組み</a></li>
<li><a href="../ja463175/index.html">機械学習モデル間の依存関係と継承の可視化</a></li>
<li><a href="../ja463177/index.html">ホームクレジットのサービスデスク。そして中身は？...</a></li>
<li><a href="../ja463179/index.html">ビッグデータビッグビリング：テレコムのビッグデータについて</a></li>
<li><a href="../ja463181/index.html">figma-デザイナーにとってはシンプルなソリューション、レイアウトデザイナーにとっては難しいソリューション</a></li>
<li><a href="../ja463183/index.html">Cisco 200-125 CCNA v3.0のトレーニング。13日目。VLANを構成する</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>