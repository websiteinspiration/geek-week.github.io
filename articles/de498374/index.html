<!doctype html>
<html class="no-js" lang="de">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🈸 🚶 ✅ GPU Computing - Warum, wann und wie. Plus einige Tests 🧖🏾 🧥 👼🏼</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Jeder weiß seit langem, dass man auf Grafikkarten nicht nur Spielzeug spielen, sondern auch Dinge ausführen kann, die nicht mit Spielen zusammenhängen...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>GPU Computing - Warum, wann und wie. Plus einige Tests</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/dbtc/blog/498374/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Jeder weiß seit langem, dass man auf Grafikkarten nicht nur Spielzeug spielen, sondern auch Dinge ausführen kann, die nicht mit Spielen zusammenhängen, z. B. ein neuronales Netzwerk trainieren, sich an Kryptowährung erinnern oder wissenschaftliche Berechnungen durchführen. Wie es passiert ist </font><font style="vertical-align: inherit;">, man kann es lesen </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">hier</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , aber ich wollte das Thema berühren , </font></font><i><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">warum die GPU auf die interessant sein können</font></font></strong></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> durchschnittliche Programmierer (nicht zu GameDev bezogen) , </font></font><i><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">wie zu nähern</font></font></strong></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Entwicklung auf der GPU , </font><font style="vertical-align: inherit;">ohne viel Zeit damit zu verbringen, </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">entscheiden</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , ob Schauen Sie in diese Richtung und "finden Sie </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">an Ihren Fingern heraus", welchen Gewinn Sie erzielen können.</font></font></strong>&nbsp;<br>
<br>
<div style="text-align:center;"><img width="800" src="https://habrastorage.org/getpro/habr/post_images/3ee/2ac/893/3ee2ac8936a685e6993966cfa40f53fd.jpg"></div><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Der Artikel wurde basierend auf meiner </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Präsentation</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> in HighLoad ++ geschrieben. </font><font style="vertical-align: inherit;">Es werden hauptsächlich die von NVIDIA angebotenen Technologien erörtert. </font><font style="vertical-align: inherit;">Ich habe keinen Zweck, für Produkte zu werben, ich gebe sie nur als Beispiel, und wahrscheinlich findet sich etwas Ähnliches bei konkurrierenden Herstellern.</font></font><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Warum auf die GPU zählen?</font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Zwei Prozessoren können nach unterschiedlichen Kriterien verglichen werden. Die wahrscheinlich beliebtesten sind die Häufigkeit und Anzahl der Kerne, die Größe der Caches usw. Letztendlich sind wir jedoch daran interessiert, wie viele Operationen ein Prozessor pro Zeiteinheit ausführen kann, welche Art von Operation dies ist, aber eine separate Frage Eine übliche Metrik ist die Anzahl der Gleitkommaoperationen pro Sekunde - Flops. Und wenn wir warm mit weich und in unserem Fall GPU mit CPU vergleichen möchten, ist diese Metrik nützlich. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die folgende Grafik zeigt das Wachstum dieser Flops im Laufe der Zeit für Prozessoren und Grafikkarten.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5e2/048/3f5/5e20483f59e87b0a395b0fae0e6495c5.png"></div><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(Daten werden aus offenen Quellen gesammelt, es gibt keine Daten für 2019-20 Jahre, weil dort nicht alles so schön ist, aber die GPUs trotzdem gewinnen.)</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Nun, es ist verlockend, nicht </font><i><font style="vertical-align: inherit;">wahr</font></i><font style="vertical-align: inherit;"> ? </font><font style="vertical-align: inherit;">Wir verlagern alle Berechnungen von der CPU auf die GPU und erzielen die achtfache beste Leistung! </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Aber natürlich ist nicht alles so einfach. </font><font style="vertical-align: inherit;">Sie können nicht einfach alles auf die GPU übertragen. Wir werden weiter darüber sprechen.</font></font><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GPU-Architektur und ihr Vergleich mit der CPU</font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ich bringe vielen ein vertrautes Bild mit der Architektur der CPU und den Grundelementen:</font></font><br>
<br>
<div style="text-align:center;"><img width="400" src="https://habrastorage.org/getpro/habr/post_images/df0/8c2/4c3/df08c24c3fe92cd97356670729c318cd.png"></div><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">CPU Core</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Was ist das Besondere? </font><font style="vertical-align: inherit;">Ein Kern und eine Reihe von Hilfsblöcken. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Schauen wir uns nun die GPU-Architektur an:</font></font><br>
<br>
<div style="text-align:center;"><img width="400" src="https://habrastorage.org/getpro/habr/post_images/0fe/138/0cc/0fe1380ccbb321b289d16e39a499009a.png"></div><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GPU-Kern</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Eine Grafikkarte verfügt über viele Prozessorkerne, normalerweise mehrere Tausend, aber sie sind zu Blöcken zusammengefasst. Bei NVIDIA-Grafikkarten sind es normalerweise jeweils 32, und sie haben gemeinsame Elemente, einschließlich und Register. Die Architektur des GPU-Kerns und der logischen Elemente ist viel einfacher als auf der CPU, dh es gibt keine Prefetcher, Brunch-Prädiktoren und vieles mehr. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nun, dies sind die Hauptunterschiede in der Architektur der CPU und der GPU, und tatsächlich legen sie Einschränkungen fest oder eröffnen umgekehrt die Möglichkeiten für das, was wir effektiv auf der GPU lesen können.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ich habe einen weiteren wichtigen Punkt nicht erwähnt. Normalerweise „stöbern“ die Grafikkarte und der Prozessor nicht untereinander und schreiben Daten auf die Grafikkarte und lesen das Ergebnis zurück. Dies sind separate Vorgänge, die sich als „Engpass“ in Ihrem System herausstellen können, ein Diagramm der Pumpzeit im Verhältnis zur Größe Daten werden später in dem Artikel angegeben.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GPU-Einschränkungen und -Funktionen</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Welche Einschränkungen bringt diese Architektur ausführbaren Algorithmen auf:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wenn wir auf einer GPU rechnen, können wir nicht nur einen Kern auswählen, sondern es wird ein ganzer Block von Kernen zugewiesen (32 für NVIDIA).</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Alle Kerne führen die gleichen Anweisungen aus, aber mit unterschiedlichen Daten (wir werden später darauf eingehen) werden solche Berechnungen als Single-Instruction-Multiple-Data oder SIMD bezeichnet (obwohl NVIDIA seine Verfeinerung einführt).&nbsp;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aufgrund des relativ einfachen Satzes von Logikblöcken und allgemeinen Registern mag die GPU die Verzweigung und die komplexe Logik in den Algorithmen wirklich nicht.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Welche Möglichkeiten eröffnet es:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Eigentlich ist die Beschleunigung der gleichen SIMD-Berechnungen. </font><font style="vertical-align: inherit;">Das einfachste Beispiel ist das elementweise Hinzufügen von Matrizen, und wir analysieren es.</font></font></li>
</ul><br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Reduktion klassischer Algorithmen auf SIMD-Darstellung</font></font></h1><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Transformation</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wir haben zwei Arrays, A und B, und wir möchten jedem Element von Array A ein Element aus Array B hinzufügen. Nachfolgend finden Sie ein Beispiel in C, obwohl ich hoffe, dass es für diejenigen klar ist, die diese Sprache nicht sprechen:</font></font><br>
<br>
<pre><code class="cpp hljs"><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">func</span><span class="hljs-params">(<span class="hljs-keyword">float</span> *A, <span class="hljs-keyword">float</span> *B, size)</span>
</span>{ 
   <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; size; i++) <font></font>
   { <font></font>
       A[i] += B[i]<font></font>
   } <font></font>
}<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Klassischer Loopback von Elementen in einer Schleife und linearer Laufzeit. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nun wollen wir sehen, wie ein solcher Code für die GPU aussehen wird:</font></font><br>
<br>
<pre><code class="cpp hljs"><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">func</span><span class="hljs-params">(<span class="hljs-keyword">float</span> *A, <span class="hljs-keyword">float</span> *B, size)</span> 
</span>{ 
   <span class="hljs-keyword">int</span> i = threadIdx.x; 
   <span class="hljs-keyword">if</span> (i &lt; size) <font></font>
      A[i] += B[i] <font></font>
}<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Und hier ist es schon interessant, dass die Variable threadIdx aufgetaucht ist, die wir anscheinend nirgendwo deklariert haben. Ja, sein System bietet uns. Stellen Sie sich vor, dass das Array im vorherigen Beispiel aus drei Elementen besteht und Sie es in drei parallelen Threads ausführen möchten. Dazu müssten Sie einen weiteren Parameter hinzufügen - den Index oder die Stream-Nummer. Dies ist, was die Grafikkarte für uns tut, obwohl sie den Index als statische Variable übergibt und mit mehreren Dimensionen gleichzeitig arbeiten kann - x, y, z. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Eine weitere Nuance: Wenn Sie eine große Anzahl paralleler Streams gleichzeitig starten möchten, müssen die Streams in Blöcke unterteilt werden (ein architektonisches Merkmal von Grafikkarten). Die maximale Blockgröße hängt von der Grafikkarte ab, und der Index des Elements, für das wir Berechnungen durchführen, muss wie folgt ermittelt werden:</font></font><br>
<br>
<pre><code class="cpp hljs"><span class="hljs-keyword">int</span> i = blockIdx.x * blockDim.x + threadIdx.x; <span class="hljs-comment">// blockIdx –  , blockDim –  , threadIdx –    </span>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Als Ergebnis haben wir: viele parallel laufende Threads, die denselben Code ausführen, jedoch unterschiedliche Indizes aufweisen, und dementsprechend Daten, d. H. </font><font style="vertical-align: inherit;">das gleiche SIMD. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dies ist das einfachste Beispiel. Wenn Sie jedoch mit der GPU arbeiten möchten, müssen Sie Ihre Aufgabe in dieselbe Form bringen. </font><font style="vertical-align: inherit;">Leider ist dies nicht immer möglich und kann in einigen Fällen Gegenstand einer Dissertation werden, dennoch können klassische Algorithmen in diese Form gebracht werden.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Anhäufung</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Lassen Sie uns nun sehen, wie die in die SIMD-Darstellung übertragene Aggregation aussehen wird:</font></font><br>
&nbsp;<br>
<div style="text-align:center;"><img width="400" src="https://habrastorage.org/getpro/habr/post_images/ecd/78a/bba/ecd78abbaff0c1be8799c1337f7652f8.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wir haben ein Array von n Elementen. </font><font style="vertical-align: inherit;">In der ersten Stufe starten wir n / 2 Threads und jeder Thread fügt zwei Elemente hinzu, d. H. </font><font style="vertical-align: inherit;">In einer Iteration addieren wir die Hälfte der Elemente im Array. </font><font style="vertical-align: inherit;">Und dann wiederholen wir in der Schleife dasselbe für das neu erstellte Array, bis wir die letzten beiden Elemente aggregieren. </font><font style="vertical-align: inherit;">Wie Sie sehen können, können wir umso weniger parallele Threads starten, je kleiner das Array ist, d. H. </font><font style="vertical-align: inherit;">Auf einer GPU ist es sinnvoll, Arrays mit einer ausreichend großen Größe zusammenzufassen. </font><font style="vertical-align: inherit;">Ein solcher Algorithmus kann verwendet werden, um die Summe der Elemente zu berechnen (vergessen Sie übrigens nicht den möglichen Überlauf des Datentyps, mit dem Sie arbeiten) und nach einem Maximum, Minimum oder nur einer Suche zu suchen.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sortierung</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das Sortieren sieht aber schon viel komplizierter aus. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die zwei beliebtesten Sortieralgorithmen auf der GPU sind:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bitonische Sorte</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Radix-Sortierung</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Radix-Sort wird jedoch immer noch häufiger verwendet, und in einigen Bibliotheken ist eine produktionsbereite Implementierung zu finden. </font><font style="vertical-align: inherit;">Ich werde nicht im Detail analysieren, wie diese Algorithmen funktionieren. Interessierte finden eine Beschreibung der Radix-Sortierung unter </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">https://www.codeproject.com/Articles/543451/Parallel-Radix-Sort-on-the-GPU-using-Cplusplus- AMP</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> und </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">https://stackoverflow.com/a/26229897</font></font></a><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Die Idee ist jedoch, dass selbst ein nichtlinearer Algorithmus wie das Sortieren auf eine SIMD-Ansicht reduziert werden kann. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Und bevor wir uns die reellen Zahlen ansehen, die von der GPU erhalten werden können, wollen wir herausfinden, wie man für dieses Wunder der Technologie programmiert.</font></font><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wo soll man anfangen</font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die gängigsten zwei Technologien, die für die Programmierung unter der GPU verwendet werden können:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Opencl</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cuda</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
OpenCL ist ein Standard, der von den meisten Grafikkartenherstellern unterstützt wird, einschließlich </font><font style="vertical-align: inherit;">Auf Mobilgeräten kann auch in OpenCL geschriebener Code auf der CPU ausgeführt werden. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Sie können OpenCL aus C / C ++ verwenden, es gibt Ordner für andere Sprachen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Für OpenCL hat mir das Buch </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">OpenCL in Action am</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> besten gefallen </font><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Es werden auch verschiedene Algorithmen auf der GPU beschrieben, einschließlich </font><font style="vertical-align: inherit;">Bitonische Sortierung und Radix-Sortierung. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
CUDA ist die proprietäre Technologie und das SDK von NVIDIA. </font><font style="vertical-align: inherit;">Sie können in C / C ++ schreiben oder Bindungen zu anderen Sprachen verwenden.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Der Vergleich von OpenCL und CUDA ist etwas falsch, weil </font><font style="vertical-align: inherit;">Einer ist der Standard, der andere ist das gesamte SDK. </font><font style="vertical-align: inherit;">Trotzdem wählen viele Leute CUDA für die Entwicklung von Grafikkarten, obwohl die Technologie proprietär ist, obwohl sie kostenlos ist und nur auf NVIDIA-Karten funktioniert. </font><font style="vertical-align: inherit;">Dafür gibt es mehrere Gründe:</font></font><br>
<br>
<ul>
<li>  API</li>
<li>    </li>
<li>,   GPU,      (host) </li>
<li> ,  ..  </li>
<li>   </li>
<li>  </li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Zu den Besonderheiten gehört die Tatsache, dass CUDA über einen eigenen Compiler verfügt, der auch Standard-C / C ++ - Code kompilieren kann. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das umfassendste CUDA-Buch, auf das ich gestoßen bin, war </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Professional CUDA C Programming</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , obwohl es bereits etwas veraltet ist, behandelt es dennoch viele technische Nuancen der Programmierung für NVIDIA-Karten. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Aber was ist, wenn ich nicht ein paar Monate damit verbringen möchte, diese Bücher zu lesen, mein eigenes Programm für eine Grafikkarte zu schreiben, zu testen und zu debuggen und dann herauszufinden, dass dies nichts für mich ist?&nbsp; </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wie gesagt, es gibt eine große Anzahl von Bibliotheken , </font><font style="vertical-align: inherit;">die die Komplexität der Entwicklung unter der GPU verstecken: XGBoost, cuBLAS, TensorFlow, PyTorch und andere, werden wir die betrachten </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;">Schub</font></a><font style="vertical-align: inherit;"> Bibliothek</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Da es weniger spezialisiert ist als die anderen oben genannten Bibliotheken, implementiert es gleichzeitig grundlegende Algorithmen, z. B. Sortieren, Suchen, Aggregieren, und kann mit hoher Wahrscheinlichkeit auf Ihre Aufgaben angewendet werden. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Thrust ist eine C ++ - Bibliothek, die darauf abzielt, Standard-STL-Algorithmen durch GPU-basierte Algorithmen zu "ersetzen". </font><font style="vertical-align: inherit;">Das Sortieren eines Arrays von Zahlen mithilfe dieser Bibliothek auf einer Grafikkarte sieht beispielsweise folgendermaßen aus:</font></font><br>
<br>
<pre><code class="cpp hljs"><span class="hljs-function">thrust::host_vector&lt;DataType&gt; <span class="hljs-title">h_vec</span><span class="hljs-params">(size)</span></span>; <span class="hljs-comment">//    </span>
<span class="hljs-built_in">std</span>::generate(h_vec.begin(), h_vec.end(), rand); <span class="hljs-comment">//   </span>
thrust::device_vector&lt;DataType&gt; d_vec = h_vec; <span class="hljs-comment">//        &nbsp;</span>
thrust::sort(d_vec.begin(), d_vec.end()); <span class="hljs-comment">//    </span>
thrust::copy(d_vec.begin(), d_vec.end(), h_vec.begin()); <span class="hljs-comment">//   ,     </span>
</code></pre><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(Vergessen Sie nicht, dass das Beispiel von einem Compiler von NVIDIA kompiliert werden muss.)</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wie Sie sehen können, ist push :: sort einem ähnlichen Algorithmus von STL sehr ähnlich. Diese Bibliothek verbirgt viele Schwierigkeiten, insbesondere die Entwicklung eines Unterprogramms (genauer gesagt des Kernels), das auf der Grafikkarte ausgeführt wird, aber gleichzeitig die Flexibilität verliert. Wenn wir beispielsweise mehrere Gigabyte Daten sortieren möchten, ist es logisch, ein Datenelement an die Karte zu senden, um mit dem Sortieren zu beginnen, und während der Sortierung weitere Daten an die Karte zu senden. Dieser Ansatz wird als Latenzverstecken bezeichnet und ermöglicht eine effizientere Nutzung der Serverzuordnungsressourcen. Wenn wir jedoch Bibliotheken auf hoher Ebene verwenden, bleiben diese Möglichkeiten leider verborgen. Für das Prototyping und die Messung der Leistung sind sie jedoch gleich, insbesondere mit Schub können Sie messen, welchen Overhead die Datenübertragung bietet. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ich habe einen kleinen </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Benchmark geschrieben</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Wenn Sie diese Bibliothek verwenden, in der mehrere gängige Algorithmen mit unterschiedlichen Datenmengen auf der GPU ausgeführt werden, sehen wir uns die Ergebnisse an.</font></font><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ergebnisse des GPU-Algorithmus</font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Um die GPU zu testen, habe ich eine Instanz in AWS mit einer Tesla k80-Grafikkarte erstellt. Dies ist nicht die bisher leistungsstärkste Serverkarte (die leistungsstärkste Tesla v100), aber die günstigste und hat Folgendes an Bord:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">4992 CUDA-Kernel</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">24 GB Speicher</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">480 Gbit / s - Speicherbandbreite&nbsp;</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Und für Tests auf der CPU habe ich eine Instanz mit einer Intel Xeon-Prozessor-CPU E5-2686 v4 bei 2,30 GHz genommen</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Transformation</font></font></h2><br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/930/6e1/687/9306e1687be5ee95c29c8aac7b2ae337.png"></div><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ausführungszeit der Transformation auf der GPU und der CPU in ms</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Wie Sie sehen können, ist die übliche Transformation der Array-Elemente sowohl auf der GPU als auch auf der CPU zeitlich ungefähr gleich. </font><font style="vertical-align: inherit;">Und warum? </font><font style="vertical-align: inherit;">Da der Overhead für das Senden von Daten an die Karte und zurück den gesamten Leistungsschub verschlingt (wir werden den Overhead separat behandeln) und es relativ wenige Berechnungen auf der Karte gibt. </font><font style="vertical-align: inherit;">Vergessen Sie auch nicht, dass Prozessoren auch SIMD-Anweisungen unterstützen und Compiler diese in einfachen Fällen effektiv verwenden können.&nbsp;</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Lassen Sie uns nun sehen, wie effizient die Aggregation auf der GPU erfolgt.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Anhäufung</font></font></h2><br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c92/d0e/cb9/c92d0ecb96c32866000e6948f5da61f9.png"></div><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ausführungszeit der Aggregation auf GPU und CPU in ms</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Im Aggregationsbeispiel sehen wir bereits eine signifikante Leistungssteigerung mit zunehmendem Datenvolumen. </font><font style="vertical-align: inherit;">Es lohnt sich auch, darauf zu achten, dass wir eine große Datenmenge in den Speicher der Karte pumpen und nur ein aggregierter Wert zurückgenommen wird, d. H. </font><font style="vertical-align: inherit;">Der Overhead für die Übertragung von Daten von der Karte in den RAM ist minimal. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Kommen wir zum interessantesten Beispiel - dem Sortieren.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sortierung</font></font></h2><br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fde/504/8da/fde5048da5084d1f0902c9362b21d939.png"></div><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sortierzeit für GPU und CPU in ms</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Trotz der Tatsache, dass wir das gesamte Datenarray an die Grafikkarte senden und umgekehrt, ist das Sortieren von 800 MB Daten an die GPU ungefähr 25-mal schneller als auf dem Prozessor.</font></font><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Datenübertragungsaufwand</font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wie aus dem Transformationsbeispiel hervorgeht, ist es nicht immer offensichtlich, ob die GPU auch bei parallelen Aufgaben effektiv ist. </font><font style="vertical-align: inherit;">Der Grund dafür ist ein Overhead für die Übertragung von Daten aus dem RAM des Computers in den Speicher der Grafikkarte (in Spielekonsolen wird der Speicher übrigens von der CPU und der GPU gemeinsam genutzt, und es besteht keine Notwendigkeit, Daten zu übertragen). </font><font style="vertical-align: inherit;">Eine der Eigenschaften einer Grafikkarte ist die Speicherbandbreite oder Speicherbandbreite, die die theoretische Bandbreite der Karte bestimmt. </font><font style="vertical-align: inherit;">Für Tesla k80 sind es 480 GB / s, für Tesla v100 sind es bereits 900 GB / s. </font><font style="vertical-align: inherit;">Die PCI Express-Version und die Implementierung der Datenübertragung auf die Karte wirken sich auch auf den Durchsatz aus. Dies kann beispielsweise in mehreren parallelen Streams erfolgen.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Schauen wir uns die praktischen Ergebnisse an, die für die Tesla k80-Grafikkarte in der Amazon-Cloud erzielt wurden:</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/798/fb5/613/798fb56139f6158566232bc6283b24e7.png"></div><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zeit zum Übertragen von Daten auf die GPU, Sortieren und </font></font></i><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zurücksenden von Daten in den </font></font><br>
<br><font style="vertical-align: inherit;"></font><br>
<br><font style="vertical-align: inherit;"></font></i><font style="vertical-align: inherit;"><i><font style="vertical-align: inherit;">RAM in ms </font></i><i><font style="vertical-align: inherit;">HtoD - Übertragen von Daten auf die </font></i><i><font style="vertical-align: inherit;">
GPU </font></i><i><font style="vertical-align: inherit;">-Grafikkarte </font></i><i><font style="vertical-align: inherit;">Ausführung - Sortieren auf der Grafikkarte </font></i><i><font style="vertical-align: inherit;">
DtoH - Kopieren von Daten von der Grafikkarte in den RAM</font></i></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Das erste, was zu beachten ist, ist, dass das Lesen von Daten von der Grafikkarte schneller ist als schreibe sie dort auf. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Zweitens: Wenn Sie mit einer Grafikkarte arbeiten, können Sie eine Latenz von 350 Mikrosekunden erreichen. Dies reicht möglicherweise bereits für einige Anwendungen mit geringer Latenz aus. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die folgende Tabelle zeigt einen Overhead für weitere Daten:</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d18/653/b96/d18653b96af325f35fade713bdaa8dae.png"></div><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zeit zum Übertragen von Daten an die GPU, Sortieren und Zurücksenden von Daten in den RAM in ms</font></font></i><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Servernutzung</font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die häufigste Frage ist, wie sich eine Spiel-Grafikkarte von einer Server-Grafikkarte unterscheidet. </font><font style="vertical-align: inherit;">Entsprechend den Merkmalen sind sie sehr ähnlich, aber die Preise unterscheiden sich erheblich.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/203/01b/741/20301b7418ee616d9611f42d2b4a8f5d.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die Hauptunterschiede zwischen dem Server (NVIDIA) und der Spielkarte:</font></font><br>
<br>
<ul>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Herstellergarantie</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (die Spielkarte ist nicht für die Verwendung auf Servern vorgesehen)</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mögliche </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Virtualisierungsprobleme</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> für eine Consumer-Grafikkarte</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Verfügbarkeit des Fehlerkorrekturmechanismus auf der Serverkarte</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Anzahl der parallelen Threads (keine CUDA-Kerne) oder die Unterstützung für Hyper-Q, mit der Sie mit der Karte von mehreren Threads auf der CPU aus arbeiten können. Laden Sie beispielsweise Daten von einem Thread auf eine Karte hoch und starten Sie Berechnungen von einem anderen</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dies sind vielleicht die wichtigsten Unterschiede, die ich gefunden habe.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Multithreading</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nachdem wir herausgefunden haben, wie der einfachste Algorithmus auf der Grafikkarte ausgeführt wird und welche Ergebnisse zu erwarten sind, ist die nächste logische Frage, wie sich die Grafikkarte bei der Verarbeitung mehrerer paralleler Anforderungen verhält. Als Antwort habe ich zwei Diagramme des Rechnens auf der GPU und einen Prozessor mit 4 und 32 Kernen:</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0a1/1f6/022/0a11f6022198a582929f384be357fe43.png"></div><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Zeit, die benötigt wird, um mathematische Berechnungen auf der GPU und der CPU mit Matrizen von 1000 x 60 in ms durchzuführen</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
. Dieses Diagramm führt Berechnungen mit Matrizen von 1000 x 60 Elementen durch. </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Berechnungen werden</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> aus mehreren Programmströmen </font><font style="vertical-align: inherit;">gestartet. </font><font style="vertical-align: inherit;">Für jeden CPU-Stream wird ein separater Stream für die GPU erstellt (es wird genau das Hyper-Q verwendet).&nbsp; </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wie Sie sehen, kommt der Prozessor mit dieser Last sehr gut zurecht, während die Latenz für eine Anforderung pro GPU mit zunehmender Anzahl paralleler Anforderungen erheblich zunimmt.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e09/22c/7fb/e0922c7fba0ef001cca97c7a99817c83.png"></div><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Zeit für die Durchführung mathematischer Berechnungen auf der GPU und der CPU mit Matrizen von 10.000 x 60 in ms.</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
In der zweiten Grafik sind dieselben Berechnungen, jedoch mit zehnmal längeren Matrizen, und die GPU verhält sich unter einer solchen Last viel besser. Diese Grafiken sind sehr bezeichnend, und wir können daraus schließen: Das Verhalten unter Last hängt von der Art der Last selbst ab. Ein Prozessor kann Matrixberechnungen auch recht effizient durchführen, jedoch bis zu einem gewissen Grad. Für eine Grafikkarte ist es charakteristisch, dass bei einer kleinen Rechenlast die Leistung ungefähr linear abfällt. Mit zunehmender Last und der Anzahl paralleler Threads kommt die Grafikkarte besser zurecht.&nbsp;</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Es ist schwierig zu vermuten, wie sich die GPU in verschiedenen Situationen verhält. Wie Sie jedoch sehen können, kann eine Serverkarte unter bestimmten Bedingungen Anforderungen aus mehreren parallelen Streams recht effizient verarbeiten. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wir werden einige weitere Fragen besprechen, die Sie möglicherweise haben, wenn Sie sich dennoch für die Verwendung der GPU in Ihren Projekten entscheiden.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ressourcenlimit</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wie bereits erwähnt, sind die beiden Hauptressourcen einer Grafikkarte Rechenkerne und Speicher. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Zum Beispiel haben wir mehrere Prozesse oder Container, die eine Grafikkarte verwenden, und wir möchten die Grafikkarte zwischen ihnen teilen können. </font><font style="vertical-align: inherit;">Leider gibt es dafür keine einfache API. </font><font style="vertical-align: inherit;">NVIDIA bietet </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">vGPU-</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Technologie an </font><font style="vertical-align: inherit;">, aber ich habe die Tesla k80-Karte nicht in der Liste der unterstützten Karten gefunden. Soweit ich aus der Beschreibung ersehen kann, konzentriert sich die Technologie mehr auf virtuelle Anzeigen als auf Berechnungen. </font><font style="vertical-align: inherit;">Vielleicht bietet AMD etwas passenderes an. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wenn Sie die GPU in Ihren Projekten verwenden möchten, sollten Sie sich daher darauf verlassen, dass die Anwendung ausschließlich die Grafikkarte verwendet, oder Sie steuern programmgesteuert die Größe des zugewiesenen Speichers und die Anzahl der für Berechnungen verwendeten Kerne.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Container und GPU</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wenn Sie das Ressourcenlimit herausgefunden haben, lautet die folgende logische Frage: Was ist, wenn sich mehrere Grafikkarten auf dem Server befinden? </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Auch hier können Sie auf Anwendungsebene entscheiden, welche GPU verwendet wird. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ein weiterer bequemer Weg sind Docker-Container. </font><font style="vertical-align: inherit;">Sie können normale Container verwenden, aber NVIDIA bietet seine </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">NGC-</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Container </font><font style="vertical-align: inherit;">mit optimierten Versionen verschiedener Software, Bibliotheken und Treibern an. </font><font style="vertical-align: inherit;">Für einen Container können Sie die Anzahl der verwendeten GPUs und deren Sichtbarkeit für den Container begrenzen. </font><font style="vertical-align: inherit;">Der Overhead bei der Containernutzung beträgt ca. 3%.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Arbeiten Sie in einem Cluster</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Eine andere Frage: Was tun, wenn Sie eine Aufgabe auf mehreren GPUs innerhalb desselben Servers oder Clusters ausführen möchten? </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wenn Sie eine Bibliothek gewählt haben, die Schub ähnelt, oder eine Lösung auf niedrigerer Ebene, muss die Aufgabe manuell gelöst werden. </font><font style="vertical-align: inherit;">Hochrangige Frameworks, beispielsweise für maschinelles Lernen oder neuronale Netze, unterstützen normalerweise die Möglichkeit, mehrere Karten sofort zu verwenden. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Darüber hinaus möchte ich darauf hinweisen, dass NVIDIA beispielsweise eine Schnittstelle für den direkten Datenaustausch zwischen Karten bietet - </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">NVLINK</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , die erheblich schneller als PCI Express ist. </font><font style="vertical-align: inherit;">Und es gibt eine Technologie für den direkten Zugriff auf den Speicher der Karte von anderen PCI Express-Geräten - </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GPUDirect RDMA</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , inkl. </font><font style="vertical-align: inherit;">und </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Netzwerk</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Empfehlungen</font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wenn Sie erwägen, die GPU in Ihren Projekten zu verwenden, ist die GPU höchstwahrscheinlich für Sie geeignet, wenn:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ihre Aufgabe kann auf eine SIMD-Ansicht reduziert werden</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Es ist möglich, die meisten Daten vor den Berechnungen auf die Karte zu laden (Cache)</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Herausforderung besteht in intensivem Computing</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Sie sollten auch im Voraus Fragen stellen:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wie viele parallele Abfragen werden sein&nbsp;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Welche Latenz erwarten Sie?</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Benötigen Sie eine Karte für Ihre Last? Benötigen Sie einen Server mit mehreren Karten oder einen Cluster von GPU-Servern?&nbsp;</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das ist alles, ich hoffe, dass das Material für Sie nützlich ist und Ihnen hilft, die richtige Entscheidung zu treffen!</font></font><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Verweise</font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Benchmark und Ergebnisse auf github - </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">https://github.com/tishden/gpu_benchmark/tree/master/cuda</font></font></a><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Zusätzlich zum Thema eine Aufzeichnung des Berichts </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">„GPU-Datenbanken - Architektur, Leistung und Nutzungsaussichten“</font></font></a><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
NVIDIA NGC Containers Webinar - </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">http : //bit.ly/2UmVIVt</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> oder </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">http://bit.ly/2x4vJKF</font></font></a></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de498362/index.html">Kingston ist weiterhin führend bei SSD-Sendungen: Wie machen wir das?</a></li>
<li><a href="../de498366/index.html">Welche Algorithmen implementieren Yandex-Entwickler täglich?</a></li>
<li><a href="../de498368/index.html">Die Geschichte eines Schalters</a></li>
<li><a href="../de498370/index.html">SAP UI5 und Bestätigungsfenster: Nochmals zum Kontext</a></li>
<li><a href="../de498372/index.html">Netzwerksimulator-Tutorial ns-3. Kapitel 5</a></li>
<li><a href="../de498378/index.html">Ankündigung von Slurms Abendschule durch Agile</a></li>
<li><a href="../de498380/index.html">Overtons Fenster in Aktion: Wie eine Pandemie eingesetzt wird, um unsere Freiheit einzuschränken</a></li>
<li><a href="../de498390/index.html">IAR + Clion = Freundschaft</a></li>
<li><a href="../de498392/index.html">18 GitLab-Funktionen werden Open Source</a></li>
<li><a href="../de498394/index.html">7 kostenlose Analoga von Screaming Frog und Netpeak Spider</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>