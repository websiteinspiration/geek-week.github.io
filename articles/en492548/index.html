<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üí∑ ‚úçüèø üßîüèº Unity Machine Learning: Teaching MO Agents to Jump Over Walls üòù üíÖüèø üìØ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="There have been major breakthroughs in reinforcement learning (RL) over the past few years: from the first successful use of it in raw pixel training ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Unity Machine Learning: Teaching MO Agents to Jump Over Walls</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/pixonic/blog/492548/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">There have been major breakthroughs in reinforcement learning (RL) over the past few years: from the first successful use of it in raw pixel training to the training of Open AI Roborists, and increasingly sophisticated environments are needed for further progress, to which help Unity comes. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The Unity ML-Agents tool is a new plugin in the Unity game engine, allowing you to use Unity as an environment constructor for training MO agents. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
From playing football to walking, jumping from walls, and training AI dogs to play sticks, the Unity ML-Agents Toolkit provides a wide range of conditions for training agents. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In this article, we will look at how Unity MO agents work, and then we will teach one of these agents to jump over walls.</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/9c6/5ad/bdb/9c65adbdbb38b939262c10e5ac728dde.gif" alt="image"><br>
<br>
<a name="habracut"></a><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">What is Unity ML-Agents?</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Unity ML-Agents is a new plugin for the Unity game engine, which allows you to create or use ready-made environments for training our agents. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The plugin consists of three components: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/zo/mx/m_/zomxm_3g9svsej0iarjpw1vbnta.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The first - a </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">learning environment</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ( </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">the Learning Environment</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ), containing scenes of Unity and environmental elements. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The second is the </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Python API</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , which contains the RL algorithms (such as PPO - Proximal Policy Optimization and SAC - Soft Actor-Critic). </font><font style="vertical-align: inherit;">We use this API to launch training, testing, etc. It is connected to the learning environment through the third component - </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">an external communicator</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">What the learning environment consists of</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The training component consists of various elements: </font></font><br>
 <br>
<img src="https://habrastorage.org/webt/hy/1u/pt/hy1upt9en2esnt-amk-iipklmn4.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The first agent is the stage actor. </font><font style="vertical-align: inherit;">It is he who we will train, optimizing a component called the ‚Äúbrain‚Äù (Brain), in which it is recorded what actions need to be performed in each of the possible states. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The third element, the Academy, manages agents and their decision-making processes and processes requests from the Python API. </font><font style="vertical-align: inherit;">To better understand its role, let's recall the RL process. </font><font style="vertical-align: inherit;">It can be represented as a cycle that works as follows: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/5y/ge/ef/5ygeefzasafou6bkwggdnaq7ows.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Suppose an agent needs to learn how to play a platformer. </font><font style="vertical-align: inherit;">The RL process in this case will look like this:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The agent receives state </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">S </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0</font></font></sub></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> from the environment - this will be the first frame of our game.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Based on state </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">S </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0, the</font></font></sub></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> agent performs action </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0</font></font></sub></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> and shifts to the right.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The environment goes into a new state </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">S </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Agent receives </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">R </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> reward </font><font style="vertical-align: inherit;">for not being dead ( </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Positive reward</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> +1).</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
This RL cycle forms a sequence of state, action and reward. </font><font style="vertical-align: inherit;">The agent's goal is to maximize the expected total reward. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/qm/mr/ml/qmmrmlkz7urj8cywqpsg1ejajve.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Thus, Academy sends instructions to agents and provides synchronization in their execution, namely:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Collection of observations;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The choice of action in accordance with the laid down instructions;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Action execution;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Reset if the number of steps has been reached or the goal has been reached.</font></font></li>
</ul><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We teach the agent to jump through the walls</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Now that we know how Unity agents work, we will train one to jump through walls. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Already trained models can also be downloaded on </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GitHub</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wall Jumping Learning Environment</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The goal of this environment is to teach the agent to get to the green tile. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Consider three cases: </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
1. There are no walls, and our agent just needs to get to the tile. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/17a/8fd/da8/17a8fdda8bbe767f64a34d8d05fd9be9.png" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
2. The agent needs to learn how to jump in order to reach the green tile. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/d2c/149/0d0/d2c1490d0b90c4fdefb8eb4922f446b2.png" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
3. The most difficult case: the wall is too high for the agent to jump over, so he needs to jump onto the white block first. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/458/175/9b6/4581759b653a175597054fcf60a0050e.png" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We will teach the agent two scenarios of behavior depending on the height of the wall:</font></font><br>
<br>
<ul>
<li><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SmallWallJump</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> in cases without walls or at low wall heights;</font></font></li>
<li><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">BigWallJump</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> in the case of high walls.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
This is what the reward system will look like: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/rt/lo/hm/rtlohmnturcucyikrnhfsadztgi.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In our observations, we are not using a regular frame, but 14 reykast, each of which can detect 4 possible objects. </font><font style="vertical-align: inherit;">In this case, reykast can be perceived as laser beams that can determine whether they pass through an object. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We will also use the global agent position in our program. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/a06/0fc/fa2/a060fcfa26e80154ef988d76dc9b8f05.png" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Four options are possible in our space: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ni/g6/zs/nig6zskfzterisy_hskg3k-rsnk.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The goal is to achieve a green tile </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">with an average reward of 0.8</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">So let's get started!</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
First of all, open the </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">UnitySDK</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> project </font><font style="vertical-align: inherit;">. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Among the examples you need to find and open the </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">WallJump</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> scene </font><font style="vertical-align: inherit;">. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As you can see, there are many agents on the stage, each of which is taken from the same prefab, and they all have the same ‚Äúbrain‚Äù. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/bad/195/820/bad195820da3e5b2c74309e9f44e9935.png" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As with the classic Deep Reinforcement Learning, after we launched several instances of the game (for example, 128 parallel environments), now we simply copy and paste the agents to have more different states. And since we want to train our agent from scratch, first of all we need to remove the ‚Äúbrain‚Äù from the agent. To do this, go to the prefabs folder and open Prefab.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Next, in the Prefab hierarchy, you need to select the agent and go to the settings. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In the Behavior Parameters, you need to delete the model. If we have several GPUs at our disposal, you can use the Inference Device from the CPU as a GPU. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/d0d/bdb/6ec/d0dbdb6ec05655b6fecdf675d629d126.png" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In the Wall Jump Agent component, you must remove the Brains for a case with no walls, as well as for low and high walls. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/618/e81/c8c/618e81c8c3c0cb15ae5fbca791c33c8c.png" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
After that, you can start training your agent from scratch. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
For our first training, we simply change the total number of training steps for two behavior scenarios: SmallWallJump and BigWallJump. So we can achieve the goal in just 300 thousand steps. To do this, in </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">config / trainer config.yaml,</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> change max_steps to 3e5 for the SmallWallJump and BigWallJump cases.</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/b80/bbb/1b2/b80bbb1b21f0f9e445fa8d2583c39d52.png" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
To train our agent, we will use </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PPO</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (Proximal Policy Optimization). The algorithm includes the accumulation of experience in interacting with the environment and using it to update decision-making policies. After updating it, the previous events are discarded, and the subsequent data collection is carried out already under the terms of the updated policy. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
So, first, using the Python API, we need to call an external communicator so that it instructs the Academy to launch agents. To do this, open the terminal where ml-agents-master is located and type in it: </font></font><br>
<br>
<code>mlagents-learn config/trainer_config.yaml ‚Äî run-id=‚ÄùWallJump_FirstTrain‚Äù ‚Äî train</code><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
This command will ask you to start the Unity scene. To do this, press ‚ñ∫ at the top of the editor. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/212/874/4fb/2128744fb143fec6308d95938e0383f7.png" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
You can watch the training of your agents in Tensorboard with the following command:</font></font><br>
<br>
<code>tensorboard ‚Äî logdir=summaries</code><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
When the training is over, you need to move the saved model files contained in </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ml-agents-master / models to UnitySDK / Assets / ML-Agents / examples / WallJump / TFModels</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Then, open the Unity editor again and select the </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">WallJump</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> scene </font><font style="vertical-align: inherit;">, where we open the finished </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">WallJumpArea</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> object </font><font style="vertical-align: inherit;">. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
After that, select the agent and in its behavior parameters drag the </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SmallWallJump.nn</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> file </font><font style="vertical-align: inherit;">into the Model Placeholder. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/d0d/bdb/6ec/d0dbdb6ec05655b6fecdf675d629d126.png" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Also move:</font></font><br>
<br>
<ol>
<li><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SmallWallJump.nn</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> at No Wall Brain Placeholder.</font></font></li>
<li><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SmallWallJump.nn</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> at Small Wall Brain Placeholder.</font></font></li>
<li><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">BigWallJump.nn</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> at No Wall Brain Placeholder.</font></font></li>
</ol><br>
<img src="https://habrastorage.org/getpro/habr/post_images/618/e81/c8c/618e81c8c3c0cb15ae5fbca791c33c8c.png" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
After that, press the ‚ñ∫ button at the top of the editor and you're done! </font><font style="vertical-align: inherit;">The agent training configuration algorithm is now complete.</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/9c6/5ad/bdb/9c65adbdbb38b939262c10e5ac728dde.gif" alt="image"><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Experiment time</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The best way to learn is to constantly try to bring something new. </font><font style="vertical-align: inherit;">Now that we have already achieved good results, we will try to put some hypotheses and test them.</font></font><br>
<br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Reducing the discount coefficient to 0.95</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
So we know that:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The larger the gamma, the lower the discount. </font><font style="vertical-align: inherit;">That is, the agent is more concerned about long-term rewards.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">On the other hand, the smaller the gamma, the greater the discount. </font><font style="vertical-align: inherit;">In this case, the agent's priority is short-term compensation.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The idea of ‚Äã‚Äãthis experiment is that if we increase the discount by decreasing the gamut from 0.99 to 0.95, the short-term reward will be a priority for the agent - which may help him to quickly approach the optimal behavior policy. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/u_/cm/hv/u_cmhvpc0dk-npolzz49loxofiq.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Interestingly, in the case of a jump through a low wall, the agent will strive for the same result. </font><font style="vertical-align: inherit;">This can be explained by the fact that this case is quite simple: the agent only needs to move to the green tile and, if necessary, jump if there is a wall in front. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/yc/ic/tt/ycicttb0mkwxmwn5c1ghgm2iiiu.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
On the other hand, in the case of Big Wall Jump, this works worse, because our agent cares more about the short-term reward and therefore does not understand that he needs to climb onto the white block in order to jump over the wall.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Increased neural network complexity</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Finally, we hypothesize whether our agent will become smarter if we increase the complexity of the neural network. </font><font style="vertical-align: inherit;">To do this, increase the size of the hidden level from 256 to 512. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
And we find that in this case the new agent works worse than our first agent. </font><font style="vertical-align: inherit;">This means that it makes no sense for us to increase the complexity of our network, because otherwise the learning time will also increase. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/zo/g7/qm/zog7qm1uemu7l7taqc5uuwlqauw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
So, we trained the agent to jump over the walls, and that's all for today. </font><font style="vertical-align: inherit;">Recall that to compare the results, trained models can be downloaded </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">here</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/af6/f00/ce3/af6f00ce3784c60469522296276d2651.gif" alt="image"></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../en492530/index.html">Combine-based MVVMs in UIKit and SwiftUI Applications for UIKit Developers</a></li>
<li><a href="../en492534/index.html">So are there real hurricanes in Moscow or not? We analyze the case of March 13, 2020 in hot pursuit</a></li>
<li><a href="../en492538/index.html">Wrike TechClub: Delivery infrastructure - processes and tools (DevOps + QAA). Papers in English</a></li>
<li><a href="../en492540/index.html">The game "Wait a moment!" on arduino</a></li>
<li><a href="../en492546/index.html">Checking the vulnerability of any site using Nikto</a></li>
<li><a href="../en492552/index.html">How to live and work in quarantine in Barcelona</a></li>
<li><a href="../en492558/index.html">Hello, this is COVID19: Does the coronavirus live on the surface of a smartphone?</a></li>
<li><a href="../en492560/index.html">Simple hash table for GPU</a></li>
<li><a href="../en492562/index.html">Three useful Apache Ignite webinars in your quarantine program</a></li>
<li><a href="../en492566/index.html">Analysis of the combination of a greedy click search algorithm with partial enumeration of graph vertices</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>