<!doctype html>
<html class="no-js" lang="zh-CN">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🐧 👎 🧑🏽‍🤝‍🧑🏻 通过竞争性神经网络加强学习 🧦 🏹 🗄️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="在经典的“井字游戏”中，有机会展示所有可能的动作-永不输。我利用这次机会来衡量我在游戏神经网络中的训练水平。
 
 强化培训对于决策含糊不清的任务将非常有用，因为选择每个行动具有不同结果的多种选择使其变得复杂。
 
 当然，井字游戏看起来并不难，很难用增援来训练他们。但是，它非常适合通过竞争性网络来...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>通过竞争性神经网络加强学习</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/505574/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">在经典的“井字游戏”中，有机会展示所有可能的动作-永不输。我利用这次机会来衡量我在游戏神经网络中的训练水平。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
强化培训对于决策含糊不清的任务将非常有用，因为选择每个行动具有不同结果的多种选择使其变得复杂。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
当然，井字游戏看起来并不难，很难用增援来训练他们。但是，它非常适合通过竞争性网络来掌握培训方法，这将提高质量并减少培训网络所花费的时间。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
接下来，我将在井字游戏中描述经过竞争性网络强化的一般学习算法，并演示受过训练的网络做出“有意义”的动作（即游戏）。</font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">录制经过训练的网络的游戏，</font></font></a><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">从头开始</font></font></a><font style="vertical-align: inherit;"><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"><font style="vertical-align: inherit;">训练网络</font></a><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"><font style="vertical-align: inherit;">，从</font></a></font><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">源头</font></font></a><font style="vertical-align: inherit;"><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"><font style="vertical-align: inherit;">开始，</font></a></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
您还可以通过单击相应的按钮从GitHub输入经过预先</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" rel="nofollow"><font style="vertical-align: inherit;">训练的</font></a><font style="vertical-align: inherit;">模型，以立即开始测试神经网络。</font></font><br>
<a name="habracut"></a><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">训练神经网络的第一步</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
除了神经元具有激活功能（修正了神经网络的最终解决方案）这一事实外，我们还可以说神经元是网络记忆。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
由于反误差在“上”层中的传播，每一层都增加了学习时间，并且信号在到达“上”层之前逐渐淡出，从而开始了决策的路径。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
在对网络设置进行了几种选择之后，我得出的结论是，对于一个具有3x3场的简单游戏，使用具有128个神经元的单层网络就足够了。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
网络不应有太多的内存，这可能导致重新训练-完全记住游戏结果的所有选项。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
神经网络的优势在于在有限内存条件下基于输入数据逼近解决方案的表现力。</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">推广代理商的一般规则</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
对于通过神经网络进行的相对预测，每个细胞都有动态的奖励，具体取决于其当前对代理的重要性。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
在输出中，神经网络预测代理将去往的细胞的索引。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
奖池的奖励具有以下分布：</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
越少移动，奖励从0.1到1.0越高</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
被占领的奖励有-1.0 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
的奖励受到奖励-0.4失败的奖励将获得的奖励-0.4 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
自由的奖励有0.1 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
奖励向免费的奖励增加了其奖励至0.2 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
获胜的对手的奖励有0.5 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
获奖机会带来奖励1.0</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">竞争性神经网络训练</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
在比赛中，将在竞争环境中对代理商进行培训，这将带来新的游戏结果，并提高针对新情况的培训质量。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
每个特工都有自己的领域来训练，以进入一个空的单元并建立成功的举动组合。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
特工在家里玩9场比赛，然后转移1场比赛到竞争领域，在这场比赛中，直到获胜者限步9次，然后再重复一切。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
在每个游戏结束时，两个网络都经过训练，可以在共同的运动场上获得新的对抗经验。</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">对手预防</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
该网络需要接受培训以争夺现场胜利，即 </font><font style="vertical-align: inherit;">通过增加细胞奖励来成功防止赢得对手，从而获得奖励。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
训练神经网络的另一个指标是比赛中的胜利指标。</font><font style="vertical-align: inherit;">如果一个玩家的获胜幅度太大，则网络可能学习不正确，并且其原因是对代理行为的奖励不正确，或者某些其他行为及其奖励未考虑在内。</font><font style="vertical-align: inherit;">训练的最佳结果可以认为是网络几乎相等，获胜和失败次数相同的情况。</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">与人竞争学习</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
训练与人玩耍的神经网络的实现与代理之间的竞争没有太大不同。</font><font style="vertical-align: inherit;">唯一的严重区别是该人最初的玩法合理。</font><font style="vertical-align: inherit;">与这样的对手的聚会会为坐席创造更多的情况，这将有利地影响他的比赛经验，并因此影响他的训练。</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">完成时间</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
仅在引入竞争算法之后，神经网络才学习打井字游戏，这使它能够学习如何根据对手的动作来做出动作，尽管并非如最初计划的那样完美。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
总的来说，我认为该项目已经成功完成-目标已经实现。</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">感谢您的关注！</font></font></h2><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ps训练竞争网络，这使您可以从不同的角度看待简单的游戏。</font></font></i></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN505554/index.html">在“青年技术员与发明家”竞赛中，不需要真正的青年发明家</a></li>
<li><a href="../zh-CN505556/index.html">苹果追踪被抢劫的iPhone并向警方提供抢劫者</a></li>
<li><a href="../zh-CN505558/index.html">Amazon DeepLens深度学习相机。解包，连接和部署项目</a></li>
<li><a href="../zh-CN505560/index.html">CS中心的产品管理计划的第二组：学生怎么说</a></li>
<li><a href="../zh-CN505568/index.html">在Delphi上使用管道和其他小东西传输文件</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>