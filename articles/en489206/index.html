<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👨🏽‍🎨 👩🏼‍🚀 🌠 Crash stories with Patroni, or How to drop a PostgreSQL cluster 🖐🏿 🧑🏻‍🤝‍🧑🏻 🌉</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="PostgreSQL does not have High Availability out of the box. To achieve HA, you need to put something in, set up - make an effort. There are several too...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Crash stories with Patroni, or How to drop a PostgreSQL cluster</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/oleg-bunin/blog/489206/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PostgreSQL does not have High Availability out of the box. To achieve HA, you need to put something in, set up - make an effort. There are several tools that can help increase the availability of PostgreSQL, and one of them is Patroni. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
At first glance, putting Patroni in a test environment, you can see what a great tool it is and how it easily handles our attempts to break up the cluster. But in practice, in a production environment, not everything always happens so beautifully and elegantly. Data Egret started using Patroni at the end of 2018 and gained some experience: how to diagnose it, configure it, and when not to rely on the auto-filer at all. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
At HighLoad ++, Alexei Lesovsky described in detail, using examples and log analysis, the typical problems that arise when working with Patroni, and best practice to overcome them.</font></font><br>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/lqh1eJwVPtk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The article will not: Patroni installation instructions and configuration examples; </font><font style="vertical-align: inherit;">problems outside of Patroni and PostgreSQL; </font><font style="vertical-align: inherit;">stories based on other people's experiences, but only those problems that the Data Egret figured out for themselves.</font></font><br>
<a name="habracut"></a><br>
<strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">About the speaker:</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Alexey Lesovsky (</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" class="user_link"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">lesovsky</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">) started as a system administrator (Linux system administrator), worked in web development (PostgreSQL database administrator). </font><font style="vertical-align: inherit;">Since 2014 she has been working at Data Egret. </font><font style="vertical-align: inherit;">Data Egret is engaged in consulting in the field of PostgreSQL, helps many, many companies to use PostgreSQL correctly and, of course, has gained extensive experience in operating the database. </font></font><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The report that this article is based on is called "Patroni Failure Stories or How to crash your PostgreSQL cluster", here is a </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">link to the presentation</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font></i><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Before you start</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Let me remind you what Patroni is, what it is intended for and what it can do. </font></font><br>
<br>
<strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Patroni is a template for building HA out of the box.</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> So it is written in the documentation and from my point of view - this is a very correct clarification. That is, Patroni is not a silver bullet that he set and it will solve all problems. It is necessary to make efforts so that it starts to work and bring benefits. </font></font><br>
<br>
<strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Patroni - agent service.</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> It is installed on each server with a database and is a kind of init-system for PostgreSQL: it starts, stops, restarts, changes the configuration and topology of the cluster. </font></font><br>
<br>
<strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Patroni stores "cluster state" in DCS.</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">To store the cluster state, its current view, you need storage. </font><font style="vertical-align: inherit;">Patroni stores state in an external system — a distributed configuration repository. </font><font style="vertical-align: inherit;">This can be one of the options: Etcd, Consul, ZooKeeper or Etcd Kubernetes. </font></font><br>
<br>
<strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Patroni AutoFile is enabled by default. </font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">You get an auto-filer out of the box, immediately after installing Patroni. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Plus there are many other things, such as: servicing configurations, creating new replicas, backing up, etc. </font><font style="vertical-align: inherit;">But this time it will remain outside the scope of the report.</font></font><br>
<blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The main objective of Patroni is to provide a reliable auto-filer.</font></font></blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Patroni does not have to monitor equipment, send out notifications, do complicated things about potential accident prevention, etc. </font><font style="vertical-align: inherit;">It is required that the cluster remain operational and the application can continue to work with the database, regardless of any changes in the cluster topology. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
But when we start using Patroni with PostgreSQL, our system gets a little more complicated. </font><font style="vertical-align: inherit;">Now, in addition to the database itself, when the master or the replica fails, the Patroni itself, the distributed storage of cluster states, or the network may break. </font><font style="vertical-align: inherit;">Consider all the options as they become more complicated in terms of how difficult it is to understand their causes.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Problem 1. DBMS and DCS on the same cluster</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Consider the simplest case: we took a database cluster and deployed DCS on the same cluster. </font><font style="vertical-align: inherit;">This common error is not only related to PostgreSQL and Patroni deployment errors. </font><font style="vertical-align: inherit;">This is a mistake in the general construction of architectures - combining many different components in one place. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
So, there was a failover. </font><font style="vertical-align: inherit;">We begin to understand what happened. </font><font style="vertical-align: inherit;">Here we are interested in when the feylover occurred, that is, the </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">point in time when the cluster state changed.</font></font></strong><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">When did the failover happen?</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A faylover does not always happen instantly; it can be time-consuming. </font><font style="vertical-align: inherit;">Therefore, it has a start time and an end time. </font><font style="vertical-align: inherit;">All events are divided into “before”, “during” and “after” the feylover. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
First of all, when the feylover happened, we are looking for the reason.</font></font><br>
<br>
<pre><code class="xml hljs">pgdb-2 patroni: INFO: promoted self to leader by acquiring session lock<font></font>
pgdb-2 patroni: WARNING: Loop time exceeded, rescheduling immediately.<font></font>
pgdb-2 patroni: INFO: Lock owner: pgdb-2; I am pgdb-2<font></font>
pgdb-2 patroni: INFO: updated leader lock during promote<font></font>
pgdb-2 patroni: server promoting<font></font>
pgdb-2 patroni: INFO: cleared rewind state after becoming the leader<font></font>
pgdb-2 patroni: INFO: Lock owner: pgdb-2; I am pgdb-2</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Above are the standard Patroni logs, where he reports that the server role has changed and the role of the wizard has moved from another to this node.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Why did the failover happen?</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Next, you need to understand what events made the master’s role go from one node to another, what happened to the old master.</font></font><br>
<br>
<pre><code class="xml hljs">pgdb-2 patroni: patroni.utils.RetryFailedError: 'Exceeded retry deadline'<font></font>
pgdb-2 patroni: ERROR: Error communicating with DCS<font></font>
pgdb-2 patroni: INFO: demoted self because DCS is not accessible and i was a leader<font></font>
pgdb-2 patroni: WARNING: Loop time exceeded, rescheduling immediately.</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In this case, everything is simple: </font></font><code>Error communicating with DCS</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- an error interacting with the configuration storage system. </font><font style="vertical-align: inherit;">The master realized that he could not work with DCS, and said that he could no longer be a master and resigned. </font></font><code>demoted self</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">speaks about this.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">What preceded the feylover?</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
If you look at the events that preceded the feylover, you can see the very reasons that became a problem for the wizard to continue:</font></font><br>
<br>
<pre><code class="xml hljs">pgdb-2 patroni: ERROR: touch_member<font></font>
                ... python trace <font></font>
pgdb-2 patroni: socket.timeout: timed out <font></font>
pgdb-2 patroni: During handling of the above exception, another exception occurred:<font></font>
                ... python trace <font></font>
pgdb-2 patroni:   raise ReadTimeoutError(self, url, "Read timed out. (read timeout=%s)" % timeout_value) <font></font>
pgdb-2 patroni: urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='127.0.0.1', port=8500): Read timed out. (read timeout=3.3333333333333335) <font></font>
pgdb-2 patroni: During handling of the above exception, another exception occurred:<font></font>
                ... python trace <font></font>
pgdb-2 patroni:     raise MaxRetryError(_pool, url, error or ResponseError(cause)) <font></font>
pgdb-2 patroni: urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=8500): Max retries exceeded with url: /v1/kv/service/pgdb/members/pgdb-2?acquire=19598b72-c0d5-f066-5d24-09c1a9ad61ee&amp;dc=maindb (Caused by ReadTimeoutError("HTTPConnectionPool(host='127.0.0.1', port=8500): Read timed out. (read timeout=3.3333333333333335)",)) <font></font>
pgdb-2 patroni: INFO: no action.  i am the leader with the lock <font></font>
pgdb-2 patroni: WARNING: Loop time exceeded, rescheduling immediately.</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Patroni logs show a lot of different timeout errors. </font><font style="vertical-align: inherit;">Patroni Agent was unable to work with DCS (in this case, it is Consul, port = 8500). </font><font style="vertical-align: inherit;">Patroni and the database were running on the same host, and Consul servers were running on the same host. </font><font style="vertical-align: inherit;">Having created the load on the server, we also created problems for the Consul server, because of which they could not communicate normally.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Everything is back as it was</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
After a while, when the load subsided, our Patroni was able to communicate again with the agents, everything resumed:</font></font><br>
<br>
<pre><code class="xml hljs">pgdb-2 patroni: INFO: promoted self to leader by acquiring session lock<font></font>
pgdb-2 patroni: WARNING: Loop time exceeded, rescheduling immediately.<font></font>
pgdb-2 patroni: INFO: Lock owner: pgdb-2; I am pgdb-2<font></font>
pgdb-2 patroni: INFO: updated leader lock during promote<font></font>
pgdb-2 patroni: server promoting<font></font>
pgdb-2 patroni: INFO: cleared rewind state after becoming the leader<font></font>
pgdb-2 patroni: INFO: Lock owner: pgdb-2; I am pgdb-2</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The same pgdb-2 server again became a master. </font><font style="vertical-align: inherit;">There was a small “flip” - in a relatively short time he resigned from himself as a master, then again took them upon himself. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
This can be regarded either as a false positive, or as the fact that Patroni did everything right.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Decision</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We decided for ourselves that the problem is that the Consul servers are on the same hardware as the databases. </font><font style="vertical-align: inherit;">Accordingly, any load on the CPU and disks (heavy IO request, temporary files, auto-vacuums, migrations, backups, etc.) also affects the interaction with the Consul cluster. </font><font style="vertical-align: inherit;">We decided that this should not live on the same equipment with the database, and allocated a separate cluster for Consul. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Alternatively, you can rotate the parameters </font></font><code>ttl</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, </font></font><code>loop_wait</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, </font></font><code>retry_timeout</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, try due to their increase to survive the short-term peak loads. </font><font style="vertical-align: inherit;">But if the load is long, we will go beyond these parameters, and the method will not work.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Problem 2. Outages</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The second problem is similar to the first in that we again have problems interacting with the DCS system:</font></font><br>
<br>
<pre><code class="xml hljs">maindb-1 patroni: ERROR: get_cluster <font></font>
maindb-1 patroni: Traceback (most recent call last):<font></font>
                ... python trace <font></font>
maindb-1 patroni: RetryFailedError: 'Exceeded retry deadline' <font></font>
maindb-1 patroni: ERROR: Error communicating with DCS <font></font>
maindb-1 patroni: INFO: closed patroni connection to the postgresql cluster <font></font>
maindb-1 patroni: INFO: postmaster pid=214121 <font></font>
... <font></font>
... <font></font>
maindb-1 patroni: INFO: demoted self because DCS is not accessible and i was a leader <font></font>
maindb-1 patroni: WARNING: Loop time exceeded, rescheduling immediately.</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Patroni again says that it cannot interact with DCS, so the current master ceases to be a master and enters replica mode. </font><font style="vertical-align: inherit;">The old master becomes a replica:</font></font><br>
<br>
<pre><code class="xml hljs">maindb-1 patroni: INFO: Lock owner: maindb-2; I am maindb-1 <font></font>
maindb-1 patroni: INFO: does not have lock <font></font>
maindb-1 patroni: INFO: running pg_rewind from maindb-2 <font></font>
maindb-1 patroni: INFO: running pg_rewind from user=postgres host=192.168.11.18 port=5432 ... <font></font>
maindb-1 patroni: servers diverged at WAL location 1FA/A38FF4F8 on timeline 6 <font></font>
maindb-1 patroni: rewinding from last common checkpoint at 1FA/A38FF450 on timeline 6 <font></font>
maindb-1 patroni: INFO: Lock owner: maindb-2; I am maindb-1 <font></font>
maindb-1 patroni: INFO: running pg_rewind from maindb-2 in progress <font></font>
maindb-1 patroni: Done!</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Here Patroni works out as it should - launches </font></font><code>pg_rewind</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">to rewind the transaction log, then connect to the new master and already catch up with the new master.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">What preceded the feylover?</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Let's find the first thing that preceded the feylover - the errors that caused it. </font><font style="vertical-align: inherit;">Patroni logs are convenient in this regard: he writes the same messages with a certain interval. </font><font style="vertical-align: inherit;">You can quickly scroll them and see when the logs have changed. </font><font style="vertical-align: inherit;">At this point, some problems began. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In a normal situation, Patroni logs look something like this:</font></font><br>
<br>
<pre><code class="xml hljs">maindb-1 patroni: INFO: Lock owner: maindb-1; I am maindb-1<font></font>
maindb-1 patroni: INFO: no action. i am the leader with the lock<font></font>
maindb-1 patroni: INFO: Lock owner: maindb-1; I am maindb-1<font></font>
maindb-1 patroni: INFO: no action. i am the leader with the lock</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The owner of the lock is checked, if it changes, events may occur that Patroni should respond to. In this case, everything is in order with us. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Having scrolled to the place where the errors began to appear, we see that an auto-filer occurred. Since the errors were related to interaction with DCS, we also look at the Consul logs - what happened there. Approximately comparing the time of the faylover and the time in the Consul logs, we see that the neighbors in the Consul cluster began to doubt the existence of other participants:</font></font><br>
<br>
<pre><code class="xml hljs">maindb-2 consul[11581]: serf: EventMemberFailed: maindb-1 192.168.11.19<font></font>
maindb-2 consul[11581]: [INFO] serf: EventMemberFailed: maindb-1 192.168.11.19<font></font>
maindb-2 consul[11581]: memberlist: Suspect maindb-1 has failed, no acks received<font></font>
maindb-2 consul[11581]: [INFO] memberlist: Suspect maindb-1 has failed, no acks received</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
If you look at the logs of other Consul agents, it also shows that there is a network collapse: all members of the Consul cluster doubt each other. </font><font style="vertical-align: inherit;">This was the impetus for the feylover. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
If we look at even earlier entries, we will see that the Consul system for the PostgreSQL agent has difficulties with communication ( </font></font><code>deadline reached</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, </font></font><code>RPC failed</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">):</font></font><br>
<br>
<pre><code class="xml hljs">maindb-1 consul: memberlist: Suspect lbs-4 has failed, no acks received<font></font>
maindb-1 consul: [ERR] yamux: keepalive failed: i/o deadline reached<font></font>
maindb-1 consul: yamux: keepalive failed: i/o deadline reached<font></font>
maindb-1 consul: [ERR] consul: "KVS.List" RPC failed to server 192.168.11.115:8300: rpc error making call: EOF<font></font>
maindb-1 consul: [ERR] http: Request GET /v1/kv/service/sam_prod/?recurse=1, error: rpc error making call: EOF from=192.168.11.19<font></font>
maindb-1 consul: [ERR] consul: "KVS.List" RPC failed to server 192.168.11.115:8300: rpc error making call: EOF<font></font>
maindb-1 consul: [ERR] agent: Coordinate update error: rpc error making call: EOF<font></font>
maindb-1 consul: [ERR] http: Request GET /v1/kv/service/sam_prod/?recurse=1, error: rpc error making call: EOF from=192.168.11.19</code></pre><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Decision</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The simplest answer is to </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">repair the network</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">It is easy to advise, but circumstances can be different, and this is not always possible. </font><font style="vertical-align: inherit;">The system can live in a data center, where we can not influence the equipment. </font><font style="vertical-align: inherit;">Need other options. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
There are solution options without working with a network:</font></font><br>
<br>
<ul>
<li><strong> Consul-</strong> — <code>checks: []</code>.<br>
   ,     .    ,    Consul-    .           .</li>
<li><strong></strong> <code>raft_multiplier</code>.<br>
   Consul-,     5 (      staging-).         Consul-.  -   .</li>
<li><strong></strong> <code>renice -n -10 consul</code>.<br>
        .  renice   .          Consul-,             .</li>
<li><strong>   consul?</strong><br>
Consul-         . Patroni   Consul-    ,     :    - ,  Patroni      .   etcd,     ,    etcd   . Patroni      etcd-.    -  ,     ,   ,     Consul.</li>
<li><strong>  </strong> <code>ttl, loop_wait, retry_timeout</code>.<br>
     ,         .     Patroni       .</li>
</ul><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Problem 3. Node loss</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
If you are using Patroni, then you are familiar with the patronictl list command, which shows the current state of the cluster:</font></font><br>
<br>
<pre><code class="xml hljs">$ patronictl list<font></font>
<font></font>
+-----------------+-------------------------+--------------+--------+---------+-----------+<font></font>
|     Cluster     |          Member         |     Host     |  Role  |  State  | Lag in MB |<font></font>
+-----------------+-------------------------+--------------+--------+---------+-----------+<font></font>
| patroni_cluster | pg01.dev.zirconus.local | 10.202.1.101 |        | running |    0.0    |<font></font>
| patroni_cluster | pg03.dev.zirconus.local | 10.202.1.103 | Leader | running |    0.0    |<font></font>
+-----------------+-------------------------+--------------+--------+---------+-----------+<font></font>
</code></pre><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
At first glance, such a conclusion may seem normal - there is a master, replicas, but there is no replication lag. </font><font style="vertical-align: inherit;">But this picture is normal exactly until we know that in this cluster there should be 3 nodes, and not 2.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Why did the failover happen?</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We understand that an auto-filer occurred and after that one replica disappeared. </font><font style="vertical-align: inherit;">We need to find out why she disappeared, and bring her back. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We study the logs again to understand why the auto-filer occurred:</font></font><br>
<br>
<pre><code class="xml hljs">pg02 patroni[1425]: ERROR: failed to update leader lock<font></font>
pg02 patroni[1425]: INFO: demoted self because failed to update leader lock in DCS<font></font>
pg02 patroni[1425]: WARNING: Loop time exceeded, rescheduling immediately.<font></font>
pg02 patroni[1425]: INFO: Lock owner: None; I am pg02.dev.zirconus.local<font></font>
pg02 patroni[1425]: INFO: not healthy enough for leader race<font></font>
pg02 patroni[1425]: INFO: starting after demotion in progress</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Master pg02 was unable to update the master key </font></font><code>ERROR: failed to update leader lock</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Then the second replica of db03 became the master, everything is in order here:</font></font><br>
<br>
<pre><code class="xml hljs">pg03 patroni[9648]: INFO: promoted self to leader by acquiring session lock<font></font>
pg03 patroni[9648]: server promoting<font></font>
pg03 patroni[9648]: INFO: cleared rewind state after becoming the leader</code></pre><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">What about the old master?</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
pg02, deciding to become a replica, began by rewinding the transaction log. </font><font style="vertical-align: inherit;">Here we need to look at the replica logs, which is not in the cluster. </font><font style="vertical-align: inherit;">Open the Patroni logs and see that during the process of connecting to the cluster, a problem arose at the stage </font></font><code>pg_rewind</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font><br>
<br>
<pre><code class="xml hljs">pg02 patroni[1425]: INFO: running pg_rewind from pg03<font></font>
pg02 patroni[1425]: INFO: running pg_rewind from user=postgres host=10.202.1.103 port=5432 ...<font></font>
pg02 patroni[1425]: servers diverged at WAL location 33F/68E6AD10 on timeline 28<font></font>
pg02 patroni[1425]: could not open file "/data/pgdb/11/pg_wal/0000001C0000033F00000059": No such file or directory<font></font>
pg02 patroni[1425]: could not find previous WAL record at 33F/59FFE368 pg02 patroni[1425]: Failure, exiting<font></font>
pg02 patroni[1425]: ERROR: Failed to rewind from healty master: pg03<font></font>
pg02 patroni[1425]: WARNING: Postgresql is not running.</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
To connect to the cluster, the node must request the transaction log from the wizard and catch the wizard from it. </font><font style="vertical-align: inherit;">In this case, there is no transaction log ( </font></font><code>No such file or directory</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">), and the replica cannot start. </font><font style="vertical-align: inherit;">Accordingly, PostgreSQL stops with an error. </font><font style="vertical-align: inherit;">You need to understand why there was no transaction log. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Let's see what the new master has in WAL:</font></font><br>
<br>
<pre><code class="xml hljs">LOG: checkpoint complete:<font></font>
wrote 62928 buffers (16.0%); 0 WAL file(s) added, 0 removed, 23 recycled;<font></font>
write=12.872 s, sync=0.020 s, total=13.001 s;<font></font>
sync files=16, longest=0.009 s, average=0.001 s;<font></font>
distance=520220 kB, estimate=520220 kB</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
It turns out that while it was running </font></font><code>pg_rewind</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, a checkpoint occurred, and </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">some of the old transaction logs were renamed</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">When the old master tried to connect to the new master and request these logs, they were already renamed, they simply did not exist. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
According to the timestamp, the time between these events is literally 150 ms.</font></font><br>
<br>
<pre><code class="xml hljs">2019-08-26 00:06:11,369 LOG: checkpoint complete<font></font>
2019-08-26 00:06:11,517 INFO: running pg_rewind</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
But this was enough to make the replica unable to connect and earn.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Decision</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Initially, we used replication slots. This solution seemed good to us, although at the first stage of operation we turned off the slots. We thought that if the slots would accumulate many wal-segments, the master could fall. Having suffered for some time without slots, we realized that we needed them and returned them. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The problem is that when the wizard enters the replica state, the wizard deletes the slots and wal-segments along with them. To level this problem, we increased the parameter </font></font><code>wal_keep_segments</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. By default, it is equal to 8 segments, we </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">raised </font></font><code>wal_keep_segments</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">to 1000</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . We allocated 16 GB on </font></font><code>wal_keep_segments</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, and now, when switching, we always have 16 GB of transaction logs on all nodes.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
This is also true for long-term maintenance tasks. </font><font style="vertical-align: inherit;">Let's say we need to update one of the replicas (software, OS, something else), and we want to turn it off. </font><font style="vertical-align: inherit;">When we turn off the replica, the slot is also deleted for it. </font><font style="vertical-align: inherit;">If you use a small parameter value </font></font><code>wal_keep_segments</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, then for a long absence of a replica, it will request those transaction logs that may not even appear on the wizard - then the replica will not be able to connect. </font><font style="vertical-align: inherit;">Therefore, we keep a large supply of magazines.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Problem 4. Data Loss</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
There was a failover on a production base. </font><font style="vertical-align: inherit;">We checked the cluster: everything is in order, all replicas are in place, there is no replication lag, and there are no errors in the logs either. </font><font style="vertical-align: inherit;">But the product team reports that there </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">is not enough data</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> in the </font><strong><font style="vertical-align: inherit;">database</font></strong><font style="vertical-align: inherit;"> - they are in one source, but not in the database. </font><font style="vertical-align: inherit;">You need to understand what happened to them. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We immediately realized that this was </font></font><code>pg_rewind</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">their jam, but we need to find out why.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">When did the failover happen?</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We can always find in the logs when the feylover occurred, who became the new master, who was the master before and when he wanted to become a replica.</font></font><br>
<br>
<pre><code class="xml hljs">pgdb-1 patroni[17836]: INFO: promoted self to leader by acquiring session lock<font></font>
pgdb-1 patroni[17836]: server promoting<font></font>
pgdb-1 patroni[17836]: INFO: cleared rewind state after becoming the leader<font></font>
pgdb-1 patroni[17836]: INFO: Lock owner: pgdb-1; I am pgdb-1</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
From the logs, we can determine how much transaction logs have been lost. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
It was like this: the old wizard rebooted, after it was restarted by autorun, Patroni was launched, which then launched PostgreSQL. </font><font style="vertical-align: inherit;">PostgreSQL decided to become a member of the Patroni cluster and launched a process </font></font><code>pg_rewind</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">that in turn erased part of the transaction logs, downloaded new ones and connected. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Patroni worked exactly as intended. </font><font style="vertical-align: inherit;">The cluster recovered: there were 3 nodes, after the feylover 3 nodes and left. </font><font style="vertical-align: inherit;">But part of the data is lost, and you need to understand what part it is. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Find in the logs of the old master the moment when it happened </font></font><code>pg_rewind</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">:</font></font><br>
<br>
<pre><code class="xml hljs">pgdb-2 patroni[4149]: INFO: running pg_rewind from pgdb-1<font></font>
pgdb-2 patroni[4149]: Lock owner: pgdb-1; I am pgdb-2<font></font>
pgdb-2 patroni[4149]: Deregister service pgdb/pgdb-2<font></font>
pgdb-2 patroni[4149]: running pg_rewind from pgdb-1 in progress<font></font>
pgdb-2 patroni[4149]: running pg_rewind from user=replica host=10.10.1.31 port=5432 ...<font></font>
pgdb-2 patroni[4149]: servers diverged at WAL location 0/5AD1BFD8 on timeline 66<font></font>
pgdb-2 patroni[4149]: rewinding from last common checkpoint at 0/59DD1070 on timeline 66<font></font>
pgdb-2 patroni[4149]: Done!</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
You need to find the position in the transaction log, which stopped the old master.</font></font><br>
<br>
<pre><code class="xml hljs">pgdb-2 patroni[4149]: INFO: Local timeline=66 lsn=0/5BDA8EB8<font></font>
pgdb-2 patroni[4149]: INFO: master_timeline=67<font></font>
...<font></font>
pgdb-2 patroni[4149]: servers diverged at WAL location 0/5AD1BFD8 on timeline 66<font></font>
postgres=# select pg_wal_lsn_diff('0/5BDA8EB8','0/5AD1BFD8');<font></font>
pg_wal_lsn_diff<font></font>
----------------<font></font>
17354464</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In this case, it is the mark 0 / 5BDA8EB8. </font><font style="vertical-align: inherit;">The second mark - 0 / 5AD1BFD8 - is needed to find the distance by which the old master differs from the new one. </font><font style="vertical-align: inherit;">Using the function </font></font><code>pg_wal_lsn_diff </code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, we compare these two marks, we get 17 MB. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Whether there is a big loss of 17 MB of data, everyone decides for himself. </font><font style="vertical-align: inherit;">For some, this is insignificant, but for someone it is unacceptable a lot. </font><font style="vertical-align: inherit;">Each for himself individually determines in accordance with the needs of the business.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Decision</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
First of all, you need to decide </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">whether Patroni autostart is always needed after rebooting the system</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Most often, we should go to the old master, see how different it is from the current state, perhaps inspect the segments of the transaction log. You need to understand whether this data can be lost, or you need to run the old wizard in standalone mode to pull the data. Only after that make a decision what to do with the data and connect this node as a replica to our cluster. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In addition, there is a parameter</font></font><code>maximum_lag_on_failover</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, by default its value is 1 Mb. It works like this: if the replica is 1 MB behind the replication lag, then this replica does not take part in the elections. If suddenly a failover occurs, Patroni looks at which replicas are behind, and those that are behind a large number of transaction logs cannot become a master. This is a good security feature that prevents you from losing too much data. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
But there is a problem: the replication lag is updated at a certain interval, the </font></font><code>ttl</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">default </font><font style="vertical-align: inherit;">value </font><font style="vertical-align: inherit;">is 30 s. It is quite possible that the value of replication lag for replicas in DCS is one, but in fact it is completely different or there is no lag at all. This is not a real-time value; it does not always reflect the real picture and it is not worth tying complex logic to it. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The risk of "losses" always remains:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In the worst case: </font></font><code>maximum_lag_on_failover + ttl</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In the average case: </font></font><code>maximum_lag_on_failover + (loop_wait / 2)</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
When you plan to implement Patroni and evaluate how much data you can lose, consider these formulas to roughly represent possible losses. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The good news is that there may be a WAL from background processes in the “losses”. </font><font style="vertical-align: inherit;">This data can easily be ignored and lost, there is no problem. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
This is what the logs look like if set </font></font><code>maximum_lag_on_failover</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, a feylover occurred and you need to select a new wizard:</font></font><br>
<br>
<pre><code class="xml hljs">pgdb-1 patroni[6202]: INFO: Lock owner: None; I am pgdb-1<font></font>
pgdb-1 patroni[6202]: INFO: not healthy enough for leader race<font></font>
pgdb-1 patroni[6202]: INFO: changing primary_conninfo and restarting in progress<font></font>
...<font></font>
pgdb-1 patroni[6202]: INFO: following a different leader because i am not the healthiest node<font></font>
pgdb-1 patroni[6202]: INFO: following a different leader because i am not the healthiest node</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The replica simply sees that she </font></font><code>not healthy enough for leader race</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">is and refuses to participate in the leadership race. </font><font style="vertical-align: inherit;">Therefore, it simply waits for a new wizard to be selected to connect to it. </font><font style="vertical-align: inherit;">This is an additional measure against data loss.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Problem 5. Drives</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The product team wrote that the application has problems working with PostgreSQL. </font><font style="vertical-align: inherit;">At the same time, you cannot enter the master, because it is not available via SSH, but the auto-filer does not occur either. </font><font style="vertical-align: inherit;">Then the host was forcibly rebooted and thus launched the auto-filer. </font><font style="vertical-align: inherit;">Although it was possible to make a manual feylover. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
After the reboot, we go to see what happened to the master. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/_1/2u/rj/_12urj0aobgqg0pp2gkkuolhjog.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We knew in advance about the problems with the disks; by monitoring we knew where to dig. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In the PostgreSQL logs, we see the following:</font></font><br>
<br>
<pre><code class="xml hljs">[COMMIT] LOG: duration: 1138.868 ms statement: COMMIT<font></font>
...<font></font>
[] WARNING: autovacuum worker started without a worker entry<font></font>
...<font></font>
[SELECT] LOG: temporary file: path "base/pgsql_tmp/pgsql_tmp11247.983", size 532996096<font></font>
...</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
On the face are all indicators of problems with disks: commits that last for a second, autovacuum runs very long and strange, and temporary files on the disk. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We looked into the system dmesg - the kernel message log, and saw a problem with one of the disks:</font></font><br>
<br>
<pre><code class="xml hljs">md/raid10:md2: sde3: rescheduling sector 351273392<font></font>
blk_update_request: I/O error, dev sde, sector 64404728<font></font>
md/raid10:md2: sde3: rescheduling sector 63161592<font></font>
blk_update_request: I/O error, dev sde, sector 64404760<font></font>
...<font></font>
md2 : active raid10 sda3[0] sdc3[2] sdd3[3] sdb3[1] sdh3[7] sdf3[5] sdg3[6]<font></font>
      15623340032 blocks super 1.2 512K chunks 2 near-copies [8/7] [UUUUUUU_]<font></font>
      bitmap: 58/59 pages [232KB], 131072KB chunk</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The disk subsystem on the server was a software Raid of 8 disks, but one was missing. </font><font style="vertical-align: inherit;">The line </font></font><code>sda3[0] sdc3[2] sdd3[3] sdb3[1] sdh3[7] sdf3[5] sdg3[6]</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">is missing </font></font><code>sde[4]</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Relatively speaking, one disk fell out, this caused disk problems, and the application had problems working with the PostgreSQL cluster.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Decision</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In this case, Patroni would not be able to help, because Patroni has no task to monitor the status of the server and disks. </font><font style="vertical-align: inherit;">During the problems, Patroni continued to interact with the DCS cluster and did not see any difficulties. </font><font style="vertical-align: inherit;">For such situations, external monitoring is needed.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Problem 6. Cluster simulator</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
This is one of the strangest problems. </font><font style="vertical-align: inherit;">I studied it for a very long time, re-read a lot of logs and called it “Cluster simulator”. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The problem was that the old master could not become a normal cue. </font><font style="vertical-align: inherit;">Patroni ran it, showed that this node is present as a replica, but at the same time it was not a normal replica, now you will see why. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
It all started, as in the previous case, with disk problems:</font></font><br>
<br>
<pre><code class="xml hljs">14:48:55.601 [COMMIT] LOG: duration: 1478.118 ms statement: COMMIT<font></font>
14:48:56.287 [COMMIT] LOG: duration: 1290.203 ms statement: COMMIT<font></font>
14:48:56.287 [COMMIT] LOG: duration: 1778.465 ms statement: COMMIT<font></font>
14:48:56.287 [COMMIT] LOG: duration: 1778.449 ms statement: COMMIT</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
There were broken connections:</font></font><br>
<br>
<pre><code class="xml hljs">14:48:58.078 [idle in transaction] LOG: could not send data to client: Broken pipe<font></font>
14:48:58.078 [idle] LOG: could not send data to client: Broken pipe<font></font>
14:48:58.078 [idle] FATAL: connection to client lost<font></font>
14:48:58.107 [idle in transaction] FATAL: connection to client lost</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
There were long expectations of a response and blocking of varying severity:</font></font><br>
<br>
<pre><code class="xml hljs">14:49:26.929 [UPDATE waiting] LOG: process 4298 acquired ExclusiveLock on tuple (2,10) of relation 52082 of database 50587 after 52487.463 ms<font></font>
14:49:26.929 [UPDATE waiting] STATEMENT: UPDATE sessions SET lastaccess='1565005714' WHERE sessionid=...<font></font>
14:49:27.929 [UPDATE waiting] LOG: process 4298 still waiting for ShareLock on transaction 364118337 after 1000.088 ms<font></font>
14:49:27.929 [UPDATE waiting] DETAIL: Process holding the lock: 4294. Wait queue: 4298.</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In general, there are obvious problems with the disks, including temporary files again. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
But the most mysterious for me - it flew in </font></font><code>immediate shutdown request</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">:</font></font><br>
<br>
<pre><code class="xml hljs">14:49:34.102 MSK 5335 @ from [] LOG: received immediate shutdown request<font></font>
14:49:34.689 [authentication] WARNING: terminating connection because of crash of another server process<font></font>
14:49:34.689 [authentication] DETAIL: The postmaster has commanded this server process to roll back the current transaction and exit, because another server process exited abnormally and possibly corrupted shared memory</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
PostgreSQL has three shutdown modes:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Graceful when we wait for all clients to disconnect on their own.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fast, when we tell customers to disconnect, because we are going to shutdown.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Immediate, which does not tell clients that it is necessary to disconnect, but simply turns it off and sends an RST message to all clients (TCP signal that the connection was interrupted).</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
PostgreSQL background processes do not send immediate shutdown request signals to each other, but only respond to them. </font><font style="vertical-align: inherit;">This is an emergency restart, and who sent it is unclear. </font><font style="vertical-align: inherit;">If it was </font></font><code>kill -9</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, then I would see it in the logs, but it was not there. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Understanding further, I saw that Patroni did not write to the log for quite some time - there were simply no messages for 54 seconds. </font><font style="vertical-align: inherit;">During this time, one of the replicas made “promote” and an auto-filer occurred:</font></font><br>
<br>
<pre><code class="xml hljs">pgsql03 patroni: 14:48:25,000 INFO: Lock owner: pgsql03; I am pgsql03 <font></font>
pgsql03 patroni: 14:48:25,013 INFO: no action.  i am the leader with the lock <font></font>
pgsql03 patroni: 14:48:37,159 INFO: Lock owner: pgsql03; I am pgsql03 <font></font>
pgsql03 patroni: 14:49:31,155 WARNING: Exception hened during processing of request from 10.1.0.12 <font></font>
pgsql03 patroni: 14:49:31,297 WARNING: Exception hened during processing of request from 10.1.0.11 <font></font>
pgsql03 patroni: 14:49:31,298 WARNING: Exception hened during processing of request from 10.1.0.11</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Patroni worked perfectly again here, the old master was unavailable, so the election of a new master began.</font></font><br>
<br>
<pre><code class="xml hljs">pgsql01 patroni: 14:48:57,136 INFO: promoted self to leader by acquiring session lock<font></font>
pgsql01 patroni: server promoting<font></font>
pgsql01 patroni: 14:48:57,214 INFO: cleared rewind state after becoming the leader<font></font>
pgsql01 patroni: 14:49:05,013 INFO: Lock owner: pgsql01; I am pgsql01<font></font>
pgsql01 patroni: 14:49:05,023 INFO: updated leader lock during promote</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
pgsql01 became the new leader, and there were just problems with the second replica. </font><font style="vertical-align: inherit;">She honestly tried to reconfigure:</font></font><br>
<br>
<pre><code class="xml hljs">pgsql02 patroni: 14:48:57,124 INFO: Could not take out TTL lock <font></font>
pgsql02 patroni: 14:48:57,137 INFO: following new leader after trying and failing to obtain lock <font></font>
pgsql02 patroni: 14:49:05,014 INFO: Lock owner: pgsql01; I am pgsql02 <font></font>
pgsql02 patroni: 14:49:05,025 INFO: changing primary_conninfo and restarting in progress <font></font>
pgsql02 patroni: 14:49:15,011 INFO: Lock owner: pgsql01; I am pgsql02<font></font>
pgsql02 patroni: 14:49:15,014 INFO: changing primary_conninfo and restarting in progress <font></font>
pgsql02 patroni: 14:49:25,011 INFO: Lock owner: pgsql01; I am pgsql02 <font></font>
pgsql02 patroni: 14:49:25,014 INFO: changing primary_conninfo and restarting in progress <font></font>
pgsql02 patroni: 14:49:35,011 INFO: Lock owner: pgsql01; I am pgsql02 <font></font>
pgsql02 patroni: 14:49:35,014 INFO: changing primary_conninfo and restarting in progress</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
She tried to change recovery.conf, restart PostgreSQL, connect to the new wizard. </font><font style="vertical-align: inherit;">Every 10 seconds there are messages that she is trying, but can not. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Meanwhile, the same immediate-shutdown signal flew into the old master. </font><font style="vertical-align: inherit;">The wizard started an emergency reboot, recovery also stops. </font><font style="vertical-align: inherit;">The replica cannot connect to the master because it is in shutdown mode.</font></font><br>
<br>
<pre><code class="xml hljs">14:49:34.293 [idle] LOG:  received replication command: IDENTIFY_SYSTEM <font></font>
WARNING:  terminating connection because of crash of another server process <font></font>
DETAIL:  The postmaster has commanded this server process to roll back the current transaction and exit, because another server process exited abnormally and possibly corrupted shared memory. <font></font>
14:49:35.232 FATAL:  could not receive data from WAL stream: server closed the connection unexpectedly             <font></font>
        This probably means the server terminated abnormally <font></font>
        before or while processing the request. <font></font>
14:49:35.232 LOG:  record with incorrect prev-link 142D46/315602C at 14CF/CF38C160 <font></font>
14:49:35.305 FATAL: could not connect to the primary server: FATAL: the database system is shutting down <font></font>
14:49:40.241 FATAL: could not connect to the primary server: FATAL: the database system is shutting down</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
At some point, the replica worked, but the replication did not start.</font></font><br>
<br>
<pre><code class="xml hljs">14:50:14.024 [] LOG:  record with incorrect prev-link 142D46/315602C at 14CF/CF38C160 <font></font>
14:50:14.028 [] LOG:  fetching timeline history file for timeline 72 from primary server <font></font>
14:50:14.104 [] FATAL:  could not start WAL streaming: ERROR:  requested starting point 14CF/CF000000 on timeline 71 is not in this server's history        <font></font>
DETAIL:  This server's history forked from timeline 71 at 14CF/CEC32E40. <font></font>
14:50:14.104 [] LOG:  new timeline 72 forked off current database system timeline 71 before current recovery point 14CF/CF38C160</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
I have a single hypothesis: in recovery.conf was the address of the old master. </font><font style="vertical-align: inherit;">When a new master already appeared, the second replica tried to connect to the old master. </font><font style="vertical-align: inherit;">When Patroni started on the second replica, the node started, but could not connect via replication. </font><font style="vertical-align: inherit;">A replication lag was formed that looked something like this:</font></font><br>
<br>
<pre><code class="xml hljs">+-----------------+----------+--------------+--------+---------+-----------+<font></font>
|     Cluster     |  Member  |     Host     |  Role  |  State  | Lag in MB |<font></font>
+-----------------+----------+--------------+--------+---------+-----------+<font></font>
| patroni_cluster |  pgsql01 | 10.2.200.151 | Leader | running |       0.0 |<font></font>
| patroni_cluster |  pgsql02 | 10.2.200.152 |        | running |    9153.0 |<font></font>
| patroni_cluster |  pgsql03 | 10.2.200.153 |        | running |       0.0 |<font></font>
+-----------------+----------+--------------+--------+---------+-----------+</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
That is, all three nodes were in place, but the second node was behind. </font><font style="vertical-align: inherit;">Replication could not start because transaction logs were different. </font><font style="vertical-align: inherit;">The transaction logs offered by the wizard, indicated in recovery.conf, simply did not fit the current node. </font><font style="vertical-align: inherit;">PostgreSQL reported an error every 5 seconds</font></font><br>
<br>
<pre><code class="xml hljs">14:50:44.143 FATAL:  could not start WAL streaming: ERROR:  requested starting point 14CF/CF000000 on timeline 71 is not in this server's history        <font></font>
         DETAIL:  This server's history forked from timeline 71 at 14CF/CEC32E40. <font></font>
14:50:44.143 LOG:  new timeline 72 forked off current database system timeline 71 before current recovery point 14CF/ CF38C160</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Here I made a mistake and did not test my hypothesis that we are connecting to the wrong master. </font><font style="vertical-align: inherit;">I just restarted Patroni on the replica. </font><font style="vertical-align: inherit;">Honestly, I already put an end to it and thought that I would have to redo it, but still decided to try restarting it.</font></font><br>
<br>
<pre><code class="xml hljs">15:14:13.511 LOG: consistent recovery state reached at 14CF/A3F657B0<font></font>
15:14:13.511 LOG: database system is ready to accept read only connections</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Recovery started, and even the database opened, it was ready to accept the connection, replication started:</font></font><br>
<br>
<pre><code class="xml hljs">15:14:17.072 LOG: record with incorrect prev-link 142D46/315602C at 14CF/CF38C160<font></font>
15:14:17.077 LOG: started streaming WAL from primary at 14CF/CF000000 on timeline 72<font></font>
15:14:17.536 LOG: invalid record length at 14CF/CF38C160: wanted 24, got 1</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
But a minute later she fell off with an error </font></font><code>terminating walreceiver process due to administrator command</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- the replica said that transaction logs were not suitable for her.</font></font><br>
<br>
<pre><code class="xml hljs">15:15:27.823 FATAL: terminating walreceiver process due to administrator command<font></font>
15:15:27.895 LOG: invalid record length at 14CF/CF38C160: wanted 24, got 1<font></font>
15:15:27.895 LOG: invalid record length at 14CF/CF38C160: wanted 24, got 1</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
And I did not restart PostgreSQL, but it was Patroni that was restarted in the hope that it would magically launch the database. </font><font style="vertical-align: inherit;">Replication started again, but the database opened in the same place:</font></font><br>
<br>
<pre><code class="xml hljs">15:17:33.553 LOG: consistent recovery state reached at 14CF/A3F657B0<font></font>
15:17:33.554 LOG: database system is ready to accept read only connections</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Marks in the transaction log were different, they were different than in the previous launch attempt - the transaction log position was earlier:</font></font><br>
<br>
<pre><code class="xml hljs">15:17:37.299 LOG: invalid contrecord length 5913 at 14CF/CEFFF7B0<font></font>
15:17:37.304 LOG: started streaming WAL from primary at 14CF/CE000000 on timeline 72</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The replication stopped again, and the error message was different and again not very informative:</font></font><br>
<br>
<pre><code class="xml hljs">15:18:12.208 FATAL: terminating walreceiver process due to administrator command<font></font>
15:18:12.240 LOG: record with incorrect prev-link 60995000/589DF000 at 14CF/CEFFF7B0<font></font>
15:18:12.240 LOG: record with incorrect prev-link 60995000/589DF000 at 14CF/CEFFF7B0</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
For the experiment, restarted again, the base opened at the same place:</font></font><br>
<br>
<pre><code class="xml hljs">15:21:25.135 LOG: consistent recovery state reached at 14CF/A3F657B0<font></font>
15:21:25.135 LOG: database system is ready to accept read only connections</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Then the idea came to me: that if I restart PostgreSQL, at this point on the current wizard I will do a checkpoint to move the point in the transaction log a little bit ahead and recovery started from another moment. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Launched Patroni, made a couple of checkpoints on the master, a couple of restart points on the replica when it opened:</font></font><br>
<br>
<pre><code class="xml hljs">15:22:43.727 LOG: invalid record length at 14D1/BCF3610: wanted 24, got 0<font></font>
15:22:43.731 LOG: started streaming WAL from primary at 14D1/B000000 on timeline 72</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
It worked - replication started from a different place and no longer broke. </font><font style="vertical-align: inherit;">But for me this is one of the most mysterious problems over which I am still racking my brains. </font><font style="vertical-align: inherit;">Especially that weird immediate shutdown request. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
From this we can conclude: Patroni can work out as planned and without errors, but this is not an absolute guarantee that everything is actually in order. </font><font style="vertical-align: inherit;">After the feylover, you should always double-check that everything is fine with the cluster: the right number of replicas, no replication lag.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Summary</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Based on these and many other similar problems, I formulated general recommendations that I advise you to keep in mind when operating Patroni.</font></font><br>
<br>
<blockquote><ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">When you use Patroni, you must have monitoring. </font><font style="vertical-align: inherit;">You always need to know when an auto-file occurred, because if you do not know that you have an auto-file, you do not control the cluster.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">&nbsp;After each feylover always check a cluster. </font><font style="vertical-align: inherit;">You must make sure that: replicas are always the current number; </font><font style="vertical-align: inherit;">no lag of replication; </font><font style="vertical-align: inherit;">there are no errors in the logs related to streaming replication, with Patroni, with the DCS system.</font></font></li>
</ul></blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Patroni is a very good tool, but no matter how you say it is not a silver bullet. </font><font style="vertical-align: inherit;">Automation can work out successfully, but at the same time, automation objects can be in a half-working state. </font><font style="vertical-align: inherit;">Anyway, you need to have an idea of ​​how PostgreSQL and replication work, how Patroni manages PostgreSQL, and how interaction between nodes is ensured. </font><font style="vertical-align: inherit;">This is necessary in order to be able to repair the problems that arise with your hands.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Diagnostic steps</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
It so happened that we work with different clients, for the most part they do not have an ELK stack, and you have to sort out the logs by opening 2 tabs and 6 consoles: in one Patroni tab for each node, in the other - Consul or PostgreSQL logs. </font><font style="vertical-align: inherit;">Diagnosing all this is difficult. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
I have developed the following approach. </font><font style="vertical-align: inherit;">I always watch when a failover occurred. </font><font style="vertical-align: inherit;">For me it's a kind of watershed. </font><font style="vertical-align: inherit;">I look at what happened before, during and after the feylover. </font><font style="vertical-align: inherit;">Failover has two timestamps - start and end. </font><font style="vertical-align: inherit;">In the logs, I look what preceded the feylover, that is, I am looking for reasons. </font><font style="vertical-align: inherit;">This gives an understanding of the picture of what happened and what can be done in the future so that the feylover does not occur under the same circumstances. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
For this, I look:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">First of all, the Patroni logs.</font></font></li>
<li>  PostgreSQL   DCS    ,     Patroni.</li>
<li>  —     ,    .</li>
</ul><br>
<h3></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
There are many other products for auto-filer: stolon, repmgr, pg_auto_failover, PAF. I tried all 4 tools and in my opinion Patroni is the best there is on the market today. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Do I recommend Patroni? Definitely, yes, because I like Patroni, I think I learned how to cook it. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
If you are interested in looking at what other problems there are with Patroni, besides those described in the article, you can always go to the page </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">https://github.com/zalando/patroni/issues</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . There are many different stories. Despite the fact that half of them are from illiterate users who ask stupid questions without bothering to bother with a simple search, interesting problems are also discussed there and, as a result of discussions, tasks for fixing bugs are opened if necessary.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Thanks to Zalando for developing this project, as well as to the two people who started working on this product: Alexander Kukushkin and Alexey Klyukin. Thanks a lot to all Patroni contributors. </font></font><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">After this report, we recorded a short interview in which Alex tried to fit his report into one council and told why to participate and speak at conferences. Take on arms and come to check the approach on Saint HighLoad ++.</font></font></i><br>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/lnw02pCpgRw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>
<blockquote> HighLoad++   6-7 .      ,    .          ,        ( ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="> </a>).      -  <strong></strong>     ,  ,     ,     .<br>
<br>
    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=">Saint HighLoad++</a>!    ,  ,      PostgreSQL:   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="></a>,     PostgreSQL    JSON    ;   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="></a>   PostgreSQL 13,  ;   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="></a>     .</blockquote></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../en489194/index.html">How OpenShift is changing the organizational structure of an IT organization. The evolution of organizational models when moving to PaaS</a></li>
<li><a href="../en489196/index.html">Magic Smoke: Microcontrollers vs. Linear Regulators</a></li>
<li><a href="../en489198/index.html">How not to shoot yourself in the foot using Liquibase</a></li>
<li><a href="../en489200/index.html">What startups is looking for Y Combinator in 2020</a></li>
<li><a href="../en489204/index.html">An Inside Look at the Reliability of Facebook Services</a></li>
<li><a href="../en489208/index.html">Virtual server for online trading</a></li>
<li><a href="../en489210/index.html">Linux code performance testing with examples</a></li>
<li><a href="../en489212/index.html">1C-Bitrix prevents unsubscribing from the newsletter by the requirement to submit their personal data</a></li>
<li><a href="../en489214/index.html">Modern approach to testing localization on iOS</a></li>
<li><a href="../en489218/index.html">It is naive. Super: code and architecture of a simple game</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>