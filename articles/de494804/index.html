<!doctype html>
<html class="no-js" lang="de">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>❓ 👩🏾‍🎨 🐃 "Entschuldigung, ich habe erkannt ..." oder Himbeeren und Controller mithilfe der Tensorflow-Objekterkennungs-API erkannt 👨🏽‍⚖️ 👩🏿‍🚒 🤢</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ende letzten Jahres schrieb ich einen Artikel darüber, wie fasziniert mich die Fähigkeit war, Objekte in Bildern mithilfe neuronaler Netze zu erkennen...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>"Entschuldigung, ich habe erkannt ..." oder Himbeeren und Controller mithilfe der Tensorflow-Objekterkennungs-API erkannt</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/494804/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ende letzten Jahres schrieb ich </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">einen Artikel</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> darüber, wie fasziniert mich die Fähigkeit war, Objekte in Bildern mithilfe neuronaler Netze zu erkennen. In diesem Artikel haben wir mit PyTorch entweder Himbeeren oder einen Arduino-ähnlichen Controller auf Video kategorisiert. Und trotz der Tatsache, dass ich PyTorch mochte, wandte ich mich an ihn, weil ich nicht sofort mit TensorFlow umgehen konnte. Aber ich habe versprochen, dass ich zum Thema Erkennung von Objekten im Video zurückkehren werde. Es scheint an der Zeit zu sein, das Versprechen zu halten. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In diesem Artikel werden wir auf unserem lokalen Computer versuchen, das fertige Modell in Tensorflow 1.13 und die Objekterkennungs-API für unsere eigenen Bilder neu zu trainieren und dann mit OpenCV Beeren und Controller im Videostream einer Webkamera zu erkennen.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Möchten Sie Ihre Fähigkeiten zur Beerenerkennung bis zum Sommer verbessern? </font><font style="vertical-align: inherit;">Dann sind Sie unter Katze willkommen.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/fu/do/rd/fudordve5xz-8gwdnbvlnkkjusm.png"><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Inhalt: </font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Teil I: Einführung </font></font></a><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Teil II: Trainieren des Modells in TenosrFlow </font></font></a><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Teil III: </font></font></a><font style="vertical-align: inherit;"><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;">Anwenden </font></a><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;">des Modells in OpenCV </font></a></font><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Teil IV: Schlussfolgerung</font></font></a><br>
<a name="I"></a><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Teil I: Einführung</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Diejenigen, die den vorherigen Artikel über PyTorch gelesen haben, wissen bereits, dass ich ein Amateur in Fragen neuronaler Netze bin. Nehmen Sie diesen Artikel daher nicht als die ultimative Wahrheit wahr. Trotzdem hoffe ich, dass ich jemandem helfen kann, mit den Grundlagen der Videoerkennung mithilfe der Tensorflow-Objekterkennungs-API umzugehen. </font></font><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dieses Mal habe ich nicht versucht, ein Tutorial zu erstellen, daher ist der Artikel kürzer als gewöhnlich.</font></font></i><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Zunächst ist das </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">offizielle Tutorial</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> zur Verwendung der Objekterkennungs-API auf einem lokalen Computer, gelinde gesagt, kaum erschöpfend. Als Neuling war ich völlig unzureichend und musste mich auf Blog-Artikel konzentrieren.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Um ehrlich zu sein, würde ich gerne TensorFlow 2.0 ausprobieren, aber in den meisten Veröffentlichungen waren Migrationsprobleme zum Zeitpunkt dieses Schreibens nicht vollständig gelöst. </font><font style="vertical-align: inherit;">Daher habe ich mich am Ende für TF 1.13.2 entschieden.</font></font><br>
<a name="II"></a><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Teil II: Modellunterricht bei TensorFlow </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ich habe Anweisungen zum Unterrichten des Modells </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">aus diesem Artikel</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> bzw. aus der ersten Hälfte gezogen, bis JavaScript angewendet wurde </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(Wenn Sie kein Englisch sprechen, können Sie einen Artikel zum gleichen Thema </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">in Habré sehen</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> )</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In meinem Fall gibt es zwar mehrere Unterschiede:</font></font><br>
<br>
<ol>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ich habe Linux verwendet, weil Anaconda für Linux bereits Protobuf und Pycocoapi erstellt hat, sodass ich sie nicht selbst erstellen musste.</font></font></li>
<li>   TensorFlow 1.13.2,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow">Object Detection API 1.13</a> ,       TensorFlow 1.13.2.   master        TF 1.15,         1.13.</li>
<li>      numpy — 1.17.5,  1.18    .</li>
<li>  faster_rcnn_inception_v2_coco    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow">ssd_mobilenet_v2_coco</a>,    ,     .</li>
</ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 Für alle Fälle möchte ich sagen, dass ich keinen Grafikbeschleuniger verwendet habe. Die Schulung wurde nur zu Prozessorkapazitäten durchgeführt. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Eine Reihe von Bildern, eine Konfigurationsdatei, ein gespeichertes Diagramm sowie ein Skript zum Erkennen von Bildern mit OpenCV können wie immer von </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GitHub</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> heruntergeladen werden </font><font style="vertical-align: inherit;">. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Eine lange 23-stündige Modellschulung ist vergangen, der gesamte Tee im Haus wurde bereits getrunken: „Was? Wo? Wann?" inspiziert und jetzt ging meine Geduld endlich zu Ende. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wir beenden das Training und speichern das Modell. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Installieren Sie OpenCV in derselben Umgebung wie "Anaconda" mit dem folgenden Befehl:</font></font><br>
<br>
<pre><code class="plaintext hljs">conda install -c conda-forge opencv</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ich habe schließlich Version 4.2 installiert. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Außerdem werden </font><font style="vertical-align: inherit;">wir </font><font style="vertical-align: inherit;">die Anweisungen </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">aus diesem Artikel</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> nicht mehr benötigen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nach dem Speichern des Modells habe ich einen Fehler gemacht, der mir nicht klar war: Ich habe sofort versucht, die zuvor im Training / Ordner in der Funktion verwendete Datei graph.pbtxt zu ersetzen:</font></font><br>
<br>
<pre><code class="python hljs">cv2.dnn.readNetFromTensorflow()</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Leider funktioniert dies nicht auf diese Weise und wir müssen noch eine Manipulation durchführen, um graph.pbtxt für OpenCV zu erhalten. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Höchstwahrscheinlich ist die Tatsache, dass ich jetzt berate, kein sehr guter Weg, aber für mich funktioniert es. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Laden Sie </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">tf_text_graph_ssd.py</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> herunter </font><font style="vertical-align: inherit;">und </font><font style="vertical-align: inherit;">legen Sie sie in </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">tf_text_graph_common.py</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> in dem Ordner ab, in dem sich unser gespeichertes Diagramm befindet (ich habe diesen Ordner inference_graph). </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Gehen Sie dann zur Konsole in diesem Ordner und führen Sie von dort aus einen Befehl mit ungefähr den folgenden Inhalten aus:</font></font><br>
<br>
<pre><code class="plaintext hljs">python tf_text_graph_ssd.py --input frozen_inference_graph.pb --config pipeline.config --output graph.pbtxt</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Und das ist alles, was Sie brauchen, um unser Modell auf OpenCV hochzuladen.</font></font><br>
<br>
<a name="III"></a><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Teil III: Wenden Sie das Modell in OpenCV an </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wie im Artikel über PyTorch über die Arbeit mit OpenCV habe ich den Programmcode aus </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">dieser Veröffentlichung zugrunde gelegt</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ich habe kleine Änderungen vorgenommen, um es ein wenig zu vereinfachen, aber da ich den Code nicht vollständig verstehe, werde ich ihn nicht kommentieren. </font><font style="vertical-align: inherit;">Funktioniert und schön. </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Es ist klar, dass der Code besser hätte sein können, aber ich habe noch keine Zeit, mich für OpenCV-Tutorials zu setzen</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<div class="spoiler" role="button" tabindex="0">
                        <b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">OpenCV-Code</font></font></b>
                        <div class="spoiler_text"><pre><code class="python hljs">
<span class="hljs-comment"># USAGE</span>
<span class="hljs-comment"># based on this code https://proglib.io/p/real-time-object-detection/</span>
<span class="hljs-comment"># import the necessary packages</span>
<span class="hljs-keyword">from</span> imutils.video <span class="hljs-keyword">import</span> VideoStream
<span class="hljs-keyword">from</span> imutils.video <span class="hljs-keyword">import</span> FPS
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> imutils
<span class="hljs-keyword">import</span> time
<span class="hljs-keyword">import</span> cv2<font></font>
<font></font>
prototxt=<span class="hljs-string">"graph.pbtxt"</span>
model=<span class="hljs-string">"frozen_inference_graph.pb"</span>
min_confidence = <span class="hljs-number">0.5</span><font></font>
<font></font>
<span class="hljs-comment"># initialize the list of class labels MobileNet SSD was trained to</span>
<span class="hljs-comment"># detect, then generate a set of bounding box colors for each class</span>
CLASSES = [<span class="hljs-string">"background"</span>, <span class="hljs-string">"duino"</span>,<span class="hljs-string">"raspb"</span>]<font></font>
COLORS = [(<span class="hljs-number">40</span>,<span class="hljs-number">50</span>,<span class="hljs-number">60</span>),((<span class="hljs-number">140</span>,<span class="hljs-number">55</span>,<span class="hljs-number">130</span>)),(<span class="hljs-number">240</span>,<span class="hljs-number">150</span>,<span class="hljs-number">25</span>)]<font></font>
<font></font>
<span class="hljs-comment"># load our serialized model from disk</span>
print(<span class="hljs-string">"[INFO] loading model..."</span>)<font></font>
<font></font>
net =cv2.dnn.readNetFromTensorflow(model,prototxt)<font></font>
<font></font>
<span class="hljs-comment"># initialize the video stream, allow the cammera sensor to warmup,</span>
<span class="hljs-comment"># and initialize the FPS counter</span>
print(<span class="hljs-string">"[INFO] starting video stream..."</span>)<font></font>
vs = VideoStream(src=<span class="hljs-number">0</span>).start()<font></font>
time.sleep(<span class="hljs-number">0.5</span>)<font></font>
fps = FPS().start()<font></font>
<font></font>
<span class="hljs-comment"># loop over the frames from the video stream</span>
<span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:
	<span class="hljs-comment"># grab the frame from the threaded video stream and resize it</span>
	<span class="hljs-comment"># to have a maximum width of 400 pixels</span><font></font>
	frame = vs.read()<font></font>
	frame = imutils.resize(frame, width=<span class="hljs-number">300</span>)<font></font>
<font></font>
	<span class="hljs-comment"># grab the frame dimensions and convert it to a blob</span>
	(h, w) = frame.shape[:<span class="hljs-number">2</span>]<font></font>
	blob = cv2.dnn.blobFromImage(frame, size=(<span class="hljs-number">300</span>, <span class="hljs-number">300</span>), swapRB=<span class="hljs-literal">True</span>)<font></font>
<font></font>
	<span class="hljs-comment"># pass the blob through the network and obtain the detections and</span>
	<span class="hljs-comment"># predictions</span><font></font>
	net.setInput(blob)<font></font>
	detections = net.forward()<font></font>
<font></font>
	<span class="hljs-comment"># loop over the detections</span>
	<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> np.arange(<span class="hljs-number">0</span>, detections.shape[<span class="hljs-number">2</span>]):
		<span class="hljs-comment"># extract the confidence (i.e., probability) associated with</span>
		<span class="hljs-comment"># the prediction</span>
		<span class="hljs-keyword">print</span> (detections)<font></font>
		confidence = detections[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, i, <span class="hljs-number">2</span>]<font></font>
<font></font>
		<span class="hljs-keyword">if</span> confidence &gt; min_confidence:
			<span class="hljs-comment"># extract the index of the class label from the</span>
			<span class="hljs-comment"># `detections`, then compute the (x, y)-coordinates of</span>
			<span class="hljs-comment"># the bounding box for the object</span>
			idx = int(detections[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, i, <span class="hljs-number">1</span>])<font></font>
			box = detections[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, i, <span class="hljs-number">3</span>:<span class="hljs-number">7</span>] * np.array([w, h, w, h])<font></font>
			(startX, startY, endX, endY) = box.astype(<span class="hljs-string">"int"</span>)<font></font>
<font></font>
			<span class="hljs-comment"># draw the prediction on the frame</span>
			label = <span class="hljs-string">"{}: {:.2f}%"</span>.format(CLASSES[idx],<font></font>
				confidence * <span class="hljs-number">100</span>)<font></font>
			cv2.rectangle(frame, (startX, startY), (endX, endY),<font></font>
				COLORS[idx], <span class="hljs-number">2</span>)<font></font>
			y = startY - <span class="hljs-number">15</span> <span class="hljs-keyword">if</span> startY - <span class="hljs-number">15</span> &gt; <span class="hljs-number">15</span> <span class="hljs-keyword">else</span> startY + <span class="hljs-number">15</span>
			cv2.putText(frame, label, (startX, y+<span class="hljs-number">3</span>),<font></font>
				cv2.FONT_HERSHEY_SIMPLEX, <span class="hljs-number">0.5</span>, COLORS[idx], <span class="hljs-number">1</span>)<font></font>
<font></font>
	<span class="hljs-comment"># show the output frame</span>
	cv2.imshow(<span class="hljs-string">"Frame"</span>, frame)<font></font>
	key = cv2.waitKey(<span class="hljs-number">1</span>) &amp; <span class="hljs-number">0xFF</span><font></font>
<font></font>
	<span class="hljs-comment"># if the `q` key was pressed, break from the loop</span>
	<span class="hljs-keyword">if</span> key == ord(<span class="hljs-string">"q"</span>):
		<span class="hljs-keyword">break</span><font></font>
<font></font>
	<span class="hljs-comment"># update the FPS counter</span><font></font>
	fps.update()<font></font>
<font></font>
<span class="hljs-comment"># stop the timer and display FPS information</span><font></font>
fps.stop()<font></font>
print(<span class="hljs-string">"[INFO] elapsed time: {:.2f}"</span>.format(fps.elapsed()))<font></font>
print(<span class="hljs-string">"[INFO] approx. FPS: {:.2f}"</span>.format(fps.fps()))<font></font>
<font></font>
<span class="hljs-comment"># do a bit of cleanup</span><font></font>
cv2.destroyAllWindows()<font></font>
vs.stop()<font></font>
</code></pre><br>
</div>
                    </div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Also ist alles fertig. Wir starten das Modell, richten das Objektiv auf meinen alten CraftDuino und genießen das Ergebnis: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/bw/hj/yd/bwhjyd9pddoeop9yaz7fxbqozzo.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Auf den ersten Blick ist es überhaupt nicht schlecht, aber nur auf den ersten Blick. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Es sieht so aus, als ob das Modell in 23 Stunden umgeschult wurde, daher gibt es schwerwiegende Fehler bei der Definition von Objekten. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Hier ist eine visuelle Demonstration: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/1w/3y/gf/1w3ygfo-ufytpuyct1kaarpsgls.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wie Sie sehen können, definiert dieses Modell nicht nur ein Messer, sondern auch nur einen schwarzen Hintergrund als arduinoähnlichen Controller. Vielleicht liegt das daran, dass in den Trainingsdaten dunkle Bilder mit dem Arduino und seinen Analoga waren, auf denen das Modell in 23 Stunden stoßen konnte. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Infolgedessen musste ich meinen Computer für weitere 8 Stunden laden und ein neues Modell trainieren. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bei ihr läuft es viel besser. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Hier ist ein Beispiel mit CraftDuino:</font></font><br>
<br>
<img src="https://habrastorage.org/webt/_c/8m/62/_c8m62y2q6as-l8sun5ah5ivppk.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Lebende Himbeeren sind nicht zur Hand. </font><font style="vertical-align: inherit;">Ich musste Bilder drucken. </font><font style="vertical-align: inherit;">Auf dem Bildschirm des Telefons oder Monitors können Sie auch erkennen, aber auf dem Papier war es bequemer. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/63/_k/ou/63_koujmchte7jor0ulqzxcvgcs.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Lassen Sie uns überprüfen, wie das Modell den Arduino Nano erkennt, was zu gegebener Zeit</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" class="user_link"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Drzugrik</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Für mich habe ich mit Sensoren in mein Mega-Gerät eingelötet: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ub/33/61/ub3361ozwkiwvl2sosx6yldvsou.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wie Sie sehen, erkennt es recht gut, aber bei einem sehr schlechten Winkel und bei warmem Licht kann es einige Fragmente wie Himbeeren erkennen. Tatsächlich war es jedoch schwierig, einen fehlerhaften Rahmen in der Linse zu erfassen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Lassen Sie uns nun überprüfen, wie sie die Objekte klassifiziert, für die sie nicht trainiert wurde. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wieder ein Beispiel mit einem Messer und einem schwarzen Hintergrund: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/io/ja/6a/ioja6aexferclondu4228nsr06y.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Diesmal funktioniert alles so, wie es sollte. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wir werden unser Modell anbieten, um den winzigen Canny 3-Controller zu erkennen, über den ich in einem </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">früheren Artikel geschrieben habe</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br>
<img src="https://habrastorage.org/webt/xp/ay/14/xpay14o7clhp1y1twu4vyltiay4.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Da unser Modell nichts anderes als Himbeeren und Arduino-ähnliche Controller kennt, können wir sagen, dass das Modell den Canny-Controller recht erfolgreich erkannt hat.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wie beim Arduino Nano hängt vieles vom Winkel und der Beleuchtung ab. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Mit dem warmen Licht einer Glühlampe und mit einem erfolglosen Winkel kann der Controller nicht nur nicht erkannt, sondern sogar als Himbeere definiert werden. </font><font style="vertical-align: inherit;">Zwar mussten diese Winkel wie im vergangenen Fall immer noch versuchen, sich in der Linse zu verfangen. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/01/ut/h_/01uth_-raiwnzasg7ypn-aoxezs.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nun, der letzte Fall ist eine Art Knicks für den Artikel über die </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Klassifizierung von Bildern in PyTorch</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Wie beim letzten Mal sind der Einplatinencomputer Raspberry Pi 2 und sein Logo in einem Frame kompatibel. </font><font style="vertical-align: inherit;">Im Gegensatz zum vorherigen Artikel, in dem wir das Klassifizierungsproblem gelöst und ein wahrscheinlichstes Objekt für das Bild ausgewählt haben, werden in diesem Fall sowohl das Logo als auch die Himbeere selbst erkannt.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/vx/fv/us/vxfvusfgitn6vk1pe6o4rvoen9i.png"><br>
<br>
<a name="IV"></a><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Teil IV: Schlussfolgerung </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Abschließend möchte ich sagen, dass ich trotz der Unerfahrenheit dieses kleinen Beispiels für die Arbeit mit der Tensorflow-Objekterkennungs-API sowohl freie Tage als auch einen Teil des Montags nicht bereue. Wenn zumindest ein wenig Verständnis für die Verwendung alles unglaublich neugierig wird. Während des Lernprozesses beginnen Sie, das Modell als lebendiges Modell zu betrachten und seine Erfolge und Misserfolge zu verfolgen. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Daher empfehle ich jedem, der mit diesem einen Tag nicht vertraut ist, etwas Eigenes zu erkennen.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Da es dabei gestiegen ist, müssen Sie nicht einmal eine echte Webcam kaufen. Tatsache ist, dass ich während der Vorbereitung des Artikels meine Webcam kaputt gemacht habe (den Fokusmechanismus gebrochen habe) und bereits gedacht habe, dass ich alles aufgeben müsste. Es stellte sich jedoch heraus, dass Sie mit Hilfe von Droidcam ein Smartphone anstelle einer Webcam verwenden können (nicht für Werbung zählen). Darüber hinaus erwies sich die Aufnahmequalität als viel besser als die einer kaputten Kamera, was die Qualität der Erkennung von Objekten im Bild stark beeinflusste. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Übrigens, da Anaconda einen normalen </font><b><font style="vertical-align: inherit;">Pycocotool-</font></b><font style="vertical-align: inherit;"> Aufbau hat</font></font><b><font style="vertical-align: inherit;"></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ich fand nur für Linux, und ich war zu faul, um zwischen Betriebssystemen zu wechseln. Ich habe diesen gesamten Artikel nur mit Open-Source-Software vorbereitet. </font><font style="vertical-align: inherit;">Es gab Analoga von Word und Photoshop und sogar einen Treiber für den Drucker. </font><font style="vertical-align: inherit;">Das erste Mal in meinem Leben geschah dies. </font><font style="vertical-align: inherit;">Es stellte sich heraus, dass moderne Versionen von Linux-Betriebssystemen und Anwendungsprogrammen sehr praktisch sein können, selbst für Personen, die Microsoft OS seit mehr als 25 Jahren verwenden. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
PS Wenn jemand weiß, wie die Objekterkennungs-API </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
für Tensorflow Version 2 und höher </font><font style="vertical-align: inherit;">ordnungsgemäß ausgeführt wird, </font><font style="vertical-align: inherit;">melden Sie sich bitte in PM oder in einem Kommentar ab. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Einen schönen Tag und gute Gesundheit!</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de488464/index.html">Webpack 5 - Asset-Module</a></li>
<li><a href="../de488468/index.html">Einführung in FastAPI</a></li>
<li><a href="../de488470/index.html">Kim Dotcom: Gefangen, die meistgesuchte Person online. Teil 4</a></li>
<li><a href="../de488472/index.html">Wochenendlesung: 10 Materialien zu Audiogeräten - von sowjetischen Autoradios bis hin zu geräuschunterdrückenden Steckern</a></li>
<li><a href="../de494800/index.html">Reverse Engineering des chinesischen USB-IR-Transceiver-Protokolls</a></li>
<li><a href="../de494806/index.html">Cyber-Ziele 2019 als Trends 2020 - Hacker haben ihren Fokus geändert</a></li>
<li><a href="../de494808/index.html">Produktanalyst: Was macht es, wie viel verdient es, welche Vorteile bringt das Unternehmen?</a></li>
<li><a href="../de494810/index.html">Einführung in 3D: Three.js Basics</a></li>
<li><a href="../de494814/index.html">Ist Slurm nützlich?</a></li>
<li><a href="../de494818/index.html">So wählen Sie ein Handelsterminal für die Arbeit an der Börse aus</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>