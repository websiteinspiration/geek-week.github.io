<!doctype html>
<html class="no-js" lang="ja">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👩‍💻 🥪 🙅🏿 写真内のGPT-2（Transformer言語モデルの視覚化） 💑 🔱 🐸</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="2019年には、機械学習のすばらしい使用法を目にしました。OpenAI GPT-2モデルは、現代の言語モデルが生成できるものについての私たちの理解よりも優れた、一貫性のある感情的なテキストを書くための印象的な能力を実証しました。GPT-2は特に新しいアーキテクチャではありません。トランスフォーマーデ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>写真内のGPT-2（Transformer言語モデルの視覚化）</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/490842/"><p><img src="https://habrastorage.org/webt/1k/58/ea/1k58ea5w9egy2dc5z3jtsiip3sc.png" alt="openAI-GPT-2-3"></p><br>
<p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2019年には、機械学習のすばらしい使用法を目にしました。</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">OpenAI GPT-2モデルは</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、現代の言語モデルが生成できるものについての私たちの理解よりも優れた、一貫性のある感情的なテキストを書くための印象的な能力を実証しました。</font><font style="vertical-align: inherit;">GPT-2は特に新しいアーキテクチャではありません。トランスフォーマーデコーダー（デコーダーのみのトランスフォーマー）を彷彿とさせます。</font><font style="vertical-align: inherit;">GPT-2の違いは、印象的なデータセットでトレーニングされた、Transformerに基づく本当に巨大な言語モデルであることです。</font><font style="vertical-align: inherit;">この記事では、そのような結果を達成できるようにするモデルのアーキテクチャーについて説明します。自己注意レイヤーと、言語モデリングを超えるタスクのためのデコードトランスフォーマーの使用について詳しく検討します。</font></font></p><a name="habracut"></a><br>
<p><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">コンテンツ</font></font></strong></p><br>
<ul>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="> 1: GPT-2   </a><br>
<ul>
<li>   </li>
<li>   </li>
<li>   BERT'</li>
<li>  </li>
<li>-  :   GPT-2</li>
<li> </li>
<li>  : GPT-2,   </li>
</ul></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="> 2:   </a><br>
<ul>
<li>  ( )</li>
<li>1 –   ,   </li>
<li>2 –  </li>
<li>3 – </li>
<li>   </li>
<li>    GPT-2</li>
<li>  !</li>
</ul></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="> 3:    </a><br>
<ul>
<li> </li>
<li></li>
<li> </li>
<li> </li>
</ul></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="></a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="></a></li>
</ul><br>
<h1 id="chast-1-gpt-2-i-yazykovoe-modelirovaniea-namepart_1a"> 1: GPT-2   </h1><br>
<p>    ?</p><br>
<h2 id="chto-takoe-yazykovaya-model">   </h2><br>
<p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=">Word2vec  </a>  ,     –  ,    ,           .     –   ,        .</p><br>
<p><img src="https://habrastorage.org/webt/98/yv/br/98yvbr3q4hpjs8hfsg1rajz3n9m.png" alt="swiftkey-keyboard"></p><br>
<p>    ,  GPT-2        ,     ,  ,     . GPT-2        40  (WebText),  OpenAI        .      ,  ,  SwiftKey,   78 ,         GPT-2   500      ,     GPT-2 –  13   (      6,5 ).</p><br>
<p><img src="https://habrastorage.org/webt/md/qg/ve/mdqgveo0tsyxqapfzqpomuu5nzo.png" alt="gpt2-sizes"></p><br>
<p>    GPT-2   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow">AllenAI GPT-2 Explorer</a>.   GPT-2        (   ),     .</p><br>
<h2 id="transformery-dlya-yazykovogo-modelirovaniya">   </h2><br>
<p>     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=">  </a>,         –     ..  .       – ,   -      .</p><br>
<p><img src="https://habrastorage.org/webt/sv/tx/hm/svtxhmcbpksa7rjn0wb0o2jj_wc.png" alt="トランスエンコーダーデコーダー"></p><br>
<p>    ,              ,    ,   ,                (                AlphaStar).</p><br>
<p><img src="https://habrastorage.org/webt/j4/sb/4r/j4sb4rysituzcqsafgktspu0pa8.png" alt="gpt-2-transformer-xl-bert-3"></p><br>
<p>      ? ,           GPT-2  :</p><br>
<p><img src="https://habrastorage.org/webt/gg/og/nr/ggognri38aojdzkkzazvnetxfvy.png" alt="gpt2-sizes-hyperparameters-3"></p><br>
<h2 id="odno-otlichie-ot-berta">   BERT'</h2><br>
<blockquote><strong>  :</strong><br>
         ,     .</blockquote><p> GPT-2      . BERT , ,   .         .       ,  GPT-2,      ,       .   ,     GPT-2    :</p><br>
<p><img src="https://habrastorage.org/webt/so/pv/tj/sopvtjhr-crr3rgtf_zaoqomgck.gif" alt="gpt-2-output"></p><br>
<p>   :  ,     ,     .           .    «» (auto-regression)    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"> RNN   </a>. </p><br>
<p><img src="https://habrastorage.org/webt/un/7x/ir/un7xirwndekafrdeixsce0ocmmm.gif" alt="gpt-2-autoregression-2"></p><br>
<p>GPT-2      TransformerXL  XLNet    . BERT .     .  , BERT            . XLNet   ,          .</p><br>
<h2 id="evolyuciya-bloka-transformera">  </h2><br>
<p><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"> </a>     .</p><br>
<h3 id="blok-enkodera"> </h3><br>
<p> –   :</p><br>
<p><img src="https://habrastorage.org/webt/f_/ym/xb/f_ymxbcpvszucrhogiurzwqypzk.png" alt="トランスエンコーダーブロック2"></p><br>
<p><em>             (, 512 ).      ,        .</em></p><br>
<h3 id="blok-dekodera"> </h3><br>
<p> –   ,      .        :</p><br>
<p><img src="https://habrastorage.org/webt/gh/so/u6/ghsou6icfirnjbj1abkj7ouqohg.png" alt="トランスデコーダブロック2"></p><br>
<p>            ,          [mask] ,   BERT',             ,     ,     .</p><br>
<p>, ,      #4,   ,         :</p><br>
<p><img src="https://habrastorage.org/webt/ta/9r/td/ta9rtdiimdxkwmwpmdxy2ig-0h8.png" alt="トランスデコーダブロックセルフアテンション2"></p><br>
<p>     ,   BERT,     GPT-2.         .       :</p><br>
<p><img src="https://habrastorage.org/webt/43/cq/pd/43cqpdiypmyuccr1gznrhr4tioo.png" alt="自己注意と覆面自己注意"></p><br>
<h3 id="blok-dekodirovaniya"> </h3><br>
<p>  ,  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow">«Generating Wikipedia by Summarizing Long Sequences»</a>     ,    :    .      «-».            6   :</p><br>
<p><img src="https://habrastorage.org/webt/rd/l7/k-/rdl7k--_z3kg4ajejaycksxqpt0.png" alt="トランスデコーダーイントロ"></p><br>
<p><em>  .       ,     . ,       4000     –      512   .</em></p><br>
<p>       ,  ,       .       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow">«         »</a>,         /    .</p><br>
<p> GPT-2  OpenAI     .</p><br>
<h2 id="ekspress-kurs-po-neyrohirurgii-zaglyadyvaya-vnutr-gpt-2">-  :   GPT-2</h2><br>
<blockquote> ,   ,       .  ,  ,      , , . (Budgie)</blockquote><p>   GPT-2       ,   .</p><br>
<p><img src="https://habrastorage.org/webt/he/ig/hj/heighjb23joyolbax5b7__fvj6a.png" alt="gpt-2-layers-2"></p><br>
<p><em>GPT-2   1024 .           .</em></p><br>
<p>     GPT-2 –      (     )     (),        (..    ).          ,      (       &lt;|endoftext|&gt;;     &lt;|s|&gt;).</p><br>
<p><img src="https://habrastorage.org/webt/ho/yz/o0/hoyzo0yvmbi3gf-1awm__udx0fi.gif" alt="gpt2-simple-output-2"></p><br>
<p>      ,       .          ,    (score)   –  ,    (50    GPT-2).          – «the».    -  –  ,  ,         ,      ,     –       .      . GPT-2   top-k,      ,      ,      (,  ,   top-k = 1).</p><br>
<p>              :</p><br>
<p><img src="https://habrastorage.org/webt/hg/2x/ps/hg2xpsd1kuverowaqcd4z_w--ki.gif" alt="gpt-2-simple-output-3"></p><br>
<p> ,        .   GPT-2              (          ). GPT-2       .</p><br>
<h2 id="zaglyanem-poglubzhe"> </h2><br>
<h3 id="kodirovanie-vhoda"> </h3><br>
<p>   .   .     NLP-,     ,          –   ,    .</p><br>
<p><img src="https://habrastorage.org/webt/qo/ml/jl/qomljlikh-rqxyud6otmaxrtyum.png" alt="gpt2-token-embeddings-wte-2"></p><br>
<p><em>    –  ,     -   .            GPT-2.       768  /.</em></p><br>
<p>,       &lt;|s|&gt;   .        ,      – ,         .     ,        1024   . </p><br>
<p><img src="https://habrastorage.org/webt/zp/mu/cg/zpmucgftth4mabqo60oolwdohey.png" alt="gpt2-positional-encoding"></p><br>
<p>          .      ,     GPT-2.</p><br>
<p><img src="https://habrastorage.org/webt/me/r8/bx/mer8bx3ofjqcpydqnfsvo1fk4ok.png" alt="gpt2-input-embedding-positional-encoding-3"></p><br>
<p><em>                 #1.</em></p><br>
<h3 id="puteshestvie-vverh-po-steku">   </h3><br>
<p>     ,       ,      .       ,            .      ,       ,      .</p><br>
<p><img src="https://habrastorage.org/webt/ij/9e/3a/ij9e3alc1pc5ihgpd8njpbh2-vy.png" alt="gpt2-transformer-block-vectors-2"></p><br>
<h3 id="obzor-vnutrennego-vnimaniya">  </h3><br>
<p>      . ,     :</p><br>
<blockquote>    ,   <strong></strong> ,   ,  <strong> </strong>  <strong> </strong>.</blockquote><p>     ,      .         ,    .     ,    , :</p><br>
<ul>
<li><strong></strong>   ;</li>
<li><strong> </strong>      («   »);</li>
<li><strong> </strong>     .</li>
</ul><br>
<p>     :          ,      ,       (   ).      ,         ,        .</p><br>
<p> ,           «a robot»     «it». ,       ,        ,    .</p><br>
<p><img src="https://habrastorage.org/webt/yx/vw/ou/yxvwoujcadhlalw2xgq3dnoezjq.png" alt="gpt2-self-attention-example-2"></p><br>
<h3 id="algoritm-vnutrennego-vnimaniya">  </h3><br>
<p>         .     :</p><br>
<ul>
<li><strong></strong> –    ,          (  ).      ,     ;</li>
<li><strong></strong> –       .        ;</li>
<li><strong></strong> –     ;      ,   ,        .</li>
</ul><br>
<p><img src="https://habrastorage.org/webt/ds/30/gu/ds30gubjhv2r7ubicxiq-idgdlg.png" alt="self-attention-example-folders-3"></p><br>
<p>          .  –     ,   .       .   ,      –  .           ,      .</p><br>
<p>              (:      ).</p><br>
<p><img src="https://habrastorage.org/webt/br/mp/xd/brmpxdc2ngiqgrkmrgn1wb5aud4.png" alt="self-attention-example-folders-scores-3"></p><br>
<p>      ,     .</p><br>
<p><img src="https://habrastorage.org/webt/xv/7n/ez/xv7nezgfhygrknpndfsptjwucpe.png" alt="gpt2-value-vector-sum"></p><br>
<p>       ,  50%      «robot», 30%   «a»  19% –   «it».         .           .</p><br>
<h3 id="vyhod-modeli"> </h3><br>
<p>       (          ),      .</p><br>
<p><img src="https://habrastorage.org/webt/vz/tt/mf/vzttmfwinmu2z7v3m0hkxijm5o8.png" alt="gpt2-output-projection-2"></p><br>
<p>  ,           .          .</p><br>
<p><img src="https://habrastorage.org/webt/fg/ib/yq/fgibyqwocamlo1cax73ck1qxrwg.png" alt="gpt2-output-scores-2"></p><br>
<p>        (top_k = 1).    ,        .         ,      ,      (          ).   –  top_k  40:       40    .</p><br>
<p><img src="https://habrastorage.org/webt/ji/zl/mk/jizlmk0ywrvlljh6ynxq_eqacjs.png" alt="gpt2-output"></p><br>
<p> ,        .       ,       (1024 )       .</p><br>
<h2 id="konec-pervoy-chasti-gpt-2-damy-i-gospoda">  : GPT-2,   </h2><br>
<p>,    ,   GPT-2.    ,       ,       .     ,                 (  TransformerXL  XLNet).</p><br>
<p>    ,       :</p><br>
<ul>
<li>«»  «»        ;     GPT-2     (Byte Pair Encoding)     .  ,      .</li>
<li>    GPT-2    / (inference/evaluation mode).         .                .           (512),      1,     .</li>
<li>     /       .        .</li>
<li>      ,    .        <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=">Transformer  </a>,          .</li>
<li>          .      «zoom in», : </li>
</ul><br>
<p><img src="https://habrastorage.org/webt/q_/kq/35/q_kq35nsjwiycnwcafi8ctxl_tm.png" alt="zoom-in"></p><br>
<h1 id="chast-2-vizualizaciya-vnutrennego-vnimaniyaa-namepart_2a"> 2:   </h1><br>
<p>             ,   «it»:</p><br>
<p><img src="https://habrastorage.org/webt/ss/7h/dk/ss7hdk8hwx43lbghil2t3os95bm.png" alt="gpt2-self-attention-1-2"></p><br>
<p>       ,   .      ,      ,       .              ,      ,      .</p><br>
<h2 id="vnutrennee-vnimanie-bez-maskirovaniya">  ( )</h2><br>
<p>     ,     .    ,   4   .</p><br>
<p>       :</p><br>
<ol>
<li>  ,      ;</li>
<li>             ;</li>
<li>          .</li>
</ol><br>
<p><img src="https://habrastorage.org/webt/ab/l8/ax/abl8ax5sj4nuqdq8jihtayycpvc.png" alt="self-attention-summary"></p><br>
<h2 id="1--sozdanie-vektorov-zaprosa-klyucha-i-znacheniya">1 –   ,   </h2><br>
<p>   .          .      .              (  «» ):</p><br>
<p><img src="https://habrastorage.org/webt/qc/7c/0l/qc7c0ll_eqaebzsk5jqluip1j7k.png" alt="self-attention-1"></p><br>
<p><em>      ,            WQ, WK, WV</em></p><br>
<h2 id="2--podschet-koefficientov">2 –  </h2><br>
<p>,     ,           №2:                   .</p><br>
<p><img src="https://habrastorage.org/webt/47/tl/vm/47tlvmotgdunt0od7rzfptulwxi.png" alt="self-attention-2"></p><br>
<p><em> ( )            ,     </em></p><br>
<h2 id="3--summirovanie">3 – </h2><br>
<p>       .            ,     .</p><br>
<p><img src="https://habrastorage.org/webt/nz/aa/sl/nzaaslrrr6etmrs-qlkhlb9o6oc.png" alt="self-attention-3-2"></p><br>
<p>&nbsp;<em>         </em></p><br>
<p>  ,       –   ,            .</p><br>
<p>       ,   ,         .          (   ).</p><br>
<h2 id="vizualizaciya-maskirovannogo-vnutrennego-vnimaniya">   </h2><br>
<p>,     ,       ,      .        №2. ,           .       .        ,      :</p><br>
<p><img src="https://habrastorage.org/webt/di/h6/vx/dih6vxt0pa-5ipj_qtruxbmbcae.png" alt="masked-self-attention-2"></p><br>
<p>      ,    (attention mask). , ,     («robot must obey orders»).         4 :     ( ,    –   ). ..    ,      4    ,      (  4 )   .</p><br>
<p><img src="https://habrastorage.org/webt/ot/_d/k9/ot_dk95ismt5lcax-xeomlzxigu.png" alt="transformer-decoder-attention-mask-dataset"></p><br>
<p>     ,      .    ,   ,        ( ),    :</p><br>
<p><img src="https://habrastorage.org/webt/2_/kz/ws/2_kzws5vewj2dvdy-rtcgwjxdd8.png" alt="queries-keys-attention-mask"></p><br>
<p>   «»    .   ,    ,     – (-inf)      (, -1   GPT-2):</p><br>
<p><img src="https://habrastorage.org/webt/86/aw/oh/86awoh2nlrcsdh3_hveet0rh3vw.png" alt="transformer-attention-mask"></p><br>
<p>,      ,   ,       :</p><br>
<p><img src="https://habrastorage.org/webt/cr/-o/ku/cr-okuz6tuuuaeht--j8mjblnqo.png" alt="transformer-attention-masked-scores-softmax"></p><br>
<p>    :</p><br>
<ul>
<li>        ( №1),      («robot»), 100%      .</li>
<li>        ( №2),    («robot must»),     «must» 48%     «robot»  52%    «must».</li>
<li> ..</li>
</ul><br>
<h2 id="maskirovannoe-vnutrennee-vnimanie-v-gpt-2">    GPT-2</h2><br>
<p>       GPT-2.</p><br>
<h3 id="vremya-ocenki-obrabotka-odnogo-tokena-za-raz"> :     </h3><br>
<p>   ,  GPT-2     ,    .    ,           ,          ,    .</p><br>
<p>       (  &lt;|s|&gt;).</p><br>
<p><img src="https://habrastorage.org/webt/oc/r2/ps/ocr2ps037_1vvr2uajz1idbe76g.png" alt="gpt2-self-attention-qkv-1-2"></p><br>
<p>GPT-2       «a».             :</p><br>
<p><img src="https://habrastorage.org/webt/0e/wf/vx/0ewfvx7snx6azpvwietxqfd9zzg.png" alt="gpt2-self-attention-qkv-2-2"></p><br>
<p>  ,     «robot»,      ,      «a» –    ,    :</p><br>
<p><img src="https://habrastorage.org/webt/re/qe/ff/reqeffjn9neoql-qqpiqp8hwmjq.png" alt="gpt2-self-attention-qkv-3-2"></p><br>
<h3 id="vnutrennee-vnimanie-gpt-2-1--sozdanie-vektorov-zaprosa-klyucha-i-znacheniya">  GPT-2: 1 –   ,   </h3><br>
<p>,     «it».    ,         «it» +      #9:</p><br>
<p><img src="https://habrastorage.org/webt/k1/ol/ee/k1oleegivyrvsvcx-glsjjxdt14.png" alt="gpt2-self-attention-1"></p><br>
<p>         (     ),       ,   .</p><br>
<p><img src="https://habrastorage.org/webt/ws/nc/1k/wsnc1kowztsdydbbgkevy7ycene.png" alt="gpt2-self-attention-2"></p><br>
<p><em>            (bias vector),   </em></p><br>
<p>    ,       ,      «it».</p><br>
<p><img src="https://habrastorage.org/webt/od/oc/ag/odocagmiv-m420l-3wy8sdila4w.png" alt="gpt2-self-attention-3"></p><br>
<p><em>       (     )   ,      </em></p><br>
<h3 id="vnutrennee-vnimanie-gpt-2-15--razdelenie-na-golovy-vnimaniya">  GPT-2: 1.5 –   «» </h3><br>
<p>        ,   «» .        .           (Q),  (K)   (V).   «»  –       .   GPT-2   12 «» ,      :</p><br>
<p><img src="https://habrastorage.org/webt/zt/qf/wr/ztqfwr4l3kaoq243xhlleg4ro2i.png" alt="gpt2-self-attention-split-attention-heads-1"></p><br>
<p>    ,     «» .    «» ,        (      12 «» ):</p><br>
<p><img src="https://habrastorage.org/webt/lu/8a/5c/lu8a5cqdztpnonylf6jfxpopngg.png" alt="gpt2-self-attention-split-attention-heads-2"></p><br>
<h3 id="vnutrennee-vnimanie-gpt-2-2--podschet-koefficientov">  GPT-2: 2 –  </h3><br>
<p>       (  ,      «»        ):</p><br>
<p><img src="https://habrastorage.org/webt/ab/0b/nm/ab0bnmrgkb-3knzrtitk2e64opi.png" alt="gpt2-self-attention-scoring"></p><br>
<p>        (    «»  #1   ):</p><br>
<p><img src="https://habrastorage.org/webt/zc/ll/nt/zcllnthmdvddhniljejpbgke8is.png" alt="gpt2-self-attention-scoring-2"></p><br>
<h3 id="vnutrennee-vnimanie-gpt-2-3--summirovanie">  GPT-2: 3 – </h3><br>
<p>   ,         , ,      «»  #1:</p><br>
<p><img src="https://habrastorage.org/webt/dv/rf/-i/dvrf-ik3yxkidhzgihbuuqv4sww.png" alt="gpt2-self-attention-multihead-sum-1"></p><br>
<h3 id="vnutrennee-vnimanie-gpt-2-35--obedinenie-golov-vnimaniya">  GPT-2: 3.5 –  «» </h3><br>
<p>  «»  ,  ,      :</p><br>
<p><img src="https://habrastorage.org/webt/p9/jg/bg/p9jgbgr9-c8lzl-zjnqfdortvxe.png" alt="gpt2-self-attention-merge-heads-1"></p><br>
<p>        .         .</p><br>
<h3 id="vnutrennee-vnimanie-gpt-2-4--proecirovanie">  GPT-2: 4 – </h3><br>
<p>   ,         ,        .       ,    «»       :</p><br>
<p><img src="https://habrastorage.org/webt/mc/mz/qq/mcmzqqb80hq14cyooubulhh3hhu.png" alt="gpt2-self-attention-project-1"></p><br>
<p> ,   ,      :</p><br>
<p><img src="https://habrastorage.org/webt/af/tl/0o/aftl0oqxhho-m6uph1uqer2a4_c.png" alt="gpt2-self-attention-project-2"></p><br>
<h3 id="polnosvyaznaya-neyronnaya-set-gpt-2-sloy-1">   GPT-2:  #1</h3><br>
<p>   –  ,       ,         .     .    4     (  GPT-2   768,      768*4 = 3072 ).    ?      (   512   #1 – 2048). ,           ,      .</p><br>
<p><img src="https://habrastorage.org/webt/gu/4d/ci/gu4dciafj9oa7wcbnt1pwtpfcu8.gif" alt="gpt2-mlp1"></p><br>
<p><em>(   )</em></p><br>
<h3 id="polnosvyaznaya-neyronnaya-set-gpt-2-proecirovanie-na-razmernost-modeli">   GPT-2:    </h3><br>
<p>        (768   GPT-2).          .</p><br>
<p><img src="https://habrastorage.org/webt/x-/eh/kj/x-ehkj3y5dyunh4nf3v7aykhp70.gif" alt="gpt2-mlp-2"></p><br>
<p><em>(   )</em></p><br>
<h2 id="vy-sdelali-eto">  !</h2><br>
<p>     ,    - .       ,      .  , ,         :</p><br>
<p><img src="https://habrastorage.org/webt/yz/n7/20/yzn720lcfsuwwky-r3kanjys3w0.png" alt="gpt2-transformer-block-weights-2"></p><br>
<p>      .   ,            :</p><br>
<p><img src="https://habrastorage.org/webt/xl/22/mc/xl22mctq_nzefihu84kxryqj5be.png" alt="gpt2-weights-2"></p><br>
<p>       ,    :</p><br>
<p><img src="https://habrastorage.org/webt/vk/it/nh/vkitnhrnvo0djn0gn1_e0fpzhp0.png" alt="gpt2-117-parameters"></p><br>
<p> -    124   117.    ,  ,         (,    ).</p><br>
<h1 id="chast-3-za-predelami-yazykovogo-modelirovaniyaa-namepart_3a"> 3:    </h1><br>
<p>      ,    .      ,     .        .</p><br>
<h2 id="mashinnyy-perevod"> </h2><br>
<p>      .         :</p><br>
<p><img src="https://habrastorage.org/webt/ni/vt/bw/nivtbwcuuxla5cx5p2srhoohfgm.png" alt="デコーダのみのトランスフォーマー翻訳"></p><br>
<h2 id="summarizaciya"></h2><br>
<p>  ,        .  ,       (  ,   )   .           :</p><br>
<p><img src="https://habrastorage.org/webt/gd/3u/h5/gd3uh5vlhfzoarket8i2eeudj_4.png" alt="ウィキペディアの要約"></p><br>
<p>         .</p><br>
<p><img src="https://habrastorage.org/webt/f2/-z/8v/f2-z8vum88usd2nyshvwwpesjq8.png" alt="デコーダのみの要約"></p><br>
<h2 id="transfernoe-obuchenie"> </h2><br>
<p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow">Sample Efficient Text Summarization Using a Single Pre-Trained Transformer</a>        ,      . ,        ,     -    .</p><br>
<p>   GPT-2           .</p><br>
<h2 id="generaciya-muzyki"> </h2><br>
<p><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"> </a>          . « »      –             (,     « »).</p><br>
<p>   ,      . ,           (),    ( ).    (, ,  )       «» –  ,     .</p><br>
<p><img src="https://habrastorage.org/webt/en/x-/g2/enx-g2e1om4ctwjcuvnsspcjzcm.png" alt="music-transformer-performance-encoding-3"></p><br>
<p> –     one-hot .   midi      .       :</p><br>
<p><img src="https://habrastorage.org/webt/q2/0s/vv/q20svvnlw_1ji4s_fjejnlnrckc.png" alt="音楽表現の例"></p><br>
<p> one-hot         :</p><br>
<p><img src="https://habrastorage.org/webt/gh/av/o5/ghavo5srziiagcrg-zqzg0vbnjg.png" alt="music-transformer-input-representation-2"></p><br>
<p>         :</p><br>
<p><img src="https://habrastorage.org/webt/gg/up/ou/ggupou0idkyeu2oc-dtgvxayxsa.png" alt="music-transformer-self-attention-2"></p><br>
<p>        , . <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"></a>.</p><br>
<h1 id="zaklyucheniea-nameconclusiona"></h1><br>
<p>     GPT-2     –  . ,              ,   ,    ,         .</p><br>
<h1 id="materialya-nameresourcesa"></h1><br>
<ul>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"> GPT-2</a>  OpenAI</li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">参照してください</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">pytorch-transformers2の</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ライブラリーを</font><font style="vertical-align: inherit;">から</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">抱きフェイス</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">れ、GPT-2に加えて、実装のBERT、トランス-XL、XLNetおよびその他の高度な変圧器モデル、。</font></font></li>
</ul><br>
<h1 id="avtory"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">著者</font></font></h1><br>
<ul>
<li><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">オリジナル</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">による</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ジェイAlammar</font></font></a></li>
<li><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">翻訳</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">エカテリーナスミルノワ</font></font></a></li>
<li><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">編集とレイアウト</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Shkarinセルゲイ</font></font></a></li>
</ul></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../ja490830/index.html">Zimbra Collaboration Suite Open-Source Editionのパスワード管理</a></li>
<li><a href="../ja490832/index.html">靴のマーキング：200万のマーキングコード「1C」+「正直な​​サイン」を流通させたとき</a></li>
<li><a href="../ja490836/index.html">コメントを開いてスパムに溺れない方法</a></li>
<li><a href="../ja490838/index.html">WiFi 6が登場：市場が提供するものと、なぜこのテクノロジーが必要なのか</a></li>
<li><a href="../ja490840/index.html">リソースプランニング。パート4.1。リソース計画を立てる前に</a></li>
<li><a href="../ja490844/index.html">本「Kubernetesパターン：ネイティブクラウドアプリケーション開発パターン」</a></li>
<li><a href="../ja490846/index.html">カラシニコフアカデミーのウェブサイトをどのように開発し、2つのコンテストの受賞者になったか</a></li>
<li><a href="../ja490850/index.html">コンパイラがあなたを助けるのを助ける</a></li>
<li><a href="../ja490852/index.html">SpinLaunchの詳細-宇宙業界で最も熱心に秘密にされてきた</a></li>
<li><a href="../ja490854/index.html">AIを使用してアイテムを並べ替える倉庫ロボットの準備が整いました</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>