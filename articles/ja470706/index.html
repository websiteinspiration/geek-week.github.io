<!doctype html>
<html class="no-js" lang="ja">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>📹 ⛔️ 🐯 Python + Keras + LSTM：30分でテキスト翻訳を行います 🏄 👂🏼 👨🏽‍🏭</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="こんにちは、ハブル。
 
 で前の部分、私は、ニューラルネットワークに基づいて、単純なテキスト認識を作成するを見ました。今日は、同様のアプローチを使用して、テキストを英語からドイツ語に自動翻訳します。
 
 
 
 これがどのように機能するかに興味がある人のために、詳細はカットの下にあります。
 
...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Python + Keras + LSTM：30分でテキスト翻訳を行います</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/470706/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">こんにちは、ハブル。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
で</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">前の部分、</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">私は、ニューラルネットワークに基づいて、単純なテキスト認識を作成するを見ました。</font><font style="vertical-align: inherit;">今日は、同様のアプローチを使用して、テキストを英語からドイツ語に自動翻訳します。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/gf/ft/jx/gfftjxwflb7yxrwtffish1hkqsc.jpeg"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
これがどのように機能するかに興味がある人のために、詳細はカットの下にあります。</font></font><br>
<a name="habracut"></a><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">注</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">：このニューラルネットワークを使用した翻訳プロジェクトは、教育のみを目的としているため、「なぜ」という質問は考慮されていません。</font><font style="vertical-align: inherit;">楽しみのためだけに。</font><font style="vertical-align: inherit;">私は、この方法またはその方法が良いか悪いかを証明するつもりはありません。何が起こるかを確認するのは興味深いことでした。</font><font style="vertical-align: inherit;">もちろん、以下で使用する方法は単純化されていますが、30分後に2つ目のLingvoを作成することを期待している人はいないでしょう。</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">データ収集</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 ネットワークで見つかった、タブで区切られた英語とドイツ語のフレーズを含むファイルがソースデータセットとして使用されました。</font><font style="vertical-align: inherit;">一連のフレーズは次のようになります。</font></font><br>
<br>
<pre><code class="python hljs">            Hi.		Hallo!<font></font>
            Hi.		Grüß Gott!<font></font>
            Run!	Lauf!<font></font>
            Wow!	Potzdonner!<font></font>
            Wow!	Donnerwetter!<font></font>
            Fire!	Feuer!<font></font>
            Help!	Hilfe!<font></font>
            Help!	Zu Hülf!<font></font>
            Stop!	Stopp!<font></font>
            Wait!	Warte!<font></font>
            Go on.	Mach weiter.<font></font>
            Hello!	Hallo!<font></font>
            I ran.	Ich rannte.<font></font>
            I see.	Ich verstehe.<font></font>
            ...<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
このファイルには192千行が含まれ、サイズは13 MBです。</font><font style="vertical-align: inherit;">テキストをメモリにロードし、データを英語とドイツ語の2つのブロックに分割します。</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">read_text</span>(<span class="hljs-params">filename</span>):</span>
    <span class="hljs-keyword">with</span> open(filename, mode=<span class="hljs-string">'rt'</span>, encoding=<span class="hljs-string">'utf-8'</span>) <span class="hljs-keyword">as</span> file:<font></font>
        text = file.read()<font></font>
        sents = text.strip().split(<span class="hljs-string">'\n'</span>)
        <span class="hljs-keyword">return</span> [i.split(<span class="hljs-string">'\t'</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> sents]<font></font>
<font></font>
data = read_text(<span class="hljs-string">"deutch.txt"</span>)<font></font>
deu_eng = np.array(data)<font></font>
<font></font>
deu_eng = deu_eng[:<span class="hljs-number">30000</span>,:]<font></font>
print(<span class="hljs-string">"Dictionary size:"</span>, deu_eng.shape)<font></font>
<font></font>
<span class="hljs-comment"># Remove punctuation </span>
deu_eng[:,<span class="hljs-number">0</span>] = [s.translate(str.maketrans(<span class="hljs-string">''</span>, <span class="hljs-string">''</span>, string.punctuation)) <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> deu_eng[:,<span class="hljs-number">0</span>]] <font></font>
deu_eng[:,<span class="hljs-number">1</span>] = [s.translate(str.maketrans(<span class="hljs-string">''</span>, <span class="hljs-string">''</span>, string.punctuation)) <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> deu_eng[:,<span class="hljs-number">1</span>]] <font></font>
<font></font>
<span class="hljs-comment"># convert text to lowercase </span>
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(deu_eng)): <font></font>
    deu_eng[i,<span class="hljs-number">0</span>] = deu_eng[i,<span class="hljs-number">0</span>].lower() <font></font>
    deu_eng[i,<span class="hljs-number">1</span>] = deu_eng[i,<span class="hljs-number">1</span>].lower()
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
また、すべての単語を小文字に変換し、句読点を削除しました。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
次のステップは、ニューラルネットワーク用のデータを準備することです。</font><font style="vertical-align: inherit;">ネットワークは単語が何であるかを認識しておらず、数字のみを処理します。</font><font style="vertical-align: inherit;">幸いにも、kerasにはすでにTokenizerクラスが組み込まれており、文の単語をデジタルコードに置き換えます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
その使用法は簡単な例で示されています：</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> keras.preprocessing.text <span class="hljs-keyword">import</span> Tokenizer
<span class="hljs-keyword">from</span> keras.preprocessing.sequence <span class="hljs-keyword">import</span> pad_sequences<font></font>
<font></font>
s = <span class="hljs-string">"To be or not to be"</span><font></font>
eng_tokenizer = Tokenizer()<font></font>
eng_tokenizer.fit_on_texts([s])<font></font>
<font></font>
seq = eng_tokenizer.texts_to_sequences([s])<font></font>
seq = pad_sequences(seq, maxlen=<span class="hljs-number">8</span>, padding=<span class="hljs-string">'post'</span>)<font></font>
print(seq)<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
「するかしないか」というフレーズは、配列[1 2 3 4 1 2 0 0]に置き換えられます。ここでは、1 =に、2 =に、3 =または、4 =と、推測するのは難しくありません。</font><font style="vertical-align: inherit;">これらのデータはすでにニューラルネットワークに送信できます。</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ニューラルネットワークのトレーニング</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
私たちのデータはデジタルで準備されています。</font><font style="vertical-align: inherit;">配列を入力（英語の行）と出力（ドイツの行）のデータ用に2つのブロックに分割します。</font><font style="vertical-align: inherit;">また、学習プロセスを検証するための別のユニットも用意します。</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-comment"># split data into train and test set </span>
train, test = train_test_split(deu_eng, test_size=<span class="hljs-number">0.2</span>, random_state=<span class="hljs-number">12</span>)<font></font>
<font></font>
<span class="hljs-comment"># prepare training data </span>
trainX = encode_sequences(eng_tokenizer, eng_length, train[:, <span class="hljs-number">0</span>])<font></font>
trainY = encode_sequences(deu_tokenizer, deu_length, train[:, <span class="hljs-number">1</span>])<font></font>
<font></font>
<span class="hljs-comment"># prepare validation data </span>
testX = encode_sequences(eng_tokenizer, eng_length, test[:, <span class="hljs-number">0</span>])<font></font>
testY = encode_sequences(deu_tokenizer, deu_length, test[:, <span class="hljs-number">1</span>])
</code></pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
これで、ニューラルネットワークのモデルを作成し、そのトレーニングを実行できます。</font><font style="vertical-align: inherit;">ご覧のとおり、ニューラルネットワークには、メモリセルを持つLSTM層が含まれています。</font><font style="vertical-align: inherit;">おそらく「通常の」ネットワークで動作しますが、希望する人は自分で確認できます。</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">make_model</span>(<span class="hljs-params">in_vocab, out_vocab, in_timesteps, out_timesteps, n</span>):</span><font></font>
    model = Sequential()<font></font>
    model.add(Embedding(in_vocab, n, input_length=in_timesteps, mask_zero=<span class="hljs-literal">True</span>))<font></font>
    model.add(LSTM(n))<font></font>
    model.add(Dropout(<span class="hljs-number">0.3</span>))<font></font>
    model.add(RepeatVector(out_timesteps))<font></font>
    model.add(LSTM(n, return_sequences=<span class="hljs-literal">True</span>))<font></font>
    model.add(Dropout(<span class="hljs-number">0.3</span>))<font></font>
    model.add(Dense(out_vocab, activation=<span class="hljs-string">'softmax'</span>))<font></font>
    model.compile(optimizer=optimizers.RMSprop(lr=<span class="hljs-number">0.001</span>), loss=<span class="hljs-string">'sparse_categorical_crossentropy'</span>)
    <span class="hljs-keyword">return</span> model<font></font>
<font></font>
eng_vocab_size = len(eng_tokenizer.word_index) + <span class="hljs-number">1</span> 
deu_vocab_size = len(deu_tokenizer.word_index) + <span class="hljs-number">1</span>
eng_length, deu_length = <span class="hljs-number">8</span>, <span class="hljs-number">8</span>
model = make_model(eng_vocab_size, deu_vocab_size, eng_length, deu_length, <span class="hljs-number">512</span>)<font></font>
<font></font>
num_epochs = <span class="hljs-number">40</span>
model.fit(trainX, trainY.reshape(trainY.shape[<span class="hljs-number">0</span>], trainY.shape[<span class="hljs-number">1</span>], <span class="hljs-number">1</span>), epochs=num_epochs, batch_size=<span class="hljs-number">512</span>, validation_split=<span class="hljs-number">0.2</span>, callbacks=<span class="hljs-literal">None</span>, verbose=<span class="hljs-number">1</span>)<font></font>
model.save(<span class="hljs-string">'en-de-model.h5'</span>)
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
トレーニング自体は次のようになります。</font></font><br>
<br>
<img src="https://habrastorage.org/webt/fj/xl/b4/fjxlb4yorszz5ojzpcih4ixlria.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ご覧の</font><font style="vertical-align: inherit;">とおり</font><font style="vertical-align: inherit;">、プロセスは高速ではなく、Core i7 + GeForce 1060で3万行のセットに対して約30分かかります。</font><font style="vertical-align: inherit;">トレーニングが終了すると（1回だけ実行する必要があります）、モデルはファイルに保存され、再利用できます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
翻訳を取得するには、predict_classes関数を使用し、その入力にいくつかの単純なフレーズを送信します。</font><font style="vertical-align: inherit;">get_word関数は、単語を数値に変換するために使用されます。</font></font><br>
<br>
<pre><code class="python hljs">model = load_model(<span class="hljs-string">'en-de-model.h5'</span>)<font></font>
<font></font>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_word</span>(<span class="hljs-params">n, tokenizer</span>):</span>
    <span class="hljs-keyword">if</span> n == <span class="hljs-number">0</span>:
        <span class="hljs-keyword">return</span> <span class="hljs-string">""</span>
    <span class="hljs-keyword">for</span> word, index <span class="hljs-keyword">in</span> tokenizer.word_index.items():
        <span class="hljs-keyword">if</span> index == n:
            <span class="hljs-keyword">return</span> word
    <span class="hljs-keyword">return</span> <span class="hljs-string">""</span><font></font>
<font></font>
phrs_enc = encode_sequences(eng_tokenizer, eng_length, [<span class="hljs-string">"the weather is nice today"</span>, <span class="hljs-string">"my name is tom"</span>, <span class="hljs-string">"how old are you"</span>, <span class="hljs-string">"where is the nearest shop"</span>])<font></font>
<font></font>
preds = model.predict_classes(phrs_enc)<font></font>
print(<span class="hljs-string">"Preds:"</span>, preds.shape)<font></font>
print(preds[<span class="hljs-number">0</span>])<font></font>
print(get_word(preds[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>], deu_tokenizer), get_word(preds[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>], deu_tokenizer), get_word(preds[<span class="hljs-number">0</span>][<span class="hljs-number">2</span>], deu_tokenizer), get_word(preds[<span class="hljs-number">0</span>][<span class="hljs-number">3</span>], deu_tokenizer))<font></font>
print(preds[<span class="hljs-number">1</span>])<font></font>
print(get_word(preds[<span class="hljs-number">1</span>][<span class="hljs-number">0</span>], deu_tokenizer), get_word(preds[<span class="hljs-number">1</span>][<span class="hljs-number">1</span>], deu_tokenizer), get_word(preds[<span class="hljs-number">1</span>][<span class="hljs-number">2</span>], deu_tokenizer), get_word(preds[<span class="hljs-number">1</span>][<span class="hljs-number">3</span>], deu_tokenizer))<font></font>
print(preds[<span class="hljs-number">2</span>])<font></font>
print(get_word(preds[<span class="hljs-number">2</span>][<span class="hljs-number">0</span>], deu_tokenizer), get_word(preds[<span class="hljs-number">2</span>][<span class="hljs-number">1</span>], deu_tokenizer), get_word(preds[<span class="hljs-number">2</span>][<span class="hljs-number">2</span>], deu_tokenizer), get_word(preds[<span class="hljs-number">2</span>][<span class="hljs-number">3</span>], deu_tokenizer))<font></font>
print(preds[<span class="hljs-number">3</span>])<font></font>
print(get_word(preds[<span class="hljs-number">3</span>][<span class="hljs-number">0</span>], deu_tokenizer), get_word(preds[<span class="hljs-number">3</span>][<span class="hljs-number">1</span>], deu_tokenizer), get_word(preds[<span class="hljs-number">3</span>][<span class="hljs-number">2</span>], deu_tokenizer), get_word(preds[<span class="hljs-number">3</span>][<span class="hljs-number">3</span>], deu_tokenizer))
</code></pre><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">結果</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
さて、実際には、最も興味深いのは結果です。ニューラルネットワークがどのように英語とドイツ語のフレーズ間の対応を学習および「記憶」するかを確認するのは興味深いことです。具体的には、2つのフレーズをより簡単に、2つのフレーズを使用して違いを確認しました。</font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">5分間のトレーニング</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
「今日は天気がいい」-「das ist ist tom」</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
「私の名前はtom」-「wiefürtom tom」</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
「何歳ですか」-「wie geht ist es」</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
「最寄りの店はどこ</font><font style="vertical-align: inherit;">ですか</font><font style="vertical-align: inherit;">」 -「wo ist der」</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ご覧のとおり、まだ「ヒット」はそれほど多くありません。 「あなたは何歳ですか」というフレーズの断片は、ニューラルネットワークを「あなたはどうですか」というフレーズと混同し、「wie geht ist es」という翻訳を生成しました（元気ですか）。 「どこにある」というフレーズでは、ニューラルネットワークは「どこにありますか？」という動詞のある場所だけを特定し、翻訳を発行しました。これは、原則として無意味ではありません。一般的に、それはグループA1のドイツの新人にも翻訳されます。）</font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">10分のトレーニング</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
「今日は天気がいい」-「das haus ist bereit」</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
「私の名前はトム」-「meinheißeheißetom」</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
「何歳ですか」 -「wie alt sind sie」</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
「最寄りの店はどこ</font><font style="vertical-align: inherit;">ですか</font><font style="vertical-align: inherit;">」-「wo ist paris」</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ある程度の進展が見られます。</font><font style="vertical-align: inherit;">最初のフレーズは完全に場違いです。</font><font style="vertical-align: inherit;">2番目のフレーズでは、ニューラルネットワークは動詞heißen（呼ばれる）を「学習」しましたが、「meinheißeheißetom」はまだ意味が推測できますが、まだ正しくありません。</font><font style="vertical-align: inherit;">3番目のフレーズは既に正しいです。</font><font style="vertical-align: inherit;">4番目では、最初の部分は「wo ist」ですが、最寄りのショップが何らかの理由でパリに置き換えられました。</font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">トレーニングの30分の</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
「今日の天気はいいです」 - 「ダスイストイストAUS」</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
「私の名前はトムです」 - 「」トム「イスト焼きそば名」</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
「あなたは何歳」 - 「ウィーALTシンドSIE」</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
「ここで、最も近いですショップ”-“ wo ist der” </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ご覧のとおり、デザインは少し変わっていますが、2番目のフレーズは正しくなっています。</font><font style="vertical-align: inherit;">3番目のフレーズは正しいですが、1番目と4番目のフレーズはまだ「学習」されていません。</font><font style="vertical-align: inherit;">これで、私は</font></font><s><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">エネルギーを節約しています</font></font></s><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> プロセスを終了しました。</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">結論</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ご覧のとおり、これは原則として機能します。</font><font style="vertical-align: inherit;">このような速度で新しい言語を覚えたいと思います:)もちろん、結果はこれまでのところ完璧ではありませんが、19万行のフルセットでのトレーニングには1時間以上かかります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
自分で実験したい人のために、ソースコードはネタバレの下にあります。</font><font style="vertical-align: inherit;">プログラムは、理論的には英語とドイツ語だけでなく、任意の言語のペアを使用できます（ファイルはUTF-8エンコードにする必要があります）。</font><font style="vertical-align: inherit;">翻訳品質の問題も未解決のままです。テストする必要があります。</font></font><br>
<br>
<div class="spoiler"><b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">keras_translate.py</font></font></b><div class="spoiler_text"><pre><code class="python hljs"><span class="hljs-keyword">import</span> os
<span class="hljs-comment"># os.environ["CUDA_VISIBLE_DEVICES"] = "-1"  # Force CPU</span>
os.environ[<span class="hljs-string">'TF_CPP_MIN_LOG_LEVEL'</span>] = <span class="hljs-string">'3'</span>  <span class="hljs-comment"># 0 = all messages are logged, 3 - INFO, WARNING, and ERROR messages are not printed</span><font></font>
<font></font>
<span class="hljs-keyword">import</span> string 
<span class="hljs-keyword">import</span> re
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> Sequential 
<span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> Dense, LSTM, Embedding, RepeatVector
<span class="hljs-keyword">from</span> keras.preprocessing.text <span class="hljs-keyword">import</span> Tokenizer
<span class="hljs-keyword">from</span> keras.callbacks <span class="hljs-keyword">import</span> ModelCheckpoint 
<span class="hljs-keyword">from</span> keras.preprocessing.sequence <span class="hljs-keyword">import</span> pad_sequences
<span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> load_model 
<span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> optimizers 
<span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<font></font>
<font></font>
pd.set_option(<span class="hljs-string">'display.max_colwidth'</span>, <span class="hljs-number">200</span>)<font></font>
<font></font>
<span class="hljs-comment"># Read raw text file</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">read_text</span>(<span class="hljs-params">filename</span>):</span>
    <span class="hljs-keyword">with</span> open(filename, mode=<span class="hljs-string">'rt'</span>, encoding=<span class="hljs-string">'utf-8'</span>) <span class="hljs-keyword">as</span> file:<font></font>
        text = file.read()<font></font>
        sents = text.strip().split(<span class="hljs-string">'\n'</span>)
        <span class="hljs-keyword">return</span> [i.split(<span class="hljs-string">'\t'</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> sents]<font></font>
<font></font>
data = read_text(<span class="hljs-string">"deutch.txt"</span>)<font></font>
deu_eng = np.array(data)<font></font>
<font></font>
deu_eng = deu_eng[:<span class="hljs-number">30000</span>,:]<font></font>
print(<span class="hljs-string">"Dictionary size:"</span>, deu_eng.shape)<font></font>
<font></font>
<span class="hljs-comment"># Remove punctuation </span>
deu_eng[:,<span class="hljs-number">0</span>] = [s.translate(str.maketrans(<span class="hljs-string">''</span>, <span class="hljs-string">''</span>, string.punctuation)) <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> deu_eng[:,<span class="hljs-number">0</span>]] <font></font>
deu_eng[:,<span class="hljs-number">1</span>] = [s.translate(str.maketrans(<span class="hljs-string">''</span>, <span class="hljs-string">''</span>, string.punctuation)) <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> deu_eng[:,<span class="hljs-number">1</span>]] <font></font>
<font></font>
<span class="hljs-comment"># Convert text to lowercase </span>
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(deu_eng)): <font></font>
    deu_eng[i,<span class="hljs-number">0</span>] = deu_eng[i,<span class="hljs-number">0</span>].lower() <font></font>
    deu_eng[i,<span class="hljs-number">1</span>] = deu_eng[i,<span class="hljs-number">1</span>].lower()<font></font>
    <font></font>
<span class="hljs-comment"># Prepare English tokenizer</span><font></font>
eng_tokenizer = Tokenizer()<font></font>
eng_tokenizer.fit_on_texts(deu_eng[:, <span class="hljs-number">0</span>])<font></font>
eng_vocab_size = len(eng_tokenizer.word_index) + <span class="hljs-number">1</span> 
eng_length = <span class="hljs-number">8</span> <font></font>
<font></font>
<span class="hljs-comment"># Prepare Deutch tokenizer </span><font></font>
deu_tokenizer = Tokenizer()<font></font>
deu_tokenizer.fit_on_texts(deu_eng[:, <span class="hljs-number">1</span>])<font></font>
deu_vocab_size = len(deu_tokenizer.word_index) + <span class="hljs-number">1</span> 
deu_length = <span class="hljs-number">8</span> <font></font>
<font></font>
<span class="hljs-comment"># Encode and pad sequences </span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">encode_sequences</span>(<span class="hljs-params">tokenizer, length, lines</span>):</span>          
    <span class="hljs-comment"># integer encode sequences</span><font></font>
    seq = tokenizer.texts_to_sequences(lines)<font></font>
    <span class="hljs-comment"># pad sequences with 0 values</span>
    seq = pad_sequences(seq, maxlen=length, padding=<span class="hljs-string">'post'</span>)
    <span class="hljs-keyword">return</span> seq<font></font>
     <font></font>
<span class="hljs-comment"># Split data into train and test set </span>
train, test = train_test_split(deu_eng, test_size=<span class="hljs-number">0.2</span>, random_state=<span class="hljs-number">12</span>)<font></font>
<font></font>
<span class="hljs-comment"># Prepare training data </span>
trainX = encode_sequences(eng_tokenizer, eng_length, train[:, <span class="hljs-number">0</span>])<font></font>
trainY = encode_sequences(deu_tokenizer, deu_length, train[:, <span class="hljs-number">1</span>])<font></font>
<font></font>
<span class="hljs-comment"># Prepare validation data </span>
testX = encode_sequences(eng_tokenizer, eng_length, test[:, <span class="hljs-number">0</span>])<font></font>
testY = encode_sequences(deu_tokenizer, deu_length, test[:, <span class="hljs-number">1</span>])<font></font>
<font></font>
<span class="hljs-comment"># Build NMT model </span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">make_model</span>(<span class="hljs-params">in_vocab, out_vocab, in_timesteps, out_timesteps, n</span>):</span><font></font>
    model = Sequential()<font></font>
    model.add(Embedding(in_vocab, n, input_length=in_timesteps, mask_zero=<span class="hljs-literal">True</span>))<font></font>
    model.add(LSTM(n))<font></font>
    model.add(Dropout(<span class="hljs-number">0.3</span>))<font></font>
    model.add(RepeatVector(out_timesteps))<font></font>
    model.add(LSTM(n, return_sequences=<span class="hljs-literal">True</span>))<font></font>
    model.add(Dropout(<span class="hljs-number">0.3</span>))<font></font>
    model.add(Dense(out_vocab, activation=<span class="hljs-string">'softmax'</span>))<font></font>
    model.compile(optimizer=optimizers.RMSprop(lr=<span class="hljs-number">0.001</span>), loss=<span class="hljs-string">'sparse_categorical_crossentropy'</span>)
    <span class="hljs-keyword">return</span> model<font></font>
<font></font>
print(<span class="hljs-string">"deu_vocab_size:"</span>, deu_vocab_size, deu_length)<font></font>
print(<span class="hljs-string">"eng_vocab_size:"</span>, eng_vocab_size, eng_length)<font></font>
<font></font>
<span class="hljs-comment"># Model compilation (with 512 hidden units)</span>
model = make_model(eng_vocab_size, deu_vocab_size, eng_length, deu_length, <span class="hljs-number">512</span>)<font></font>
<font></font>
<span class="hljs-comment"># Train model</span>
num_epochs = <span class="hljs-number">250</span>
history = model.fit(trainX, trainY.reshape(trainY.shape[<span class="hljs-number">0</span>], trainY.shape[<span class="hljs-number">1</span>], <span class="hljs-number">1</span>), epochs=num_epochs, batch_size=<span class="hljs-number">512</span>, validation_split=<span class="hljs-number">0.2</span>, callbacks=<span class="hljs-literal">None</span>, verbose=<span class="hljs-number">1</span>)
<span class="hljs-comment"># plt.plot(history.history['loss'])</span>
<span class="hljs-comment"># plt.plot(history.history['val_loss'])</span>
<span class="hljs-comment"># plt.legend(['train','validation'])</span>
<span class="hljs-comment"># plt.show()</span>
model.save(<span class="hljs-string">'en-de-model.h5'</span>)<font></font>
<font></font>
<span class="hljs-comment"># Load model</span>
model = load_model(<span class="hljs-string">'en-de-model.h5'</span>)<font></font>
<font></font>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_word</span>(<span class="hljs-params">n, tokenizer</span>):</span>
    <span class="hljs-keyword">if</span> n == <span class="hljs-number">0</span>:
        <span class="hljs-keyword">return</span> <span class="hljs-string">""</span>
    <span class="hljs-keyword">for</span> word, index <span class="hljs-keyword">in</span> tokenizer.word_index.items():
        <span class="hljs-keyword">if</span> index == n:
            <span class="hljs-keyword">return</span> word
    <span class="hljs-keyword">return</span> <span class="hljs-string">""</span><font></font>
<font></font>
<font></font>
phrs_enc = encode_sequences(eng_tokenizer, eng_length, [<span class="hljs-string">"the weather is nice today"</span>, <span class="hljs-string">"my name is tom"</span>, <span class="hljs-string">"how old are you"</span>, <span class="hljs-string">"where is the nearest shop"</span>])<font></font>
print(<span class="hljs-string">"phrs_enc:"</span>, phrs_enc.shape)<font></font>
<font></font>
preds = model.predict_classes(phrs_enc)<font></font>
print(<span class="hljs-string">"Preds:"</span>, preds.shape)<font></font>
print(preds[<span class="hljs-number">0</span>])<font></font>
print(get_word(preds[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>], deu_tokenizer), get_word(preds[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>], deu_tokenizer), get_word(preds[<span class="hljs-number">0</span>][<span class="hljs-number">2</span>], deu_tokenizer), get_word(preds[<span class="hljs-number">0</span>][<span class="hljs-number">3</span>], deu_tokenizer))<font></font>
print(preds[<span class="hljs-number">1</span>])<font></font>
print(get_word(preds[<span class="hljs-number">1</span>][<span class="hljs-number">0</span>], deu_tokenizer), get_word(preds[<span class="hljs-number">1</span>][<span class="hljs-number">1</span>], deu_tokenizer), get_word(preds[<span class="hljs-number">1</span>][<span class="hljs-number">2</span>], deu_tokenizer), get_word(preds[<span class="hljs-number">1</span>][<span class="hljs-number">3</span>], deu_tokenizer))<font></font>
print(preds[<span class="hljs-number">2</span>])<font></font>
print(get_word(preds[<span class="hljs-number">2</span>][<span class="hljs-number">0</span>], deu_tokenizer), get_word(preds[<span class="hljs-number">2</span>][<span class="hljs-number">1</span>], deu_tokenizer), get_word(preds[<span class="hljs-number">2</span>][<span class="hljs-number">2</span>], deu_tokenizer), get_word(preds[<span class="hljs-number">2</span>][<span class="hljs-number">3</span>], deu_tokenizer))<font></font>
print(preds[<span class="hljs-number">3</span>])<font></font>
print(get_word(preds[<span class="hljs-number">3</span>][<span class="hljs-number">0</span>], deu_tokenizer), get_word(preds[<span class="hljs-number">3</span>][<span class="hljs-number">1</span>], deu_tokenizer), get_word(preds[<span class="hljs-number">3</span>][<span class="hljs-number">2</span>], deu_tokenizer), get_word(preds[<span class="hljs-number">3</span>][<span class="hljs-number">3</span>], deu_tokenizer))<font></font>
print()<font></font>
</code></pre><br>
</div></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
辞書自体が大きすぎて記事に添付できません。コメントにリンクしてください。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
いつものように、すべての成功した実験。</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../ja470688/index.html">VMworld 2019について知られていること</a></li>
<li><a href="../ja470692/index.html">新しいロスバンクのウェブサイトをどのように作成したか、そしてそれから何が生まれたのか</a></li>
<li><a href="../ja470694/index.html">メールマーケティングプラットフォームの選択：ロシア企業に注意を払うべきこと</a></li>
<li><a href="../ja470696/index.html">カルディが音声認識に適しているのはなぜですか？（2019年12月25日更新）</a></li>
<li><a href="../ja470700/index.html">デスクトップ。金属。サイレント あなたのです？</a></li>
<li><a href="../ja470714/index.html">Digital Breakthrough Finalへの行き方</a></li>
<li><a href="../ja470718/index.html">人間の言葉による「代数的影響」</a></li>
<li><a href="../ja470726/index.html">日常生活で溺れない方法、またはストレステスト中にAWRダンプを比較した経験</a></li>
<li><a href="../ja470730/index.html">Azure PowerShell：「基本的に無害」</a></li>
<li><a href="../ja470732/index.html">秋の編集：スマート傘についてどう思いますか？</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>