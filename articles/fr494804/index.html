<!doctype html>
<html class="no-js" lang="fr">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🧗 🔟 🐯 "Désolé, j'ai reconnu ..." ou reconnaissez les framboises et les contrôleurs à l'aide de l'API de détection d'objets Tensorflow 👩‍👩‍👧‍👧 🤙 🏂🏾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="À la fin de l'année dernière, j'ai écrit un article sur la façon dont j'ai été intrigué par la capacité de reconnaître des objets dans des images à l'...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>"Désolé, j'ai reconnu ..." ou reconnaissez les framboises et les contrôleurs à l'aide de l'API de détection d'objets Tensorflow</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/494804/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">À la fin de l'année dernière, j'ai écrit </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">un article</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> sur la façon dont j'ai été intrigué par la capacité de reconnaître des objets dans des images à l'aide de réseaux de neurones. Dans cet article, en utilisant PyTorch, nous avons classé les framboises ou un contrôleur de type arduino en vidéo. Et malgré le fait que j'aimais PyTorch, je me suis tourné vers lui parce que je ne pouvais pas traiter TensorFlow tout de suite. Mais j'ai promis de revenir sur la question de la reconnaissance des objets dans la vidéo. Il semble que le moment soit venu de tenir la promesse. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dans cet article, nous allons essayer sur notre machine locale de recycler le modèle fini dans Tensorflow 1.13 et l'API de détection d'objets sur notre propre ensemble d'images, puis de l'utiliser pour reconnaître les baies et les contrôleurs dans le flux vidéo d'une caméra Web à l'aide d'OpenCV.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Vous voulez améliorer vos compétences de reconnaissance des baies d'ici l'été? </font><font style="vertical-align: inherit;">Alors vous êtes les bienvenus sous chat.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/fu/do/rd/fudordve5xz-8gwdnbvlnkkjusm.png"><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Contenu: </font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Partie I: introduction </font></font></a><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Partie II: former le modèle dans TenosrFlow </font></font></a><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Partie III: appliquer le modèle dans OpenCV </font></font></a><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Partie IV: conclusion</font></font></a><br>
<a name="I"></a><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Partie I: Introduction</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ceux qui ont lu l'article précédent sur PyTorch savent déjà que je suis un amateur de questions de réseaux de neurones. Par conséquent, ne percevez pas cet article comme la vérité ultime. Mais quoi qu'il en soit, j'espère pouvoir aider quelqu'un à gérer les bases de la reconnaissance vidéo en utilisant l'API de détection d'objets Tensorflow. </font></font><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cette fois, je n'ai pas essayé de faire un tutoriel, donc l'article sera plus court que d'habitude.</font></font></i><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Pour commencer, le </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">didacticiel officiel</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> sur l'utilisation de l'API de détection d'objets sur une machine locale, pour le moins, n'est pas exhaustif. En tant que novice, j'étais complètement insuffisant et j'ai dû me concentrer sur les articles de blog.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pour être honnête, je voudrais essayer TensorFlow 2.0, mais dans la plupart des publications, au moment d'écrire ces lignes, les problèmes de migration n'étaient pas complètement résolus. </font><font style="vertical-align: inherit;">J'ai donc finalement opté pour TF 1.13.2.</font></font><br>
<a name="II"></a><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Partie II: enseigner un modèle à TensorFlow </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
J'ai tiré des instructions pour enseigner le modèle à </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">partir de cet article</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , ou plutôt de sa première moitié, jusqu'à ce que JavaScript soit appliqué </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(si vous ne parlez pas anglais, vous pouvez voir un article sur le même sujet </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">dans Habré</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> )</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Certes, dans mon cas, il existe plusieurs différences:</font></font><br>
<br>
<ol>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">J'ai utilisé Linux car Anaconda pour Linux a déjà construit protobuf et pycocoapi, donc je n'ai pas eu à les construire moi-même.</font></font></li>
<li>   TensorFlow 1.13.2,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow">Object Detection API 1.13</a> ,       TensorFlow 1.13.2.   master        TF 1.15,         1.13.</li>
<li>      numpy — 1.17.5,  1.18    .</li>
<li>  faster_rcnn_inception_v2_coco    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow">ssd_mobilenet_v2_coco</a>,    ,     .</li>
</ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 Au cas où, je dirais que je n'ai pas utilisé d'accélérateur graphique. La formation a été effectuée uniquement sur les capacités du processeur. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Un ensemble d'images, un fichier de configuration, un graphique enregistré, ainsi qu'un script pour reconnaître les images à l'aide d'OpenCV, comme toujours, peuvent être téléchargés depuis </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GitHub</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Une longue formation de modèle de 23 heures s'est écoulée, tout le thé de la maison a déjà été bu, «Quoi? Où? Quand?" inspecté et maintenant ma patience a finalement pris fin. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nous arrêtons la formation et enregistrons le modèle. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Installez OpenCV dans le même environnement que "Anaconda" avec la commande suivante:</font></font><br>
<br>
<pre><code class="plaintext hljs">conda install -c conda-forge opencv</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
J'ai finalement installé la version 4.2. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
De plus, les instructions </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">de cet article</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ne seront plus nécessaires. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Après avoir enregistré le modèle, j'ai fait une erreur qui n'était pas évidente pour moi, à savoir, j'ai immédiatement essayé de remplacer le fichier graph.pbtxt utilisé précédemment dans le dossier training / dans la fonction:</font></font><br>
<br>
<pre><code class="python hljs">cv2.dnn.readNetFromTensorflow()</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Malheureusement, cela ne fonctionne pas de cette façon et nous devrons effectuer une autre manipulation pour obtenir graph.pbtxt pour OpenCV. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Très probablement, le fait que je conseille maintenant n'est pas un très bon moyen, mais pour moi, cela fonctionne. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Téléchargez </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">tf_text_graph_ssd.py</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , et aussi </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">tf_text_graph_common.py</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> mettez-les dans le dossier où se trouve notre graphique enregistré (j'ai ce dossier inference_graph). </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Accédez ensuite à la console dans ce dossier et exécutez-y une commande d'environ le contenu suivant:</font></font><br>
<br>
<pre><code class="plaintext hljs">python tf_text_graph_ssd.py --input frozen_inference_graph.pb --config pipeline.config --output graph.pbtxt</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Et c'est tout ce qu'il reste pour télécharger notre modèle sur OpenCV.</font></font><br>
<br>
<a name="III"></a><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Partie III: appliquer le modèle dans OpenCV </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Comme dans l'article sur PyTorch concernant le travail avec OpenCV, j'ai pris comme base le code du programme de </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">cette publication</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
J'ai fait de petits changements pour le simplifier un peu plus, mais comme je ne comprends pas bien le code, je ne le commenterai pas. </font><font style="vertical-align: inherit;">Fonctionne et sympa. </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Il est clair que le code aurait pu être meilleur, mais je n'ai pas encore le temps de m'asseoir pour les tutoriels OpenCV</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<div class="spoiler" role="button" tabindex="0">
                        <b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Code OpenCV</font></font></b>
                        <div class="spoiler_text"><pre><code class="python hljs">
<span class="hljs-comment"># USAGE</span>
<span class="hljs-comment"># based on this code https://proglib.io/p/real-time-object-detection/</span>
<span class="hljs-comment"># import the necessary packages</span>
<span class="hljs-keyword">from</span> imutils.video <span class="hljs-keyword">import</span> VideoStream
<span class="hljs-keyword">from</span> imutils.video <span class="hljs-keyword">import</span> FPS
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> imutils
<span class="hljs-keyword">import</span> time
<span class="hljs-keyword">import</span> cv2<font></font>
<font></font>
prototxt=<span class="hljs-string">"graph.pbtxt"</span>
model=<span class="hljs-string">"frozen_inference_graph.pb"</span>
min_confidence = <span class="hljs-number">0.5</span><font></font>
<font></font>
<span class="hljs-comment"># initialize the list of class labels MobileNet SSD was trained to</span>
<span class="hljs-comment"># detect, then generate a set of bounding box colors for each class</span>
CLASSES = [<span class="hljs-string">"background"</span>, <span class="hljs-string">"duino"</span>,<span class="hljs-string">"raspb"</span>]<font></font>
COLORS = [(<span class="hljs-number">40</span>,<span class="hljs-number">50</span>,<span class="hljs-number">60</span>),((<span class="hljs-number">140</span>,<span class="hljs-number">55</span>,<span class="hljs-number">130</span>)),(<span class="hljs-number">240</span>,<span class="hljs-number">150</span>,<span class="hljs-number">25</span>)]<font></font>
<font></font>
<span class="hljs-comment"># load our serialized model from disk</span>
print(<span class="hljs-string">"[INFO] loading model..."</span>)<font></font>
<font></font>
net =cv2.dnn.readNetFromTensorflow(model,prototxt)<font></font>
<font></font>
<span class="hljs-comment"># initialize the video stream, allow the cammera sensor to warmup,</span>
<span class="hljs-comment"># and initialize the FPS counter</span>
print(<span class="hljs-string">"[INFO] starting video stream..."</span>)<font></font>
vs = VideoStream(src=<span class="hljs-number">0</span>).start()<font></font>
time.sleep(<span class="hljs-number">0.5</span>)<font></font>
fps = FPS().start()<font></font>
<font></font>
<span class="hljs-comment"># loop over the frames from the video stream</span>
<span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:
	<span class="hljs-comment"># grab the frame from the threaded video stream and resize it</span>
	<span class="hljs-comment"># to have a maximum width of 400 pixels</span><font></font>
	frame = vs.read()<font></font>
	frame = imutils.resize(frame, width=<span class="hljs-number">300</span>)<font></font>
<font></font>
	<span class="hljs-comment"># grab the frame dimensions and convert it to a blob</span>
	(h, w) = frame.shape[:<span class="hljs-number">2</span>]<font></font>
	blob = cv2.dnn.blobFromImage(frame, size=(<span class="hljs-number">300</span>, <span class="hljs-number">300</span>), swapRB=<span class="hljs-literal">True</span>)<font></font>
<font></font>
	<span class="hljs-comment"># pass the blob through the network and obtain the detections and</span>
	<span class="hljs-comment"># predictions</span><font></font>
	net.setInput(blob)<font></font>
	detections = net.forward()<font></font>
<font></font>
	<span class="hljs-comment"># loop over the detections</span>
	<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> np.arange(<span class="hljs-number">0</span>, detections.shape[<span class="hljs-number">2</span>]):
		<span class="hljs-comment"># extract the confidence (i.e., probability) associated with</span>
		<span class="hljs-comment"># the prediction</span>
		<span class="hljs-keyword">print</span> (detections)<font></font>
		confidence = detections[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, i, <span class="hljs-number">2</span>]<font></font>
<font></font>
		<span class="hljs-keyword">if</span> confidence &gt; min_confidence:
			<span class="hljs-comment"># extract the index of the class label from the</span>
			<span class="hljs-comment"># `detections`, then compute the (x, y)-coordinates of</span>
			<span class="hljs-comment"># the bounding box for the object</span>
			idx = int(detections[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, i, <span class="hljs-number">1</span>])<font></font>
			box = detections[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, i, <span class="hljs-number">3</span>:<span class="hljs-number">7</span>] * np.array([w, h, w, h])<font></font>
			(startX, startY, endX, endY) = box.astype(<span class="hljs-string">"int"</span>)<font></font>
<font></font>
			<span class="hljs-comment"># draw the prediction on the frame</span>
			label = <span class="hljs-string">"{}: {:.2f}%"</span>.format(CLASSES[idx],<font></font>
				confidence * <span class="hljs-number">100</span>)<font></font>
			cv2.rectangle(frame, (startX, startY), (endX, endY),<font></font>
				COLORS[idx], <span class="hljs-number">2</span>)<font></font>
			y = startY - <span class="hljs-number">15</span> <span class="hljs-keyword">if</span> startY - <span class="hljs-number">15</span> &gt; <span class="hljs-number">15</span> <span class="hljs-keyword">else</span> startY + <span class="hljs-number">15</span>
			cv2.putText(frame, label, (startX, y+<span class="hljs-number">3</span>),<font></font>
				cv2.FONT_HERSHEY_SIMPLEX, <span class="hljs-number">0.5</span>, COLORS[idx], <span class="hljs-number">1</span>)<font></font>
<font></font>
	<span class="hljs-comment"># show the output frame</span>
	cv2.imshow(<span class="hljs-string">"Frame"</span>, frame)<font></font>
	key = cv2.waitKey(<span class="hljs-number">1</span>) &amp; <span class="hljs-number">0xFF</span><font></font>
<font></font>
	<span class="hljs-comment"># if the `q` key was pressed, break from the loop</span>
	<span class="hljs-keyword">if</span> key == ord(<span class="hljs-string">"q"</span>):
		<span class="hljs-keyword">break</span><font></font>
<font></font>
	<span class="hljs-comment"># update the FPS counter</span><font></font>
	fps.update()<font></font>
<font></font>
<span class="hljs-comment"># stop the timer and display FPS information</span><font></font>
fps.stop()<font></font>
print(<span class="hljs-string">"[INFO] elapsed time: {:.2f}"</span>.format(fps.elapsed()))<font></font>
print(<span class="hljs-string">"[INFO] approx. FPS: {:.2f}"</span>.format(fps.fps()))<font></font>
<font></font>
<span class="hljs-comment"># do a bit of cleanup</span><font></font>
cv2.destroyAllWindows()<font></font>
vs.stop()<font></font>
</code></pre><br>
</div>
                    </div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Donc, tout est prêt. Nous lançons le modèle, pointons l'objectif vers mon ancien CraftDuino et apprécions le résultat: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/bw/hj/yd/bwhjyd9pddoeop9yaz7fxbqozzo.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
à première vue, ce n'est pas mal du tout, mais ce n'est qu'à première vue. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
On dirait qu'en 23 heures, le modèle a été recyclé, donc il donne de graves erreurs lors de la définition des objets. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Voici une démonstration visuelle: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/1w/3y/gf/1w3ygfo-ufytpuyct1kaarpsgls.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Comme vous pouvez le voir, non seulement un couteau, mais même juste un fond noir, ce modèle le définit comme un contrôleur de type Arduino. C'est peut-être parce que dans les données d'entraînement, il y avait des images sombres avec l'Arduino et ses analogues, sur lesquelles le modèle a réussi à se cogner en 23 heures. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En conséquence, j'ai dû charger mon ordinateur pendant encore 8 heures et former un nouveau modèle. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Les choses vont beaucoup mieux avec elle. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Voici un exemple avec CraftDuino:</font></font><br>
<br>
<img src="https://habrastorage.org/webt/_c/8m/62/_c8m62y2q6as-l8sun5ah5ivppk.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Les framboises vivantes ne sont pas à portée de main. </font><font style="vertical-align: inherit;">J'ai dû imprimer des photos. </font><font style="vertical-align: inherit;">Depuis l'écran du téléphone ou du moniteur, vous pouvez également reconnaître, mais à partir du papier, c'était plus pratique. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/63/_k/ou/63_koujmchte7jor0ulqzxcvgcs.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Vérifions comment le modèle reconnaît l'Arduino nano, qui en temps voulu</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" class="user_link"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Drzugrik</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">pour moi, j'ai soudé dans mon méga appareil avec des capteurs: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ub/33/61/ub3361ozwkiwvl2sosx6yldvsou.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
comme vous pouvez le voir, il reconnaît assez bien, mais avec un très mauvais angle et sous un éclairage chaud, il peut reconnaître certains fragments comme les framboises. Mais en fait, une monture avec une erreur était difficile à saisir dans l'objectif. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Voyons maintenant comment elle classe les objets sur lesquels elle n'a pas été formée. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Encore une fois, un exemple avec un couteau et un fond noir: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/io/ja/6a/ioja6aexferclondu4228nsr06y.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
cette fois, tout fonctionne comme il se doit. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nous proposerons notre modèle pour reconnaître le petit contrôleur Canny 3, dont j'ai parlé dans un </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">article précédent</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br>
<img src="https://habrastorage.org/webt/xp/ay/14/xpay14o7clhp1y1twu4vyltiay4.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Étant donné que notre modèle ne sait rien, sauf les framboises et les contrôleurs de type arduino, nous pouvons dire que le modèle a reconnu le contrôleur Canny avec succès.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Certes, comme dans le cas de l'Arduino nano, cela dépend beaucoup de l'angle et de l'éclairage. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Avec la lumière chaude d'une lampe à incandescence et avec un angle infructueux, le contrôleur peut non seulement ne pas être reconnu, mais même être défini comme framboise. </font><font style="vertical-align: inherit;">Certes, comme dans le cas précédent, ces angles devaient encore essayer de s'accrocher à l'objectif. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/01/ut/h_/01uth_-raiwnzasg7ypn-aoxezs.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Eh bien, le dernier cas est une sorte de révérence pour l'article sur la </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">classification des images dans PyTorch</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Comme la dernière fois, l'ordinateur monocarte Raspberry Pi 2 et son logo sont compatibles dans un seul cadre. </font><font style="vertical-align: inherit;">Contrairement à l'article précédent, dans lequel nous avons résolu le problème de classification et choisi un objet le plus probable pour l'image, dans ce cas, le logo et la framboise elle-même sont reconnus.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/vx/fv/us/vxfvusfgitn6vk1pe6o4rvoen9i.png"><br>
<br>
<a name="IV"></a><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Partie IV: Conclusion </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En conclusion, je tiens à dire que malgré l'inexpérience de ce petit exemple de travail avec l'API de détection d'objets Tensorflow, cela a pris deux jours de congé et une partie de lundi, je ne regrette rien. Quand au moins un peu de compréhension sur la façon de l'utiliser, tout devient incroyablement curieux. Dans le processus d'apprentissage, vous commencez à considérer le modèle comme un modèle vivant, à suivre ses succès et ses échecs. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Par conséquent, je recommande à tous ceux qui ne connaissent pas ce jour d'essayer de reconnaître quelque chose qui leur est propre.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
De plus, comme il a augmenté au cours du processus, vous n'avez même pas besoin d'acheter une vraie webcam. Le fait est que lors de la préparation de l'article, j'ai réussi à casser ma webcam (cassé le mécanisme de mise au point) et pensais déjà que je devrais tout abandonner. Mais il s'est avéré qu'avec l'aide de Droidcam, vous pouvez utiliser un smartphone au lieu d'une webcam (ne comptez pas pour la publicité). De plus, la qualité de prise de vue s'est avérée bien meilleure que celle d'un appareil photo cassé, ce qui a grandement influencé la qualité de reconnaissance des objets dans l'image. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Soit dit en passant, puisque Anaconda a une construction normale de </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">pycocotools</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Je n'ai trouvé que pour Linux, et j'étais trop paresseux pour basculer entre les systèmes d'exploitation, j'ai préparé cet article entier uniquement en utilisant un logiciel open source. </font><font style="vertical-align: inherit;">Il y avait des analogues de Word et de Photoshop et même un pilote pour l'imprimante. </font><font style="vertical-align: inherit;">La première fois de ma vie, cela s'est produit. </font><font style="vertical-align: inherit;">Il s'est avéré que les versions modernes du système d'exploitation Linux et des programmes d'application peuvent être très pratiques, même pour une personne utilisant Microsoft OS depuis plus de 25 ans. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
PS Si quelqu'un sait comment exécuter correctement l'API de détection d'objets </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
pour Tensorflow version 2 et supérieure, veuillez vous désabonner en PM ou dans un commentaire. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Passez une bonne journée et bonne santé!</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr488468/index.html">Présentation de FastAPI</a></li>
<li><a href="../fr488470/index.html">Kim Dotcom: Caught, la personne la plus recherchée en ligne. Partie 4</a></li>
<li><a href="../fr488472/index.html">Week-end de lecture: 10 documents sur les gadgets audio - des autoradios soviétiques aux prises antibruit</a></li>
<li><a href="../fr488474/index.html">A propos de la couleur, du son et de «l'exploration des foules» en tant que beauté</a></li>
<li><a href="../fr494800/index.html">Ingénierie inverse du protocole émetteur-récepteur infrarouge USB chinois</a></li>
<li><a href="../fr494806/index.html">Cyber ​​vise 2019 comme tendances 2020 - les pirates ont changé d'orientation</a></li>
<li><a href="../fr494808/index.html">Analyste de produit: que fait-il, combien gagne-t-il, quels avantages apporte l'entreprise</a></li>
<li><a href="../fr494810/index.html">Introduction à la 3D: principes de base de Three.js</a></li>
<li><a href="../fr494814/index.html">Slurm est-il utile?</a></li>
<li><a href="../fr494818/index.html">Comment choisir un terminal de trading pour travailler sur la bourse</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>