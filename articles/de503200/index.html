<!doctype html>
<html class="no-js" lang="de">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üßëüèº ‚ú≥Ô∏è üôãüèΩ YOLOv4 - das genaueste neuronale Echtzeitnetzwerk im Microsoft COCO-Dataset üåã üôáüèª üå≥</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Darknet YOLOv4 ist schneller / genauer als Google TensorFlow EfficientDet und FaceBook Pytorch / Detectron RetinaNet / MaskRCNN. 
 
 Der gleiche Artik...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>YOLOv4 - das genaueste neuronale Echtzeitnetzwerk im Microsoft COCO-Dataset</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/503200/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Darknet YOLOv4 ist schneller / genauer als Google TensorFlow EfficientDet und FaceBook Pytorch / Detectron RetinaNet / MaskRCNN. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Der gleiche Artikel auf Medium</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Medium </font></font></a><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Code</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/AlexeyAB/darknet </font></font></a><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Artikel</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">arxiv.org/abs/2004.10934</font></font></a><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/3h/nc/sr/3hncsroz9wt8u3ycqskubgu1xk8.gif"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wir werden einige Nuancen des Vergleichs und der Verwendung neuronaler Netze zur Erkennung von Objekten zeigen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Unser Ziel war es, einen Objekterkennungsalgorithmus f√ºr die Verwendung in realen Produkten zu entwickeln und nicht nur die Wissenschaft voranzutreiben. </font><font style="vertical-align: inherit;">Die Genauigkeit des neuronalen YOLOv4-Netzwerks (608 x 608) betr√§gt 43,5% AP / 65,7% AP50 Microsoft-COCO-Testdev. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">62 FPS</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - YOLOv4 (608 x 608 Batch = 1) unter Tesla V100 - unter Verwendung von Darknet-Framework </font></font><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">400 FPS</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - YOLOv4 (416 x 416 Batch = 4) unter RTX 2080 Ti - unter Verwendung von TensorRT + tkDNN </font></font><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">32 FPS</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - YOLOv4 (416 x 416 Batch = 1) unter Jetson AGX Xavier - mit TensorRT + tkDNN</font></font><br>
<br>
<img src="https://habrastorage.org/webt/p_/ep/cl/p_epcl_aaw_trgeltekatagtqkg.jpeg"> <br>
<a name="habracut"></a><br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/1_SiUOYUoOI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zun√§chst einige n√ºtzliche Links.</font></font></h3><br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Eine ausf√ºhrliche Beschreibung der in YOLOv4 verwendeten Funktionen finden Sie in diesem Artikel: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">medium.com/@jonathan_hui/yolov4-c9901eaa8e61</font></font></a></li>
<li>  YOLOv4: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://lutzroeder.github.io/netron/%3Furl%3D" rel="nofollow">lutzroeder.github.io/netron/?url=https%3A%2F%2Fraw.githubusercontent.com%2FAlexeyAB%2Fdarknet%2Fmaster%2Fcfg%2Fyolov4.cfg</a></li>
<li>     YOLOv4  GPU   Google-cloud  Jupyter Notebook ‚Äì      ,   - ¬´Open in Playground¬ª,         [ ] ‚Äì    ,  ,  ,    5    : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow">colab.research.google.com/drive/12QusaaRj_lUwCGDvQNfICpa7kA7_a2dE</a>      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow">www.youtube.com/watch?v=mKAEGSxwOAY</a></li>
<li>  Darknet   : <br>
 ‚Äî <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow">  Darknet YOLOv4</a><br>
 ‚Äî <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow">  Darknet YOLOv4</a><br>
 </li>
</ul><br>
<h3>   </h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Unser neuronales YOLOv4-Netzwerk und unser eigenes Darknet DL-Framework (C / C ++ / CUDA) bieten eine bessere FPS-Geschwindigkeit und AP50: 95- und AP50-Genauigkeit f√ºr Microsoft COCO-Datens√§tze als DL-Frameworks und neuronale Netzwerke: Google TensorFlow EfficientDet, FaceBook Detectron RetinaNet / MaskRCNN, PyTorch Yolov3-ASFF und viele andere ... YOLOv4 erreicht beim Microsoft COCO-Test eine Genauigkeit von 43,5% AP / 65,7% AP50 bei einer Geschwindigkeit von 62 FPS TitanV oder 34 FPS RTX 2070. Im Gegensatz zu anderen modernen Detektoren kann YOLOv4 jeden trainieren Wer hat die nVidia Gaming-Grafikkarte mit 8-16 GB VRAM. Jetzt k√∂nnen nicht nur gro√üe Unternehmen ein neuronales Netzwerk auf Hunderten von GPU / TPUs trainieren, um gro√üe Mini-Batch-Gr√∂√üen zu verwenden, um eine h√∂here Genauigkeit zu erzielen. Daher geben wir die Kontrolle √ºber k√ºnstliche Intelligenz an normale Benutzer zur√ºck, da f√ºr YOLOv4 kein gro√ües Mini-Lot erforderlich ist.kann auf eine Gr√∂√üe von 2 - 8 begrenzt werden.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
YOLOV4 ist optimal f√ºr die Verwendung in Echtzeit, weil Das Netzwerk liegt </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">auf der Pareto-Optimalit√§tskurve</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> im Diagramm AP (Genauigkeit) / FPS (Geschwindigkeit). </font></font><br>
<br>
<img src="https://habrastorage.org/webt/2k/77/as/2k77aszzprngk0qmtistcehkz8c.png"><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Diagramme der Genauigkeit (AP) und Geschwindigkeit (FPS) vieler neuronaler Netze zur Erkennung von Objekten, die auf GPUs TitanV / TeslaV100, TitanXP / TeslaP100, TitanX / TeslaM40 gemessen wurden, f√ºr die beiden Hauptindikatoren f√ºr die Genauigkeit AP50: 95 und AP50</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
F√ºr einen fairen Vergleich nehmen wir Daten aus Artikeln und Vergleichen Sie nur auf der GPU mit der gleichen Architektur. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die meisten praktischen Aufgaben stellen die Mindestanforderungen an Detektoren - dies sind die minimal akzeptable Genauigkeit und Geschwindigkeit. Normalerweise die minimal zul√§ssige Geschwindigkeit von 30 FPS (Frames pro Sekunde) und h√∂her f√ºr Echtzeitsysteme. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wie aus den Grafiken ersichtlich, sind in Echtzeitsystemen mit FPS 30 oder mehr:</font></font><br>
<br>
<ul>
<li> YOLOv4-608   RTX 2070  <b>450$</b> (34 FPS)   <b>43.5% AP / 65.7% AP50</b></li>
<li> EfficientDet-D2   TitanV  <b>2250$</b> (42 FPS)   <b>43.0% AP / 62.3% AP50</b></li>
<li> EfficientDet-D0   RTX 2070  <b>450$</b> (34 FPS)   <b>33.8% AP / 52.2% AP50</b></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Jene. YOLOv4 erfordert </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">f√ºnfmal billigere Ger√§te</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> und ist genauer als EfficientDet-D2 (Google-TensorFlow). Sie k√∂nnen EfficientDet-D0 (Google-TensorFlow) verwenden, dann sind die Kosten f√ºr die Ausr√ºstung gleich, aber die Genauigkeit ist um 10% AP niedriger. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dar√ºber hinaus haben einige industrielle Systeme Einschr√§nkungen hinsichtlich der W√§rmeableitung oder der Verwendung eines passiven K√ºhlsystems. In diesem Fall k√∂nnen Sie TitanV auch mit Geld nicht verwenden. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wenn Sie YOLOv4 (416x416) auf einer RTX 2080 Ti-GPU mit TensorRT + tkDNN verwenden, erreichen wir eine Geschwindigkeit von 2x schneller und bei Verwendung von Batch = 4 3x-4x schneller - f√ºr einen ehrlichen Vergleich pr√§sentieren wir diese Ergebnisse nicht in einem Artikel √ºber arxiv. org:</font></font><br>
<img src="https://habrastorage.org/webt/ci/j7/uq/cij7uqas0ypsjcpsfkhvdxuyxzs.png"><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
YOLOv4 neuronales Netzwerk (416x416) FP16 (Tensorkerne) Batch = </font><font style="vertical-align: inherit;">1 erreicht 32 FPS-Rechner nVidia Jetson AGX Xavier unter Verwendung von Bibliotheken + tkDNN TensorRT: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/ceccocats/tkDNN</font></font></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
etwas langsamere Geschwindigkeit ergibt OpenCV-dnn-Bibliothek, die mit CUDA: </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;">docs</font></a><font style="vertical-align: inherit;"> kompiliert wurde </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.opencv.org / master / da / d9d / tutorial_dnn_yolo.html</font></font></a><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Manchmal wird die Geschwindigkeit (FPS) einiger neuronaler Netze in Artikeln angegeben, wenn eine hohe Stapelgr√∂√üe verwendet wird oder wenn mit einer speziellen Software (TensorRT) getestet wird, die das Netzwerk optimiert und einen erh√∂hten FPS-Wert anzeigt. Der Vergleich einiger Netzwerke auf TRT mit anderen Netzwerken ohne TRT ist nicht fair. Die Verwendung einer hohen Chargengr√∂√üe erh√∂ht die FPS, erh√∂ht aber auch die Latenz (anstatt sie zu verringern) im Vergleich zu Charge = 1. Wenn das Netzwerk mit Batch = 1 40 FPS und mit Batch = 32 60 FPS anzeigt, betr√§gt die Verz√∂gerung 25 ms f√ºr Batch = 1 und ~ 500 ms f√ºr Batch = 32, weil Pro Sekunde werden nur ~ 2 Pakete (jeweils 32 Bilder) verarbeitet, weshalb die Verwendung von Batch = 32 in vielen industriellen Systemen nicht akzeptabel ist. Daher haben wir die Ergebnisse in den Diagrammen nur mit Batch = 1 und ohne Verwendung von TensorRT verglichen.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Jeder Prozess kann entweder von Personen oder von Computern gesteuert werden. Wenn ein Computersystem aufgrund niedriger Geschwindigkeit mit einer gro√üen Verz√∂gerung arbeitet und zu viele Fehler macht, kann es nicht mit der vollst√§ndigen Kontrolle der Aktionen betraut werden. In diesem Fall steuert die Person den Prozess, und das Computersystem gibt nur Hinweise - dies ist ein Empfehlungssystem - die Person arbeitet und nur das System erz√§hlt, wo Fehler gemacht wurden. Wenn das System schnell und mit hoher Genauigkeit arbeitet, kann ein solches System den Prozess unabh√§ngig steuern, und eine Person k√ºmmert sich nur darum. Daher sind Genauigkeit und Systemgeschwindigkeit immer wichtig. Wenn Sie der Meinung sind, dass 120 FPS f√ºr YOLOv4 416x416 zu viel f√ºr Ihre Aufgabe sind und es besser ist, den Algorithmus langsamer und genauer zu verwenden, verwenden Sie bei realen Aufgaben h√∂chstwahrscheinlich etwas Schw√§cheres als den Tesla V100 250 Watt.Beispiel: RTX 2060 / Jetson-Xavier 30-80 Watt. In diesem Fall erhalten Sie 30 FPS f√ºr YOLOv4 416x416 und andere neuronale Netze mit 1-15 FPS oder starten √ºberhaupt nicht.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Merkmale des Trainings verschiedener neuronaler Netze</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Sie m√ºssen EfficientDet mit Mini-Batch = 128 Gr√∂√üe auf mehreren Tesla V100 32-GB-GPUs trainieren, w√§hrend YOLOv4 auf nur einer Tesla V100 32-GB-GPU mit Mini-Batch = 8 = Batch / Unterteilungen trainiert wurde und auf einem regul√§ren Spiel trainiert werden kann Grafikkarte 8-16 GB GPU-VRAM. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die n√§chste Nuance ist die Schwierigkeit, ein neuronales Netzwerk zu trainieren, um seine eigenen Objekte zu erkennen. </font><font style="vertical-align: inherit;">Unabh√§ngig davon, wie viel Zeit Sie andere Netzwerke auf derselben 1080 Ti-GPU trainieren, erhalten Sie nicht die in der obigen Grafik angegebene Genauigkeit. </font><font style="vertical-align: inherit;">Die meisten Netzwerke (EfficientDet, ASFF, ...) m√ºssen auf 4 - 128 GPUs (mit einer gro√üen Mini-Batch-Gr√∂√üe unter Verwendung von syncBN) trainiert werden, und es ist erforderlich, jedes Mal f√ºr jede Netzwerkaufl√∂sung neu zu trainieren, ohne beide Bedingungen zu erf√ºllen. Es ist unm√∂glich, die von ihnen angegebene AP- oder AP50-Genauigkeit zu erreichen.</font></font><br>
 <br>
<img src="https://habrastorage.org/webt/p4/sx/p3/p4sxp3ewxd9owskis23n6dyrv58.jpeg"><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sie k√∂nnen die Abh√§ngigkeit der Erkennungsgenauigkeit von Objekten von der Gr√∂√üe des Minibatch in anderen Detektoren sehen, d. H. </font><font style="vertical-align: inherit;">Bei Verwendung von 128 Grafikkarten anstelle von 8 Grafikkarten ist die Lerngeschwindigkeit 10-mal h√∂her und die Endgenauigkeit 1,5 AP h√∂her - MegDet: Ein gro√üer Mini-Batch-Objektdetektor </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">arxiv.org/abs/1711.07240</font></font></a></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Yolo ASFF: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">arxiv.org/abs/1911.09516</font></font></a> <br>
<blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Im Anschluss an [43] f√ºhren wir eine Reihe von Tricks in den Trainingsprozess ein, z. B. den Mischalgorithmus [12], den Kosinus [26] -Lernratenplan und die synchronisierte Batch-Normalisierungstechnik [30].</font></font></blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
EfficientDet: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">arxiv.org/abs/1911.09070</font></font></a> <br>
<blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nach jeder Faltung wird eine synchronisierte Chargennormalisierung mit einem Chargennormabfall von 0,99 und Epsilon 1e-3 hinzugef√ºgt. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Jedes Modell trainiert 300 Epochen mit einer Gesamtgr√∂√üe von 128 auf 32 TPUv3-Kernen.</font></font></blockquote><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow">cloud.google.com/tpu/docs/types-zones#europe</a><br>
<blockquote>v3-32 TPU type (v3) ‚Äì 32 TPU v3 cores ‚Äì 512 GiB Total TPU memory</blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Sie m√ºssen 512 GB TPU / GPU-RAM verwenden, um das EfficientDet-Modell mit synchronisierter Batch-Normalisierung bei Batch = 128 zu trainieren, w√§hrend Mini-Batch = 8 und nur 32 GB GPU-RAM zum Trainieren von YOLOv4 verwendet wurden. Trotzdem ist YOLOv4 schneller / genauer als √∂ffentliche Netzwerke, obwohl es nur einmal mit einer Aufl√∂sung von 512 x 512 pro GPU (Tesla V100 32 GB / 16 GB) trainiert wird. Gleichzeitig f√ºhrt die Verwendung der kleineren Mini-Batch-Gr√∂√üe und des GPU-VRAM nicht zu einem so dramatischen Genauigkeitsverlust wie in anderen neuronalen Netzen: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/xi/ol/rs/xiolrsvx4vzpjvahb6kvambdvgq.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Quelle: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">arxiv.org/abs/2004.10934</font></font></a></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Sie k√∂nnen also k√ºnstliche Intelligenz lokal auf Ihrem PC trainieren, anstatt sie herunterzuladen Datensatz in die Cloud - dies garantiert den Schutz Ihrer pers√∂nlichen Daten und macht das Training f√ºr k√ºnstliche Intelligenz f√ºr alle verf√ºgbar.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Es reicht aus, unser Netzwerk einmal mit einer Netzwerkaufl√∂sung von 512 x 512 zu trainieren, und dann kann es mit verschiedenen Netzwerkaufl√∂sungen im Bereich [416 x 416 - 512 x 512 - 608 x 608] verwendet werden. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die meisten anderen Modelle m√ºssen jedes Mal f√ºr jede Netzwerkaufl√∂sung separat trainiert werden. Aus diesem Grund dauert das Training um ein Vielfaches l√§nger.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Merkmale der Messgenauigkeit von Objekterkennungsalgorithmen</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Sie k√∂nnen immer ein Bild finden, bei dem ein Algorithmus schlecht funktioniert und ein anderer Algorithmus gut funktioniert und umgekehrt. Daher wird zum Testen von Erkennungsalgorithmen ein gro√üer Satz von ~ 20.000 Bildern und 80 verschiedenen Objekttypen verwendet - MSCOCO-Testentwicklungsdatensatz. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Damit der Algorithmus nicht einfach versucht, sich den Hash jedes Bildes und die Koordinaten darauf zu merken (√úberanpassung), wird die Genauigkeit der Objekterkennung immer auf Bildern und Beschriftungen √ºberpr√ºft, die der Algorithmus w√§hrend des Trainings nicht gesehen hat - dies stellt sicher, dass der Algorithmus Objekte auf Bildern / Videos erkennen kann, die er hat Noch nie gesehen.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Damit niemand einen Fehler bei der Berechnung der Genauigkeit machen kann, gibt es im √∂ffentlichen Bereich nur Test-Dev-Testbilder, auf denen Sie die Ergebnisse erkennen und an den CodaLab-Evaluierungsserver senden, auf dem das Programm selbst Ihre Ergebnisse mit Testanmerkungen vergleicht, auf die niemand zugreifen kann . </font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Der MSCOCO-Datensatz besteht aus 3 Teilen</font></font></a><br>
<br>
<ol>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tutorial: 120.000 Bilder und eine JSON-Datei mit den Koordinaten jedes Objekts</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Validierungssatz: 5.000 Bilder und eine JSON-Datei mit den Koordinaten jedes Objekts</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Testsuite: 41.000 JPG-Bilder ohne die Koordinaten von Objekten (einige dieser Bilder werden verwendet, um die Genauigkeit bei Aufgaben zu bestimmen: Objekterkennung, Instanzsegmentierung, Schl√ºsselpunkte, ...)</font></font></li>
</ol><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Merkmale der Entwicklung von YOLOv4</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bei der Entwicklung von YOLOv4 musste ich sowohl das neuronale YOLOv4-Netzwerk als auch das Darknet-Framework unter C / C ++ / CUDA selbst entwickeln. </font><font style="vertical-align: inherit;">weil </font><font style="vertical-align: inherit;">In Darknet gibt es keine automatische Differenzierung und automatische Ausf√ºhrung der Kettenregel, dann m√ºssen alle Verl√§ufe manuell implementiert werden. </font><font style="vertical-align: inherit;">Auf der anderen Seite k√∂nnen wir von der strikten Einhaltung der Kettenregel abweichen, die Backpropagation √§ndern und sehr triviale Dinge versuchen, um die Lernstabilit√§t und -genauigkeit zu erh√∂hen.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zus√§tzliche Erkenntnisse bei der Erstellung neuronaler Netze</font></font></h3><br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nicht immer ist das beste Netzwerk zum Klassifizieren von Objekten das beste als R√ºckgrat f√ºr das Netzwerk, das zum Erkennen von Objekten verwendet wird</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Verwendung von Gewichten, die mit Merkmalen trainiert wurden, deren Genauigkeit bei der Klassifizierung erh√∂ht wurde, kann die Genauigkeit des Detektors beeintr√§chtigen (in einigen Netzwerken).</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nicht alle in verschiedenen Studien genannten Funktionen verbessern die Netzwerkgenauigkeit.</font></font></li>
<li>                .</li>
<li>      BFLOPS  ,   BFLOPS    </li>
<li>                  ,     receptive field     ,       stride=2 / conv3x3,    weights (filters)      . </li>
</ul><br>
<h3>   YOLOv4</h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die Objekterkennung mit trainierten YOLOv4-Modellen ist in die OpenCV-dnn-Bibliothek </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/opencv/opencv/issues/17148 integriert,</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> sodass Sie YOLOv4 direkt von OpenCV aus verwenden k√∂nnen, ohne das Darknet-Framework zu verwenden. </font><font style="vertical-align: inherit;">Die OpenCV-Bibliothek unterst√ºtzt die Implementierung neuronaler Netze auf der CPU, GPU (nVidia GPU) und VPU (Intel Myriad X). </font><font style="vertical-align: inherit;">Weitere Details: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">docs.opencv.org/master/da/d9d/tutorial_dnn_yolo.html </font></font></a><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">OpenCV</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (dnn) Framework:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">C ++ - Beispiel: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/opencv/opencv/blob/master/samples/dnn/object_detection.cpp</font></font></a></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Python-Beispiel: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/opencv/opencv/blob/master/samples/dnn/object_detection.py</font></font></a></li>
</ul><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Darknet-</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Framework:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Anweisungen zur Verwendung von YOLOv4 zum Erkennen von Objekten: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/AlexeyAB/darknet#how-to-use-on-the-command-line</font></font></a></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Anweisungen zum Trainieren eines neuronalen Netzwerks zum Erkennen eigener Objekte: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/AlexeyAB/darknet#how-to-train-to-detect-your-custom-objects</font></font></a></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Anweisungen zum Trainieren des CSPDarknet53-Klassifikators im ILSVRC2012-Dataset (ImageNet): </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/AlexeyAB/darknet/wiki/Train-Classifier-on-ImageNet-</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (ILSVRC2012)</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Anweisungen zum Trainieren von YOLOv4 im MS COCO-Datensatz: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/AlexeyAB/darknet/wiki/Train-Detector-on-MS-COCO-</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (trainvalno5k-2014) -dataset</font></font></li>
</ul><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">tkDNN + TensorRT</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - Maximale Objekterkennungsgeschwindigkeit mit YOLOv4: TensorRT + tkDNN </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/ceccocats/tkDNN</font></font></a><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">400 FPS - YOLOv4 (416 √ó 416 Charge = 4) auf RTX 2080 Ti</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">32 FPS - YOLOv4 (416 x 416 Batch = 1) auf Jetson AGX Xavier</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die Verwendung von YOLOv4 kann erweitert werden, um 3D-gedrehte B-Boxen oder wichtige Punkte / Gesichtspunkte zu erkennen, z. B.: </font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Github.com/ouyanghuiyu/darknet_face_with_landmark</font></font></a><br>
<br>
<img src="https://habrastorage.org/webt/z7/vs/dv/z7vsdvhcpfbrgmdv1byhbpzd1cu.jpeg"></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de503182/index.html">"Ich bin der erste blinde Entwickler in meinem Unternehmen." Teil 1</a></li>
<li><a href="../de503184/index.html">Wir laden Sie zum Online-Meeting Zabbix ein</a></li>
<li><a href="../de503192/index.html">OVirt in 2 Stunden. Teil 4. Grundlegende Operationen</a></li>
<li><a href="../de503194/index.html">ISA vergibt keine Fehler</a></li>
<li><a href="../de503196/index.html">450 kostenlose Kurse aus der Ivy League</a></li>
<li><a href="../de503204/index.html">So flashen Sie Xiaomi Redmi 4 Prime / Pro / Premium auf Android 10</a></li>
<li><a href="../de503208/index.html">Wann ist der beste Zeitpunkt f√ºr eine Investition?</a></li>
<li><a href="../de503210/index.html">K√∂nnen Phishing-Sites ausgerottet werden?</a></li>
<li><a href="../de503212/index.html">30 Life Hacks, um den Online-Kurs abzuschlie√üen</a></li>
<li><a href="../de503214/index.html">Lastoptimierung f√ºr ein Highload-Projekt mit ElasticSearch</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>