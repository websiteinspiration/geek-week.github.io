<!doctype html>
<html class="no-js" lang="de">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§∂üèº ‚ôÇÔ∏è ‚úÖ Trainieren Sie Smart-Gaming-Rivalen in Unity mithilfe der "Play with yourself" -Methode mithilfe von ML-Agents ‚ôâÔ∏è ü¶ê üè†</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo Habr! 
 
 Wie unsere regelm√§√üigen Leser wissen, haben wir lange und erfolgreich B√ºcher √ºber Unity ver√∂ffentlicht . Im Rahmen der Untersuchung de...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Trainieren Sie Smart-Gaming-Rivalen in Unity mithilfe der "Play with yourself" -Methode mithilfe von ML-Agents</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/507212/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hallo Habr! </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wie unsere regelm√§√üigen Leser wissen, haben wir lange und erfolgreich </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">B√ºcher</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> √ºber </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Unity ver√∂ffentlicht</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Im Rahmen der Untersuchung des Themas haben wir uns insbesondere f√ºr das </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ML-Agents Toolkit</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> interessiert </font><font style="vertical-align: inherit;">. Heute machen wir Sie auf eine √úbersetzung eines Artikels aus dem Unity-Blog aufmerksam, in dem es darum geht, Spielagenten mithilfe der Methode ‚Äûmit sich selbst‚Äú effektiv zu schulen. Insbesondere hilft der Artikel zu verstehen, warum diese Methode effektiver ist als traditionelles verst√§rktes Lernen. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/vb/sz/5m/vbsz5musziyq5yighwu_iq0eggu.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Viel Spa√ü beim Lesen!</font></font><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dieser Artikel gibt dann einen √úberblick √ºber die Self-Play-Technologie (mit sich selbst spielen) und zeigt, wie sie mit dem </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ML-Agents Toolkit</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> zu einem stabilen und effektiven Training in der Soccer-Demo-Umgebung beitr√§gt </font><font style="vertical-align: inherit;">. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In den Tennis- und Fu√üball-Demo-Umgebungen des Unity ML-Agents Toolkit treten Agenten wie Rivalen gegeneinander an. Die Schulung von Agenten in einem solchen Wettbewerbsszenario ist manchmal eine nicht triviale Aufgabe. In fr√ºheren Versionen des ML-Agents Toolkit war eine ernsthafte Untersuchung der Auszeichnung erforderlich, damit der Agent sicher lernen konnte. In </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Version 0.14</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Es wurde eine M√∂glichkeit hinzugef√ºgt, die es dem Benutzer erm√∂glicht, Agenten mithilfe des auf Selbstspiel basierenden Verst√§rkungslernens (RL) zu schulen. Dieser Mechanismus ist entscheidend, um einige der hochwertigsten Ergebnisse des Verst√§rkungslernens zu erzielen, wie </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">OpenAI Five</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> und </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">DeepMinds AlphaStar</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Beim Selbstspiel bei der Arbeit werden die aktuellen und vergangenen Hypostasen des Agenten miteinander verglichen. Auf diese Weise erhalten wir einen Gegner f√ºr unseren Agenten, der sich mithilfe traditioneller Verst√§rkungslernalgorithmen schrittweise verbessern kann. </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ein voll ausgebildeter Agent</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> kann erfolgreich mit fortgeschrittenen menschlichen Spielern konkurrieren.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das Selbstspiel bietet eine Lernumgebung, die auf den gleichen Prinzipien beruht wie die Konkurrenz aus menschlicher Sicht. Zum Beispiel wird eine Person, die Tennis spielen lernt, sich daf√ºr entscheiden, Gegner auf ungef√§hr dem gleichen Niveau wie er selbst zu schonen, da ein zu starker oder zu schwacher Gegner nicht so praktisch ist, um das Spiel zu meistern. Unter dem Gesichtspunkt der Entwicklung eigener F√§higkeiten kann es f√ºr einen unerfahrenen Tennisspieler viel wertvoller sein, dieselben Anf√§nger zu schlagen, als beispielsweise ein Vorschulkind oder Novak Djokovic. Der erste wird nicht einmal in der Lage sein, den Ball zu schlagen, und der zweite wird Ihnen keinen solchen Aufschlag geben, den Sie schlagen k√∂nnen. Wenn ein Anf√§nger gen√ºgend Kraft entwickelt, kann er zum n√§chsten Level √ºbergehen oder sich f√ºr ein ernsthafteres Turnier bewerben, um gegen erfahrenere Gegner zu spielen.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In diesem Artikel werden wir einige technische Feinheiten betrachten, die mit der Dynamik des Spiels verbunden sind, sowie Beispiele f√ºr die Arbeit in virtuellen Umgebungen Tennis und Fu√üball, die so √ºberarbeitet wurden, dass sie das Spiel mit sich selbst veranschaulichen.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Geschichte eines Spiels mit sich selbst in Spielen</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das Ph√§nomen des Spielens mit sich selbst hat eine lange Geschichte, die sich in der Praxis der Entwicklung k√ºnstlicher Spielagenten widerspiegelt, die im Wettbewerb mit Menschen in Spielen entwickelt wurden. </font><font style="vertical-align: inherit;">Einer der ersten, der dieses System verwendete, war Arthur Samuel, der in den 1950er Jahren einen Schach-Simulator entwickelte und </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">diese Arbeit</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 1959 ver√∂ffentlichte. </font><font style="vertical-align: inherit;">Dieses System wurde zum Vorl√§ufer eines wegweisenden Ergebnisses des von Gerald Tesauro in TD-Gammon erzielten verst√§rkten Lernens. </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Summen ver√∂ffentlicht</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">im Jahr 1995. TD-Gammon verwendete den TD (Œª) -Zeitdifferenzalgorithmus mit der Funktion, mit sich selbst zu spielen, um den Agenten zu trainieren, Backgammon zu spielen, damit er mit einer professionellen Person konkurrieren kann. In einigen F√§llen wurde beobachtet, dass TD-Gammon eine sicherere Sicht auf Positionen hat als Weltklassespieler. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das Spielen mit sich selbst spiegelt sich in vielen der mit RL verbundenen ikonischen Errungenschaften wider. Es ist wichtig zu beachten, dass das Spielen mit sich selbst die Entwicklung von Agenten f√ºr </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Schach und</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> √ºbermenschliche F√§higkeiten, Elite- </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">DOTA-2-</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Agenten </font><font style="vertical-align: inherit;">sowie komplexe Strategien und Gegenstrategien in Spielen wie </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wrestling</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> und </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Verstecken unterst√ºtzt hat</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Bei den Ergebnissen, die durch das Spielen mit sich selbst erzielt werden, wird h√§ufig festgestellt, dass Spielagenten Strategien w√§hlen, die Experten √ºberraschen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das Spielen mit sich selbst verleiht den Agenten eine gewisse Kreativit√§t, die unabh√§ngig von der Kreativit√§t der Programmierer ist. </font><font style="vertical-align: inherit;">Der Agent erh√§lt nur die Spielregeln und dann - Informationen dar√ºber, ob er gewonnen oder verloren hat. </font><font style="vertical-align: inherit;">Basierend auf diesen Grundprinzipien muss der Agent ein kompetentes Verhalten entwickeln. </font><font style="vertical-align: inherit;">Laut dem Erfinder von TD-Gammon befreit ein solcher Lernansatz "... in dem Sinne, dass das Programm nicht durch menschliche Neigungen und Vorurteile eingeschr√§nkt wird, die fehlerhaft und unzuverl√§ssig sein k√∂nnen." </font><font style="vertical-align: inherit;">Dank dieser Freiheit entdecken Agenten brillante Spielstrategien, die die Art und Weise, wie Ingenieure √ºber bestimmte Spiele denken, v√∂llig ver√§ndern.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wettkampf-Verst√§rkungstraining</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Im Rahmen der traditionellen Aufgabe des verst√§rkten Lernens versucht der Agent, eine Verhaltenslinie zu entwickeln, die die Gesamtbelohnung maximiert. Das Belohnungssignal codiert die Aufgabe des Agenten - eine solche Aufgabe kann beispielsweise das Zeichnen eines Kurses oder das Sammeln von Gegenst√§nden sein. Das Verhalten des Agenten unterliegt Umwelteinschr√§nkungen. Zum Beispiel die Schwerkraft, Hindernisse sowie der relative Einfluss der vom Agenten selbst ergriffenen Ma√ünahmen - zum Beispiel die Anwendung von Gewalt auf die eigene Bewegung. Diese Faktoren begrenzen das Verhalten des Agenten und sind externe Kr√§fte, mit denen er umgehen muss, um eine hohe Belohnung zu erhalten. Somit konkurriert der Agent mit der Dynamik der Umgebung und muss sich genau von Staat zu Staat bewegen, damit die maximale Belohnung erreicht wird.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/if/jf/xi/ifjfxil_iqm9pznr61qbpxujs38.png"><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Das typische Verst√§rkungstrainingsszenario ist links dargestellt: Der Agent agiert in der Umgebung, wechselt in den n√§chsten Zustand und erh√§lt eine Belohnung. Das Trainingsszenario wird rechts gezeigt, wo der Agent mit einem Rivalen konkurriert, der aus Sicht des Agenten tats√§chlich ein Element der Umgebung ist.</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Bei Wettbewerbsspielen konkurriert der Agent nicht nur mit der Dynamik der Umgebung, sondern auch mit einem anderen (m√∂glicherweise intellektuellen) Agenten. Wir k√∂nnen davon ausgehen, dass der Gegner in die Umgebung eingebaut ist und seine Handlungen sich direkt auf den n√§chsten Zustand auswirken, den der Agent ‚Äûsieht‚Äú, sowie auf die Belohnung, die er erhalten wird. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/zl/kv/xo/zlkvxoa9_bt0cdsthmddwll-fvc.png"><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Beispiel Tennis aus dem ML-Agents Toolkit</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Betrachten Sie die Demo von ML-Agents Tennis. Der blaue Schl√§ger (links) ist der Lernagent und der lila (rechts) ist sein Gegner. Um den Ball √ºber das Netz zu werfen, muss der Agent die Flugbahn des vom Gegner fliegenden Balls ber√ºcksichtigen und den Winkel und die Geschwindigkeit des fliegenden Balls unter Ber√ºcksichtigung der Umgebungsbedingungen (Schwerkraft) anpassen. In einem Wettbewerb mit einem Gegner ist es jedoch nur die halbe Miete, den Ball √ºber das Netz zu werfen. Ein starker Gegner kann mit einem unwiderstehlichen Schlag reagieren, und infolgedessen verliert der Agent. Ein schwacher Gegner kann den Ball ins Netz schlagen. Ein gleichberechtigter Gegner kann den Aufschlag zur√ºckgeben, und daher wird das Spiel fortgesetzt. In jedem Fall h√§ngen sowohl der n√§chste Zustand als auch die entsprechende Belohnung von den Umgebungsbedingungen und dem Gegner ab. In all diesen Situationen macht der Agent jedoch die gleiche Tonh√∂he. Daher als Training in Pflichtspielen,und das Pumpen rivalisierender Verhaltensweisen durch einen Agenten ist ein komplexes Problem.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
√úberlegungen f√ºr einen geeigneten Gegner sind nicht trivial. Wie aus dem Obigen hervorgeht, beeinflusst die relative St√§rke des Gegners das Ergebnis eines bestimmten Spiels erheblich. Wenn der Gegner zu stark ist, kann es f√ºr den Agenten schwierig sein, das Spielen von Grund auf zu erlernen. Wenn der Gegner dagegen zu schwach ist, kann der Agent lernen, zu gewinnen, aber diese F√§higkeiten k√∂nnen im Wettbewerb mit einem st√§rkeren oder einfach anderen Gegner nutzlos sein. Deshalb brauchen wir einen Gegner, dessen St√§rke ungef√§hr der des Agenten entspricht (unnachgiebig, aber nicht un√ºberwindbar). Da sich die F√§higkeiten unseres Agenten mit jedem abgeschlossenen Spiel verbessern, m√ºssen wir au√üerdem die St√§rke seines Gegners in gleichem Ma√üe erh√∂hen. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/fn/vk/pt/fnvkptgsipqopi2iiyflsa5ec_a.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wenn Sie mit sich selbst spielen, ist ein Schnappschuss aus der Vergangenheit oder ein Agent in seinem aktuellen Zustand der Gegner, der in die Umgebung eingebaut ist.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Hier bietet sich das Spiel mit uns an! Der Agent selbst erf√ºllt beide Anforderungen an den gew√ºnschten Gegner. Er ist definitiv ungef√§hr so ‚Äã‚Äãstark wie er selbst und seine F√§higkeiten verbessern sich im Laufe der Zeit. In diesem Fall ist die eigene Richtlinie des Agenten in die Umgebung integriert (siehe Abbildung). Diejenigen, die mit der </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">schrittweise Erh√∂hung der Komplexit√§t</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (Lehrplanlernen) </font><font style="vertical-align: inherit;">vertraut sind </font><font style="vertical-align: inherit;">, zeigen Ihnen, dass wir davon ausgehen k√∂nnen, dass das System den Lehrplan auf nat√ºrliche Weise entwickelt, woraufhin der Agent lernt, gegen immer m√§chtigere Gegner zu k√§mpfen. Wenn Sie mit sich selbst spielen, k√∂nnen Sie die Umgebung selbst nutzen, um wettbewerbsf√§hige Agenten f√ºr wettbewerbsf√§hige Spiele zu schulen!</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In den n√§chsten beiden Abschnitten werden wir weitere technische Details zur Schulung von Wettbewerbsagenten ber√ºcksichtigen, insbesondere in Bezug auf die Implementierung und Verwendung des Spiels mit sich selbst im ML-Agents Toolkit.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Praktische √úberlegungen</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Einige praktische Probleme ergeben sich hinsichtlich des Rahmens f√ºr das Spielen mit sich selbst. </font><font style="vertical-align: inherit;">Insbesondere ist eine Umschulung m√∂glich, bei der der Agent nur mit einem bestimmten Spielstil zu gewinnen lernt, sowie die dem Lernprozess innewohnende Instabilit√§t, die aufgrund der Unstetigkeit der √úbergangsfunktion (dh aufgrund sich st√§ndig √§ndernder Gegner) entstehen kann. </font><font style="vertical-align: inherit;">Das erste Problem entsteht, weil wir m√∂chten, dass unsere Agenten √ºber ein allgemeines Wissen und die F√§higkeit verf√ºgen, Gegner verschiedener Typen zu bek√§mpfen.</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das zweite Problem kann in der Tennisumgebung veranschaulicht werden: Verschiedene Gegner schlagen den Ball mit unterschiedlichen Geschwindigkeiten und Winkeln. Aus Sicht des Lernagenten bedeutet dies, dass beim Lernen dieselben Entscheidungen zu unterschiedlichen Ergebnissen f√ºhren und sich der Agent dementsprechend in unterschiedlichen Folgesituationen befindet. Beim traditionellen Lernen der Verst√§rkung sind station√§re √úbergangsfunktionen impliziert. Leider k√∂nnen wir, nachdem wir eine Auswahl verschiedener Gegner f√ºr den Agenten vorbereitet haben, um das erste Problem zu l√∂sen, das zweite Problem verschlimmern, da wir nachl√§ssig sind.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Um dies zu bew√§ltigen, werden wir einen Puffer mit fr√ºheren Agentenrichtlinien beibehalten, aus dem wir langfristig potenzielle Rivalen f√ºr unseren "Studenten" ausw√§hlen werden. </font><font style="vertical-align: inherit;">Wenn wir einen Agenten aus fr√ºheren Richtlinien ausw√§hlen, erhalten wir f√ºr ihn eine Auswahl verschiedener Gegner. </font><font style="vertical-align: inherit;">Dar√ºber hinaus stabilisieren wir die √úbergangsfunktion und schaffen eine konsistentere Lernumgebung, damit der Agent lange Zeit mit einem festen Gegner trainieren kann. </font><font style="vertical-align: inherit;">Schlie√ülich k√∂nnen diese algorithmischen Aspekte mithilfe von Hyperparametern gesteuert werden, die im n√§chsten Abschnitt erl√§utert werden.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Implementierungs- und Verwendungsdetails</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bei der Auswahl von Hyperparametern f√ºr das Spielen mit uns selbst ber√ºcksichtigen wir zun√§chst einen Kompromiss zwischen dem Niveau des Gegners, der Universalit√§t der endg√ºltigen Politik und der Stabilit√§t des Trainings. Das Training im Wettbewerb mit einer Gruppe von Gegnern, die sich langsam oder gar nicht √§ndern, was bedeutet, dass sie eine geringere Streuung der Ergebnisse liefern, ist ein stabilerer Prozess als das Training im Wettbewerb mit vielen verschiedenen Gegnern, die sich schnell √§ndern. Mit den verf√ºgbaren Hyperparametern k√∂nnen Sie steuern, wie oft die aktuelle Richtlinie des Agenten f√ºr die sp√§tere Verwendung als einer der Gegner in der Stichprobe gespeichert wird, wie oft der neue Gegner gespeichert wird, anschlie√üend f√ºr das Sparring ausgew√§hlt wird, wie oft der neue Gegner ausgew√§hlt wird, wie viele Gegner gespeichert werden und wie wahrscheinlich es istIn diesem Fall muss der Sch√ºler gegen sein eigenes Alter Ego spielen und nicht gegen einen aus dem Pool ausgew√§hlten Gegner.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bei Pflichtspielen ist die von der Umwelt vergebene ‚Äûkumulative‚Äú Auszeichnung m√∂glicherweise nicht die aussagekr√§ftigste Messgr√∂√üe f√ºr die Verfolgung des Lernfortschritts. Tatsache ist, dass die kumulierte Auszeichnung vollst√§ndig vom Level des Gegners abh√§ngt. Ein Agent mit einer bestimmten Spielf√§higkeit erh√§lt eine gr√∂√üere oder geringere Belohnung, abh√§ngig von einem weniger geschickten bzw. geschickteren Gegner. Wir schlagen die Implementierung </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">des ELO-Bewertungssystems vor</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , mit dem Sie die relativen Spielf√§higkeiten von zwei Spielern aus einer bestimmten Population berechnen k√∂nnen, wenn Sie mit einem Betrag von Null spielen. W√§hrend eines einzelnen Trainingslaufs sollte dieser Wert stetig ansteigen. Sie k√∂nnen es zusammen mit anderen </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Lernmetriken</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , z. B. der Gesamtpr√§mie, mit </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;">TensorBoard verfolgen</font></a><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mit dir selbst im Fu√üball spielen</font></font></h3> <br>
<img src="https://habrastorage.org/webt/gl/m_/rh/glm_rhop1whkyve0i-wpubxfpqw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die neuesten Versionen des ML-Agent Toolkit enthalten keine Agentenrichtlinien f√ºr die Fu√üball-Lernumgebung, da der zuverl√§ssige Trainingsprozess nicht darin integriert wurde. </font><font style="vertical-align: inherit;">Wenn wir das Spiel jedoch mit uns selbst und einigen Umgestaltungen verwenden, k√∂nnen wir den Agenten in nicht trivialen Verhaltensweisen schulen. </font><font style="vertical-align: inherit;">Die wichtigste √Ñnderung ist das Entfernen von "Spielpositionen" aus den Eigenschaften des Agenten. </font><font style="vertical-align: inherit;">Fr√ºher im Fu√üballumfeld stachen der ‚ÄûTorh√ºter‚Äú und der ‚ÄûSt√ºrmer‚Äú deutlich heraus, sodass das gesamte Gameplay logischer aussah. </font><font style="vertical-align: inherit;">In </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">diesem Video</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Es wird eine neue Umgebung vorgestellt, in der man sehen kann, wie sich spontan ein Rollenverhalten bildet, in der einige Agenten als Angreifer und andere als Torh√ºter auftreten. Jetzt lernen die Agenten selbst, diese Positionen zu spielen! Die Belohnungsfunktion f√ºr alle vier Agenten ist definiert als +1,0 f√ºr ein erzieltes Tor und -1,0 f√ºr ein Gegentor mit einer zus√§tzlichen Strafe von -0,0003 pro Schritt. Diese Strafe wird bereitgestellt, um die Agenten zum Angriff anzuregen.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Hier betonen wir noch einmal, dass die Agenten in der Fu√üball-Lernumgebung selbst kooperatives Verhalten lernen, und daf√ºr wird kein expliziter Algorithmus verwendet, der sich auf das Verhalten mehrerer Agenten oder die Rollenzuweisung bezieht. </font><font style="vertical-align: inherit;">Dieses Ergebnis zeigt, dass ein Agent mit relativ einfachen Algorithmen in komplexen Verhaltensweisen geschult werden kann - vorausgesetzt, die Aufgabe ist gut formuliert. </font><font style="vertical-align: inherit;">Die wichtigste Voraussetzung daf√ºr ist, dass Agenten ihre Teamkollegen beobachten k√∂nnen, dh Informationen √ºber die relative Position des Teamkollegen erhalten. </font><font style="vertical-align: inherit;">Der Agent erzwingt einen aggressiven Kampf um den Ball und teilt dem Teamkollegen indirekt mit, dass er sich zur Verteidigung bewegen soll. </font><font style="vertical-align: inherit;">Im Gegenteil, der Agent bewegt sich zur Verteidigung weg und provoziert einen Teamkollegen zum Angriff.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Was weiter</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wenn Sie jemals eine der neuen Funktionen dieser Version verwendet haben, teilen Sie uns diese mit. </font><font style="vertical-align: inherit;">Wir machen Sie auf die Seite mit den </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GitHub-Problemen von ML-Agents</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> aufmerksam, auf der </font><font style="vertical-align: inherit;">Sie √ºber gefundene Fehler sprechen k√∂nnen, sowie auf die Seite mit den </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Unity ML-Agents-Foren</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , auf der allgemeine Fragen und Probleme besprochen werden.</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de507200/index.html">7 M√∂glichkeiten, wie Data Scientists Sie t√§uschen</a></li>
<li><a href="../de507202/index.html">Avito Analytics-Treffen</a></li>
<li><a href="../de507204/index.html">Innenk√ºche im Industriedesign: von der Skizze bis zum Produkt im Karton</a></li>
<li><a href="../de507206/index.html">Architektur Y Messenger</a></li>
<li><a href="../de507210/index.html">Die Leistung von modernem Java beim Arbeiten mit gro√üen Datenmengen, Teil 2</a></li>
<li><a href="../de507214/index.html">So erstellen und √§ndern Sie interaktive PDF-Formulare oder die neue F√§higkeit ABBYY FineReader PDF</a></li>
<li><a href="../de507218/index.html">Lesen Sie mich oder warum der Text nicht bis zum Ende gelesen wird</a></li>
<li><a href="../de507222/index.html">Warum jeder Masken tragen sollte</a></li>
<li><a href="../de507224/index.html">So beseitigen Sie blinde Flecken mit visuellen Tests</a></li>
<li><a href="../de507226/index.html">OCR f√ºr PDF in .NET - So extrahieren Sie Text aus unzug√§nglichen PDF-Dokumenten</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>