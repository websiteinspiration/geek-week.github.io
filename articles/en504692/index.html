<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèª‚Äçüöí üö¥ üßöüèª ZFS Basics: Storage and Performance üë®‚Äçüîß üïô üéª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="This spring, we have already discussed some introductory topics, such as how to check the speed of your disks and what RAID is . In the second of them...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>ZFS Basics: Storage and Performance</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/504692/"><img src="https://habrastorage.org/getpro/habr/post_images/abf/883/e96/abf883e96b01dbc78420e0dc1a158460.jpg"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
This spring, we have already discussed some introductory topics, such </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">as how to check the speed of your disks</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> and </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">what RAID is</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">In the second of them, we even promised to continue studying the performance of various multi-disk topologies in ZFS. </font><font style="vertical-align: inherit;">This is the next generation file system that is being implemented everywhere: from </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Apple</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> to </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ubuntu</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Well, today is the best day to get to know ZFS, curious readers. </font><font style="vertical-align: inherit;">Just be aware that, according to a conservative assessment by OpenZFS developer Matt Arens, "it's really complicated." </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
But before we get to the numbers - and they will, I promise - for all variants vosmidiskovoy ZFS configuration, you need to talk about </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">how to</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> do ZFS stores data on disk.</font></font><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zpool, vdev and device</font></font></h1><br>
<img src="https://habrastorage.org/getpro/habr/post_images/674/1c6/ab3/6741c6ab310f4e0edf2adf7e2ca4c6bb.png"><br>
<i><font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">This full pool diagram includes three helper vdevs, one for each class, and four for RAIDz2. </font></font></font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/b9f/82c/887/b9f82c88748c44d1f86cc412a053bf94.png"><br>
<i><font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">There is usually no reason to create a pool of inappropriate </font></font></font></i><font style="vertical-align: inherit;"><i><font color="gray"><font style="vertical-align: inherit;">vdev </font></font></i><i><font color="gray"><font style="vertical-align: inherit;">types and sizes - but if you want, nothing prevents you from doing this.</font></font></i></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
To really understand the ZFS file system , you need to carefully look at its actual structure. </font><font style="vertical-align: inherit;">First, ZFS combines traditional levels of volume management and the file system. </font><font style="vertical-align: inherit;">Secondly, it uses a transactional copy mechanism when writing. </font><font style="vertical-align: inherit;">These features mean that the system is structurally very different from ordinary file systems and RAID arrays. </font><font style="vertical-align: inherit;">The first set of basic building blocks to understand: a storage pool (zpool), a virtual device (vdev), and a real device (device).</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">zpool</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The zpool storage pool is the topmost ZFS structure. Each pool contains one or more virtual devices. In turn, each of them contains one or more real devices (device). Virtual pools are autonomous blocks. One physical computer may contain two or more separate pools, but each is completely independent of the others. Pools cannot share virtual devices. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Redundancy of ZFS is at the level of virtual devices, but not at the level of pools. At the pool level, there is absolutely no redundancy - if any vdev drive or special vdev is lost, then the entire pool is lost along with it.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Modern storage pools can survive the loss of a cache or virtual device log - although they can lose a small amount of dirty data if they lose the vdev log during a power outage or system crash. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
There is a common misconception that ‚Äúdata bands‚Äù (strips) of ZFS are recorded across the entire pool. This is not true. Zpool is not a fun RAID0 at all, it's rather a fun </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">JBOD</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> with a complex changeable distribution mechanism.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
For the most part, entries are distributed among available virtual devices according to available space, so theoretically they will all be filled at the same time. In later versions of ZFS, the current use (disposal) of vdev is taken into account - if one virtual device is significantly more loaded than the other (for example, due to read load), it will be temporarily skipped for writing, despite the presence of the highest free space coefficient. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A recycling detection mechanism built into modern ZFS record distribution methods can reduce latency and increase throughput during periods of unusually high load - but this is not </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">carte blanche</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">involuntarily mixing slow HDDs and fast SSDs in one pool. </font><font style="vertical-align: inherit;">Such an unequal pool will still work at the speed of the slowest device, that is, as if it were entirely composed of such devices.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">vdev</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Each storage pool consists of one or more virtual devices (virtual device, vdev). </font><font style="vertical-align: inherit;">In turn, each vdev includes one or more real devices. </font><font style="vertical-align: inherit;">Most virtual devices are used to easily store data, but there are several helper vdev classes, including CACHE, LOG, and SPECIAL. </font><font style="vertical-align: inherit;">Each of these vdev types can have one of five topologies: single-device, RAIDz1, RAIDz2, RAIDz3, or mirror.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
RAIDz1, RAIDz2, and RAIDz3 are special variations of what the olds call RAID double (diagonal) parity. 1, 2, and 3 refer to how many parity blocks are allocated for each data band. Instead of separate disks for parity, virtual RAIDz devices evenly distribute this parity across disks. A RAIDz array can lose as many disks as it has parity blocks; if he loses another one, he will fail and take the storage pool with him.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In mirrored virtual devices (mirror vdev), each block is stored on each device in vdev. Although the most common two-wide mirrors, there can be any arbitrary number of devices in the mirror - in large installations, triple ones are often used to increase read performance and fault tolerance. The vdev mirror can survive any failure while at least one device in vdev continues to work. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Single vdevs are inherently dangerous. Such a virtual device will not survive a single failure - and if it is used as storage or a special vdev, then its failure will lead to the destruction of the entire pool. Be very, very careful here.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
CACHE, LOG, and SPECIAL virtual appliances can be created using any of the above topologies - but remember that losing a SPECIAL virtual appliance means losing a pool, so an excessive topology is strongly recommended.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">device</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
This is probably the easiest term to understand in ZFS - it is literally a block random access device. Remember that virtual devices are made up of individual devices, and the pool is made up of virtual devices. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Disks - magnetic or solid-state - are the most common block devices that are used as vdev building blocks. However, any device with a handle in / dev is suitable - so you can use entire hardware RAID arrays as separate devices. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A simple raw file is one of the most important alternative block devices vdev can be built from. Test pools from </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">sparse files</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">&nbsp;- A very convenient way to check pool commands and see how much space is available in the pool or virtual device of this topology. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/5cf/aa5/62c/5cfaa562cb208b654af113f7535b8f57.png"><br>
<i><font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">You can create a test pool from sparse files in just a few seconds - but do not forget to delete the entire pool and its components later.</font></font></font></i> <br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Suppose you want to put a server on eight disks and plan to use 10 TB disks (~ 9300 GiB) - but you are not sure which Topology best suits your needs. In the above example, in a matter of seconds we build a test pool from sparse files - and now we know that RAIDz2 vdev from eight 10 TB drives provides 50 TiB of useful capacity.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Another special class of devices is SPARE (spare). Hot-swappable devices, unlike conventional devices, belong to the entire pool, not just one virtual device. If some vdev in the pool fails, and the spare device is connected to the pool and available, then it will automatically join the affected vdev. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
After connecting to the affected vdev, the spare device starts receiving copies or reconstruction of data that should be on the missing device. In traditional RAID, this is called rebuilding, while in ZFS it is called ‚Äúresilvering‚Äù.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
It is important to note that replacement devices do not permanently replace failed devices. </font><font style="vertical-align: inherit;">This is only a temporary replacement to reduce the time during which vdev degradation is observed. </font><font style="vertical-align: inherit;">After the administrator replaced the failed vdev device, redundancy is restored to this permanent device, and SPARE disconnects from vdev and returns to work as a spare for the entire pool.</font></font><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Datasets, Blocks, and Sectors</font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The next set of building blocks that you need to understand on our journey through ZFS is not so much hardware, but how the data is organized and stored. </font><font style="vertical-align: inherit;">We skip several levels here - such as metaslab - so as not to pile up the details while maintaining an understanding of the overall structure.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dataset</font></font></h3><br>
<img src="https://habrastorage.org/getpro/habr/post_images/bd3/c48/d9d/bd3c48d9dff6e0f493a5d90d1dca6d1d.png"><br>
<i><font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">When we first create a dataset, it shows all available pool space. Then we set the quota - and change the mount point. Magic! </font></font></font></i> <br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/a18/3de/210/a183de210cdc57cd1421652201cbf2c3.png"><br>
<i><font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zvol is for the most part just a dataset, devoid of its file system layer, which we replace here with a completely normal ext4</font></font></font></i> <br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
file system. The ZFS dataset is roughly the same as a standard mounted file system. Like a regular file system, at first glance it seems to be ‚Äújust another folder‚Äù. But also, like conventional mounted file systems, each ZFS dataset has its own set of basic properties.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
First of all, a dataset may have an assigned quota. If installed</font></font><code>zfs set quota=100G poolname/datasetname</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, then you cannot write to the mounted folder</font></font><code>/poolname/datasetname</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">more than 100 GiB.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Notice the presence - and absence - of slashes at the beginning of each line? Each data set has its own place both in the ZFS hierarchy and in the system mount hierarchy. There is no leading slash in the ZFS hierarchy - you start with the name of the pool, and then the path from one data set to the next. For example, </font></font><code>pool/parent/child</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">for a dataset named </font></font><code>child</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">under the parent dataset </font></font><code>parent</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">in a pool with a creative name </font></font><code>pool</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
By default, the mount point of the dataset will be equivalent to its name in the ZFS hierarchy, with a slash at the beginning - the pool with the name is </font></font><code>pool</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">mounted as </font></font><code>/pool</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, the dataset is </font></font><code>parent</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">mounted in </font></font><code>/pool/parent</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, and the child </font><font style="vertical-align: inherit;">dataset </font><font style="vertical-align: inherit;">is mounted </font></font><code>child</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">in </font></font><code>/pool/parent/child</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. However, the system mount point for the dataset can be changed. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
If we indicate</font></font><code>zfs set mountpoint=/lol pool/parent/child</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, then the data set is </font></font><code>pool/parent/child</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">mounted in the system as </font></font><code>/lol</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In addition to datasets, we should mention volumes (zvols). </font><font style="vertical-align: inherit;">A volume is approximately similar to a data set, except that it actually does not have a file system - it is just a block device. </font><font style="vertical-align: inherit;">You can, for example, create </font></font><code>zvol</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">with a name </font></font><code>mypool/myzvol</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, then format it with the ext4 file system, and then mount this file system - now you have the ext4 file system, but with support for all ZFS security features! </font><font style="vertical-align: inherit;">This may seem silly on one computer, but it makes much more sense as a backend when exporting an iSCSI device.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Blocks</font></font></h3><br>
<img src="https://habrastorage.org/getpro/habr/post_images/74b/4dd/d00/74b4ddd009e67db1b1b6c4467bcf6fa3.png"><br>
<i><font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A file is represented by one or more blocks. Each block is stored on one virtual device. The block size is usually equal to the </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">recordsize</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> parameter </font><font style="vertical-align: inherit;">, but can be reduced to </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2 ^ ashift</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> if it contains metadata or a small file. </font></font></font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/8d1/7fd/ad2/8d17fdad2eda641c801e5e6a302f6e38.png"><br>
<i><font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We really, </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">really are</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> not joking about the huge performance damage if you install too small ashift</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
In the ZFS pool, all data, including metadata, is stored in blocks. The maximum block size for each data set is defined in the property</font></font><code>recordsize</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(record size). The size of the record may vary, but this will not change the size or location of any blocks that have already been written to the dataset - it is valid only for new blocks as they are written.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Unless otherwise specified, the current recording size is 128 KiB by default. This is a kind of difficult compromise in which performance will not be ideal, but not terrible in most cases. </font></font><code>Recordsize</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">can be set to any value from 4K to 1M (with additional settings </font></font><code>recordsize</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">you can set even more, but this is rarely a good idea). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Any block refers to the data of only one file - you cannot squeeze two different files into one block. Each file consists of one or more blocks, depending on the size. If the file size is smaller than the record size, it will be saved in a smaller block ‚Äî for example, a block with a 2 KiB file will occupy only one 4 KiB sector on the disk. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
If the file is large enough and requires several blocks, then all records with this file will have a size</font></font><code>recordsize</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">&nbsp;- including the last record, the main part of which may turn out to be </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">unused space</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Zvol volumes do not have a property </font></font><code>recordsize</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">&nbsp;- instead they have an equivalent property </font></font><code>volblocksize</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sectors</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The last, most basic building block is the sector. This is the smallest physical unit that can be written to or read from the base unit. For several decades, most disks used 512-byte sectors. Recently, most drives are configured for 4 KiB sectors, and in some - especially SSDs - 8 KiB sectors or even more. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ZFS has a property that allows you to manually set the sector size. This is a property </font></font><code>ashift</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. It is somewhat confusing that ashift is a power of two. For example, it </font></font><code>ashift=9</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">means a sector size of 2 ^ 9, or 512 bytes.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ZFS asks the operating system for detailed information about each block device when it is added to the new vdev, and theoretically automatically sets ashift properly based on this information. Unfortunately, many disks lie about their sector size in order to maintain compatibility with Windows XP (which was unable to understand disks with other sector sizes). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
This means that the ZFS administrator is strongly advised to know the actual sector size of their devices and manually install</font></font><code>ashift</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. If too small an ashift is set, then the number of read / write operations astronomically increases. So, writing 512-byte ‚Äúsectors‚Äù to the real 4 KiB sector means writing the first ‚Äúsector‚Äù, then reading the 4 KiB sector, changing it with the second 512-byte ‚Äúsector‚Äù, writing it back to the new 4 KiB sector, and so on for each entry. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In the real world, such a penalty beats Samsung EVO </font></font><code>ashift=13</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SSDs </font><font style="vertical-align: inherit;">, for which it must act </font><font style="vertical-align: inherit;">, but these SSDs lie about their sector size, and therefore it is set by default </font></font><code>ashift=9</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. If an experienced system administrator does not change this setting, then this SSD is </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">slower than a</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> regular magnetic HDD. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
For comparison, for too large a size</font></font><code>ashift</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">there is virtually no penalty. </font><font style="vertical-align: inherit;">There is no real decrease in productivity, and the increase in unused space is infinitely small (or equal to zero with compression enabled). </font><font style="vertical-align: inherit;">Therefore, we strongly recommend that even those drives that really use 512-byte sectors be installed </font></font><code>ashift=12</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">or even </font></font><code>ashift=13</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">to look confidently into the future. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The property is </font></font><code>ashift</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">set for each vdev virtual device, and </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">not for the pool</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , as many mistakenly think - and does not change after installation. </font><font style="vertical-align: inherit;">If you accidentally knocked down </font></font><code>ashift</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">when adding a new vdev to the pool, then you irrevocably contaminated this pool with a low-performance device and, as a rule, there is no other way but to destroy the pool and start all over again. </font><font style="vertical-align: inherit;">Even removing vdev will not save you from a broken setup</font></font><code>ashift</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">!</font></font><br>
<br>
<h3>   </h3><br>
<img src="https://habrastorage.org/getpro/habr/post_images/38b/a1e/4a8/38ba1e4a8fa0e255081ed8db259a302f.gif"><br>
<i><font color="gray">      &nbsp;‚Äî     ,   </font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/90d/4cb/a35/90d4cba35ffa5a3e44a7ca5f61d4491b.gif"><br>
<i><font color="gray">         ,     </font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/c8b/2af/ffb/c8b2afffbc46f63f6a7fe1167edf5dcb.gif"><br>
<i><font color="gray">  ,      ,   ¬´ ¬ª   ¬´ ¬ª,        </font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/4c1/e2b/818/4c1e2b818077cb07d651527e214363fe.gif"><br>
<i><font color="gray">     ,       ‚Äî      ,     ,       </font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Copy on Write (CoW) is the fundamental foundation of what makes ZFS so awesome. The basic concept is simple - if you ask the traditional file system to modify the file, it will do exactly what you requested. If you ask the file system with copying during recording to do the same, it will say ‚Äúgood‚Äù - but it will lie to you. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Instead, the copy-write file system writes the new version of the modified block, and then updates the file metadata to break the link with the old block and associate the new block you just wrote to it.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Disconnecting the old unit and linking the new one is done in one operation, so it cannot be interrupted - if you reset the power after this happens, you have a new version of the file, and if you reset the power earlier, then you have the old version. </font><font style="vertical-align: inherit;">In any case, there will be no conflict in the file system. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Copying when writing to ZFS takes place not only at the file system level, but also at the disk management level. </font><font style="vertical-align: inherit;">This means that ZFS is not subject to a space in the record (a </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">hole in the RAID</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ) - a phenomenon when the strip only managed to partially record before the system crashed, with the array damaged after a reboot. </font><font style="vertical-align: inherit;">Here the strip is atomic, vdev is always consistent, and </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bob is your uncle</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ZIL: ZFS Intent Log</font></font></h3><br>
<img src="https://habrastorage.org/getpro/habr/post_images/567/71c/73f/56771c73f9a28ebaed161e02313deadb.png"><br>
<i><font color="gray"> ZFS     &nbsp;‚Äî  ,      ZIL,            </font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/cec/7c5/437/cec7c5437087f6816f9cdea5f6829820.png"><br>
<i><font color="gray"> ,   ZIL,    .      </font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/075/65a/d6b/07565ad6b2f431db3e6bc20cd24a653b.png"><br>
<i><font color="gray">SLOG,   LOG-, ‚Äî   &nbsp;‚Äî , ,  &nbsp;‚Äî&nbsp;vdev,  ZIL      </font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/927/0f7/539/9270f7539b759aa37896d41e04c4ec47.png"><br>
<i><font color="gray">      ZIL &nbsp;‚Äî    ZIL   SLOG,      </font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
There are two main categories of write operations - synchronous (sync) and asynchronous (async). For most workloads, the vast majority of write operations are asynchronous - the file system allows you to aggregate them and deliver them in batches, reducing fragmentation and significantly increasing throughput. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Synchronous recordings are a completely different matter. When an application requests a synchronous write, it tells the file system: ‚ÄúYou need to commit this to non-volatile memory </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">right now</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , and until then I can do nothing more.‚Äù Therefore, synchronous recordings should be immediately committed to disk - and if that increases fragmentation or reduces bandwidth, then so be it.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ZFS processes synchronous records differently than regular file systems - instead of immediately uploading them to regular storage, ZFS records them in a special storage area called the ZFS intent log - ZFS Intent Log, or ZIL. The trick is that these records </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">also</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> remain in memory, being aggregated along with regular asynchronous write requests, to later be dumped into storage as perfectly normal TXGs (Transaction Groups, Transaction Groups). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In normal operation, ZIL is recorded and never read again. When, after a few moments, recordings from ZIL are fixed in the main storage in ordinary TXG from RAM, they are disconnected from ZIL. The only thing when something is read from ZIL is when importing the pool.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
If ZFS crashes - operating system crashes or power outages - when there is data in ZIL, this data will be read during the next pool import (for example, when the emergency system restarts). Everything that is in the ZIL will be read, combined into TXG groups, committed to the main storage, and then disconnected from the ZIL during the import process. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
One of the vdev helper classes is called LOG or SLOG, the secondary LOG device. He has one task - to provide the pool with a separate and, preferably, much faster, with very high write resistance, vdev device for storing ZIL, instead of storing ZIL in the main vdev storage. ZIL itself behaves the same regardless of the storage location, but if vdev with LOG has very high write performance, then synchronous writes will be faster.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Adding vdev with LOG to the pool </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">cannot</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> improve asynchronous write performance - even if you force all writes to ZIL using </font></font><code>zfs set sync=always</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, they will still be tied to the main repository in TXG in the same way and at the same pace as without a log. </font><font style="vertical-align: inherit;">The only direct performance improvement is the delay in synchronous recording (since higher log speed speeds up operations </font></font><code>sync</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
However, in an environment that already requires a large number of synchronous writes, vdev LOG can indirectly speed up asynchronous writes and uncached reads. </font><font style="vertical-align: inherit;">Uploading ZIL records to a separate vdev LOG means less competition for IOPS in the primary storage, which to some extent improves the performance of all read and write operations.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Snapshots</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The write copy mechanism is also an essential foundation for atomic ZFS snapshots and incremental asynchronous replication. </font><font style="vertical-align: inherit;">The active file system has a pointer tree that marks all records with current data - when you take a snapshot, you simply make a copy of this pointer tree. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
When a record is overwritten in the active file system, ZFS first writes the new version of the block to unused space. </font><font style="vertical-align: inherit;">It then detaches the old version of the block from the current file system. </font><font style="vertical-align: inherit;">But if some snapshot refers to the old block, it still remains unchanged. </font><font style="vertical-align: inherit;">The old block will not actually be restored as free space until all snapshots that link to this block are destroyed!</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Replication</font></font></h3><br>
<img src="https://habrastorage.org/getpro/habr/post_images/e69/167/01d/e6916701d4aa3ff27bb42efc43be60da.png"><br>
<i><font color="gray">  Steam  2015   158&nbsp;   126&nbsp;927 .        rsync&nbsp;‚Äî  ZFS    ¬´ ¬ª  750% .</font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/25f/376/0ab/25f3760ab64d6647571b9c02804b39f0.png"><br>
<i><font color="gray">      40-     Windows 7&nbsp;‚Äî   .  ZFS   289  ,  rsync&nbsp;‚Äî  ¬´¬ª  161  ,    ,   rsync   --inplace.</font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/776/46b/a3a/77646ba3ac20eeb0933d7dc7d644296c.png"><br>
<i><font color="gray">    ,  rsync    .  1,9         &nbsp;‚Äî    ,   ZFS   1148  ,  rsync,    rsync --inplace</font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Once you understand how snapshots work, it‚Äôs easy to grasp the essence of replication. Since a snapshot is just a tree of pointers to records, it follows that if we make a </font></font><code>zfs send</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">snapshot, then we send this tree and all the records associated with it. When we pass this </font></font><code>zfs send</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">in </font></font><code>zfs receive</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">to the target object, it writes both the actual contents of the block and the tree of pointers that reference the blocks to the target data set. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Everything becomes even more interesting in the second </font></font><code>zfs send</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Now we have two systems, each of which contains </font></font><code>poolname/datasetname@1</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, and you shoot a new snapshot </font></font><code>poolname/datasetname@2</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Therefore, in the source pool you have </font></font><code>datasetname@1</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">and </font></font><code>datasetname@2</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, and in the target pool so far only the first snapshot </font></font><code>datasetname@1</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Since we have a common snapshot between the source and the target</font></font><code>datasetname@1</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, we can do </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">incremental</font></font></i> <code>zfs send</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> on top of it. When we tell the system </font></font><code>zfs send -i poolname/datasetname@1 poolname/datasetname@2</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, it compares two pointer trees. Any pointers that exist only in </font></font><code>@2</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, obviously, refer to new blocks - so we need the contents of these blocks. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
On a remote system, incremental processing is </font></font><code>send</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">just as simple. First, we record all the new entries included in the stream </font></font><code>send</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, and then add pointers to these blocks. Voila, in our </font></font><code>@2</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">new system! </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ZFS asynchronous incremental replication is a huge improvement over earlier non-snapshot methods like rsync. In both cases, only changed data is transmitted - but rsync must first </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">read</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">from the disk all the data on both sides to check the amount and compare it. </font><font style="vertical-align: inherit;">In contrast, ZFS replication reads nothing but pointer trees - and any blocks that are not represented in the general snapshot.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Inline compression</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The copy-on-write mechanism also simplifies the built-in compression system. In a traditional file system, compression is problematic - both the old version and the new version of the changed data are in the same space. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
If you consider a piece of data in the middle of a file that begins its life as a megabyte of zeros from 0x00000000 and so on - it is very easy to compress it to one sector on the disk. But what happens if we replace this megabyte of zeros with a megabyte of incompressible data such as JPEG or pseudo-random noise? Suddenly, this megabyte of data will require not one, but 256 sectors of 4 KiB, and in this place on the disk only one sector is reserved.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ZFS does not have such a problem, since changed records are always written to unused space - the original block occupies only one 4 KiB sector, and a new record will take 256, but this is not a problem - a recently changed fragment from the middle of the file would be written into unused space regardless of whether its size has changed or not, so for ZFS this is a normal situation. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Built-in ZFS compression is disabled by default, and the system offers plug-in algorithms - now among them are LZ4, gzip (1-9), LZJB and ZLE.</font></font><br>
<br>
<ul>
<li><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">LZ4</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> is a streaming algorithm that offers extremely fast compression and decompression and performance gains for most use cases - even on fairly slow CPUs.</font></font><br>
</li>
<li><b>GZIP</b> ‚Äî  ,       Unix-.        1-9,       CPU      9.       (   )  ,    &nbsp;   c CPU&nbsp;‚Äî    ,     .<br>
</li>
<li><b>LZJB</b> ‚Äî    ZFS.       , LZ4     .<br>
</li>
<li><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ZLE</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - zero level encoding, Zero Level Encoding. </font><font style="vertical-align: inherit;">It does not touch normal data at all, but compresses large sequences of zeros. </font><font style="vertical-align: inherit;">Useful for completely incompressible data sets (for example, JPEG, MP4, or other already compressed formats), since it ignores incompressible data, but compresses unused space in the resulting records.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We recommend LZ4 compression for almost all use cases; </font><font style="vertical-align: inherit;">The performance penalty for encountering incompressible data is very small, and the </font><font style="vertical-align: inherit;">performance </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">gain</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> for typical data is significant. </font><font style="vertical-align: inherit;">Copying a virtual machine image for a new installation of the Windows operating system (freshly installed OS, no data inside yet) with </font></font><code>compression=lz4</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">passed 27% faster than with </font></font><code>compression=none</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, in </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">this 2015 test</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ARC - adaptive replacement cache</font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ZFS is the only modern file system known to us that uses its own read caching mechanism, and does not rely on the operating system page cache to store copies of recently read blocks in RAM. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Although its own cache is not without its problems - ZFS cannot respond to new memory allocation requests as fast as the kernel, so a new </font></font><code>malloc()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">memory allocation </font><font style="vertical-align: inherit;">call </font><font style="vertical-align: inherit;">may fail if it needs RAM currently occupied by ARC. </font><font style="vertical-align: inherit;">But there are good reasons to use your own cache, at least for now.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
All well-known modern operating systems, including MacOS, Windows, Linux and BSD, use the LRU algorithm (Least Recently Used) to implement the page cache. This is a primitive algorithm that raises the cached block ‚Äúup the queue‚Äù after each reading and pushes the ‚Äúdown queue‚Äù blocks as necessary to add new cache misses (blocks that should have been read from disk, not from the cache) up. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Usually the algorithm works fine, but on systems with large working datasets, LRU easily leads to thrashing - crowding out frequently needed blocks to make room for blocks that will never be read from the cache again. </font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ARC</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">&nbsp;- a much less naive algorithm, which can be considered as a "weighted" cache. After each reading of the cached block, it becomes a little ‚Äúheavier‚Äù and it becomes harder to crowd out - and even after crowding out the block is </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">tracked</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> for a certain period of time. A block that has been squeezed out but then needs to be read back to the cache will also become ‚Äúheavier‚Äù.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The end result of all this is a cache with a much larger hit ratio ‚Äî the ratio between hits in the cache (read from the cache) and misses (read from disk). </font><font style="vertical-align: inherit;">This is extremely important statistics - not only do the cache hits themselves take orders of magnitude faster, cache misses can also be served faster, because the more cache hits, the fewer concurrent disk requests and the less the delay for those remaining misses that should be served with drive.</font></font><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Conclusion</font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
After studying the basic semantics of ZFS ‚Äî how copying works when writing, as well as the relationships between storage pools, virtual devices, blocks, sectors, and files ‚Äî we are ready to discuss real performance with real numbers. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In the next part, we will look at the actual performance of pools with mirrored vdev and RAIDz, in comparison with each other, as well as in comparison with traditional Linux kernel RAID topologies, which we examined </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">earlier</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
At first we wanted to consider only the basics - the ZFS topologies themselves - but after </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">this</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> we will be ready to talk about more advanced ZFS tuning and tuning, including the use of auxiliary vdev types such as L2ARC, SLOG and Special Allocation.</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../en504680/index.html">Overview of SpaL's NLP Library</a></li>
<li><a href="../en504682/index.html">Nostalgia Post: j2me, Gravity Defied, 64kb</a></li>
<li><a href="../en504686/index.html">How to draw a cat</a></li>
<li><a href="../en504688/index.html">Masks are useless: scientific criticism of social policy at KOVID-19</a></li>
<li><a href="../en504690/index.html">The Tale of How I Configured Azure AD B2C on React and React Native Part 3 (Tutorial)</a></li>
<li><a href="../en504694/index.html">How to compile a decorator - C ++, Python and its own implementation. Part 1</a></li>
<li><a href="../en504696/index.html">News from the world of OpenStreetMap No. 513 (12.05.2020-18.05.2020)</a></li>
<li><a href="../en504698/index.html">Onboarding on a remote site</a></li>
<li><a href="../en504700/index.html">Soviet graphic tablet "sketch"</a></li>
<li><a href="../en504702/index.html">People do not want to know English</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>