<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🏩 👦 🎡 Machine learning on R: expert techniques for predictive analysis 👩🏿‍🤝‍👩🏾 ❌ 😠</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello, habrozhiteli! The R language offers a powerful set of machine learning methods that allow you to quickly conduct non-trivial analysis of your d...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Machine learning on R: expert techniques for predictive analysis</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/496256/"><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><img src="https://habrastorage.org/webt/lf/4u/ao/lf4uaojxjnbk8d6no9t-3j-v7ww.jpeg" align="left" alt="image"></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hello, habrozhiteli! The R language offers a powerful set of machine learning methods that allow you to quickly conduct non-trivial analysis of your data. The book is a guide that will help apply machine learning methods to solve everyday problems. Brett Lanz will teach you everything you need for data analysis, forecasting and data visualization. Here you will find information on new and improved libraries, tips on the ethical aspects of machine learning and bias issues, as well as in-depth training.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In this book - Fundamentals of machine learning and features of computer training on examples. </font><font style="vertical-align: inherit;">- Preparation of data for use in machine learning by means of the language R. - Classification of the significance of the results. </font><font style="vertical-align: inherit;">- Prediction of events using decision trees, rules and reference vectors. </font><font style="vertical-align: inherit;">- Prediction of numerical data and assessment of financial data using regression methods. </font><font style="vertical-align: inherit;">- Modeling complex processes using neural networks is the foundation of deep learning. </font><font style="vertical-align: inherit;">- Evaluation of models and improving their performance. </font><font style="vertical-align: inherit;">- The latest technologies for processing big data, in particular R 3.6, Spark, H2O and TensorFlow.</font></font><br>
<a name="habracut"></a><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Who is the book for?</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The book is intended for those who expect to use data in a specific area. </font><font style="vertical-align: inherit;">You may already be a little familiar with machine learning, but you have never worked with the R language; </font><font style="vertical-align: inherit;">or, conversely, you know a little about R, but almost do not know about machine learning. </font><font style="vertical-align: inherit;">In any case, this book will help you get started quickly. </font><font style="vertical-align: inherit;">It would be useful to refresh a little the basic concepts of mathematics and programming, but no prior experience would be required. </font><font style="vertical-align: inherit;">All you need is a desire to learn.</font></font><br>
<br>
<div class="spoiler"><b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">What will you read in the publication</font></font></b><div class="spoiler_text"> 1 «   »    ,          ,      ,   ,     .<br>
<br>
 2 «    »            R.         ,   ,    .<br>
<br>
 3 « :      »      ,           :     .<br>
<br>
 4 « :      »     ,       .    ,      .<br>
<br>
 5 «  :       »    ,     ,    .      ,    .<br>
<br>
 6 «  :  »     ,    .        ,     ,     .<br>
<br>
 7 « “ ”:      »   ,     .        ,    ,    .<br>
<br>
 8 « :       »  ,    ,      .   -   ,         ,   ,      .<br>
<br>
 9 «  :   k-»      .         -.<br>
<br>
 10 «  »    ,                  .<br>
<br>
 11 «  »  ,  ,        .                ,         .<br>
<br>
 12 «   »    :        R.  ,        ,       R.<br>
</div></div><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Example: modeling concrete strength using a neural network</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In the field of civil engineering, it is extremely important to have accurate estimates of the effectiveness of building materials. </font><font style="vertical-align: inherit;">These assessments are necessary to develop safety rules governing the use of materials in the construction of buildings, bridges and roads. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Of particular interest is the assessment of concrete strength. </font><font style="vertical-align: inherit;">Concrete is used in almost any construction, the performance characteristics of concrete are very different, since it consists of a huge number of ingredients that interact in a complex. </font><font style="vertical-align: inherit;">As a result, it is difficult to say exactly what the strength of the finished product will be. </font><font style="vertical-align: inherit;">A model that would allow determining the strength of concrete for sure, taking into account the composition of the starting materials, could provide a higher level of safety for construction sites.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Step 1. Data collection</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
For this analysis, we will use the concrete compressive strength data provided by I-Cheng Yeh to the UCI Machine Learning Repository (http://archive.ics.uci.edu/ml). </font><font style="vertical-align: inherit;">Since Ai-Cheng Ye successfully used neural networks to model this data, we will try to reproduce his work by applying a simple model of a neural network in R.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Judging by the site, this dataset contains 1030 records about different brands of concrete with eight characteristics that describe the components used in the composition of the concrete mix. </font><font style="vertical-align: inherit;">It is believed that these characteristics affect the final compressive strength. </font><font style="vertical-align: inherit;">These include: the amount (in kilograms per cubic meter) of cement, water, various additives, large and small aggregates such as crushed stone and sand used in the finished product, as well as setting time (in days).</font></font><br>
<br>
<blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">To run this example, download the concrete.csv file and save it in the R working directory.</font></font></blockquote><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Step 2. Research and data preparation</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As usual, we start the analysis by loading the data into the R-object using the read.csv () function and make sure that the result corresponds to the expected structure:</font></font><br>
<br>
<pre><code class="java hljs">&gt; concrete &lt;- read.csv(<span class="hljs-string">"concrete.csv"</span>)<font></font>
&gt; str(concrete)<font></font>
<span class="hljs-string">'data.frame'</span>:        <span class="hljs-number">1030</span> obs. of <span class="hljs-number">9</span> variables:<font></font>
$ cement       : num <span class="hljs-number">141</span> <span class="hljs-number">169</span> <span class="hljs-number">250</span> <span class="hljs-number">266</span> <span class="hljs-number">155</span> ...<font></font>
$ slag            : num <span class="hljs-number">212</span> <span class="hljs-number">42.2</span> <span class="hljs-number">0</span> <span class="hljs-number">114</span> <span class="hljs-number">183.4</span> ...<font></font>
$ ash             : num <span class="hljs-number">0</span> <span class="hljs-number">124.3</span> <span class="hljs-number">95.7</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span> ...<font></font>
$ water          : num <span class="hljs-number">204</span> <span class="hljs-number">158</span> <span class="hljs-number">187</span> <span class="hljs-number">228</span> <span class="hljs-number">193</span> ...<font></font>
$ superplastic : num <span class="hljs-number">0</span> <span class="hljs-number">10.8</span> <span class="hljs-number">5.5</span> <span class="hljs-number">0</span> <span class="hljs-number">9.1</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">6.4</span> <span class="hljs-number">0</span> <span class="hljs-number">9</span> ...<font></font>
$ coarseagg    : num <span class="hljs-number">972</span> <span class="hljs-number">1081</span> <span class="hljs-number">957</span> <span class="hljs-number">932</span> <span class="hljs-number">1047</span> ...<font></font>
$ fineagg        : num <span class="hljs-number">748</span> <span class="hljs-number">796</span> <span class="hljs-number">861</span> <span class="hljs-number">670</span> <span class="hljs-number">697</span> ...<font></font>
$ age             : <span class="hljs-keyword">int</span> <span class="hljs-number">28</span> <span class="hljs-number">14</span> <span class="hljs-number">28</span> <span class="hljs-number">28</span> <span class="hljs-number">28</span> <span class="hljs-number">90</span> <span class="hljs-number">7</span> <span class="hljs-number">56</span> <span class="hljs-number">28</span> <span class="hljs-number">28</span> ...<font></font>
$ strength      : num <span class="hljs-number">29.9</span> <span class="hljs-number">23.5</span> <span class="hljs-number">29.2</span> <span class="hljs-number">45.9</span> <span class="hljs-number">18.3</span> ...</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nine variables in the data frame correspond to eight characteristics and one expected result, but it became obvious that there is a problem. </font><font style="vertical-align: inherit;">Neural networks work best when the input data is scaled to a narrow range centered around 0, and here we see values ​​in the range from 0 to more than 1000.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Typically, the solution to this problem is to scale the data using the normalization or standardization function. If the data distribution corresponds to a bell-shaped curve (normal distribution, see chapter 2), then it may make sense to use standardization using the built-in scale () function. If the data distribution is close to uniform or very different from normal, then normalization to the range from 0 to 1 may be more suitable. In this case, we will use the latter option. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In chapter 3, we created our own normalize () function:</font></font><br>
<br>
<pre><code class="java hljs">&gt; normalize &lt;- function(x) {
       <span class="hljs-keyword">return</span>((x - min(x)) / (max(x) — min(x)))<font></font>
}</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
After this code is executed, you can apply the normalize () function to all columns of the selected data frame using the lapply () function: </font></font><br>
<br>
<code>&gt; concrete_norm &lt;- as.data.frame(lapply(concrete, normalize))</code><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
To verify that the normalization has worked, you can check whether the minimum and maximum values ​​of the strength attribute are 0 and 1, respectively:</font></font><br>
<br>
<pre><code class="java hljs">&gt; summary(concrete_norm$strength)<font></font>
       Min.     <span class="hljs-number">1</span>st Qu.         Median     Mean      <span class="hljs-number">3</span>rd Qu.      Max.
   <span class="hljs-number">0.0000</span>     <span class="hljs-number">0.2664</span>         <span class="hljs-number">0.4001</span>  <span class="hljs-number">0.4172</span>      <span class="hljs-number">0.5457</span>   <span class="hljs-number">1.0000</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
For comparison: the initial minimum and maximum values ​​of this attribute were 2.33 and 82.60, respectively:</font></font><br>
<br>
<pre><code class="java hljs">&gt; summary(concrete$strength)<font></font>
     Min.       <span class="hljs-number">1</span>st Qu.     Median       Mean      <span class="hljs-number">3</span>rd Qu.       Max.
    <span class="hljs-number">2.33</span>         <span class="hljs-number">23.71</span>       <span class="hljs-number">34.44</span>      <span class="hljs-number">35.82</span>        <span class="hljs-number">46.14</span>      <span class="hljs-number">82.60</span></code></pre><br>
<blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Any conversion applied to the data before training the model should subsequently be applied in the reverse order to convert the attribute back to the original units. </font><font style="vertical-align: inherit;">To facilitate scaling, it is advisable to save the source data, or at least a summary of the statistics of the source data.</font></font></blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Following the scenario described by Ye in the original article, we will divide the data into a training set, which includes 75% of all examples, and a test set, consisting of 25%. </font><font style="vertical-align: inherit;">The CSV file used is sorted in random order, so we can only divide it into two parts: </font><font style="vertical-align: inherit;">
We will use a training dataset to build a neural network and a test dataset to assess how well the model generalizes for future results. </font><font style="vertical-align: inherit;">Since the neural network is easily brought to a state of retraining, this step is very important.</font></font><br>
<br>
<code>&gt; concrete_train &lt;- concrete_norm[1:773, ]<br>
&gt; concrete_test &lt;- concrete_norm[774:1030, ]</code><br>
<br><font style="vertical-align: inherit;"></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Step 3. Training the model on data</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
To model the relationship between the ingredients used in concrete production and the strength of the finished product, we will build a multilayer direct distribution neural network. The neuralnet package, developed by Stefan Fritsch and Frauke Guenther, provides a standard and easy-to-use implementation of such networks. This package also includes a function for building a network topology. Implementing neuralnet is a good way to get additional information about neural networks, although this does not mean that it cannot be used to do real work either - as you will soon see, it is a rather powerful tool.</font></font><br>
<br>
<blockquote> R    ,       ,        .  nnet      R,  , ,      .       ,      .    —  RSNNS,       ,     ,     .</blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Since the neuralnet package is not included in the base R, you will need to install it by typing install.packages ("neuralnet") and download it using the library (neuralnet) command. </font><font style="vertical-align: inherit;">The neuralnet () function in the package can be used to train neural networks in numerical prediction using the following syntax.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Neural network syntax</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Using the neuralnet () function from the neuralnet package </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Building a model:</font></font><br>
<br>
<pre><code class="java hljs">m &lt;- neuralnet(target ~ predictors, data = mydata,<font></font>
                       hidden = <span class="hljs-number">1</span>, act.fct = <span class="hljs-string">"logistic"</span>)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
• target - a model that will be built as a result of training on the mydata data frame; </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
• predictors - R-formula that determines the characteristics from the mydata data frame to be used in forecasting; </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
• data - data frame to which target and predictors belong; </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
• hidden - the number of neurons in the hidden layer (default is 1). Note: to describe several hidden layers, a vector of integers is used, for example, c (2, 2); </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
• act.fct - activation function: "logistic" or "tanh". Note: any other differentiable function can also be used. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The function returns a neural network object that can be used for forecasting. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Prediction:</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
p &lt;- compute (m, test)</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
• m - model trained using the neuralnet () function; </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
• test - a data frame containing test data with the same characteristics as the training data used to construct the classifier. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The function returns a list consisting of two components: $ neurons, where neurons are stored for each network layer, and $ net.result, where the values ​​predicted using this model are stored.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Examples:</font></font></h4><br>
<br>
<pre><code class="java hljs">concrete_model &lt;- neuralnet(strength ~ cement + slag + ash,<font></font>
      data = concrete, hidden = c(<span class="hljs-number">5</span>, <span class="hljs-number">5</span>), act.fct = <span class="hljs-string">"tanh"</span>)<font></font>
model_results &lt;- compute(concrete_model, concrete_data)<font></font>
strength_predictions &lt;- model_results$net.result</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Let's start by training the simplest multi-level direct distribution network with default parameters, which has only one hidden node:</font></font><br>
<br>
<pre><code class="java hljs">&gt; concrete_model &lt;- neuralnet(strength ~ cement + slag<font></font>
         + ash + water + superplastic + coarseagg + fineagg + age,<font></font>
         data = concrete_train)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Then, as shown in fig. </font><font style="vertical-align: inherit;">7.11, you can visualize the network topology using the plot () function and passing it the resulting model object:</font></font><br>
<br>
<code>&gt; plot(concrete_model)</code><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/mt/qh/og/mtqhogfq-pcptln4uzbl9ofb8po.png" alt="image"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In this simple model, there is one input node for each of the eight features, then there is one hidden and one output node, which gives a forecast of concrete strength. </font><font style="vertical-align: inherit;">The diagram also shows the weights for each connection and the offset value indicated for the nodes marked with the number 1. The offset value is a numerical constant that allows you to shift the value in the specified node up or down, approximately like a shift in a linear equation.</font></font><br>
<br>
<blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A neural network with one hidden node can be considered the “cousin” of the linear regression models discussed in Chapter 6. The weights between the input nodes and the hidden node are similar to beta coefficients, and the offset weight is like a shift.</font></font><br>
</blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
At the bottom of the figure, the number of training steps and the magnitude of the error are displayed - the total mean square error (Sum of Squared Errors, SSE), which, as expected, is the sum of the squared differences between the predicted and actual values. </font><font style="vertical-align: inherit;">The smaller the SSE, the more accurately the model matches the training data, which indicates the effectiveness of these data, but says little about how the model will work with unknown data.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Step 4. Assessing the effectiveness of the model</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The network topology diagram provides an opportunity to look into the “black box” of a neural network, but it does not provide much information on how well the model matches future data. To generate forecasts on a test data set, you can use the compute () </font></font><br>
<br>
<code>&gt; model_results &lt;- compute(concrete_model, concrete_test[1:8])</code><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
function </font><font style="vertical-align: inherit;">: </font><font style="vertical-align: inherit;">The compute () function works a little differently than the predict () functions that we have used so far. It returns a list consisting of two components: $ neurons, where neurons are stored for each network layer, and $ net.result, where predicted values ​​are stored. It is $ net.result that we need:</font></font><br>
<br>
<code>&gt; predicted_strength &lt;- model_results$net.result</code><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Since we have the task of numerical forecasting, and not classification, we cannot use the matrix of inconsistencies to verify the accuracy of the model. </font><font style="vertical-align: inherit;">We measure the correlation between the predicted and the true value of concrete strength. </font><font style="vertical-align: inherit;">If the predicted and actual values ​​will strongly correlate, then, probably, the model will be useful for determining the strength of concrete. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Let me remind you that to obtain the correlation between two numerical vectors, the cor () function is used:</font></font><br>
<br>
<pre><code class="java hljs">&gt; cor(predicted_strength, concrete_test$strength)<font></font>
                    [,<span class="hljs-number">1</span>]<font></font>
[<span class="hljs-number">1</span>,] <span class="hljs-number">0.8064655576</span></code></pre><br>
<blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Do not be alarmed if your result differs from ours. </font><font style="vertical-align: inherit;">Since the neural network starts working with random weights, the predictions presented in the book may be different for different models. </font><font style="vertical-align: inherit;">If you want to accurately match the results, try the set.seed (12345) command before you start building a neural network.</font></font></blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
If the correlation is close to 1, this indicates a strong linear relationship between the two variables. </font><font style="vertical-align: inherit;">Therefore, a correlation of approximately 0.806 indicates a rather strong relationship. </font><font style="vertical-align: inherit;">This means that the model works quite well even with a single hidden node. </font><font style="vertical-align: inherit;">Given that we used only one hidden node, it is likely that we can improve the efficiency of the model, which we will try to do.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Step 5. Improving Model Efficiency</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Since networks with a more complex topology are able to study more complex concepts, let's see what happens if you increase the number of hidden nodes to five. </font><font style="vertical-align: inherit;">We will use the neuralnet () function, as before, but add the hidden = 5 parameter:</font></font><br>
<br>
<pre><code class="java hljs">&gt; concrete_model2 &lt;- neuralnet(strength ~ cement + slag +<font></font>
                                               ash + water + superplastic +<font></font>
                                               coarseagg + fineagg + age,<font></font>
                                               data = concrete_train, hidden = <span class="hljs-number">5</span>)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Having built the network diagram again (Fig. 7.12), we will see a sharp increase in the number of connections. How has this affected efficiency? </font></font><br>
<br>
<code>&gt; plot(concrete_model2)</code><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Please note that the resulting error (again measured as SSE) decreased from 5.08 in the previous model to 1.63. In addition, the number of training stages increased from 4882 to 86,849 - which is not surprising, given how complicated the model is. The more complex the network, the more iterations are required to find the optimal weights. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Applying the same steps to compare the predicted values ​​with the true ones, we get a correlation of about 0.92, which is much better compared to the previous result of 0.80 for a network with one hidden node:</font></font><br>
<br>
<pre><code class="java hljs">&gt; model_results2 &lt;- compute(concrete_model2, concrete_test[<span class="hljs-number">1</span>:<span class="hljs-number">8</span>])<font></font>
&gt; predicted_strength2 &lt;- model_results2$net.result<font></font>
&gt; cor(predicted_strength2, concrete_test$strength)<font></font>
                  [,<span class="hljs-number">1</span>]<font></font>
[<span class="hljs-number">1</span>,] <span class="hljs-number">0.9244533426</span></code></pre><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/x2/57/55/x25755f5wjbx5meuttyzyv0fczi.png" alt="image"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Despite significant improvements, you can go even further to increase the effectiveness of the model. In particular, it is possible to introduce additional hidden layers and change the network activation function. By making these changes, we are laying the foundations for building a simple deep neural network. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The choice of activation function is very important for deep learning. The best function for a particular learning task is usually found experimentally, and then is widely used by the community of machine learning researchers.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Recently, the activation function, called the distillation function, or rectifier, has become very popular due to its successful application in complex tasks, such as image recognition. A neural network node in which a rectifier is used as an activation function is called a Rectified Linear Unit (ReLU). As shown in fig. 7.13, the rectifier type activation function is described in such a way that returns x if x is greater than or equal to 0, and 0 otherwise. The importance of this function is that, on the one hand, it is non-linear, and on the other, it has simple mathematical properties that make it computationally inexpensive and highly efficient for gradient descent. Unfortunately, for x = 0, the rectifier derivative is not defined,therefore, the rectifier cannot be used in conjunction with the neuralnet () function.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Instead, you can use a smoothed approximation of ReLU called softplus or SmoothReLU, an activation function defined as log (1 + ex). </font><font style="vertical-align: inherit;">As shown in fig. </font><font style="vertical-align: inherit;">7.13, the softplus function is close to zero for x values ​​less than 0 and approximately equal to x for x greater than 0.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/k9/us/lu/k9uslumivm12vadxdm5d__dhzoq.png" alt="image"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
To define the softplus () function in R, we use the following code: </font></font><br>
<br>
<code>&gt; softplus &lt;- function(x) { log(1 + exp(x)) }</code><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Such an activation function can be provided to the input neuralnet () using the act.fct parameter. </font><font style="vertical-align: inherit;">In addition, we add a second hidden layer consisting of five nodes, assigning the hidden parameter the value of the integer vector c (5, 5). </font><font style="vertical-align: inherit;">As a result, we get a two-layer network, each of the layers of which has five nodes, and all of them use the softplus activation function:</font></font><br>
<br>
<pre><code class="java hljs">&gt; set.seed(<span class="hljs-number">12345</span>)<font></font>
&gt; concrete_model3 &lt;- neuralnet(strength ~ cement + slag +<font></font>
                                               ash + water + superplastic +<font></font>
                                               coarseagg + fineagg + age,<font></font>
                                               data = concrete_train,<font></font>
                                               hidden = c(<span class="hljs-number">5</span>, <span class="hljs-number">5</span>),<font></font>
                                               act.fct = softplus)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As before, the network can be visualized (Fig. 7.14):</font></font><br>
<br>
<code>&gt; plot(concrete_model3)</code><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/yw/yy/om/ywyyomaklvohzlzh-jp2ljggypi.png" alt="image"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The correlation between the predicted and actual strength of concrete can be calculated as follows:</font></font><br>
<br>
<pre><code class="java hljs">&gt; model_results3 &lt;- compute(concrete_model3, concrete_test[<span class="hljs-number">1</span>:<span class="hljs-number">8</span>])<font></font>
&gt; predicted_strength3 &lt;- model_results3$net.result<font></font>
&gt; cor(predicted_strength3, concrete_test$strength)<font></font>
                  [,<span class="hljs-number">1</span>]<font></font>
[<span class="hljs-number">1</span>,] <span class="hljs-number">0.9348395359</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The correlation between the predicted and the actual strength was 0.935, which is the best indicator obtained so far. Interestingly, in the original publication, Ye reported a correlation of 0.885. This means that, with relatively little effort, we were able to get a comparable result and even surpass the results of an expert in this field. True, the results of Ye were published in 1998, which gave us a head start on more than 20 years of additional research in the field of neural networks! </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Another important detail should be taken into account: since we normalized the data before training the model, the forecasts are also in the normalized interval from 0 to 1. For example, the following code shows a data frame that compares line-by-line the concrete strength values ​​from the initial data set with the corresponding forecasts:</font></font><br>
<br>
<pre><code class="java hljs">&gt; strengths &lt;- data.frame(<font></font>
      actual = concrete$strength[<span class="hljs-number">774</span>:<span class="hljs-number">1030</span>],<font></font>
      pred = predicted_strength3<font></font>
   )<font></font>
&gt; head(strengths, n = <span class="hljs-number">3</span>)<font></font>
      actual        pred<font></font>
<span class="hljs-number">774</span> <span class="hljs-number">30.14</span> <span class="hljs-number">0.2860639091</span>
<span class="hljs-number">775</span> <span class="hljs-number">44.40</span> <span class="hljs-number">0.4777304648</span>
<span class="hljs-number">776</span> <span class="hljs-number">24.50</span> <span class="hljs-number">0.2840964250</span></code></pre><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Examining the correlation, we see that the choice of normalized or abnormalized data does not affect the calculated performance statistics - just like before, the correlation is 0.935: </font><font style="vertical-align: inherit;">
But if we calculated another performance indicator, for example, the absolute difference between the predicted and actual values, then the choice of scale would be very important. </font><font style="vertical-align: inherit;">
With this in mind, you can create the unnormalize () function that would perform the reverse of minimax normalization and would allow you to convert normalized forecasts to the original scale:</font></font><br>
<br>
<code>&gt; cor(strengths$pred, strengths$actual)<br>
[1] 0.9348395359</code><br>
<br><font style="vertical-align: inherit;"></font><br>
<br><font style="vertical-align: inherit;"></font><br>
<br>
<pre><code class="java hljs">&gt; unnormalize &lt;- function(x) {
     <span class="hljs-keyword">return</span>((x * (max(concrete$strength)) -<font></font>
           min(concrete$strength)) + min(concrete$strength))<font></font>
   }</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
After applying the unnormalize () function that we wrote to forecasts, it becomes clear that the scale of the new forecasts is similar to the initial values ​​of concrete strength. </font><font style="vertical-align: inherit;">This allows you to calculate the meaningful value of the absolute error. </font><font style="vertical-align: inherit;">In addition, the correlation between abnormal and initial strength values ​​remains unchanged:</font></font><br>
<br>
<pre><code class="java hljs">&gt; strengths$pred_new &lt;- unnormalize(strengths$pred)<font></font>
&gt; strengths$error &lt;- strengths$pred_new — strengths$actual<font></font>
&gt; head(strengths, n = <span class="hljs-number">3</span>)<font></font>
           actual                pred             pred_new                    error<font></font>
<span class="hljs-number">774</span>          <span class="hljs-number">30.14</span>         <span class="hljs-number">0.2860639091</span>               <span class="hljs-number">23.62887889</span>      -<span class="hljs-number">6.511121108</span>
<span class="hljs-number">775</span>          <span class="hljs-number">44.40</span>         <span class="hljs-number">0.4777304648</span>               <span class="hljs-number">39.46053639</span>      -<span class="hljs-number">4.939463608</span>
<span class="hljs-number">776</span>          <span class="hljs-number">24.50</span>         <span class="hljs-number">0.2840964250</span>               <span class="hljs-number">23.46636470</span>      -<span class="hljs-number">1.033635298</span><font></font>
<font></font>
&gt; cor(strengths$pred_new, strengths$actual)<font></font>
[<span class="hljs-number">1</span>] <span class="hljs-number">0.9348395359</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Applying neural networks to your projects, you need to follow a similar sequence of steps to return the data to its original scale.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
You may also find that neural networks are rapidly becoming more complex as they are used for increasingly difficult learning tasks. For example, you may encounter the so-called “disappearing” small gradient problem and the closely related “exploding” gradient problem, when the back propagation algorithm does not find a useful solution, since it does not converge in a reasonable time. To solve these problems, you can try to change the number of hidden nodes, apply various activation functions, such as ReLU, adjust the learning speed, etc. On the help page for the neuralnet function, you will find additional information about the various parameters that can be configured. However, this leads to another problem,when the bottleneck in building a highly efficient model is checking a large number of parameters. This is the price of using neural networks, and even more so deep learning networks: their huge potential requires a lot of time and processing power.</font></font><br>
<br>
<blockquote>       ,  ML     .     ,   Amazon Web Services (AWS)  Microsoft Azure,          .       12.</blockquote><br>
<h3>  </h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The Support Vector Machine (SVM) method can be represented as a surface that forms the boundary between data points plotted in a multidimensional space that describes examples and values ​​of their attributes. The goal of SVM is to build a flat border - a hyperplane that divides the space in such a way that homogeneous groups form on both sides of it. Thus, SVM training combines aspects of both nearest-neighbor training based on the instances described in Chapter 3 and linear regression modeling, discussed in Chapter 6. This is an extremely powerful combination that allows SVMs to model very complex relationships.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Despite the fact that the basic mathematics underlying SVM have existed for decades, interest in these methods has grown significantly after they began to be applied to ML. The popularity of these methods increased after high-profile success stories in solving complex learning problems, as well as after the development of SVM algorithms, which were awarded and implemented in well-supported libraries in many programming languages, including R. After that, SVM methods were accepted by a wide audience. Otherwise, it would probably be impossible to apply the complex math required to implement SVM. The good news is that although the mathematics are possibly complex, the basic concepts are understandable.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
SVM methods can be adapted to use almost any type of training task, including classification and numerical forecasting. </font><font style="vertical-align: inherit;">Many of the key successes of this algorithm relate to pattern recognition. </font><font style="vertical-align: inherit;">The best-known applications for these methods include the following:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">classification of data on the expression of microarray genes in bioinformatics for the detection of cancer and other genetic diseases;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">text categorization, such as determining the language used in a document, or classifying documents by topic;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Detection of rare but important events, such as the failure of an internal combustion engine, a safety violation, or an earthquake.</font></font></li>
</ul><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
SVM methods are easiest to understand using binary classification as an example - this is how they are usually used. </font><font style="vertical-align: inherit;">Therefore, in the remaining sections, we will focus only on SVM classifiers. </font><font style="vertical-align: inherit;">Principles similar to those presented here are also used when adapting SVM methods for numerical forecasting.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">about the author</font></font></h3><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Brett Lantz</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (@DataSpelunking) has been using innovative data processing techniques to study human behavior for over a decade. </font><font style="vertical-align: inherit;">Being a sociologist by training, Brett first became interested in machine learning while exploring a large database of teen profiles on social networks. </font><font style="vertical-align: inherit;">Brett is a teacher at DataCamp and often makes presentations at machine learning conferences and seminars around the world. </font><font style="vertical-align: inherit;">He is a well-known enthusiast in the field of practical application of data science in the field of sports, unmanned vehicles, the study of foreign languages ​​and fashion, as well as in many other industries. </font><font style="vertical-align: inherit;">Brett hopes to one day write about all of this on dataspelunking.com, an exchange of knowledge about finding patterns in data.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">About Science Editor</font></font></h3><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Raghav Bali</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (Raghav Bali) - senior researcher of one of the world's largest health care organizations. He is engaged in research and development of corporate solutions based on machine learning, deep learning and natural language processing for use in healthcare and insurance. In his previous position at Intel, he participated in proactive initiatives in the field of information technology, based on big data, using natural language processing, deep learning and traditional statistical methods. At American Express, he worked in digital engagement and customer retention.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Raghav is the author of several books published by leading publishers. </font><font style="vertical-align: inherit;">His latest book is about the latest in transfer study. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Raghav graduated from the International Institute of Information Technology in Bangalore, has a master's degree (honors degree). </font><font style="vertical-align: inherit;">In those rare moments when he is not busy solving scientific problems, Raghav likes to read and photograph everything in a row. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
»More information about the book can be found </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">on the publisher’s website</font></font></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
» </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Contents</font></font></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
» </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Excerpt</font></font></a><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
For Khabrozhiteley 25% discount on the coupon - </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Machine Learning</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Upon payment of the paper version of the book, an electronic book is sent by e-mail.</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../en496242/index.html">Cyberpunk is already here: schoolchildren study at Minecraft, and students take physical tests in CS: GO</a></li>
<li><a href="../en496248/index.html">Challenges, bald heads and wine. How we at HFLabs endure self-isolation</a></li>
<li><a href="../en496250/index.html">How strange code hides errors? TensorFlow.NET Analysis</a></li>
<li><a href="../en496252/index.html">Aerodynamically offset centering aircraft</a></li>
<li><a href="../en496254/index.html">How Rostelecom mistakenly redirected traffic to Google, AWS, Cloudflare, etc.</a></li>
<li><a href="../en496258/index.html">Online holivar: a new format for the exchange of experience. This Saturday</a></li>
<li><a href="../en496260/index.html">Cybersecurity tips for working from home</a></li>
<li><a href="../en496262/index.html">Getting CVE ID</a></li>
<li><a href="../en496266/index.html">Coronavirus: shall we all die?</a></li>
<li><a href="../en496268/index.html">Huawei Enterprise networking products and solutions for enterprise customers in 2020</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>