<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üö¢ üöà üóûÔ∏è GlusterFS as external storage for Kubernetes „Ä∞Ô∏è üè¥Û†ÅßÛ†Å¢Û†Å≥Û†Å£Û†Å¥Û†Åø üè§</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Finding the optimal storage is a rather complicated process, everything has its pros and cons. Of course, the leader in this category is CEPH, but it ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>GlusterFS as external storage for Kubernetes</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/498160/"><img src="https://portworx.com/wp-content/uploads/2018/10/Twitter-Social-Graphic-68.png" alt="image"><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Finding the optimal storage is a rather complicated process, everything has its pros and cons. </font><font style="vertical-align: inherit;">Of course, the leader in this category is CEPH, but it is a rather complex system, albeit with very rich functionality. </font><font style="vertical-align: inherit;">For us, such a system is redundant, given that we needed a simple replicated storage in master-master mode for a couple of terabytes. </font><font style="vertical-align: inherit;">Having studied a lot of material, it was decided to test the most fashionable product on the market for the circuit we are interested in. </font><font style="vertical-align: inherit;">Due to the fact that no ready-made solution of such a plan was found, I would like to share my best practices on this topic and describe the problems that we encountered in the deployment process.</font></font><br>
<a name="habracut"></a><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Goals</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
What did we expect from the new repository:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Ability to work with an even number of nodes for replication.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Easy installation, setup, support</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> The system must be adult, time-tested and users</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Ability to expand storage space without downtime</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Storage must be compatible with Kubernetes</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> There should be an automatic failover when one of the nodes crashes</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
It is on the last point that we have a lot of questions.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Deployment</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
For deployment, two virtual machines were created on CentOs 8. Each of them is connected via an additional disk with storage.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Preliminary preparation</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
For GlusterFS, you need to allocate a separate disk with XFS so that it does not affect the system in any way. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Select the partition:</font></font><br>
<br>
<pre><code class="bash hljs">$ fdisk /dev/sdb<font></font>
Command (m <span class="hljs-keyword">for</span> <span class="hljs-built_in">help</span>): n<font></font>
Partition <span class="hljs-built_in">type</span><font></font>
   p   primary (0 primary, 0 extended, 4 free)<font></font>
   e   extended (container <span class="hljs-keyword">for</span> logical partitions)<font></font>
Select (default p): p<font></font>
Partition number (1-4, default 1):  1<font></font>
First sector (2048-16777215, default 2048): <font></font>
Last sector, +sectors or +size{K,M,G,T,P} (2048-16777215, default 16777215): <font></font>
&nbsp;<font></font>
Created a new partition 1 of <span class="hljs-built_in">type</span> ‚ÄòLinux‚Äô and of size 8 GiB.<font></font>
Command (m <span class="hljs-keyword">for</span> <span class="hljs-built_in">help</span>): w <font></font>
<font></font>
The partition table has been altered.<font></font>
Calling ioctl() to re-read partition table. Syncing disks.<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Format in XFS and mount:</font></font><br>
<br>
<pre><code class="bash hljs">$ mkfs.xfs /dev/sdb1<font></font>
$ mkdir /gluster<font></font>
$ mount /dev/sdb1 /gluster</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
And to top it off, drop the entry in / etc / fstab to automatically mount the directory at system startup:</font></font><br>
<br>
<pre><code class="bash hljs">/dev/sdb1       /gluster        xfs     defaults        0       0</code></pre><br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Installation</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Concerning the installation, many articles have been written, in this connection we will not go deep into the process, we will just consider what it is worth paying attention to. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
On both nodes, install and run the latest version glusterfs:</font></font><br>
<br>
<pre><code class="bash hljs">$ wget -P /etc/yum.repos.d  https://download.gluster.org/pub/gluster/glusterfs/LATEST/CentOS/glusterfs-rhel8.repo<font></font>
$ yum -y install yum-utils<font></font>
$ yum-config-manager --<span class="hljs-built_in">enable</span> PowerTools<font></font>
$ yum install -y glusterfs-server<font></font>
$ systemctl start glusterd<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Next, you need to tell the glaster where his neighboring node is. </font><font style="vertical-align: inherit;">It is done with only one node. </font><font style="vertical-align: inherit;">An important point: if you have a domain network, then you must specify the server name with the domain, otherwise in the future you will have to redo everything.</font></font><br>
<br>
<pre><code class="bash hljs">$ gluster peer probe gluster-02.example.com</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
If it was successful, then we check the connection with the command from both servers:</font></font><br>
<br>
<pre><code class="bash hljs">$ gluster peer status<font></font>
Number of Peers: 1<font></font>
<font></font>
Hostname: gluster-02.example.com<font></font>
Uuid: a6de3b23-ee31-4394-8bff-0bd97bd54f46<font></font>
State: Peer <span class="hljs-keyword">in</span> Cluster (Connected)<font></font>
Other names:<font></font>
10.10.6.72<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Now you can create a Volume in which we will write.</font></font><br>
<br>
<pre><code class="bash hljs">gluster volume create main replica 2 gluster-01.example.com:/gluster/main gluster-02.example.com:/gluster/main force</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Where:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">main - name Volume </font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">replica - type Volume (more details can be found in the </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">official documentation</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> )</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2 - number of replicas </font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Run Volume and check its performance:</font></font><br>
<br>
<pre><code class="bash hljs">gluster volume start main<font></font>
gluster volume status main</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
For replicated Volume, it is recommended that you set the following parameters:</font></font><br>
<br>
<pre><code class="bash hljs">$ gluster volume <span class="hljs-built_in">set</span> main network.ping-timeout 5<font></font>
$ gluster volume <span class="hljs-built_in">set</span> main cluster.quorum-type fixed<font></font>
$ gluster volume <span class="hljs-built_in">set</span> main cluster.quorum-count 1<font></font>
$ gluster volume <span class="hljs-built_in">set</span> main performance.quick-read on</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
With these simple steps, we have built a GlusterFS cluster. </font><font style="vertical-align: inherit;">It remains to connect to it and check the performance. </font><font style="vertical-align: inherit;">Ubuntu is installed on the client machine, for mounting you need to install the client:</font></font><br>
<br>
<pre><code class="bash hljs">$ add-apt-repository ppa:gluster/glusterfs-7<font></font>
$ apt install glusterfs-client<font></font>
$ mkdir /gluster<font></font>
$ mount.glusterfs gluster-01.example.com:/main /gluster</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Gluster, when connected to one of the nodes, gives the addresses of all the nodes and automatically connects to all. </font><font style="vertical-align: inherit;">If the client has already connected, the failure of one of the nodes will not lead to a halt. </font><font style="vertical-align: inherit;">But if the first node is unavailable, it will not work to connect in the event of a session break. </font><font style="vertical-align: inherit;">To do this, when mounting, you can pass the backupvolfile parameter indicating the second node.</font></font><br>
<pre><code class="bash hljs">mount.glusterfs gluster-01.example.com:/main /gluster -o backupvolfile-server=gluster-02.example.com</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
An important point: gluster synchronizes files between nodes only if their change was through the mounted volume. </font><font style="vertical-align: inherit;">If you make changes directly on the nodes, the file will be out of sync.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Connect to Kubernetes</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
At this stage, the questions began: ‚ÄúHow to connect it?‚Äù. </font><font style="vertical-align: inherit;">And there are several options. </font><font style="vertical-align: inherit;">Consider them.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Heketi</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The most popular and recommended is to use an external service: heketi. </font><font style="vertical-align: inherit;">heketi is a layer between kubernetes and gluster that allows you to manage and work with the repository via http. </font><font style="vertical-align: inherit;">But heketi will be that single point of failure, because </font><font style="vertical-align: inherit;">service is not clustered. </font><font style="vertical-align: inherit;">The second instance of this service will not be able to work independently, because </font><font style="vertical-align: inherit;">any changes are stored in the local database. </font><font style="vertical-align: inherit;">Running this service in kubernetes is also not suitable, because </font><font style="vertical-align: inherit;">he needs a static disk on which his database will be stored. </font><font style="vertical-align: inherit;">In this regard, this option turned out to be the most inappropriate.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Endpoint at Kubernetes</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
If you have Kubernetes on systems with package managers, then this is a very convenient option. </font><font style="vertical-align: inherit;">The point is that for all GlusteFS servers in Kubernetes, a common Endpoint is created. </font><font style="vertical-align: inherit;">A service is hung on this Endpoint and we will already be mounted on this service. </font><font style="vertical-align: inherit;">For this option to work, it is necessary to install glusterfs-client on each Kubernetes node and make sure that it can be mounted. </font><font style="vertical-align: inherit;">In Kubernetes, deploy the following config:</font></font><br>
<br>
<pre><code class="python hljs">apiVersion: v1<font></font>
kind: Endpoints<font></font>
metadata: <font></font>
  name: glusterfs-cluster<font></font>
subsets:<font></font>
  - addresses:<font></font>
      <span class="hljs-comment">#  ip  </span>
      - ip: <span class="hljs-number">10.10</span><span class="hljs-number">.6</span><span class="hljs-number">.71</span><font></font>
    ports:<font></font>
      <span class="hljs-comment">#    1,    </span>
      - port: <span class="hljs-number">1</span><font></font>
  - addresses:<font></font>
      - ip: <span class="hljs-number">10.10</span><span class="hljs-number">.6</span><span class="hljs-number">.72</span><font></font>
    ports:<font></font>
      - port: <span class="hljs-number">1</span><font></font>
<font></font>
---<font></font>
apiVersion: v1<font></font>
kind: Service<font></font>
metadata:<font></font>
  name: glusterfs-cluster<font></font>
spec:<font></font>
  ports:<font></font>
  - port: <span class="hljs-number">1</span>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Now we can create a simple test deployment and check how the mounting works. </font><font style="vertical-align: inherit;">Below is an example of a simple test deployment:</font></font><br>
<br>
<pre><code class="python hljs">apiVersion: apps/v1<font></font>
kind: Deployment<font></font>
metadata:<font></font>
  name: gluster-test<font></font>
spec:<font></font>
  replicas: <span class="hljs-number">1</span><font></font>
  selector:<font></font>
    matchLabels:<font></font>
      app: gluster-test<font></font>
  template:<font></font>
    metadata:<font></font>
      labels:<font></font>
        app: gluster-test<font></font>
    spec:<font></font>
      volumes:<font></font>
      - name: gluster<font></font>
        glusterfs:<font></font>
          endpoints: glusterfs-cluster<font></font>
          path: main<font></font>
      containers:<font></font>
      - name: gluster-test<font></font>
        image: nginx<font></font>
        volumeMounts:<font></font>
        - name: gluster<font></font>
          mountPath: /gluster</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
This option did not suit us, because we have container-linux on all Kubernetes nodes. </font><font style="vertical-align: inherit;">The package manager is not there, so it was not possible to install gluster-client for mounting. </font><font style="vertical-align: inherit;">In this regard, the third option was found, which it was decided to use.</font></font><br>
<br>
<h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GlusterFS + NFS + keepalived</font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Until recently, GlusterFS offered its own NFS server, but now NFS uses the external nfs-ganesha service. </font><font style="vertical-align: inherit;">Quite a bit has been written about this, in connection with this we will figure out how to configure it. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The repository must be registered manually. </font><font style="vertical-align: inherit;">To do this, in the file /etc/yum.repos.d/nfs-ganesha.repo we add:</font></font><br>
<br>
<pre><code class="bash hljs">[nfs-ganesha]<font></font>
name=nfs-ganesha<font></font>
baseurl=https://download.nfs-ganesha.org/2.8/2.8.0/RHEL/el-8/<span class="hljs-variable">$basearch</span>/<font></font>
enabled=1<font></font>
gpgcheck=1<font></font>
[nfs-ganesha-noarch]<font></font>
name=nfs-ganesha-noarch<font></font>
baseurl=https://download.nfs-ganesha.org/2.8/2.8.0/RHEL/el-8/noarch/<font></font>
enabled=1<font></font>
gpgcheck=1<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
And install:</font></font><br>
<br>
<pre><code class="bash hljs">yum -y install nfs-ganesha-gluster --nogpgcheck
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
After installation, we conduct the basic configuration in the file /etc/ganesha/ganesha.conf.</font></font><br>
<br>
<pre><code class="json hljs"># create new<font></font>
NFS_CORE_PARAM {<font></font>
    # possible to mount with NFSv3 to NFSv4 Pseudo path<font></font>
    mount_path_pseudo = true;<font></font>
    # NFS protocol<font></font>
    Protocols = 3,4;<font></font>
}<font></font>
EXPORT_DEFAULTS {<font></font>
    # default access mode<font></font>
    Access_Type = RW;<font></font>
}<font></font>
EXPORT {<font></font>
    # uniq ID<font></font>
    Export_Id = 101;<font></font>
    # mount path of Gluster Volume<font></font>
    Path = <span class="hljs-attr">"/gluster/main"</span>;<font></font>
    FSAL {<font></font>
        # any name<font></font>
        name = GLUSTER;<font></font>
        # hostname or IP address of this Node<font></font>
        hostname=<span class="hljs-attr">"gluster-01.example.com"</span>;<font></font>
        # Gluster volume name<font></font>
        volume=<span class="hljs-attr">"main"</span>;<font></font>
    }<font></font>
    # config for root Squash<font></font>
    Squash=<span class="hljs-string">"No_root_squash"</span>;<font></font>
    # NFSv4 Pseudo path<font></font>
    Pseudo=<span class="hljs-string">"/main"</span>;<font></font>
    # allowed security options<font></font>
    SecType = <span class="hljs-string">"sys"</span>;<font></font>
}<font></font>
LOG {<font></font>
    # default log level<font></font>
    Default_Log_Level = WARN;<font></font>
}<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We need to start the service, enable nfs for our volume and check that it is turned on.</font></font><br>
<br>
<pre><code class="bash hljs">$ systemctl start nfs-ganesha<font></font>
$ systemctl <span class="hljs-built_in">enable</span> nfs-ganesha<font></font>
$ gluster volume <span class="hljs-built_in">set</span> main nfs.disable off<font></font>
$ gluster volume status main<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As a result, the status should indicate that the nfs server has started for our volume. </font><font style="vertical-align: inherit;">You need to do mount and check.</font></font><br>
<br>
<pre><code class="bash hljs">mkdir /gluster-nfs<font></font>
mount.nfs gluster-01.example.com:/main /gluster-nfs</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
But this option is not fault tolerant, so you need to make a VIP address that will travel between our two nodes and help switch traffic if one of the nodes falls. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Installing keepalived in CentOs is done immediately through the package manager.</font></font><br>
<br>
<pre><code class="bash hljs">$ yum install -y keepalived</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We configure the service in the file /etc/keepalived/keepalived.conf:</font></font><br>
<br>
<pre><code class="json hljs">global_defs {<font></font>
    notification_email {<font></font>
        admin@example.com<font></font>
    }<font></font>
    notification_email_from alarm@example.com<font></font>
    smtp_server mail.example.com<font></font>
    smtp_connect_timeout <span class="hljs-number">30</span><font></font>
<font></font>
    vrrp_garp_interval <span class="hljs-number">10</span>
    vrrp_garp_master_refresh <span class="hljs-number">30</span><font></font>
}<font></font>
<font></font>
#C   ,   .    , VIP .<font></font>
vrrp_script chk_gluster {<font></font>
    script <span class="hljs-attr">"pgrep glusterd"</span><font></font>
    interval 2<font></font>
}<font></font>
<font></font>
vrrp_instance gluster {<font></font>
    interface ens192<font></font>
    state MASTER #     BACKUP<font></font>
    priority 200 #      ,  100<font></font>
    virtual_router_id 1<font></font>
    virtual_ipaddress {<font></font>
        10.10.6.70/24<font></font>
    }<font></font>
<font></font>
    unicast_peer {<font></font>
        10.10.6.72 #        <font></font>
    }<font></font>
<font></font>
    track_script {<font></font>
        chk_gluster<font></font>
    }<font></font>
}<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Now we can start the service and check that the VIP on the node appears:</font></font><br>
<br>
<pre><code class="bash hljs">$ systemctl start keepalived<font></font>
$ systemctl <span class="hljs-built_in">enable</span> keepalived<font></font>
$ ip addr<font></font>
1: ens192: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000<font></font>
    link/ether 00:50:56:97:55:eb brd ff:ff:ff:ff:ff:ff<font></font>
    inet 10.10.6.72/24 brd 10.10.6.255 scope global noprefixroute ens192<font></font>
       valid_lft forever preferred_lft forever<font></font>
    inet 10.10.6.70/24 scope global secondary ens192<font></font>
       valid_lft forever preferred_lft forever<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
If everything worked for us, then it remains to add PersistentVolume to Kubernetes and create a test service to verify operation.</font></font><br>
<br>
<pre><code class="python hljs">---<font></font>
apiVersion: v1<font></font>
kind: PersistentVolume<font></font>
metadata:<font></font>
  name: gluster-nfs<font></font>
spec:<font></font>
  capacity:<font></font>
    storage: <span class="hljs-number">10</span>Gi<font></font>
  accessModes:<font></font>
    - ReadWriteMany<font></font>
  persistentVolumeReclaimPolicy: Retain<font></font>
  nfs:<font></font>
    server: <span class="hljs-number">10.10</span><span class="hljs-number">.6</span><span class="hljs-number">.70</span><font></font>
    path: /main<font></font>
<font></font>
---<font></font>
apiVersion: v1<font></font>
kind: PersistentVolumeClaim<font></font>
metadata:<font></font>
 name: gluster-nfs<font></font>
spec:<font></font>
 accessModes:<font></font>
 - ReadWriteMany<font></font>
 resources:<font></font>
   requests:<font></font>
     storage: <span class="hljs-number">10</span>Gi<font></font>
 volumeName: <span class="hljs-string">"gluster-nfs"</span><font></font>
<font></font>
---<font></font>
apiVersion: apps/v1<font></font>
kind: Deployment<font></font>
metadata:<font></font>
  name: gluster-test<font></font>
  labels:<font></font>
    app: gluster-test<font></font>
spec:<font></font>
  replicas: <span class="hljs-number">1</span><font></font>
  selector:<font></font>
    matchLabels:<font></font>
      app: gluster-test<font></font>
  template:<font></font>
    metadata:<font></font>
      labels:<font></font>
        app: gluster-test<font></font>
    spec:<font></font>
      volumes:<font></font>
      - name: gluster<font></font>
        persistentVolumeClaim:<font></font>
          claimName: gluster-nfs<font></font>
      containers:<font></font>
      - name: gluster-test<font></font>
        image: nginx<font></font>
        volumeMounts:<font></font>
        - name: gluster<font></font>
          mountPath: /gluster</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
With this configuration, in case of a fall of the main node, it will be idle for about a minute until mount falls off in timeout and switches. </font><font style="vertical-align: inherit;">Simple for a minute for this storage, let's say that this is not a regular situation and we will rarely meet with it, but in this case the system will automatically switch and continue working, and we will be able to solve the problem and carry out the recovery without worrying about the simple.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Summary</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In this article, we examined 3 possible options for connecting GlusterFS to Kubernetes, in our version it is possible to add a provisioner to Kubernetes, but we do not need it yet. </font><font style="vertical-align: inherit;">It remains to add the results of performance tests between NFS and Gluster on the same nodes. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Files on 1Mb:</font></font><br>
<br>
<pre><code class="bash hljs">sync; dd <span class="hljs-keyword">if</span>=/dev/zero of=tempfile bs=1M count=1024; sync<font></font>
Gluster: 1073741824 bytes (1.1 GB, 1.0 GiB) copied, 2.63496 s, 407 MB/s<font></font>
NFS: 1073741824 bytes (1.1 GB, 1.0 GiB) copied, 5.4527 s, 197 MB/s<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Files on 1Kb:</font></font><br>
<br>
<pre><code class="bash hljs">sync; dd <span class="hljs-keyword">if</span>=/dev/zero of=tempfile bs=1K count=1048576; sync<font></font>
Gluster: 1073741824 bytes (1.1 GB, 1.0 GiB) copied, 70.0508 s, 15.3 MB/s<font></font>
NFS: 1073741824 bytes (1.1 GB, 1.0 GiB) copied, 6.95208 s, 154 MB/s<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
NFS works the same for any file size, the speed difference is not particularly noticeable, unlike GlusterFS, which is very degraded with small files. </font><font style="vertical-align: inherit;">But at the same time, with large file sizes, NFS shows performance 2-3 times lower than Gluster.</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../en498146/index.html">Six smart security trends to watch out for</a></li>
<li><a href="../en498150/index.html">Plumber programmers, or the story of one leak and the difficulties of dealing with it</a></li>
<li><a href="../en498154/index.html">Calendar of free IT events online from April 20 to 26</a></li>
<li><a href="../en498156/index.html">F #, morphology of binary images</a></li>
<li><a href="../en498158/index.html">Remote Marathon Week 1: Workplace</a></li>
<li><a href="../en498162/index.html">We invite you to an IT internship at Alfa Bank</a></li>
<li><a href="../en498164/index.html">Product Strategies for Transition Costs</a></li>
<li><a href="../en498168/index.html">New neural network architectures</a></li>
<li><a href="../en498172/index.html">Java Digest for April 21</a></li>
<li><a href="../en498174/index.html">How to stop worrying and start believing in A / B tests</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>