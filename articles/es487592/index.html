<!doctype html>
<html class="no-js" lang="es">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü•ó üòù üå§Ô∏è VMware vSAN 6.7 - Y el trueno golpe√≥ üëø üë©üèø‚Äçü§ù‚Äçüë©üèæ üê∫</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="El a√±o 2018 estaba terminando ... 
 
 Una vez, en un claro d√≠a de diciembre, nuestra Compa√±√≠a decidi√≥ comprar un nuevo hardware. No, por supuesto, est...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>VMware vSAN 6.7 - Y el trueno golpe√≥</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/487592/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El a√±o 2018 estaba terminando ... </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Una vez, en un claro d√≠a de diciembre, nuestra Compa√±√≠a decidi√≥ comprar un nuevo hardware. No, por supuesto, esto no sucedi√≥ de la noche a la ma√±ana. La decisi√≥n fue tomada antes. Mucho m√°s temprano. Pero, como siempre, no siempre nuestros deseos coinciden con las capacidades de los accionistas. Y no hab√≠a dinero, y aguantamos. Pero finalmente, ese momento alegre lleg√≥ cuando la adquisici√≥n fue aprobada en todos los niveles. Todo estuvo bien, los trabajadores de cuello blanco aplaudieron alegremente, estaban cansados ‚Äã‚Äãde procesar documentos durante 25 horas mensuales en servidores de 7 a√±os, y pidieron al Departamento de TI con mucha persistencia algo para darles m√°s tiempo para otras cosas igualmente importantes. .</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Prometimos reducir el tiempo de procesamiento de documentos en 3 veces, hasta 8 horas. </font><font style="vertical-align: inherit;">Para esto, se dispar√≥ un gorri√≥n desde un ca√±√≥n. </font><font style="vertical-align: inherit;">Esta opci√≥n parec√≠a la √∫nica, ya que nuestro equipo no ten√≠a, y nunca tuvo, un administrador de base de datos para aplicar todo tipo de optimizaci√≥n de consultas (DBA).</font></font><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La configuraci√≥n del equipo seleccionado fue, por supuesto, alt√≠sima. </font><font style="vertical-align: inherit;">Estos fueron tres servidores de la compa√±√≠a </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">HPE</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : DL560 Gen10. </font><font style="vertical-align: inherit;">Cada uno de ellos contaba con 4 procesadores Intel Xeon Platinum 8164 2.0Ghz con 26 n√∫cleos, 256 DDR4 RAM, as√≠ como 8 SSD 800Gb SAS (SSD 800Gb WD Ultrastar DC SS530 WUSTR6480ASS204) + 8 SSD 1.92Tb (Western Digital Ultrastar DC SS530 )</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Estas "piezas de hierro" estaban destinadas al cl√∫ster VMware (HA + DRS + vSAN). Que ha estado trabajando con nosotros durante casi 3 a√±os en servidores similares de las generaciones 7 y 8, tambi√©n de HPE. Por cierto, no hubo problemas hasta que HPE se neg√≥ a admitirlos y actualizar ESXi desde la versi√≥n 6.0, incluso a la 6.5, sin una pandereta. Bueno, bueno, como resultado, fue posible actualizar. Al cambiar la imagen de instalaci√≥n, eliminar m√≥dulos de problemas incompatibles de la imagen de instalaci√≥n, etc. Esto tambi√©n agreg√≥ combustible al fuego de nuestro deseo de combinar todo lo nuevo. Por supuesto, si no fuera por los nuevos "trucos" de vSAN, en el ata√∫d vimos una actualizaci√≥n de todo el sistema desde la versi√≥n 6.0 a una m√°s nueva, y no habr√≠a necesidad de escribir un art√≠culo, pero no estamos buscando formas f√°ciles ...</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Entonces, compramos este equipo y decidimos reemplazar el obsoleto. Aplicamos el √∫ltimo </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SPP</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> a cada nuevo servidor, instalamos en cada uno de ellos dos tarjetas de red Ethernet 10G (una para redes de usuarios y la segunda para SAN, 656596-B21 HP Ethernet 10Gb 2-port 530T). S√≠, cada nuevo servidor ven√≠a con una tarjeta de red SFP + sin m√≥dulos, pero nuestra infraestructura de red implicaba Ethernet (dos pilas de conmutadores DELL 4032N para redes LAN y SAN), y el distribuidor de HP en Mosc√∫ no ten√≠a m√≥dulos HPE 813874-B21 y nosotros No esperaron.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Cuando lleg√≥ el momento de instalar ESXi e incorporar nuevos nodos en un centro de datos VMware com√∫n, ocurri√≥ un "milagro". Al final result√≥ que, HPE ESXi Custom ISO versi√≥n 6.5 y posteriores no est√° dise√±ado para instalarse en nuevos servidores Gen10. Solo hardcore, solo 6.7. Y tuvimos que seguir inconscientemente los preceptos de la "empresa virtual". </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Se cre√≥ un nuevo cl√∫ster HA + DRS, se cre√≥ un cl√∫ster vSAN, todo cumpliendo estrictamente con VMware HCL y </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">este documento</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Todo se configur√≥ de acuerdo con el Feng Shui y solo las "alarmas" peri√≥dicas eran sospechosas al monitorear vSAN sobre los valores de los par√°metros distintos de cero en esta secci√≥n:</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/800/65a/ff5/80065aff58945762f1660b556d61216e.png" alt="imagen"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Con tranquilidad, trasladamos todas las m√°quinas virtuales (aproximadamente 50 unidades) a nuevos servidores y a un nuevo almacenamiento vSAN integrado en discos SSD, verificamos el rendimiento del procesamiento de documentos en el nuevo entorno (por cierto, result√≥ ahorrar mucho m√°s tiempo del que planeamos) . Hasta que la base m√°s pesada se transfiri√≥ al nuevo cl√∫ster, la operaci√≥n, que se mencion√≥ al principio del art√≠culo, ¬°tom√≥ alrededor de 4 horas en lugar de 25! Esta fue una contribuci√≥n significativa al estado de √°nimo de A√±o Nuevo de todos los participantes en el proceso. Algunos probablemente comenzaron a so√±ar con un premio. Luego, todos se fueron felices para las vacaciones de A√±o Nuevo.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Cuando comenzaron los d√≠as laborables del nuevo a√±o 2019, nada auguraba una cat√°strofe. ¬°Todos los servicios, transferidos a nuevas capacidades, sin exagerar, despegaron! Solo los eventos en la secci√≥n de resincronizaci√≥n de objetos se volvieron mucho m√°s. Y despu√©s de un par de semanas ocurrieron problemas. Temprano en la ma√±ana, casi todos los servicios clave de la Compa√±√≠a (1s, MSSQL, SMB, Exchange, etc.) dejaron de responder o comenzaron a responder con un largo retraso. Toda la infraestructura se sumi√≥ en un completo caos, y nadie sab√≠a qu√© suced√≠a y qu√© hacer. Todas las m√°quinas virtuales en vCenter se ve√≠an "verdes", no hubo errores en su monitoreo. Reiniciar no ayud√≥. Adem√°s, despu√©s de un reinicio, algunas m√°quinas ni siquiera pudieron arrancar, mostrando varios errores de proceso en la consola. El infierno parec√≠a venir a nosotros y el diablo se frotaba las manos con anticipaci√≥n.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bajo la presi√≥n de un estr√©s grave, fue posible determinar la fuente del desastre. Este problema result√≥ ser el almacenamiento distribuido vSAN. La corrupci√≥n no controlada de datos en discos de m√°quinas virtuales ocurri√≥, a primera vista, sin ninguna raz√≥n. En ese momento, la √∫nica soluci√≥n que parec√≠a racional era ponerse en contacto con el soporte t√©cnico de VMware con gritos: SOS, guardar-ayuda! </font></font><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Y esta decisi√≥n, posteriormente, salv√≥ a la Compa√±√≠a de la p√©rdida de datos relevantes, incluidos los buzones de los empleados, las bases de datos y los archivos compartidos. Juntos, estamos hablando de m√°s de 30 terabytes de informaci√≥n.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Est√° obligado a rendir homenaje al personal de soporte de VMware que no "jug√≥ al f√∫tbol" con el titular de la suscripci√≥n de soporte t√©cnico b√°sico, pero incluy√≥ este caso en el segmento Enterpise, y el proceso gir√≥ las 24 horas. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Lo que pas√≥ despu√©s:</font></font><br>
<br>
<ol>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El soporte t√©cnico de VMware plante√≥ dos preguntas principales: c√≥mo recuperar datos y c√≥mo resolver el problema de la corrupci√≥n de datos "fantasma" en discos de m√°quinas virtuales en el cl√∫ster de combate "vSAN". </font><font style="vertical-align: inherit;">Por cierto, los datos no estaban en ninguna parte para recuperarse, ya que el almacenamiento adicional estaba ocupado por copias de seguridad y simplemente no hab√≠a ning√∫n lugar para desplegar servicios de "combate".</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mientras que, junto con VMware, intent√© juntar los objetos "da√±ados" en el cl√∫ster de vSAN, mis colegas extrajeron con urgencia un nuevo almacenamiento que pudiera acomodar los m√°s de 30 terabytes de datos de la Compa√±√≠a.</font></font></li>
<li>  ,   ,     VMware      ,           ,     ¬´¬ª      - -   .         , ? </li>
<li>        .</li>
<li> ,  ¬´ ¬ª   .</li>
<li>          ,   ,   ¬´¬ª        .</li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tuve que sacrificar temporalmente (durante un par de d√≠as) la eficiencia del correo, en aras de 6 terabytes adicionales de espacio libre en el almacenamiento, para lanzar los servicios clave de los que depend√≠an los ingresos de la Compa√±√≠a.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Miles de l√≠neas de chat con colegas de VMware de habla inglesa se guardaron "para la memoria", aqu√≠ hay un breve extracto de nuestras conversaciones:</font></font></li>
</ol><br>
<pre><code class="plaintext">I understood that you are now migrating all the VMs out of vSAN datastore.<font></font>
May I know, how the migration task is going on.? How many VMs left and how much time is expected to migrate the remaining VMs. ?<font></font>
There are 6 vms still need to be migrated. 1 of them is fail so far.<font></font>
How much time is expected to complete the migration for the working VMs..?<font></font>
I think atleast 2-3 hours<font></font>
ok<font></font>
Can you please SSH to vCenter server ?<font></font>
you on it<font></font>
/localhost/Datacenter ###CLUB/computers/###Cluster&gt; vsan.check_state .<font></font>
2019-02-02 05:22:34 +0300: Step 1: Check for inaccessible vSAN objects<font></font>
Detected 3 objects to be inaccessible<font></font>
Detected 7aa2265c-6e46-2f49-df40-20677c0084e0 on esxi-dl560-gen10-2.####.lan to be inaccessible<font></font>
Detected 99c3515c-bee0-9faa-1f13-20677c038dd8 on esxi-dl560-gen10-3.####.lan to be inaccessible<font></font>
Detected f1ca455c-d47e-11f7-7e90-20677c038de0 on esxi-dl560-gen10-1.####.lan to be inaccessible<font></font>
2019-02-02 05:22:34 +0300: Step 2: Check for invalid/inaccessible VMs<font></font>
Detected VM 'i.#####.ru' as being 'inaccessible'<font></font>
2019-02-02 05:22:34 +0300: Step 3: Check for VMs for which VC/hostd/vmx are out of sync<font></font>
Did not find VMs for which VC/hostd/vmx are out of sync<font></font>
/localhost/Datacenter ###CLUB/computers/###Cluster&gt;<font></font>
Thank you<font></font>
second vm with issues: sd.####.ru</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
C√≥mo se manifest√≥ este problema (adem√°s de los servicios de organizaci√≥n firmemente ca√≠dos). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Crecimiento exponencial de errores de suma de comprobaci√≥n (CRC) "de la nada" durante el intercambio de datos con discos en modo HBA. </font><font style="vertical-align: inherit;">C√≥mo verificar esto: ingrese el siguiente comando en la consola de cada nodo ESXi:</font></font><br>
<br>
<pre><code class="cs">while true; do clear; for disk in $(localcli vsan storage list | grep -B10 'ity Tier: tr' |grep "VSAN UUID"|awk '{print $3}'|sort -u);do echo ==DISK==$disk====;vsish -e get /vmkModules/lsom/disks/$disk/checksumErrors | grep -v ':0';done; sleep 3; done</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Como resultado de la ejecuci√≥n, puede ver errores de CRC para cada disco en el cl√∫ster de vSAN de este nodo (no se mostrar√°n valores cero). </font><font style="vertical-align: inherit;">Si tiene valores positivos y, adem√°s, est√°n en constante crecimiento, entonces hay una raz√≥n para que surjan tareas constantemente en la secci√≥n Monitor -&gt; vSAN -&gt; Resincing de objetos del cl√∫ster. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
¬øC√≥mo recuperar discos de m√°quinas virtuales que no se clonan ni migran por medios est√°ndar? </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
¬øQui√©n hubiera pensado usar el poderoso comando cat:</font></font><br>
<br>
<pre><code class="bash">1. cd      vSAN<font></font>
[root@esxi-dl560-gen10-1:~] cd /vmfs/volumes/vsanDatastore/estaff<font></font>
<font></font>
2. grep vmdk     uuid<font></font>
<font></font>
[root@esxi-dl560-gen10-1:/vmfs/volumes/vsan:52f53dfd12dddc84-f712dbefac32cd1a/2636a75c-e8f1-d9ca-9a00-20677c0084e0] grep vsan *vmdk<font></font>
estaff.vmdk:RW 10485760 VMFS "vsan://3836a75c-d2dc-5f5d-879c-20677c0084e0"<font></font>
estaff_1.vmdk:RW 41943040 VMFS "vsan://3736a75c-e412-a6c8-6ce4-20677c0084e0"<font></font>
[root@esxi-dl560-gen10-1:/vmfs/volumes/vsan:52f53dfd12dddc84-f712dbefac32cd1a/2636a75c-e8f1-d9ca-9a00-20677c0084e0]<font></font>
<font></font>
3.    VM  ,  :<font></font>
<font></font>
mkdir /vmfs/volumes/POWERVAULT/estaff<font></font>
<font></font>
4.   vmx  <font></font>
<font></font>
cp *vmx *vmdk /vmfs/volumes/POWERVAULT/estaff<font></font>
<font></font>
5.      ,      ^_^<font></font>
<font></font>
/usr/lib/vmware/osfs/bin/objtool open -u 3836a75c-d2dc-5f5d-879c-20677c0084e0; sleep 1; cat /vmfs/devices/vsan/3836a75c-d2dc-5f5d-879c-20677c0084e0 &gt;&gt; /vmfs/volumes/POWERVAULT/estaff/estaff-flat.vmdk<font></font>
<font></font>
6. cd   :<font></font>
<font></font>
 cd /vmfs/volumes/POWERVAULT/estaff<font></font>
<font></font>
7.    - estaff.vmdk     <font></font>
<font></font>
[root@esxi-dl560-gen10-1:/tmp] cat estaff.vmdk<font></font>
# Disk DescriptorFile<font></font>
version=4<font></font>
encoding="UTF-8"<font></font>
CID=a7bb7cdc<font></font>
parentCID=ffffffff<font></font>
createType="vmfs"<font></font>
<font></font>
# Extent description<font></font>
RW 10485760 VMFS "vsan://3836a75c-d2dc-5f5d-879c-20677c0084e0"      &lt;&lt;&lt;&lt;&lt;     "estaff-flat.vmdk"<font></font>
<font></font>
# The Disk Data Base<font></font>
#DDB<font></font>
<font></font>
ddb.adapterType = "ide"<font></font>
ddb.deletable = "true"<font></font>
ddb.geometry.cylinders = "10402"<font></font>
ddb.geometry.heads = "16"<font></font>
ddb.geometry.sectors = "63"<font></font>
ddb.longContentID = "6379fa7fdf6009c344bd9a64a7bb7cdc"<font></font>
ddb.thinProvisioned = "1"<font></font>
ddb.toolsInstallType = "1"<font></font>
ddb.toolsVersion = "10252"<font></font>
ddb.uuid = "60 00 C2 92 c7 97 ca ae-8d da 1c e2 3c df cf a5"<font></font>
ddb.virtualHWVersion = "8"<font></font>
[root@esxi-dl560-gen10-1:/tmp]</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
C√≥mo reconocer discos naa.xxxx ... en grupos de discos:</font></font><br>
<br>
<pre><code class="bash">[root@esxi-dl560-gen10-1:/vmfs/volumes] vdq -Hi<font></font>
Mappings:<font></font>
   DiskMapping[0]:<font></font>
           SSD:  naa.5000c5003024eb43<font></font>
            MD:  naa.5000cca0aa0025f4<font></font>
            MD:  naa.5000cca0aa00253c<font></font>
            MD:  naa.5000cca0aa0022a8<font></font>
            MD:  naa.5000cca0aa002500<font></font>
<font></font>
   DiskMapping[2]:<font></font>
           SSD:  naa.5000c5003024eb47<font></font>
            MD:  naa.5000cca0aa002698<font></font>
            MD:  naa.5000cca0aa0029c4<font></font>
            MD:  naa.5000cca0aa002950<font></font>
            MD:  naa.5000cca0aa0028cc<font></font>
<font></font>
   DiskMapping[4]:<font></font>
           SSD:  naa.5000c5003024eb4f<font></font>
            MD:  naa.5000c50030287137<font></font>
            MD:  naa.5000c50030287093<font></font>
            MD:  naa.5000c50030287027<font></font>
            MD:  naa.5000c5003024eb5b<font></font>
            MD:  naa.5000c50030287187<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
C√≥mo averiguar los UUID de vUAN para cada naa ...:</font></font><br>
<br>
<pre><code class="bash">[root@esxi-dl560-gen10-1:/vmfs/volumes] localcli vsan storage list | grep -B15 'ity Tier: tr' | grep -E '^naa|VSAN UUID'<font></font>
<font></font>
naa.5000cca0aa002698:<font></font>
   VSAN UUID: 52247b7d-fed5-a2f2-a2e8-5371fa7ef8ed<font></font>
naa.5000cca0aa0029c4:<font></font>
   VSAN UUID: 52309c55-3ecc-3fe8-f6ec-208701d83813<font></font>
naa.5000c50030287027:<font></font>
   VSAN UUID: 523d7ea5-a926-3acd-2d58-0c1d5889a401<font></font>
naa.5000cca0aa0022a8:<font></font>
   VSAN UUID: 524431a2-4291-cb49-7070-8fa1d5fe608d<font></font>
naa.5000c50030287187:<font></font>
   VSAN UUID: 5255739f-286c-8808-1ab9-812454968734<font></font>
naa.5000cca0aa0025f4: &lt;&lt;&lt;&lt;&lt;&lt;&lt;<font></font>
   VSAN UUID: 52b1d17e-02cc-164b-17fa-9892df0c1726<font></font>
naa.5000cca0aa00253c:<font></font>
   VSAN UUID: 52bd28f3-d84e-e1d5-b4dc-54b75456b53f<font></font>
naa.5000cca0aa002950:<font></font>
   VSAN UUID: 52d6e04f-e1af-cfb2-3230-dd941fd8a032<font></font>
naa.5000c50030287137:<font></font>
   VSAN UUID: 52df506a-36ea-f113-137d-41866c923901<font></font>
naa.5000cca0aa002500:<font></font>
   VSAN UUID: 52e2ce99-1836-c825-6600-653e8142e10f<font></font>
naa.5000cca0aa0028cc:<font></font>
   VSAN UUID: 52e89346-fd30-e96f-3bd6-8dbc9e9b4436<font></font>
naa.5000c50030287093:<font></font>
   VSAN UUID: 52ecacbe-ef3b-aa6e-eba3-6e713a0eb3b2<font></font>
naa.5000c5003024eb5b:<font></font>
   VSAN UUID: 52f1eecb-befa-12d6-8457-a031eacc1cab</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Y lo m√°s importante. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El problema result√≥ ser la operaci√≥n incorrecta del firmware del controlador RAID y el controlador HPE con vSAN. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Anteriormente, en VMware 6.7 U1, el firmware compatible para el controlador HPE Smart Array P816i-a SR Gen10 en vSAN HCL era la versi√≥n 1.98 (que result√≥ ser fatal para nuestra organizaci√≥n), y ahora </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">dice 1.65</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Adem√°s, la versi√≥n 1.99, que resolvi√≥ el problema en ese momento (31 de enero de 2019), ya estaba en los contenedores de HPE, pero no la pasaron ni a VMware ni a nosotros, citando la falta de certificaci√≥n, a pesar de nuestras renuncias y todo eso. , dicen, lo principal para nosotros es resolver el problema con el almacenamiento y eso es todo. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Como resultado, el problema finalmente se resolvi√≥ solo despu√©s de tres meses, cuando se lanz√≥ la versi√≥n de firmware 1.99 para el controlador de disco.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
¬øQu√© conclusiones he sacado?</font></font><br>
<br>
<ol>
<li>     (   ),         .</li>
<li>     !   .</li>
<li>       ¬´¬ª ,   ¬´¬ª    ¬´¬ª   ,    30%      ¬´¬ª.</li>
<li>HPE,    ,           .</li>
<li> ,       :<br>
<br>
<ul>
<li>HPE -           .   ,     Enterprise       . ,    ,       ).</li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">No preve√≠a una situaci√≥n en la que se pudiera necesitar espacio en disco adicional para colocar copias de todos los servidores de la Compa√±√≠a en caso de emergencia.</font></font></li>
</ul></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Adem√°s, a la luz de lo que sucedi√≥, para VMware ya no comprar√© hardware para grandes empresas, ning√∫n proveedor que no sea DELL. </font><font style="vertical-align: inherit;">Por qu√©, porque DELL, hasta donde yo s√©, adquiri√≥ VMware, y ahora se espera que la integraci√≥n de hardware y software en esta direcci√≥n sea lo m√°s cercana posible.</font></font></li>
</ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Como dicen, quemados en leche, soplar en el agua. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Eso es todo chicos. </font><font style="vertical-align: inherit;">Deseo que nunca te metas en situaciones tan terribles. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Seg√∫n recuerdo, ¬°ya me sorprender√©!</font></font></div>
      
    </div><p class="reference-to-source js-reference-to-source">Source: https://habr.com/ru/post/undefined/</p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es487574/index.html">Trabajando con audio: progreso y visualizaci√≥n de datos</a></li>
<li><a href="../es487578/index.html">Optimizaci√≥n de CMake para bibliotecas est√°ticas</a></li>
<li><a href="../es487582/index.html">Ning√∫n dios quema ollas</a></li>
<li><a href="../es487584/index.html">Githabificaci√≥n de la seguridad de la informaci√≥n</a></li>
<li><a href="../es487588/index.html">Quarkus: veterinario subat√≥mico supers√≥nico</a></li>
<li><a href="../es487594/index.html">Entretenida mnemotecnia: recolectamos memoria auditiva de visual</a></li>
<li><a href="../es487596/index.html">Eugene Varavva, desarrollador de Google. C√≥mo describir Google en 5 palabras</a></li>
<li><a href="../es487604/index.html">Incruste las notas breves del programador: duplicaci√≥n de secci√≥n en la memoria del microcontrolador</a></li>
<li><a href="../es487606/index.html">Visualizaci√≥n de l√≠neas de tensi√≥n y movimientos de cargas electrost√°ticas, simulaci√≥n del movimiento planetario del sistema solar.</a></li>
<li><a href="../es487610/index.html">RealWorld: aiohttp, Tortoise ORM</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>