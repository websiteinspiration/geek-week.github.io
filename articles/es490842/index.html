<!doctype html>
<html class="no-js" lang="es">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ˜¥ ğŸˆº ğŸ˜• GPT-2 en imÃ¡genes (visualizaciÃ³n de modelos de lenguaje Transformer) ğŸ§—ğŸ» â“ ğŸ‘©ğŸ¿â€âœˆï¸</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="En 2019, fuimos testigos del uso brillante del aprendizaje automÃ¡tico. El modelo OpenAI GPT-2 ha demostrado una capacidad impresionante para escribir ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>GPT-2 en imÃ¡genes (visualizaciÃ³n de modelos de lenguaje Transformer)</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/490842/"><p><img src="https://habrastorage.org/webt/1k/58/ea/1k58ea5w9egy2dc5z3jtsiip3sc.png" alt="openAI-GPT-2-3"></p><br>
<p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En 2019, fuimos testigos del uso brillante del aprendizaje automÃ¡tico. </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El modelo OpenAI GPT-2 ha</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> demostrado una capacidad impresionante para escribir textos coherentes y emocionales que son superiores a nuestra comprensiÃ³n de lo que pueden generar los modelos de lenguaje modernos. </font><font style="vertical-align: inherit;">GPT-2 no es una arquitectura particularmente nueva: recuerda mucho al Transformador-Decodificador (Transformador solo de decodificador). </font><font style="vertical-align: inherit;">La diferencia entre GPT-2 es que es un modelo de lenguaje realmente enorme basado en Transformer, entrenado en un impresionante conjunto de datos. </font><font style="vertical-align: inherit;">En este artÃ­culo, analizaremos la arquitectura del modelo que nos permite lograr tales resultados: examinaremos en detalle la capa de auto atenciÃ³n y el uso del Transformador de decodificaciÃ³n para tareas que van mÃ¡s allÃ¡ del modelado del lenguaje.</font></font></p><a name="habracut"></a><br>
<p><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Contenido</font></font></strong></p><br>
<ul>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> 1: GPT-2   </a><br>
<ul>
<li>   </li>
<li>   </li>
<li>   BERT'</li>
<li>  </li>
<li>-  :   GPT-2</li>
<li> </li>
<li>  : GPT-2,   </li>
</ul></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> 2:   </a><br>
<ul>
<li>  ( )</li>
<li>1 â€“   ,   </li>
<li>2 â€“  </li>
<li>3 â€“ </li>
<li>   </li>
<li>    GPT-2</li>
<li>  !</li>
</ul></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> 3:    </a><br>
<ul>
<li> </li>
<li></li>
<li> </li>
<li> </li>
</ul></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a></li>
</ul><br>
<h1 id="chast-1-gpt-2-i-yazykovoe-modelirovaniea-namepart_1a"> 1: GPT-2   </h1><br>
<p>    ?</p><br>
<h2 id="chto-takoe-yazykovaya-model">   </h2><br>
<p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Word2vec  </a>  ,     â€“  ,    ,           .     â€“   ,        .</p><br>
<p><img src="https://habrastorage.org/webt/98/yv/br/98yvbr3q4hpjs8hfsg1rajz3n9m.png" alt="teclado de tecla rÃ¡pida"></p><br>
<p>    ,  GPT-2        ,     ,  ,     . GPT-2        40  (WebText),  OpenAI        .      ,  ,  SwiftKey,   78 ,         GPT-2   500      ,     GPT-2 â€“  13   (      6,5 ).</p><br>
<p><img src="https://habrastorage.org/webt/md/qg/ve/mdqgveo0tsyxqapfzqpomuu5nzo.png" alt="gpt2-tamaÃ±os"></p><br>
<p>    GPT-2   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow">AllenAI GPT-2 Explorer</a>.   GPT-2        (   ),     .</p><br>
<h2 id="transformery-dlya-yazykovogo-modelirovaniya">   </h2><br>
<p>     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">  </a>,         â€“     ..  .       â€“ ,   -      .</p><br>
<p><img src="https://habrastorage.org/webt/sv/tx/hm/svtxhmcbpksa7rjn0wb0o2jj_wc.png" alt="transformador-codificador-decodificador"></p><br>
<p>    ,              ,    ,   ,                (                AlphaStar).</p><br>
<p><img src="https://habrastorage.org/webt/j4/sb/4r/j4sb4rysituzcqsafgktspu0pa8.png" alt="gpt-2-transformer-xl-bert-3"></p><br>
<p>      ? ,           GPT-2  :</p><br>
<p><img src="https://habrastorage.org/webt/gg/og/nr/ggognri38aojdzkkzazvnetxfvy.png" alt="gpt2-tamaÃ±os-hiperparÃ¡metros-3"></p><br>
<h2 id="odno-otlichie-ot-berta">   BERT'</h2><br>
<blockquote><strong>  :</strong><br>
         ,     .</blockquote><p> GPT-2      . BERT , ,   .         .       ,  GPT-2,      ,       .   ,     GPT-2    :</p><br>
<p><img src="https://habrastorage.org/webt/so/pv/tj/sopvtjhr-crr3rgtf_zaoqomgck.gif" alt="gpt-2-output"></p><br>
<p>   :  ,     ,     .           .    Â«Â» (auto-regression)    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow"> RNN   </a>. </p><br>
<p><img src="https://habrastorage.org/webt/un/7x/ir/un7xirwndekafrdeixsce0ocmmm.gif" alt="gpt-2-autoregression-2"></p><br>
<p>GPT-2      TransformerXL  XLNet    . BERT .     .  , BERT            . XLNet   ,          .</p><br>
<h2 id="evolyuciya-bloka-transformera">  </h2><br>
<p><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow"> </a>     .</p><br>
<h3 id="blok-enkodera"> </h3><br>
<p> â€“   :</p><br>
<p><img src="https://habrastorage.org/webt/f_/ym/xb/f_ymxbcpvszucrhogiurzwqypzk.png" alt="transformador-codificador-bloque-2"></p><br>
<p><em>             (, 512 ).      ,        .</em></p><br>
<h3 id="blok-dekodera"> </h3><br>
<p> â€“   ,      .        :</p><br>
<p><img src="https://habrastorage.org/webt/gh/so/u6/ghsou6icfirnjbj1abkj7ouqohg.png" alt="transformador-decodificador-bloque-2"></p><br>
<p>            ,          [mask] ,   BERT',             ,     ,     .</p><br>
<p>, ,      #4,   ,         :</p><br>
<p><img src="https://habrastorage.org/webt/ta/9r/td/ta9rtdiimdxkwmwpmdxy2ig-0h8.png" alt="transformador-decodificador-bloque-auto-atenciÃ³n-2"></p><br>
<p>     ,   BERT,     GPT-2.         .       :</p><br>
<p><img src="https://habrastorage.org/webt/43/cq/pd/43cqpdiypmyuccr1gznrhr4tioo.png" alt="auto-atenciÃ³n-y-auto-atenciÃ³n enmascarada"></p><br>
<h3 id="blok-dekodirovaniya"> </h3><br>
<p>  ,  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow">Â«Generating Wikipedia by Summarizing Long SequencesÂ»</a>     ,    :    .      Â«-Â».            6   :</p><br>
<p><img src="https://habrastorage.org/webt/rd/l7/k-/rdl7k--_z3kg4ajejaycksxqpt0.png" alt="introducciÃ³n-decodificador-transformador"></p><br>
<p><em>  .       ,     . ,       4000     â€“      512   .</em></p><br>
<p>       ,  ,       .       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow">Â«         Â»</a>,         /    .</p><br>
<p> GPT-2  OpenAI     .</p><br>
<h2 id="ekspress-kurs-po-neyrohirurgii-zaglyadyvaya-vnutr-gpt-2">-  :   GPT-2</h2><br>
<blockquote> ,   ,       .  ,  ,      , , . (Budgie)</blockquote><p>   GPT-2       ,   .</p><br>
<p><img src="https://habrastorage.org/webt/he/ig/hj/heighjb23joyolbax5b7__fvj6a.png" alt="gpt-2-capas-2"></p><br>
<p><em>GPT-2   1024 .           .</em></p><br>
<p>     GPT-2 â€“      (     )     (),        (..    ).          ,      (       &lt;|endoftext|&gt;;     &lt;|s|&gt;).</p><br>
<p><img src="https://habrastorage.org/webt/ho/yz/o0/hoyzo0yvmbi3gf-1awm__udx0fi.gif" alt="gpt2-simple-output-2"></p><br>
<p>      ,       .          ,    (score)   â€“  ,    (50    GPT-2).          â€“ Â«theÂ».    -  â€“  ,  ,         ,      ,     â€“       .      . GPT-2   top-k,      ,      ,      (,  ,   top-k = 1).</p><br>
<p>              :</p><br>
<p><img src="https://habrastorage.org/webt/hg/2x/ps/hg2xpsd1kuverowaqcd4z_w--ki.gif" alt="gpt-2-simple-output-3"></p><br>
<p> ,        .   GPT-2              (          ). GPT-2       .</p><br>
<h2 id="zaglyanem-poglubzhe"> </h2><br>
<h3 id="kodirovanie-vhoda"> </h3><br>
<p>   .   .     NLP-,     ,          â€“   ,    .</p><br>
<p><img src="https://habrastorage.org/webt/qo/ml/jl/qomljlikh-rqxyud6otmaxrtyum.png" alt="gpt2-token-embeddings-wte-2"></p><br>
<p><em>    â€“  ,     -   .            GPT-2.       768  /.</em></p><br>
<p>,       &lt;|s|&gt;   .        ,      â€“ ,         .     ,        1024   . </p><br>
<p><img src="https://habrastorage.org/webt/zp/mu/cg/zpmucgftth4mabqo60oolwdohey.png" alt="codificaciÃ³n posicional gpt2"></p><br>
<p>          .      ,     GPT-2.</p><br>
<p><img src="https://habrastorage.org/webt/me/r8/bx/mer8bx3ofjqcpydqnfsvo1fk4ok.png" alt="gpt2-input-embedded-positional-encoding-3"></p><br>
<p><em>                 #1.</em></p><br>
<h3 id="puteshestvie-vverh-po-steku">   </h3><br>
<p>     ,       ,      .       ,            .      ,       ,      .</p><br>
<p><img src="https://habrastorage.org/webt/ij/9e/3a/ij9e3alc1pc5ihgpd8njpbh2-vy.png" alt="gpt2-transformer-block-vectors-2"></p><br>
<h3 id="obzor-vnutrennego-vnimaniya">  </h3><br>
<p>      . ,     :</p><br>
<blockquote>    ,   <strong></strong> ,   ,  <strong> </strong>  <strong> </strong>.</blockquote><p>     ,      .         ,    .     ,    , :</p><br>
<ul>
<li><strong></strong>   ;</li>
<li><strong> </strong>      (Â«   Â»);</li>
<li><strong> </strong>     .</li>
</ul><br>
<p>     :          ,      ,       (   ).      ,         ,        .</p><br>
<p> ,           Â«a robotÂ»     Â«itÂ». ,       ,        ,    .</p><br>
<p><img src="https://habrastorage.org/webt/yx/vw/ou/yxvwoujcadhlalw2xgq3dnoezjq.png" alt="gpt2-self-attention-example-2"></p><br>
<h3 id="algoritm-vnutrennego-vnimaniya">  </h3><br>
<p>         .     :</p><br>
<ul>
<li><strong></strong> â€“    ,          (  ).      ,     ;</li>
<li><strong></strong> â€“       .        ;</li>
<li><strong></strong> â€“     ;      ,   ,        .</li>
</ul><br>
<p><img src="https://habrastorage.org/webt/ds/30/gu/ds30gubjhv2r7ubicxiq-idgdlg.png" alt="self-attention-example-folders-3"></p><br>
<p>          .  â€“     ,   .       .   ,      â€“  .           ,      .</p><br>
<p>              (:      ).</p><br>
<p><img src="https://habrastorage.org/webt/br/mp/xd/brmpxdc2ngiqgrkmrgn1wb5aud4.png" alt="self-attention-example-folders-scores-3"></p><br>
<p>      ,     .</p><br>
<p><img src="https://habrastorage.org/webt/xv/7n/ez/xv7nezgfhygrknpndfsptjwucpe.png" alt="gpt2-value-vector-sum"></p><br>
<p>       ,  50%      Â«robotÂ», 30%   Â«aÂ»  19% â€“   Â«itÂ».         .           .</p><br>
<h3 id="vyhod-modeli"> </h3><br>
<p>       (          ),      .</p><br>
<p><img src="https://habrastorage.org/webt/vz/tt/mf/vzttmfwinmu2z7v3m0hkxijm5o8.png" alt="gpt2-output-projection-2"></p><br>
<p>  ,           .          .</p><br>
<p><img src="https://habrastorage.org/webt/fg/ib/yq/fgibyqwocamlo1cax73ck1qxrwg.png" alt="gpt2-output-scores-2"></p><br>
<p>        (top_k = 1).    ,        .         ,      ,      (          ).   â€“  top_k  40:       40    .</p><br>
<p><img src="https://habrastorage.org/webt/ji/zl/mk/jizlmk0ywrvlljh6ynxq_eqacjs.png" alt="gpt2-output"></p><br>
<p> ,        .       ,       (1024 )       .</p><br>
<h2 id="konec-pervoy-chasti-gpt-2-damy-i-gospoda">  : GPT-2,   </h2><br>
<p>,    ,   GPT-2.    ,       ,       .     ,                 (  TransformerXL  XLNet).</p><br>
<p>    ,       :</p><br>
<ul>
<li>Â«Â»  Â«Â»        ;     GPT-2     (Byte Pair Encoding)     .  ,      .</li>
<li>    GPT-2    / (inference/evaluation mode).         .                .           (512),      1,     .</li>
<li>     /       .        .</li>
<li>      ,    .        <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Transformer  </a>,          .</li>
<li>          .      Â«zoom inÂ», : </li>
</ul><br>
<p><img src="https://habrastorage.org/webt/q_/kq/35/q_kq35nsjwiycnwcafi8ctxl_tm.png" alt="zoom-in"></p><br>
<h1 id="chast-2-vizualizaciya-vnutrennego-vnimaniyaa-namepart_2a"> 2:   </h1><br>
<p>             ,   Â«itÂ»:</p><br>
<p><img src="https://habrastorage.org/webt/ss/7h/dk/ss7hdk8hwx43lbghil2t3os95bm.png" alt="gpt2-self-attention-1-2"></p><br>
<p>       ,   .      ,      ,       .              ,      ,      .</p><br>
<h2 id="vnutrennee-vnimanie-bez-maskirovaniya">  ( )</h2><br>
<p>     ,     .    ,   4   .</p><br>
<p>       :</p><br>
<ol>
<li>  ,      ;</li>
<li>             ;</li>
<li>          .</li>
</ol><br>
<p><img src="https://habrastorage.org/webt/ab/l8/ax/abl8ax5sj4nuqdq8jihtayycpvc.png" alt="self-attention-summary"></p><br>
<h2 id="1--sozdanie-vektorov-zaprosa-klyucha-i-znacheniya">1 â€“   ,   </h2><br>
<p>   .          .      .              (  Â«Â» ):</p><br>
<p><img src="https://habrastorage.org/webt/qc/7c/0l/qc7c0ll_eqaebzsk5jqluip1j7k.png" alt="self-attention-1"></p><br>
<p><em>      ,            WQ, WK, WV</em></p><br>
<h2 id="2--podschet-koefficientov">2 â€“  </h2><br>
<p>,     ,           â„–2:                   .</p><br>
<p><img src="https://habrastorage.org/webt/47/tl/vm/47tlvmotgdunt0od7rzfptulwxi.png" alt="self-attention-2"></p><br>
<p><em> ( )            ,     </em></p><br>
<h2 id="3--summirovanie">3 â€“ </h2><br>
<p>       .            ,     .</p><br>
<p><img src="https://habrastorage.org/webt/nz/aa/sl/nzaaslrrr6etmrs-qlkhlb9o6oc.png" alt="self-attention-3-2"></p><br>
<p>&nbsp;<em>         </em></p><br>
<p>  ,       â€“   ,            .</p><br>
<p>       ,   ,         .          (   ).</p><br>
<h2 id="vizualizaciya-maskirovannogo-vnutrennego-vnimaniya">   </h2><br>
<p>,     ,       ,      .        â„–2. ,           .       .        ,      :</p><br>
<p><img src="https://habrastorage.org/webt/di/h6/vx/dih6vxt0pa-5ipj_qtruxbmbcae.png" alt="masked-self-attention-2"></p><br>
<p>      ,    (attention mask). , ,     (Â«robot must obey ordersÂ»).         4 :     ( ,    â€“   ). ..    ,      4    ,      (  4 )   .</p><br>
<p><img src="https://habrastorage.org/webt/ot/_d/k9/ot_dk95ismt5lcax-xeomlzxigu.png" alt="transformer-decoder-attention-mask-dataset"></p><br>
<p>     ,      .    ,   ,        ( ),    :</p><br>
<p><img src="https://habrastorage.org/webt/2_/kz/ws/2_kzws5vewj2dvdy-rtcgwjxdd8.png" alt="queries-keys-attention-mask"></p><br>
<p>   Â«Â»    .   ,    ,     â€“ (-inf)      (, -1   GPT-2):</p><br>
<p><img src="https://habrastorage.org/webt/86/aw/oh/86awoh2nlrcsdh3_hveet0rh3vw.png" alt="transformer-attention-mask"></p><br>
<p>,      ,   ,       :</p><br>
<p><img src="https://habrastorage.org/webt/cr/-o/ku/cr-okuz6tuuuaeht--j8mjblnqo.png" alt="transformer-attention-masked-scores-softmax"></p><br>
<p>    :</p><br>
<ul>
<li>        ( â„–1),      (Â«robotÂ»), 100%      .</li>
<li>        ( â„–2),    (Â«robot mustÂ»),     Â«mustÂ» 48%     Â«robotÂ»  52%    Â«mustÂ».</li>
<li> ..</li>
</ul><br>
<h2 id="maskirovannoe-vnutrennee-vnimanie-v-gpt-2">    GPT-2</h2><br>
<p>       GPT-2.</p><br>
<h3 id="vremya-ocenki-obrabotka-odnogo-tokena-za-raz"> :     </h3><br>
<p>   ,  GPT-2     ,    .    ,           ,          ,    .</p><br>
<p>       (  &lt;|s|&gt;).</p><br>
<p><img src="https://habrastorage.org/webt/oc/r2/ps/ocr2ps037_1vvr2uajz1idbe76g.png" alt="gpt2-self-attention-qkv-1-2"></p><br>
<p>GPT-2       Â«aÂ».             :</p><br>
<p><img src="https://habrastorage.org/webt/0e/wf/vx/0ewfvx7snx6azpvwietxqfd9zzg.png" alt="gpt2-self-attention-qkv-2-2"></p><br>
<p>  ,     Â«robotÂ»,      ,      Â«aÂ» â€“    ,    :</p><br>
<p><img src="https://habrastorage.org/webt/re/qe/ff/reqeffjn9neoql-qqpiqp8hwmjq.png" alt="gpt2-self-attention-qkv-3-2"></p><br>
<h3 id="vnutrennee-vnimanie-gpt-2-1--sozdanie-vektorov-zaprosa-klyucha-i-znacheniya">  GPT-2: 1 â€“   ,   </h3><br>
<p>,     Â«itÂ».    ,         Â«itÂ» +      #9:</p><br>
<p><img src="https://habrastorage.org/webt/k1/ol/ee/k1oleegivyrvsvcx-glsjjxdt14.png" alt="gpt2-self-attention-1"></p><br>
<p>         (     ),       ,   .</p><br>
<p><img src="https://habrastorage.org/webt/ws/nc/1k/wsnc1kowztsdydbbgkevy7ycene.png" alt="gpt2-self-attention-2"></p><br>
<p><em>            (bias vector),   </em></p><br>
<p>    ,       ,      Â«itÂ».</p><br>
<p><img src="https://habrastorage.org/webt/od/oc/ag/odocagmiv-m420l-3wy8sdila4w.png" alt="gpt2-self-attention-3"></p><br>
<p><em>       (     )   ,      </em></p><br>
<h3 id="vnutrennee-vnimanie-gpt-2-15--razdelenie-na-golovy-vnimaniya">  GPT-2: 1.5 â€“   Â«Â» </h3><br>
<p>        ,   Â«Â» .        .           (Q),  (K)   (V).   Â«Â»  â€“       .   GPT-2   12 Â«Â» ,      :</p><br>
<p><img src="https://habrastorage.org/webt/zt/qf/wr/ztqfwr4l3kaoq243xhlleg4ro2i.png" alt="gpt2-self-attention-split-attention-heads-1"></p><br>
<p>    ,     Â«Â» .    Â«Â» ,        (      12 Â«Â» ):</p><br>
<p><img src="https://habrastorage.org/webt/lu/8a/5c/lu8a5cqdztpnonylf6jfxpopngg.png" alt="gpt2-self-attention-split-attention-heads-2"></p><br>
<h3 id="vnutrennee-vnimanie-gpt-2-2--podschet-koefficientov">  GPT-2: 2 â€“  </h3><br>
<p>       (  ,      Â«Â»        ):</p><br>
<p><img src="https://habrastorage.org/webt/ab/0b/nm/ab0bnmrgkb-3knzrtitk2e64opi.png" alt="gpt2-self-attention-scoring"></p><br>
<p>        (    Â«Â»  #1   ):</p><br>
<p><img src="https://habrastorage.org/webt/zc/ll/nt/zcllnthmdvddhniljejpbgke8is.png" alt="gpt2-self-attention-scoring-2"></p><br>
<h3 id="vnutrennee-vnimanie-gpt-2-3--summirovanie">  GPT-2: 3 â€“ </h3><br>
<p>   ,         , ,      Â«Â»  #1:</p><br>
<p><img src="https://habrastorage.org/webt/dv/rf/-i/dvrf-ik3yxkidhzgihbuuqv4sww.png" alt="gpt2-self-attention-multihead-sum-1"></p><br>
<h3 id="vnutrennee-vnimanie-gpt-2-35--obedinenie-golov-vnimaniya">  GPT-2: 3.5 â€“  Â«Â» </h3><br>
<p>  Â«Â»  ,  ,      :</p><br>
<p><img src="https://habrastorage.org/webt/p9/jg/bg/p9jgbgr9-c8lzl-zjnqfdortvxe.png" alt="gpt2-self-attention-merge-heads-1"></p><br>
<p>        .         .</p><br>
<h3 id="vnutrennee-vnimanie-gpt-2-4--proecirovanie">  GPT-2: 4 â€“ </h3><br>
<p>   ,         ,        .       ,    Â«Â»       :</p><br>
<p><img src="https://habrastorage.org/webt/mc/mz/qq/mcmzqqb80hq14cyooubulhh3hhu.png" alt="gpt2-self-attention-project-1"></p><br>
<p> ,   ,      :</p><br>
<p><img src="https://habrastorage.org/webt/af/tl/0o/aftl0oqxhho-m6uph1uqer2a4_c.png" alt="gpt2-self-atenciÃ³n-proyecto-2"></p><br>
<h3 id="polnosvyaznaya-neyronnaya-set-gpt-2-sloy-1">   GPT-2:  #1</h3><br>
<p>   â€“  ,       ,         .     .    4     (  GPT-2   768,      768*4 = 3072 ).    ?      (   512   #1 â€“ 2048). ,           ,      .</p><br>
<p><img src="https://habrastorage.org/webt/gu/4d/ci/gu4dciafj9oa7wcbnt1pwtpfcu8.gif" alt="gpt2-mlp1"></p><br>
<p><em>(   )</em></p><br>
<h3 id="polnosvyaznaya-neyronnaya-set-gpt-2-proecirovanie-na-razmernost-modeli">   GPT-2:    </h3><br>
<p>        (768   GPT-2).          .</p><br>
<p><img src="https://habrastorage.org/webt/x-/eh/kj/x-ehkj3y5dyunh4nf3v7aykhp70.gif" alt="gpt2-mlp-2"></p><br>
<p><em>(   )</em></p><br>
<h2 id="vy-sdelali-eto">  !</h2><br>
<p>     ,    - .       ,      .  , ,         :</p><br>
<p><img src="https://habrastorage.org/webt/yz/n7/20/yzn720lcfsuwwky-r3kanjys3w0.png" alt="gpt2-transformer-block-weights-2"></p><br>
<p>      .   ,            :</p><br>
<p><img src="https://habrastorage.org/webt/xl/22/mc/xl22mctq_nzefihu84kxryqj5be.png" alt="gpt2-weights-2"></p><br>
<p>       ,    :</p><br>
<p><img src="https://habrastorage.org/webt/vk/it/nh/vkitnhrnvo0djn0gn1_e0fpzhp0.png" alt="ParÃ¡metros gpt2-117"></p><br>
<p> -    124   117.    ,  ,         (,    ).</p><br>
<h1 id="chast-3-za-predelami-yazykovogo-modelirovaniyaa-namepart_3a"> 3:    </h1><br>
<p>      ,    .      ,     .        .</p><br>
<h2 id="mashinnyy-perevod"> </h2><br>
<p>      .         :</p><br>
<p><img src="https://habrastorage.org/webt/ni/vt/bw/nivtbwcuuxla5cx5p2srhoohfgm.png" alt="decodificador-solo-transformador-traducciÃ³n"></p><br>
<h2 id="summarizaciya"></h2><br>
<p>  ,        .  ,       (  ,   )   .           :</p><br>
<p><img src="https://habrastorage.org/webt/gd/3u/h5/gd3uh5vlhfzoarket8i2eeudj_4.png" alt="wikipedia-resumen"></p><br>
<p>         .</p><br>
<p><img src="https://habrastorage.org/webt/f2/-z/8v/f2-z8vum88usd2nyshvwwpesjq8.png" alt="resumen de solo decodificador"></p><br>
<h2 id="transfernoe-obuchenie"> </h2><br>
<p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow">Sample Efficient Text Summarization Using a Single Pre-Trained Transformer</a>        ,      . ,        ,     -    .</p><br>
<p>   GPT-2           .</p><br>
<h2 id="generaciya-muzyki"> </h2><br>
<p><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow"> </a>          . Â« Â»      â€“             (,     Â« Â»).</p><br>
<p>   ,      . ,           (),    ( ).    (, ,  )       Â«Â» â€“  ,     .</p><br>
<p><img src="https://habrastorage.org/webt/en/x-/g2/enx-g2e1om4ctwjcuvnsspcjzcm.png" alt="codificador-rendimiento-transformador-musical-3"></p><br>
<p> â€“     one-hot .   midi      .       :</p><br>
<p><img src="https://habrastorage.org/webt/q2/0s/vv/q20svvnlw_1ji4s_fjejnlnrckc.png" alt="ejemplo-representaciÃ³n-musical"></p><br>
<p> one-hot         :</p><br>
<p><img src="https://habrastorage.org/webt/gh/av/o5/ghavo5srziiagcrg-zqzg0vbnjg.png" alt="transformador-de-musica-entrada-representacion-2"></p><br>
<p>         :</p><br>
<p><img src="https://habrastorage.org/webt/gg/up/ou/ggupou0idkyeu2oc-dtgvxayxsa.png" alt="transformador de mÃºsica-auto-atenciÃ³n-2"></p><br>
<p>        , . <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow"></a>.</p><br>
<h1 id="zaklyucheniea-nameconclusiona"></h1><br>
<p>     GPT-2     â€“  . ,              ,   ,    ,         .</p><br>
<h1 id="materialya-nameresourcesa"></h1><br>
<ul>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow"> GPT-2</a>  OpenAI</li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Vea la biblioteca </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">pytorch-transformers2</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> de </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hugging Face</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , que, ademÃ¡s de GPT-2, implementa BERT, Transformer-XL, XLNet y otros modelos avanzados de Transformer.</font></font></li>
</ul><br>
<h1 id="avtory"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Autores</font></font></h1><br>
<ul>
<li><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Original</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> de </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Jay Alammar</font></font></a></li>
<li><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">TraducciÃ³n</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ekaterina Smirnova</font></font></a></li>
<li><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">EdiciÃ³n y maquetaciÃ³n</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Shkarin Sergey</font></font></a></li>
</ul></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es490830/index.html">GestiÃ³n de contraseÃ±as en Zimbra Collaboration Suite EdiciÃ³n de cÃ³digo abierto</a></li>
<li><a href="../es490832/index.html">Marcado de calzado: a medida que ponemos en circulaciÃ³n 2 millones de cÃ³digos de marcado "1C" + "Signo honesto"</a></li>
<li><a href="../es490836/index.html">CÃ³mo abrir comentarios y no ahogarse en spam</a></li>
<li><a href="../es490838/index.html">WiFi 6 estÃ¡ aquÃ­: lo que ofrece el mercado y por quÃ© necesitamos esta tecnologÃ­a</a></li>
<li><a href="../es490840/index.html">PlaneaciÃ³n de recursos. Parte 4.1. Antes de hacer un plan de recursos</a></li>
<li><a href="../es490844/index.html">Libro "Patrones de Kubernetes: Patrones de desarrollo de aplicaciones nativas en la nube"</a></li>
<li><a href="../es490846/index.html">CÃ³mo desarrollamos un sitio web para la Academia Kalashnikov y nos convertimos en galardonados en dos competiciones</a></li>
<li><a href="../es490850/index.html">Ayuda al compilador a ayudarte</a></li>
<li><a href="../es490852/index.html">En detalle sobre SpinLaunch: el secreto mÃ¡s celosamente guardado en la industria espacial</a></li>
<li><a href="../es490854/index.html">Los robots de almacÃ©n que usan IA para clasificar artÃ­culos estÃ¡n listos para funcionar</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>