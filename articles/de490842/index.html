<!doctype html>
<html class="no-js" lang="de">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👇🏿 📺 😼 GPT-2 in Bildern (Visualisierung von Transformer-Sprachmodellen) 🧜🏼 🧓🏽 🏪</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Im Jahr 2019 haben wir den brillanten Einsatz von maschinellem Lernen erlebt. Das OpenAI GPT-2-Modell hat eine beeindruckende Fähigkeit bewiesen, kohä...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>GPT-2 in Bildern (Visualisierung von Transformer-Sprachmodellen)</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/490842/"><p><img src="https://habrastorage.org/webt/1k/58/ea/1k58ea5w9egy2dc5z3jtsiip3sc.png" alt="openAI-GPT-2-3"></p><br>
<p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Im Jahr 2019 haben wir den brillanten Einsatz von maschinellem Lernen erlebt. </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Das OpenAI GPT-2-Modell hat</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> eine beeindruckende Fähigkeit bewiesen, kohärente und emotionale Texte zu schreiben, die unserem Verständnis der Möglichkeiten moderner Sprachmodelle überlegen sind. </font><font style="vertical-align: inherit;">GPT-2 ist keine besonders neue Architektur - es erinnert sehr an den Transformer-Decoder (nur Decoder-Transformer). </font><font style="vertical-align: inherit;">Der Unterschied zwischen GPT-2 besteht darin, dass es sich um ein wirklich großes Sprachmodell handelt, das auf Transformer basiert und auf einem beeindruckenden Datensatz trainiert. </font><font style="vertical-align: inherit;">In diesem Artikel werden wir uns mit der Architektur des Modells befassen, mit der wir solche Ergebnisse erzielen können: Wir betrachten detailliert die Selbstaufmerksamkeitsschicht und die Verwendung des Dekodierungstransformators für Aufgaben, die über die Sprachmodellierung hinausgehen.</font></font></p><a name="habracut"></a><br>
<p><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Inhalt</font></font></strong></p><br>
<ul>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> 1: GPT-2   </a><br>
<ul>
<li>   </li>
<li>   </li>
<li>   BERT'</li>
<li>  </li>
<li>-  :   GPT-2</li>
<li> </li>
<li>  : GPT-2,   </li>
</ul></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> 2:   </a><br>
<ul>
<li>  ( )</li>
<li>1 –   ,   </li>
<li>2 –  </li>
<li>3 – </li>
<li>   </li>
<li>    GPT-2</li>
<li>  !</li>
</ul></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> 3:    </a><br>
<ul>
<li> </li>
<li></li>
<li> </li>
<li> </li>
</ul></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a></li>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a></li>
</ul><br>
<h1 id="chast-1-gpt-2-i-yazykovoe-modelirovaniea-namepart_1a"> 1: GPT-2   </h1><br>
<p>    ?</p><br>
<h2 id="chto-takoe-yazykovaya-model">   </h2><br>
<p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Word2vec  </a>  ,     –  ,    ,           .     –   ,        .</p><br>
<p><img src="https://habrastorage.org/webt/98/yv/br/98yvbr3q4hpjs8hfsg1rajz3n9m.png" alt="Swiftkey-Tastatur"></p><br>
<p>    ,  GPT-2        ,     ,  ,     . GPT-2        40  (WebText),  OpenAI        .      ,  ,  SwiftKey,   78 ,         GPT-2   500      ,     GPT-2 –  13   (      6,5 ).</p><br>
<p><img src="https://habrastorage.org/webt/md/qg/ve/mdqgveo0tsyxqapfzqpomuu5nzo.png" alt="gpt2-größen"></p><br>
<p>    GPT-2   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow">AllenAI GPT-2 Explorer</a>.   GPT-2        (   ),     .</p><br>
<h2 id="transformery-dlya-yazykovogo-modelirovaniya">   </h2><br>
<p>     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">  </a>,         –     ..  .       – ,   -      .</p><br>
<p><img src="https://habrastorage.org/webt/sv/tx/hm/svtxhmcbpksa7rjn0wb0o2jj_wc.png" alt="Transformator-Encoder-Decoder"></p><br>
<p>    ,              ,    ,   ,                (                AlphaStar).</p><br>
<p><img src="https://habrastorage.org/webt/j4/sb/4r/j4sb4rysituzcqsafgktspu0pa8.png" alt="gpt-2-transformator-xl-bert-3"></p><br>
<p>      ? ,           GPT-2  :</p><br>
<p><img src="https://habrastorage.org/webt/gg/og/nr/ggognri38aojdzkkzazvnetxfvy.png" alt="gpt2-Größen-Hyperparameter-3"></p><br>
<h2 id="odno-otlichie-ot-berta">   BERT'</h2><br>
<blockquote><strong>  :</strong><br>
         ,     .</blockquote><p> GPT-2      . BERT , ,   .         .       ,  GPT-2,      ,       .   ,     GPT-2    :</p><br>
<p><img src="https://habrastorage.org/webt/so/pv/tj/sopvtjhr-crr3rgtf_zaoqomgck.gif" alt="gpt-2-Ausgabe"></p><br>
<p>   :  ,     ,     .           .    «» (auto-regression)    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"> RNN   </a>. </p><br>
<p><img src="https://habrastorage.org/webt/un/7x/ir/un7xirwndekafrdeixsce0ocmmm.gif" alt="gpt-2-Autoregression-2"></p><br>
<p>GPT-2      TransformerXL  XLNet    . BERT .     .  , BERT            . XLNet   ,          .</p><br>
<h2 id="evolyuciya-bloka-transformera">  </h2><br>
<p><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"> </a>     .</p><br>
<h3 id="blok-enkodera"> </h3><br>
<p> –   :</p><br>
<p><img src="https://habrastorage.org/webt/f_/ym/xb/f_ymxbcpvszucrhogiurzwqypzk.png" alt="Transformator-Encoder-Block-2"></p><br>
<p><em>             (, 512 ).      ,        .</em></p><br>
<h3 id="blok-dekodera"> </h3><br>
<p> –   ,      .        :</p><br>
<p><img src="https://habrastorage.org/webt/gh/so/u6/ghsou6icfirnjbj1abkj7ouqohg.png" alt="Transformator-Decoder-Block-2"></p><br>
<p>            ,          [mask] ,   BERT',             ,     ,     .</p><br>
<p>, ,      #4,   ,         :</p><br>
<p><img src="https://habrastorage.org/webt/ta/9r/td/ta9rtdiimdxkwmwpmdxy2ig-0h8.png" alt="Transformator-Decoder-Block-Selbstaufmerksamkeit-2"></p><br>
<p>     ,   BERT,     GPT-2.         .       :</p><br>
<p><img src="https://habrastorage.org/webt/43/cq/pd/43cqpdiypmyuccr1gznrhr4tioo.png" alt="Selbstaufmerksamkeit und maskierte Selbstaufmerksamkeit"></p><br>
<h3 id="blok-dekodirovaniya"> </h3><br>
<p>  ,  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow">«Generating Wikipedia by Summarizing Long Sequences»</a>     ,    :    .      «-».            6   :</p><br>
<p><img src="https://habrastorage.org/webt/rd/l7/k-/rdl7k--_z3kg4ajejaycksxqpt0.png" alt="Transformator-Decoder-Intro"></p><br>
<p><em>  .       ,     . ,       4000     –      512   .</em></p><br>
<p>       ,  ,       .       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow">«         »</a>,         /    .</p><br>
<p> GPT-2  OpenAI     .</p><br>
<h2 id="ekspress-kurs-po-neyrohirurgii-zaglyadyvaya-vnutr-gpt-2">-  :   GPT-2</h2><br>
<blockquote> ,   ,       .  ,  ,      , , . (Budgie)</blockquote><p>   GPT-2       ,   .</p><br>
<p><img src="https://habrastorage.org/webt/he/ig/hj/heighjb23joyolbax5b7__fvj6a.png" alt="gpt-2-schichten-2"></p><br>
<p><em>GPT-2   1024 .           .</em></p><br>
<p>     GPT-2 –      (     )     (),        (..    ).          ,      (       &lt;|endoftext|&gt;;     &lt;|s|&gt;).</p><br>
<p><img src="https://habrastorage.org/webt/ho/yz/o0/hoyzo0yvmbi3gf-1awm__udx0fi.gif" alt="gpt2-simple-output-2"></p><br>
<p>      ,       .          ,    (score)   –  ,    (50    GPT-2).          – «the».    -  –  ,  ,         ,      ,     –       .      . GPT-2   top-k,      ,      ,      (,  ,   top-k = 1).</p><br>
<p>              :</p><br>
<p><img src="https://habrastorage.org/webt/hg/2x/ps/hg2xpsd1kuverowaqcd4z_w--ki.gif" alt="gpt-2-simple-output-3"></p><br>
<p> ,        .   GPT-2              (          ). GPT-2       .</p><br>
<h2 id="zaglyanem-poglubzhe"> </h2><br>
<h3 id="kodirovanie-vhoda"> </h3><br>
<p>   .   .     NLP-,     ,          –   ,    .</p><br>
<p><img src="https://habrastorage.org/webt/qo/ml/jl/qomljlikh-rqxyud6otmaxrtyum.png" alt="gpt2-Token-Einbettungen-wte-2"></p><br>
<p><em>    –  ,     -   .            GPT-2.       768  /.</em></p><br>
<p>,       &lt;|s|&gt;   .        ,      – ,         .     ,        1024   . </p><br>
<p><img src="https://habrastorage.org/webt/zp/mu/cg/zpmucgftth4mabqo60oolwdohey.png" alt="gpt2-Positionscodierung"></p><br>
<p>          .      ,     GPT-2.</p><br>
<p><img src="https://habrastorage.org/webt/me/r8/bx/mer8bx3ofjqcpydqnfsvo1fk4ok.png" alt="gpt2-Eingabe-Einbettung-Positionscodierung-3"></p><br>
<p><em>                 #1.</em></p><br>
<h3 id="puteshestvie-vverh-po-steku">   </h3><br>
<p>     ,       ,      .       ,            .      ,       ,      .</p><br>
<p><img src="https://habrastorage.org/webt/ij/9e/3a/ij9e3alc1pc5ihgpd8njpbh2-vy.png" alt="gpt2-Transformator-Blockvektoren-2"></p><br>
<h3 id="obzor-vnutrennego-vnimaniya">  </h3><br>
<p>      . ,     :</p><br>
<blockquote>    ,   <strong></strong> ,   ,  <strong> </strong>  <strong> </strong>.</blockquote><p>     ,      .         ,    .     ,    , :</p><br>
<ul>
<li><strong></strong>   ;</li>
<li><strong> </strong>      («   »);</li>
<li><strong> </strong>     .</li>
</ul><br>
<p>     :          ,      ,       (   ).      ,         ,        .</p><br>
<p> ,           «a robot»     «it». ,       ,        ,    .</p><br>
<p><img src="https://habrastorage.org/webt/yx/vw/ou/yxvwoujcadhlalw2xgq3dnoezjq.png" alt="gpt2-Selbstaufmerksamkeit-Beispiel-2"></p><br>
<h3 id="algoritm-vnutrennego-vnimaniya">  </h3><br>
<p>         .     :</p><br>
<ul>
<li><strong></strong> –    ,          (  ).      ,     ;</li>
<li><strong></strong> –       .        ;</li>
<li><strong></strong> –     ;      ,   ,        .</li>
</ul><br>
<p><img src="https://habrastorage.org/webt/ds/30/gu/ds30gubjhv2r7ubicxiq-idgdlg.png" alt="Selbstaufmerksamkeit-Beispielordner-3"></p><br>
<p>          .  –     ,   .       .   ,      –  .           ,      .</p><br>
<p>              (:      ).</p><br>
<p><img src="https://habrastorage.org/webt/br/mp/xd/brmpxdc2ngiqgrkmrgn1wb5aud4.png" alt="Selbstaufmerksamkeit-Beispiel-Ordner-Scores-3"></p><br>
<p>      ,     .</p><br>
<p><img src="https://habrastorage.org/webt/xv/7n/ez/xv7nezgfhygrknpndfsptjwucpe.png" alt="gpt2-Wert-Vektor-Summe"></p><br>
<p>       ,  50%      «robot», 30%   «a»  19% –   «it».         .           .</p><br>
<h3 id="vyhod-modeli"> </h3><br>
<p>       (          ),      .</p><br>
<p><img src="https://habrastorage.org/webt/vz/tt/mf/vzttmfwinmu2z7v3m0hkxijm5o8.png" alt="gpt2-output-projection-2"></p><br>
<p>  ,           .          .</p><br>
<p><img src="https://habrastorage.org/webt/fg/ib/yq/fgibyqwocamlo1cax73ck1qxrwg.png" alt="gpt2-output-score-2"></p><br>
<p>        (top_k = 1).    ,        .         ,      ,      (          ).   –  top_k  40:       40    .</p><br>
<p><img src="https://habrastorage.org/webt/ji/zl/mk/jizlmk0ywrvlljh6ynxq_eqacjs.png" alt="gpt2-Ausgabe"></p><br>
<p> ,        .       ,       (1024 )       .</p><br>
<h2 id="konec-pervoy-chasti-gpt-2-damy-i-gospoda">  : GPT-2,   </h2><br>
<p>,    ,   GPT-2.    ,       ,       .     ,                 (  TransformerXL  XLNet).</p><br>
<p>    ,       :</p><br>
<ul>
<li>«»  «»        ;     GPT-2     (Byte Pair Encoding)     .  ,      .</li>
<li>    GPT-2    / (inference/evaluation mode).         .                .           (512),      1,     .</li>
<li>     /       .        .</li>
<li>      ,    .        <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Transformer  </a>,          .</li>
<li>          .      «zoom in», : </li>
</ul><br>
<p><img src="https://habrastorage.org/webt/q_/kq/35/q_kq35nsjwiycnwcafi8ctxl_tm.png" alt="hineinzoomen"></p><br>
<h1 id="chast-2-vizualizaciya-vnutrennego-vnimaniyaa-namepart_2a"> 2:   </h1><br>
<p>             ,   «it»:</p><br>
<p><img src="https://habrastorage.org/webt/ss/7h/dk/ss7hdk8hwx43lbghil2t3os95bm.png" alt="gpt2-Selbstaufmerksamkeit-1-2"></p><br>
<p>       ,   .      ,      ,       .              ,      ,      .</p><br>
<h2 id="vnutrennee-vnimanie-bez-maskirovaniya">  ( )</h2><br>
<p>     ,     .    ,   4   .</p><br>
<p>       :</p><br>
<ol>
<li>  ,      ;</li>
<li>             ;</li>
<li>          .</li>
</ol><br>
<p><img src="https://habrastorage.org/webt/ab/l8/ax/abl8ax5sj4nuqdq8jihtayycpvc.png" alt="Selbstaufmerksamkeitszusammenfassung"></p><br>
<h2 id="1--sozdanie-vektorov-zaprosa-klyucha-i-znacheniya">1 –   ,   </h2><br>
<p>   .          .      .              (  «» ):</p><br>
<p><img src="https://habrastorage.org/webt/qc/7c/0l/qc7c0ll_eqaebzsk5jqluip1j7k.png" alt="Selbstaufmerksamkeit-1"></p><br>
<p><em>      ,            WQ, WK, WV</em></p><br>
<h2 id="2--podschet-koefficientov">2 –  </h2><br>
<p>,     ,           №2:                   .</p><br>
<p><img src="https://habrastorage.org/webt/47/tl/vm/47tlvmotgdunt0od7rzfptulwxi.png" alt="Selbstaufmerksamkeit-2"></p><br>
<p><em> ( )            ,     </em></p><br>
<h2 id="3--summirovanie">3 – </h2><br>
<p>       .            ,     .</p><br>
<p><img src="https://habrastorage.org/webt/nz/aa/sl/nzaaslrrr6etmrs-qlkhlb9o6oc.png" alt="Selbstaufmerksamkeit-3-2"></p><br>
<p>&nbsp;<em>         </em></p><br>
<p>  ,       –   ,            .</p><br>
<p>       ,   ,         .          (   ).</p><br>
<h2 id="vizualizaciya-maskirovannogo-vnutrennego-vnimaniya">   </h2><br>
<p>,     ,       ,      .        №2. ,           .       .        ,      :</p><br>
<p><img src="https://habrastorage.org/webt/di/h6/vx/dih6vxt0pa-5ipj_qtruxbmbcae.png" alt="maskierte Selbstaufmerksamkeit-2"></p><br>
<p>      ,    (attention mask). , ,     («robot must obey orders»).         4 :     ( ,    –   ). ..    ,      4    ,      (  4 )   .</p><br>
<p><img src="https://habrastorage.org/webt/ot/_d/k9/ot_dk95ismt5lcax-xeomlzxigu.png" alt="Transformator-Decoder-Aufmerksamkeitsmasken-Datensatz"></p><br>
<p>     ,      .    ,   ,        ( ),    :</p><br>
<p><img src="https://habrastorage.org/webt/2_/kz/ws/2_kzws5vewj2dvdy-rtcgwjxdd8.png" alt="Abfragen-Schlüssel-Aufmerksamkeitsmaske"></p><br>
<p>   «»    .   ,    ,     – (-inf)      (, -1   GPT-2):</p><br>
<p><img src="https://habrastorage.org/webt/86/aw/oh/86awoh2nlrcsdh3_hveet0rh3vw.png" alt="Transformator-Aufmerksamkeitsmaske"></p><br>
<p>,      ,   ,       :</p><br>
<p><img src="https://habrastorage.org/webt/cr/-o/ku/cr-okuz6tuuuaeht--j8mjblnqo.png" alt="Transformator-Aufmerksamkeit-maskierte-Scores-Softmax"></p><br>
<p>    :</p><br>
<ul>
<li>        ( №1),      («robot»), 100%      .</li>
<li>        ( №2),    («robot must»),     «must» 48%     «robot»  52%    «must».</li>
<li> ..</li>
</ul><br>
<h2 id="maskirovannoe-vnutrennee-vnimanie-v-gpt-2">    GPT-2</h2><br>
<p>       GPT-2.</p><br>
<h3 id="vremya-ocenki-obrabotka-odnogo-tokena-za-raz"> :     </h3><br>
<p>   ,  GPT-2     ,    .    ,           ,          ,    .</p><br>
<p>       (  &lt;|s|&gt;).</p><br>
<p><img src="https://habrastorage.org/webt/oc/r2/ps/ocr2ps037_1vvr2uajz1idbe76g.png" alt="gpt2-Selbstaufmerksamkeit-qkv-1-2"></p><br>
<p>GPT-2       «a».             :</p><br>
<p><img src="https://habrastorage.org/webt/0e/wf/vx/0ewfvx7snx6azpvwietxqfd9zzg.png" alt="gpt2-Selbstaufmerksamkeit-qkv-2-2"></p><br>
<p>  ,     «robot»,      ,      «a» –    ,    :</p><br>
<p><img src="https://habrastorage.org/webt/re/qe/ff/reqeffjn9neoql-qqpiqp8hwmjq.png" alt="gpt2-Selbstaufmerksamkeit-qkv-3-2"></p><br>
<h3 id="vnutrennee-vnimanie-gpt-2-1--sozdanie-vektorov-zaprosa-klyucha-i-znacheniya">  GPT-2: 1 –   ,   </h3><br>
<p>,     «it».    ,         «it» +      #9:</p><br>
<p><img src="https://habrastorage.org/webt/k1/ol/ee/k1oleegivyrvsvcx-glsjjxdt14.png" alt="gpt2-Selbstaufmerksamkeit-1"></p><br>
<p>         (     ),       ,   .</p><br>
<p><img src="https://habrastorage.org/webt/ws/nc/1k/wsnc1kowztsdydbbgkevy7ycene.png" alt="gpt2-Selbstaufmerksamkeit-2"></p><br>
<p><em>            (bias vector),   </em></p><br>
<p>    ,       ,      «it».</p><br>
<p><img src="https://habrastorage.org/webt/od/oc/ag/odocagmiv-m420l-3wy8sdila4w.png" alt="gpt2-Selbstaufmerksamkeit-3"></p><br>
<p><em>       (     )   ,      </em></p><br>
<h3 id="vnutrennee-vnimanie-gpt-2-15--razdelenie-na-golovy-vnimaniya">  GPT-2: 1.5 –   «» </h3><br>
<p>        ,   «» .        .           (Q),  (K)   (V).   «»  –       .   GPT-2   12 «» ,      :</p><br>
<p><img src="https://habrastorage.org/webt/zt/qf/wr/ztqfwr4l3kaoq243xhlleg4ro2i.png" alt="gpt2-Selbstaufmerksamkeit-Split-Aufmerksamkeitsköpfe-1"></p><br>
<p>    ,     «» .    «» ,        (      12 «» ):</p><br>
<p><img src="https://habrastorage.org/webt/lu/8a/5c/lu8a5cqdztpnonylf6jfxpopngg.png" alt="gpt2-Selbstaufmerksamkeit-Split-Aufmerksamkeitsköpfe-2"></p><br>
<h3 id="vnutrennee-vnimanie-gpt-2-2--podschet-koefficientov">  GPT-2: 2 –  </h3><br>
<p>       (  ,      «»        ):</p><br>
<p><img src="https://habrastorage.org/webt/ab/0b/nm/ab0bnmrgkb-3knzrtitk2e64opi.png" alt="gpt2-Selbstaufmerksamkeitsbewertung"></p><br>
<p>        (    «»  #1   ):</p><br>
<p><img src="https://habrastorage.org/webt/zc/ll/nt/zcllnthmdvddhniljejpbgke8is.png" alt="gpt2-Selbstaufmerksamkeitsbewertung-2"></p><br>
<h3 id="vnutrennee-vnimanie-gpt-2-3--summirovanie">  GPT-2: 3 – </h3><br>
<p>   ,         , ,      «»  #1:</p><br>
<p><img src="https://habrastorage.org/webt/dv/rf/-i/dvrf-ik3yxkidhzgihbuuqv4sww.png" alt="gpt2-Selbstaufmerksamkeit-Multihead-Summe-1"></p><br>
<h3 id="vnutrennee-vnimanie-gpt-2-35--obedinenie-golov-vnimaniya">  GPT-2: 3.5 –  «» </h3><br>
<p>  «»  ,  ,      :</p><br>
<p><img src="https://habrastorage.org/webt/p9/jg/bg/p9jgbgr9-c8lzl-zjnqfdortvxe.png" alt="gpt2-Selbstaufmerksamkeit-Zusammenführungsköpfe-1"></p><br>
<p>        .         .</p><br>
<h3 id="vnutrennee-vnimanie-gpt-2-4--proecirovanie">  GPT-2: 4 – </h3><br>
<p>   ,         ,        .       ,    «»       :</p><br>
<p><img src="https://habrastorage.org/webt/mc/mz/qq/mcmzqqb80hq14cyooubulhh3hhu.png" alt="gpt2-Selbstaufmerksamkeit-Projekt-1"></p><br>
<p> ,   ,      :</p><br>
<p><img src="https://habrastorage.org/webt/af/tl/0o/aftl0oqxhho-m6uph1uqer2a4_c.png" alt="gpt2-Selbstaufmerksamkeit-Projekt-2"></p><br>
<h3 id="polnosvyaznaya-neyronnaya-set-gpt-2-sloy-1">   GPT-2:  #1</h3><br>
<p>   –  ,       ,         .     .    4     (  GPT-2   768,      768*4 = 3072 ).    ?      (   512   #1 – 2048). ,           ,      .</p><br>
<p><img src="https://habrastorage.org/webt/gu/4d/ci/gu4dciafj9oa7wcbnt1pwtpfcu8.gif" alt="gpt2-mlp1"></p><br>
<p><em>(   )</em></p><br>
<h3 id="polnosvyaznaya-neyronnaya-set-gpt-2-proecirovanie-na-razmernost-modeli">   GPT-2:    </h3><br>
<p>        (768   GPT-2).          .</p><br>
<p><img src="https://habrastorage.org/webt/x-/eh/kj/x-ehkj3y5dyunh4nf3v7aykhp70.gif" alt="gpt2-mlp-2"></p><br>
<p><em>(   )</em></p><br>
<h2 id="vy-sdelali-eto">  !</h2><br>
<p>     ,    - .       ,      .  , ,         :</p><br>
<p><img src="https://habrastorage.org/webt/yz/n7/20/yzn720lcfsuwwky-r3kanjys3w0.png" alt="gpt2-transformator-blockgewichte-2"></p><br>
<p>      .   ,            :</p><br>
<p><img src="https://habrastorage.org/webt/xl/22/mc/xl22mctq_nzefihu84kxryqj5be.png" alt="gpt2-weight-2"></p><br>
<p>       ,    :</p><br>
<p><img src="https://habrastorage.org/webt/vk/it/nh/vkitnhrnvo0djn0gn1_e0fpzhp0.png" alt="gpt2-117-Parameter"></p><br>
<p> -    124   117.    ,  ,         (,    ).</p><br>
<h1 id="chast-3-za-predelami-yazykovogo-modelirovaniyaa-namepart_3a"> 3:    </h1><br>
<p>      ,    .      ,     .        .</p><br>
<h2 id="mashinnyy-perevod"> </h2><br>
<p>      .         :</p><br>
<p><img src="https://habrastorage.org/webt/ni/vt/bw/nivtbwcuuxla5cx5p2srhoohfgm.png" alt="Nur Decoder-Transformator-Übersetzung"></p><br>
<h2 id="summarizaciya"></h2><br>
<p>  ,        .  ,       (  ,   )   .           :</p><br>
<p><img src="https://habrastorage.org/webt/gd/3u/h5/gd3uh5vlhfzoarket8i2eeudj_4.png" alt="Wikipedia-Zusammenfassung"></p><br>
<p>         .</p><br>
<p><img src="https://habrastorage.org/webt/f2/-z/8v/f2-z8vum88usd2nyshvwwpesjq8.png" alt="Nur-Decoder-Zusammenfassung"></p><br>
<h2 id="transfernoe-obuchenie"> </h2><br>
<p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow">Sample Efficient Text Summarization Using a Single Pre-Trained Transformer</a>        ,      . ,        ,     -    .</p><br>
<p>   GPT-2           .</p><br>
<h2 id="generaciya-muzyki"> </h2><br>
<p><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"> </a>          . « »      –             (,     « »).</p><br>
<p>   ,      . ,           (),    ( ).    (, ,  )       «» –  ,     .</p><br>
<p><img src="https://habrastorage.org/webt/en/x-/g2/enx-g2e1om4ctwjcuvnsspcjzcm.png" alt="Musik-Transformator-Performance-Codierung-3"></p><br>
<p> –     one-hot .   midi      .       :</p><br>
<p><img src="https://habrastorage.org/webt/q2/0s/vv/q20svvnlw_1ji4s_fjejnlnrckc.png" alt="Musik-Repräsentations-Beispiel"></p><br>
<p> one-hot         :</p><br>
<p><img src="https://habrastorage.org/webt/gh/av/o5/ghavo5srziiagcrg-zqzg0vbnjg.png" alt="Musik-Transformator-Eingang-Darstellung-2"></p><br>
<p>         :</p><br>
<p><img src="https://habrastorage.org/webt/gg/up/ou/ggupou0idkyeu2oc-dtgvxayxsa.png" alt="Musik-Transformator-Selbstaufmerksamkeit-2"></p><br>
<p>        , . <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"></a>.</p><br>
<h1 id="zaklyucheniea-nameconclusiona"></h1><br>
<p>     GPT-2     –  . ,              ,   ,    ,         .</p><br>
<h1 id="materialya-nameresourcesa"></h1><br>
<ul>
<li><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"> GPT-2</a>  OpenAI</li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Weitere </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Informationen finden Sie in der</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Bibliothek </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;">pytorch-transformers2</font></a><font style="vertical-align: inherit;"> von </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hugging Face</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , die neben GPT-2 auch BERT-, Transformer-XL-, XLNet- und andere fortschrittliche Transformer-Modelle implementiert.</font></font></li>
</ul><br>
<h1 id="avtory"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Autoren</font></font></h1><br>
<ul>
<li><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Original</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> von </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Jay Alammar</font></font></a></li>
<li><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Übersetzung</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ekaterina Smirnova</font></font></a></li>
<li><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bearbeitung und Layout</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Shkarin Sergey</font></font></a></li>
</ul></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de490830/index.html">Kennwortverwaltung in der Open-Source-Edition der Zimbra Collaboration Suite</a></li>
<li><a href="../de490832/index.html">Schuhmarkierung: Als wir 2 Millionen Markierungscodes „1C“ + „Ehrliches Zeichen“ in Umlauf brachten</a></li>
<li><a href="../de490836/index.html">Wie man Kommentare öffnet und nicht in Spam ertrinkt</a></li>
<li><a href="../de490838/index.html">WiFi 6 ist da: Was der Markt bietet und warum brauchen wir diese Technologie</a></li>
<li><a href="../de490840/index.html">Ressourcenplanung. Teil 4.1. Bevor Sie einen Ressourcenplan erstellen</a></li>
<li><a href="../de490844/index.html">Buch „Kubernetes Patterns: Native Cloud Application Development Patterns“</a></li>
<li><a href="../de490846/index.html">Wie wir eine Website für die Kalaschnikow-Akademie entwickelten und Preisträger von zwei Wettbewerben wurden</a></li>
<li><a href="../de490850/index.html">Helfen Sie dem Compiler, Ihnen zu helfen</a></li>
<li><a href="../de490852/index.html">Подробно о SpinLaunch — самом ревностно хранимом секрете в космической индустрии</a></li>
<li><a href="../de490854/index.html">Lagerroboter, die KI zum Sortieren von Artikeln verwenden, sind einsatzbereit</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>