<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üö£üèø üôèüèø ‚òÇÔ∏è Speech Recognition: A Very Short Introductory Course üöÅ üïç üßóüèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="It is almost impossible to tell the layman as simple as possible about the work of computer speech recognition and converting it into text. Not a sing...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Speech Recognition: A Very Short Introductory Course</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/toshibarus/blog/490732/"><img src="https://habrastorage.org/webt/tz/sh/ll/tzshllxzf2iddwai7sredy3edie.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
It is almost impossible to tell the layman as simple as possible about the work of computer speech recognition and converting it into text. </font><font style="vertical-align: inherit;">Not a single story about this is complete without complex formulas and mathematical terms. </font><font style="vertical-align: inherit;">We will try to explain as clearly and slightly simplistically as possible how your smartphone understands speech, when cars have learned to recognize a human voice and in what unexpected areas this technology is used. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Necessary warning: if you are a developer or, especially, a mathematician, you are unlikely to learn anything new from the post and even complain about the insufficient scientific nature of the material. </font><font style="vertical-align: inherit;">Our goal is to introduce the uninitiated readers to speech technologies in the simplest way and tell how and why Toshiba took up the creation of her voice AI.</font></font><br>
<a name="habracut"></a><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Important milestones in the history of speech recognition</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The history of recognition of human speech by electronic machines began a little earlier than it is customary to think: in most cases it is customary to count down from 1952, but in fact one of the first devices that responded to voice commands was the Televox robot, </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">which we already wrote about</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Created in 1927 in the USA, Herbert Televox robot was a simple device in which various relays reacted to sounds of different frequencies. The robot had three tuning forks, each of which was responsible for its tone. Depending on which tuning fork worked, one or another relay was activated.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/90/pq/4i/90pq4ixpys3c8uevjp-ovvydfny.jpeg" alt="image"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In fact, the entire ‚Äúfilling‚Äù of Televox, including the command recognition system, was located on a rack in the area of ‚Äã‚Äãthe body of the ‚Äúrobot‚Äù. It was impossible to close its lid, otherwise tuning forks could not correctly ‚Äúhear‚Äù sounds. Source: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Acme Telepictures / Wikimedia.</font></font></a></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
It was possible to communicate with Televox as separate signals with a whistle, and in short verbal cues - their tuning forks were also laid out in a sequence of sounds. The creator of the robot, Roy Wensley, even staged a fantastic demonstration for those times, saying the command ‚ÄúSesame, open‚Äù, through which Televox turned on the relay responsible for opening the door. No digital technology, neural networks, AI and machine learning - just analog technology!</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The next key invention that paved the way for true recognition of human speech was the Audrey machine, developed in 1952 at the Bell Labs Innovation Forge. The huge Audrey consumed a lot of electricity and was the size of a good cabinet, but all its functionality came down to recognizing spoken numbers from zero to nine. Just ten words, yes, but let's not forget that Audrey was an analog machine. </font></font><br>
<img src="https://habrastorage.org/webt/vd/1q/eb/vd1qebrer6czotgwty3tdyfp15i.png" alt="image"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Unfortunately, the story has not preserved public photographs of Audrey, there is only a concept. Simple on paper, difficult to translate - according to the memoirs of contemporaries, Audrey components occupied an entire cabinet. Source: Bell Labs</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
It worked like this: the announcer spoke numbers into the microphone, making intervals of at least 350 ms between words, Audrey converted the sounds he heard into electrical signals and compared them with samples recorded in analog memory. According to the results of the comparison, the car highlighted the number on the dashboard. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
It was a breakthrough, but there was no real benefit from Audrey - the machine recognized the voice of its creator with an accuracy of 97%, other specially trained speakers received an accuracy of 70-80%. Strangers who first contacted Audrey, no matter how hard they tried, saw their number on the scoreboard in only 50% of cases.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Despite the revolutionary results for its time, Audrey did not find, and could not find practical application. </font><font style="vertical-align: inherit;">It was assumed that the system could be adapted instead of telephone operators, but nevertheless, human services were more convenient, faster and much more reliable than Audrey.</font></font><br>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/rQco1sa9AwU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Presentation similar to Audrey, only much smaller, machines - IBM Shoebox. </font><font style="vertical-align: inherit;">Shoebox speed is clearly visible. </font><font style="vertical-align: inherit;">The machine could also perform simple mathematical operations of addition and subtraction</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In the early 1960s, work on creating machines for speech recognition was carried out in Japan, the UK, the USA and even the USSR, where they invented a very important algorithm for the dynamic transformation of the timeline (DTW), with the help of which it was possible to build a system that knows about 200 words. But all the developments were similar to each other, and the recognition principle became a common drawback: words were perceived as integral sound fingerprints, and then they were checked against the base of samples (dictionary). Any changes in the speed, timbre and clarity of the pronunciation of words significantly affected the quality of recognition. Scientists have a new task: to teach the machine to hear individual sounds, phonemes or syllables and then make words from them. Such an approach would make it possible to level out the effect of changing the speaker, when, depending on the speaker, the recognition level varied sharply.</font></font><br>
<br>
<i> ‚Äî     ,           . ,   ¬´ ¬ª  ¬´¬ª       ¬´¬ª.   ¬´¬ª   ¬´ ¬ª  ¬´ ¬ª      ¬´¬ª,    ‚Äî  ¬´¬ª.  ,  ,   . </i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In 1971, the Department of Defense Advanced Research Projects Agency (DARPA) launched a five-year program with a budget of $ 15 million, which was tasked with creating a recognition system that knew at least 1000 words. By 1976, Carnegie Mellon University introduced Harpy, capable of operating a dictionary of 1011 words. Harpy did not compare the completely heard words with the samples, but divided them into allophones (a sample of the sound of a phoneme depending on the letters surrounding it). This was another success, confirming that the future lies in the recognition of individual phonemes, rather than whole words. However, among the drawbacks of Harpy was an extremely low level of correct recognition of allophones (pronunciations of phonemes) - about 47%. With such a high error, the share of errors grew after the volume of the dictionary.</font></font><br>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/32KKg3aP3Vw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Description of how Harpy works. Video of the program did not survive.</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Harpy's experience has shown that building up dictionaries of holistic sound fingerprints is useless - it only increases recognition time and drastically reduces accuracy, so researchers all over the world have taken a different path - recognizing phonemes. In the mid-1980s, the IBM Tangora machine could learn to understand the speech of any speaker with any accent, dialect and pronunciation, it only required a 20-minute training, during which a database of phonemes and allophone samples was accumulated. The use of the </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">hidden Markov model</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> also increased the vocabulary of IBM Tangora to an impressive 20,000 words - 20 times more than Harpy had, and is already comparable to the teenager's vocabulary.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
All speech recognition systems from the 1950s to the mid-1990s did not know how to read a person‚Äôs natural spoken language - they had to pronounce the words separately, pausing between them. A truly revolutionary event was the introduction of the hidden Markov model developed in the 1980s - a statistical model that built precise assumptions about unknown elements based on the known ones. Simply put, with just a few recognized phonemes in one word, the hidden Markov model very accurately selects the missing phonemes, thereby greatly increasing the accuracy of speech recognition.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In 1996, the first commercial program appeared, capable of distinguishing not individual words, but a continuous flow of natural speech - IBM MedSpeak / Radiology. IBM was a specialized product that was used in medicine to shorthand describe the results of an x-ray delivered by a doctor during the study. Here, the power of computers finally became sufficient to recognize individual words "on the fly." Plus, the algorithms have become more perfect, the correct recognition of micro-pauses between the spoken words has appeared.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The first universal engine for recognizing natural speech was the program Dragon NaturallySpeaking in 1997. When working with her, the announcer (i.e. the user) did not need to undergo training or operate with a specific vocabulary, as in the case of MedSpeak, any person, even a child, could work with NaturallySpeaking, the program did not set any pronunciation rules. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/xj/m6/w-/xjm6w-kpgryltox7wquuvpl8db4.png" alt="image"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Despite the uniqueness of Dragon NaturallySpeaking, IT browsers did not show much enthusiasm for recognizing natural speech. Among the shortcomings, recognition errors and incorrect processing of commands addressed to the program itself were noted. Source: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">itWeek</font></font></a></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
It is noteworthy that the recognition engine was ready back in the 1980s, but due to the insufficient computer power, the Dragon Systems development (now owned by Nuance Communications) did not have time to determine the spaces between words on the fly, which is necessary for recognizing natural speech. </font><font style="vertical-align: inherit;">Without this, the words "while being treated", for example, could be heard by the computer as "crippled." </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ahead was the growing popularity of speech recognition systems, neural networks, the emergence of Google voice search on mobile devices and, finally, the Siri voice assistant, not only converting speech to text, but also adequately responding to queries constructed in any natural way.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">How to hear what was said and to think of what was inaudible?</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nowadays, the best tool for creating a speech recognition engine is the recurrent neural network (RNN), on which all modern services for recognizing voice, music, images, faces, objects, text are built. RNN allows you to understand words with extreme accuracy, as well as predict the most likely word in the context of the context if it was not recognized. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The neural network temporal classification of the model (CTC) selects individual phonemes in the recorded audio stream (word, phrase) and arranges them in the order in which they were pronounced. After repeated analysis, CTC very clearly identifies certain phonemes, and their text recording is compared with the database of words in the neural network and then turns into a recognized word.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Neural networks are so called because the principle of their work is similar to the work of the human brain. Neural network training is very similar to human training. For example, in order for a very small child to learn to recognize cars and distinguish them from motorcycles, you need to at least several times draw his attention to various cars and each time pronounce the corresponding word: this is big and red - the car, and this low black - the car, but this and these are motorcycles. At some point, the child will discover patterns and common signs for different cars, and will learn to correctly recognize where the car is, where the jeep, where the motorcycle, and where the ATV, even if in passing it sees them on an advertising poster on the street. In the same way, the neural network needs to be trained in a base of examples - to make hundreds and thousands of pronunciation variants of each word, letter, phoneme ‚Äúlearn‚Äù.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A recurrent neural network for speech recognition is good because after a long training the base of various pronunciations, it will learn to distinguish phonemes from words and make words from them regardless of the quality and nature of the pronunciation. And even ‚Äúthink out‚Äù with high accuracy, within the context of the word, words that could not be recognized unambiguously due to background noises or fuzzy pronunciation. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
But there is a nuance with RNN predictions - a recurrent neural network can ‚Äúthink out‚Äù a missing word only by relying on the closest context of about five words. Outside this space, analysis will not be conducted. And sometimes he is so necessary! For example, for recognition, we uttered the phrase ‚ÄúGreat Russian poet Alexander Sergeyevich </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pushkin</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚Äù, In which the word‚Äú Pushkin ‚Äù(specially in italics) was said so inaudibly that the AI ‚Äã‚Äãcould not accurately recognize it. But a recurrent neural network, based on the experience gained during the training, may suggest that the word "Pushkin" is most often found next to the words "Russian", "poet", "Alexander" and "Sergeyevich". This is a fairly simple task for an RNN trained in Russian texts, because a very specific context allows us to make assumptions with the highest accuracy.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
And if the context is vague? Take another text in which one word cannot be recognized: ‚ÄúOur everything, Alexander Sergeyevich Pushkin, tragically died in the prime of his life after a duel with Dantes. The Pushkin Theater Festival is named after the poet. ‚Äù If you remove the word "Pushkinsky", RNN simply can not guess it, based on the context of the proposal, because it only mentions a theater festival and a reference to the name of an unknown poet - there are a lot of possible options! </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
This is where the long short-term memory (LSTM) architecture for recurrent neural networks, created in 1997 (a </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;">detailed article on LSTM</font></a><font style="vertical-align: inherit;"> ) comes into </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">play.</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">) It was specially developed in order to add RNN's ability to take into account the context remote from the event being processed - the results of solving previous problems (that is, word recognition) pass through the entire recognition process, no matter how long the monologue is, and are taken into account in each case of doubt. Moreover, the removal distance has almost no effect on the efficiency of the architecture. With the help of LSTM, if necessary, a word network will take into account all the experience available within the framework of the task: in our example, RNN will look at the previous sentence, find that Pushkin and Dantes were mentioned earlier, therefore, ‚ÄúBy the Name of the Poet‚Äù most likely indicates one of them. Since there is no evidence of the existence of the Dantes Theater Festival,we are talking about Pushkinsky (all the more so since the sound imprint of an unrecognized word is very similar) - such a festival was at the base for training the neural network.</font></font><br>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/P325_hrGsDI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Confession of a voice assistant." </font><font style="vertical-align: inherit;">When a well-trained neural network comes into play, a voice assistant can figure out exactly what needs to be done with ‚Äúgreen slippers‚Äù</font></font></i><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">How does speech recognition make the world a better place?</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In each case, the application is different - it helps someone communicate with gadgets, and </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">according to PricewaterhouseCooper</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> s more than half of smartphone users give voice commands to devices - among adults (25-49 years old), the percentage of those who constantly use voice interfaces, even higher than among young people (18-25) - 65% against 59%. </font><font style="vertical-align: inherit;">And in Russia at least once, at least 71% of the population communicated with Siri, Google Assitant or Alice. </font><font style="vertical-align: inherit;">45 million Russians constantly communicate with Yandex from Alice, and Yandex.Maps / Yandex.Navigator only account for 30% of requests.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Speech recognition really helps someone at work - for example, as we said above, to doctors: in medicine since 1996 (when IBM MedSpeak came out), recognition is used to record anamnesis and when examining images - a physician can continue to work without being distracted by recordings in computer or paper card. By the way, work on dictation in medicine is conducted not only in the West - in Russia there is a Voice2Med program from the ‚ÄúCenter for Speech Technologies‚Äù.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
There are other examples, including our own. Organizing a Toshiba business involves full inclusion, that is, equal rights and opportunities for people with various health conditions, including for employees with hearing impairments. We have a corporate program called Universal Design Advisor System, in which people with various types of disabilities participate in the development of Toshiba products, making suggestions to improve their convenience for people with disabilities - that is, we do not assume how we can do better, but operate on real experience and employee reviews.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A few years ago, at the Toshiba headquarters in Japan, we faced a very interesting task, requiring the development of a new speech recognition system. During the operation of the Universal Design Advisor System, we received an important insight: employees with hearing impairments want to participate in discussions at meetings and lectures in real time, and not be limited to reading the processed transcript hours or days later. Starting voice recognition through a smartphone in such cases gives a very weak result, so Toshiba specialists had to start developing a specialized recognition system. And, of course, we immediately ran into problems.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Conversation differs enormously from written speech - we don‚Äôt speak the way we write letters, and a real conversation translated into text looks very sloppy and even unreadable. That is, even if we convert conversations on the morning plan into text with high accuracy, we will get an incoherent hash teeming with verbal parasites, interjections and thoughtful ‚Äúaaa‚Äù, ‚Äúuh‚Äù and ‚Äúmmm‚Äù. To get rid of the transcription of unnecessary sounds, words and expressions of emotions in the text, we decided to develop an AI capable of maximally accurately recognizing not always necessary elements of colloquial speech, including the emotional coloring of some words (for example, ‚Äúyes, well‚Äù may sound like skepticism or how sincere surprise, and these are literally opposite meanings).</font></font><br>
<br>
<img src="https://habrastorage.org/webt/6z/vv/od/6zvvodwnihcvdqdqfv4uuprbtb4.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">It looks like a laptop with a set of peripherals for voice recognition using Toshiba AI (left) and an application with the results for end devices (right). Source: Toshiba</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
LSTM came in handy here, without which the recognition accuracy was insufficient for the received text to be read and understood without effort. Moreover, LSTM was useful not only for more accurate prediction of words in context, but also for the correct processing of pauses in the middle of sentences and interjections-parasites - for this we taught the neural network these parasites and pauses that are natural for colloquial speech.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Does this mean that now the neural network can remove interjections from transcripts? </font><font style="vertical-align: inherit;">Yes, it can, but this is not necessary. </font><font style="vertical-align: inherit;">The fact is that (another insight received) people with hearing impairments are guided, including by the movements of the speaker's lips. </font><font style="vertical-align: inherit;">If the lips move, but the text corresponding to these movements does not appear on the screen, there is a feeling that the recognition system has missed part of the conversation. </font><font style="vertical-align: inherit;">That is, for someone who cannot hear, it is important to get as much information as possible about the conversation, including ill-fated pauses and mejometia. </font><font style="vertical-align: inherit;">Therefore, the Toshiba engine leaves these elements in the transcript, but in real time dims the brightness of the letters, making it clear that these are optional details for understanding the text.</font></font><br>
<br>
<div class="oembed"><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">https://www.toshiba-clip.com/en/detail/7655</font></font></a></div><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">This is how the recognition result on the fly looks on the client device. </font><font style="vertical-align: inherit;">The parts of the monologue that are not meaningful are painted gray.</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Now Toshiba AI works with English, Japanese and Chinese speech, and even translation between languages ‚Äã‚Äãon the fly is possible. </font><font style="vertical-align: inherit;">It is not necessary to use it for shorthand on the fly - the AI ‚Äã‚Äãcan be adapted to work with voice assistants, who finally learn to adequately perceive interjections, pauses and stutters when a person pronounces a command. </font><font style="vertical-align: inherit;">In March 2019, the system was successfully used to add subtitles to the IPSJ National Convention broadcast in Japan. </font><font style="vertical-align: inherit;">In the near future - the transformation of the Toshiba AI into a public service and experiences with the implementation of voice recognition in production.</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../en490720/index.html">Motor! or What is game physics</a></li>
<li><a href="../en490722/index.html">Gender holidays in IT. How to note</a></li>
<li><a href="../en490726/index.html">Authentication on network equipment through SSH using public keys</a></li>
<li><a href="../en490728/index.html">Security Week 10: RSA Conference and Cybersecurity Awareness</a></li>
<li><a href="../en490730/index.html">Intel x86 Root of Trust: loss of trust</a></li>
<li><a href="../en490734/index.html">Easter eggs on topographic maps of Switzerland</a></li>
<li><a href="../en490736/index.html">9 clear tools for learning and pumping English vocabulary</a></li>
<li><a href="../en490738/index.html">Lisk substitution principle</a></li>
<li><a href="../en490740/index.html">Defecting *** s is not just randomization</a></li>
<li><a href="../en490742/index.html">A new era in robotics has begun</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>