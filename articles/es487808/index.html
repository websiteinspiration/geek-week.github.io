<!doctype html>
<html class="no-js" lang="es">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë∂ üë®üèº‚Äçü§ù‚Äçüë®üèª üåü Redes neuronales recurrentes (RNN) con Keras üßöüèª ‚úçüèº üí≠</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Traducci√≥n de la Gu√≠a de red neuronal recursiva de Tensorflow.org. El material analiza tanto las capacidades integradas de Keras / Tensorflow 2.0 para...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Redes neuronales recurrentes (RNN) con Keras</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/487808/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Traducci√≥n de la Gu√≠a de red neuronal recursiva de Tensorflow.org. </font><font style="vertical-align: inherit;">El material analiza tanto las capacidades integradas de Keras / Tensorflow 2.0 para mallado r√°pido, como la posibilidad de personalizar capas y celdas. </font><font style="vertical-align: inherit;">Tambi√©n se consideran los casos y las limitaciones del uso del n√∫cleo CuDNN, lo que permite acelerar el proceso de aprendizaje de la red neuronal.</font></font><br>
 <br>
<img src="https://habrastorage.org/webt/xt/_q/nj/xt_qnjgfjengqoqd4gizkq4j_wk.png"><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Las redes neuronales recursivas (RNN) son una clase de redes neuronales que son buenas para modelar datos en serie, como series de tiempo o lenguaje natural. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Si es esquem√°ticamente, la capa RNN usa un bucle </font></font><code>for</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">para iterar sobre una secuencia ordenada en el tiempo, mientras almacena en un estado interno, informaci√≥n codificada sobre los pasos que ya ha visto. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Keras RNN API est√° dise√±ado con un enfoque en: </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Facilidad de uso</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : una funci√≥n de capas </font></font><code>tf.keras.layers.RNN</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, </font></font><code>tf.keras.layers.LSTM</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, </font></font><code>tf.keras.layers.GRU</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">le permiten construir r√°pidamente un modelo recursivo sin tener que realizar ajustes de configuraci√≥n compleja. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">F√°cil personalizaci√≥n</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : tambi√©n puede definir su propia capa de celdas RNN (parte interna del bucle</font></font><code>for</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">) con un comportamiento personalizado y √∫selo con una capa com√∫n de `tf.keras.layers.RNN` (el bucle` for` mismo). </font><font style="vertical-align: inherit;">Esto le permitir√° crear r√°pidamente prototipos de varias ideas de investigaci√≥n de manera flexible, con un m√≠nimo de c√≥digo.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Instalaci√≥n</font></font></h2><br>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> __future__ <span class="hljs-keyword">import</span> absolute_import, division, print_function, unicode_literals<font></font>
<font></font>
<span class="hljs-keyword">import</span> collections
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<font></font>
<font></font>
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<font></font>
<font></font>
<span class="hljs-keyword">from</span> tensorflow.keras <span class="hljs-keyword">import</span> layers</code></pre><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Construyendo un modelo simple</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Keras tiene tres capas RNN incorporadas:</font></font><br>
<br>
<ol>
<li><code>tf.keras.layers.SimpleRNN</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, un RNN completamente conectado en el que la salida del paso de tiempo anterior debe pasar al siguiente paso.</font></font></li>
<li><code>tf.keras.layers.GRU</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, propuesto por primera vez en el art√≠culo </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Estudio de frases con c√≥dec RNN para traducci√≥n autom√°tica estad√≠stica</font></font></a></li>
<li><code>tf.keras.layers.LSTM</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, propuesto por primera vez en el art√≠culo </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Memoria a corto plazo a largo plazo</font></font></a></li>
</ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A principios de 2015, Keras present√≥ las primeras implementaciones reutilizables de c√≥digo abierto Python y LSTM y GRU. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El siguiente es un ejemplo de un </font></font><code>Sequential</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">modelo que procesa secuencias de n√∫meros enteros anidando cada n√∫mero entero en un vector de 64 dimensiones, y luego procesando secuencias de vectores usando una capa </font></font><code>LSTM</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font><br>
<br>
<pre><code class="python hljs">model = tf.keras.Sequential()
<span class="hljs-comment">#   Embedding      1000, </span>
<span class="hljs-comment">#     64.</span>
model.add(layers.Embedding(input_dim=<span class="hljs-number">1000</span>, output_dim=<span class="hljs-number">64</span>))<font></font>
<font></font>
<span class="hljs-comment">#   LSTM  128  .</span>
model.add(layers.LSTM(<span class="hljs-number">128</span>))<font></font>
<font></font>
<span class="hljs-comment">#   Dense  10    softmax.</span>
model.add(layers.Dense(<span class="hljs-number">10</span>))<font></font>
<font></font>
model.summary()</code></pre><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Salidas y estados</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Por defecto, la salida de la capa RNN contiene un vector por elemento. Este vector es la salida de la √∫ltima celda RNN que contiene informaci√≥n sobre toda la secuencia de entrada. La dimensi√≥n de esta salida </font></font><code>(batch_size, units)</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, donde </font></font><code>units</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">corresponde al argumento </font></font><code>units</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">pasado al constructor de capa. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La capa RNN tambi√©n puede devolver la secuencia de salida completa para cada elemento (un vector para cada paso), si as√≠ lo especifica </font></font><code>return_sequences=True</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. La dimensi√≥n de esta salida es </font></font><code>(batch_size, timesteps, units)</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font><br>
<br>
<pre><code class="python hljs">model = tf.keras.Sequential()<font></font>
model.add(layers.Embedding(input_dim=<span class="hljs-number">1000</span>, output_dim=<span class="hljs-number">64</span>))<font></font>
<font></font>
<span class="hljs-comment">#  GRU  3D   (batch_size, timesteps, 256)</span>
model.add(layers.GRU(<span class="hljs-number">256</span>, return_sequences=<span class="hljs-literal">True</span>))<font></font>
<font></font>
<span class="hljs-comment">#  SimpleRNN  2D   (batch_size, 128)</span>
model.add(layers.SimpleRNN(<span class="hljs-number">128</span>))<font></font>
<font></font>
model.add(layers.Dense(<span class="hljs-number">10</span>))<font></font>
<font></font>
model.summary()</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Adem√°s, la capa RNN puede devolver sus estados internos finales. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Los estados devueltos pueden usarse m√°s tarde para reanudar la ejecuci√≥n del RNN o </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">para inicializar otro RNN</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Esta configuraci√≥n generalmente se usa en el modelo codificador-decodificador, secuencia a secuencia, donde el estado final del codificador se utiliza para el estado inicial del decodificador. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para que la capa RNN devuelva su estado interno, establezca el par√°metro </font></font><code>return_state</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">en valor </font></font><code>True</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">al crear la capa. Tenga en cuenta que hay </font></font><code>LSTM</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2 tensores de estado, y </font></font><code>GRU</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">solo uno. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para ajustar el estado inicial de una capa, simplemente llame a la capa con un argumento adicional </font></font><code>initial_state</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Tenga en cuenta que la dimensi√≥n debe coincidir con la dimensi√≥n del elemento de capa, como en el siguiente ejemplo.</font></font><br>
<br>
<pre><code class="python hljs">encoder_vocab = <span class="hljs-number">1000</span>
decoder_vocab = <span class="hljs-number">2000</span><font></font>
<font></font>
encoder_input = layers.Input(shape=(<span class="hljs-literal">None</span>, ))<font></font>
encoder_embedded = layers.Embedding(input_dim=encoder_vocab, output_dim=<span class="hljs-number">64</span>)(encoder_input)<font></font>
<font></font>
<span class="hljs-comment">#       </span><font></font>
output, state_h, state_c = layers.LSTM(<font></font>
    <span class="hljs-number">64</span>, return_state=<span class="hljs-literal">True</span>, name=<span class="hljs-string">'encoder'</span>)(encoder_embedded)<font></font>
encoder_state = [state_h, state_c]<font></font>
<font></font>
decoder_input = layers.Input(shape=(<span class="hljs-literal">None</span>, ))<font></font>
decoder_embedded = layers.Embedding(input_dim=decoder_vocab, output_dim=<span class="hljs-number">64</span>)(decoder_input)<font></font>
<font></font>
<span class="hljs-comment">#  2     LSTM    </span><font></font>
decoder_output = layers.LSTM(<font></font>
    <span class="hljs-number">64</span>, name=<span class="hljs-string">'decoder'</span>)(decoder_embedded, initial_state=encoder_state)<font></font>
output = layers.Dense(<span class="hljs-number">10</span>)(decoder_output)<font></font>
<font></font>
model = tf.keras.Model([encoder_input, decoder_input], output)<font></font>
model.summary()<font></font>
</code></pre><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Capas RNN y c√©lulas RNN</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La API RNN, adem√°s de las capas RNN incorporadas, tambi√©n proporciona API a nivel de celda. </font><font style="vertical-align: inherit;">A diferencia de las capas RNN, que procesan paquetes completos de secuencias de entrada, una celda RNN procesa solo un paso de tiempo. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La celda est√° dentro del ciclo de la </font></font><code>for</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">capa RNN. </font><font style="vertical-align: inherit;">Al envolver una celda con una capa, se </font></font><code>tf.keras.layers.RNN</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">obtiene una capa capaz de procesar paquetes de secuencia, p. </font></font><code>RNN(LSTMCell(10))</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Matem√°ticamente, </font></font><code>RNN(LSTMCell(10))</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">da el mismo resultado que </font></font><code>LSTM(10)</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">De hecho, la implementaci√≥n de esta capa dentro de TF v1.x fue solo para crear la celda RNN correspondiente y envolverla en la capa RNN. </font><font style="vertical-align: inherit;">Sin embargo, el uso de capas incrustadas </font></font><code>GRU</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">y </font></font><code>LSTM</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">permite el uso de CuDNN que puede brindarle un mejor rendimiento.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Hay tres celdas RNN incorporadas, cada una de las cuales corresponde a su propia capa RNN.</font></font><br>
<br>
<ul>
<li><code>tf.keras.layers.SimpleRNNCell</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">partidos la capa </font></font><code>SimpleRNN</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font></li>
<li><code>tf.keras.layers.GRUCell</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">partidos la capa </font></font><code>GRU</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font></li>
<li><code>tf.keras.layers.LSTMCell</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">partidos la capa </font></font><code>LSTM</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La abstracci√≥n de una celda junto con una clase com√∫n </font></font><code>tf.keras.layers.RNN</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">hace que sea muy f√°cil implementar arquitecturas RNN personalizadas para su investigaci√≥n.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Estado de guardado entre lotes</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Al procesar secuencias largas (posiblemente interminables), es posible que desee utilizar el patr√≥n de </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">estado de cross-batch</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Por lo general, el estado interno de la capa RNN se restablece con cada nuevo paquete de datos (es decir, se supone que cada ejemplo que ve la capa es independiente del pasado). La capa mantendr√° el estado solo mientras dure el procesamiento de este elemento. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Sin embargo, si tiene secuencias muy largas, es √∫til dividirlas en otras m√°s cortas y transferirlas a la capa RNN a su vez sin restablecer el estado de la capa. Por lo tanto, una capa puede almacenar informaci√≥n sobre la secuencia completa, aunque solo ver√° una subsecuencia a la vez. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Puede hacer esto estableciendo `stateful = True` en el constructor.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Si tiene la secuencia `s = [t0, t1, ... t1546, t1547]`, puede dividirla, por ejemplo, en:</font></font><br>
<br>
<pre><code class="python hljs">s1 = [t0, t1, ... t100]<font></font>
s2 = [t101, ... t201]<font></font>
...<font></font>
s16 = [t1501, ... t1547]</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Entonces puedes procesarlo con:</font></font><br>
<br>
<pre><code class="python hljs">lstm_layer = layers.LSTM(<span class="hljs-number">64</span>, stateful=<span class="hljs-literal">True</span>)
<span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> sub_sequences:<font></font>
  output = lstm_layer(s)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Cuando desee limpiar la condici√≥n, √∫sela </font></font><code>layer.reset_states()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font><br>
<blockquote><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nota:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> En este caso, se supone que el ejemplo </font></font><code>i</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">en este paquete es una continuaci√≥n del ejemplo del </font></font><code>i</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">paquete anterior. </font><font style="vertical-align: inherit;">Esto significa que todos los paquetes contienen el mismo n√∫mero de elementos (tama√±o del paquete). </font><font style="vertical-align: inherit;">Por ejemplo, si el paquete contiene </font></font><code>[sequence_A_from_t0_to_t100, sequence_B_from_t0_to_t100]</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, el siguiente paquete deber√≠a contener </font></font><code>[sequence_A_from_t101_to_t200, sequence_B_from_t101_to_t200]</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font></blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aqu√≠ hay un ejemplo completo:</font></font><br>
<br>
<pre><code class="python hljs">paragraph1 = np.random.random((<span class="hljs-number">20</span>, <span class="hljs-number">10</span>, <span class="hljs-number">50</span>)).astype(np.float32)<font></font>
paragraph2 = np.random.random((<span class="hljs-number">20</span>, <span class="hljs-number">10</span>, <span class="hljs-number">50</span>)).astype(np.float32)<font></font>
paragraph3 = np.random.random((<span class="hljs-number">20</span>, <span class="hljs-number">10</span>, <span class="hljs-number">50</span>)).astype(np.float32)<font></font>
<font></font>
lstm_layer = layers.LSTM(<span class="hljs-number">64</span>, stateful=<span class="hljs-literal">True</span>)<font></font>
output = lstm_layer(paragraph1)<font></font>
output = lstm_layer(paragraph2)<font></font>
output = lstm_layer(paragraph3)<font></font>
<font></font>
<span class="hljs-comment"># reset_states()      initial_state.</span>
<span class="hljs-comment">#  initial_state   ,      .</span>
lstm_layer.reset_states()</code></pre><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bidireccional RNN</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para secuencias que no sean series de tiempo (p. Ej., Textos), a menudo sucede que el modelo RNN funciona mejor si procesa la secuencia no solo de principio a fin, sino tambi√©n viceversa. Por ejemplo, para predecir la siguiente palabra en una oraci√≥n, a menudo es √∫til conocer el contexto alrededor de la palabra, y no solo las palabras en frente de ella. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Keras proporciona una API simple para crear tales RNN bidireccionales: un contenedor </font></font><code>tf.keras.layers.Bidirectional</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font><br>
<br>
<pre><code class="python hljs">model = tf.keras.Sequential()<font></font>
<font></font>
model.add(layers.Bidirectional(layers.LSTM(<span class="hljs-number">64</span>, return_sequences=<span class="hljs-literal">True</span>), <font></font>
                               input_shape=(<span class="hljs-number">5</span>, <span class="hljs-number">10</span>)))<font></font>
model.add(layers.Bidirectional(layers.LSTM(<span class="hljs-number">32</span>)))<font></font>
model.add(layers.Dense(<span class="hljs-number">10</span>))<font></font>
<font></font>
model.summary()</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Debajo del cap√≥, la </font></font><code>Bidirectional</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">capa RNN transferida </font></font><code>go_backwards</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">se copiar√° </font><font style="vertical-align: inherit;">y el campo de la </font><font style="vertical-align: inherit;">capa reci√©n copiada se </font><font style="vertical-align: inherit;">volcar√° </font><font style="vertical-align: inherit;">, y as√≠ los datos de entrada se procesar√°n en el orden inverso. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La salida de ` </font></font><code>Bidirectional</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">RNN por defecto ser√° la suma de la salida de la capa directa y la salida de la capa inversa. </font><font style="vertical-align: inherit;">Si necesita otro comportamiento de fusi√≥n, p. </font><font style="vertical-align: inherit;">concatenaci√≥n, cambie el par√°metro `merge_mode` en el constructor de contenedor` Bidirectional`.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Optimizaci√≥n del rendimiento y n√∫cleo CuDNN en TensorFlow 2.0</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En TensorFlow 2.0, las capas LSTM y GRU incorporadas se pueden usar por defecto en los n√∫cleos CuDNN si hay un procesador de gr√°ficos disponible. </font><font style="vertical-align: inherit;">Con este cambio, las capas anteriores </font></font><code>keras.layers.CuDNNLSTM/CuDNNGRU</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">est√°n desactualizadas y puede construir su modelo sin preocuparse por el equipo en el que funcionar√°. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dado que el n√∫cleo CuDNN est√° construido con algunas suposiciones, esto significa que la capa </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">no podr√° usar la capa del n√∫cleo CuDNN si cambia la configuraci√≥n predeterminada de las capas LSTM o GRU incorporadas</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">P.ej.</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cambiar una funci√≥n </font></font><code>activation</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">de </font></font><code>tanh</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">a otra.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cambiar una funci√≥n </font></font><code>recurrent_activation</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">de </font></font><code>sigmoid</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">a otra.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Uso </font></font><code>recurrent_dropout</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">&gt; 0.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Establecerlo </font></font><code>unroll</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">en True, lo que hace que LSTM / GRU descomponga el interno </font></font><code>tf.while_loop</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">en un bucle desplegado </font></font><code>for</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Establecer </font></font><code>use_bias</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">en falso.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Usar m√°scaras cuando los datos de entrada no est√°n justificados correctamente (si la m√°scara coincide con los datos correctos estrictamente correctos, CuDNN a√∫n puede usarse. Este es el caso m√°s com√∫n).</font></font></li>
</ul><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cuando sea posible, use n√∫cleos CuDNN</font></font></h3><br>
<pre><code class="python hljs">batch_size = <span class="hljs-number">64</span>
<span class="hljs-comment">#    MNIST    (batch_size, 28, 28).</span>
<span class="hljs-comment">#     (28, 28) (   ).</span>
input_dim = <span class="hljs-number">28</span><font></font>
<font></font>
units = <span class="hljs-number">64</span>
output_size = <span class="hljs-number">10</span>  <span class="hljs-comment">#   0  9</span><font></font>
<font></font>
<span class="hljs-comment">#  RNN </span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">build_model</span>(<span class="hljs-params">allow_cudnn_kernel=True</span>):</span>
  <span class="hljs-comment"># CuDNN     ,     .</span>
  <span class="hljs-comment">#   `LSTM(units)`    CuDNN,</span>
  <span class="hljs-comment">#   RNN(LSTMCell(units))   non-CuDNN .</span>
  <span class="hljs-keyword">if</span> allow_cudnn_kernel:
    <span class="hljs-comment">#  LSTM      CuDNN.</span>
    lstm_layer = tf.keras.layers.LSTM(units, input_shape=(<span class="hljs-literal">None</span>, input_dim))
  <span class="hljs-keyword">else</span>:
    <span class="hljs-comment">#  LSTMCell  RNN    CuDNN.</span><font></font>
    lstm_layer = tf.keras.layers.RNN(<font></font>
        tf.keras.layers.LSTMCell(units),<font></font>
        input_shape=(<span class="hljs-literal">None</span>, input_dim))<font></font>
  model = tf.keras.models.Sequential([<font></font>
      lstm_layer,<font></font>
      tf.keras.layers.BatchNormalization(),<font></font>
      tf.keras.layers.Dense(output_size)]<font></font>
  )<font></font>
  <span class="hljs-keyword">return</span> model
</code></pre><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cargando el conjunto de datos MNIST</font></font></h3><br>
<pre><code class="python hljs">mnist = tf.keras.datasets.mnist<font></font>
<font></font>
(x_train, y_train), (x_test, y_test) = mnist.load_data()<font></font>
x_train, x_test = x_train / <span class="hljs-number">255.0</span>, x_test / <span class="hljs-number">255.0</span>
sample, sample_label = x_train[<span class="hljs-number">0</span>], y_train[<span class="hljs-number">0</span>]</code></pre><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cree una instancia del modelo y comp√≠lelo</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Hemos elegido </font></font><code>sparse_categorical_crossentropy</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">en funci√≥n de las p√©rdidas. </font><font style="vertical-align: inherit;">La salida del modelo tiene una dimensi√≥n </font></font><code>[batch_size, 10]</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">La respuesta del modelo es un vector entero, cada uno de los n√∫meros est√° en el rango de 0 a 9.</font></font><br>
<br>
<pre><code class="python hljs">model = build_model(allow_cudnn_kernel=<span class="hljs-literal">True</span>)<font></font>
<font></font>
model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="hljs-literal">True</span>), <font></font>
              optimizer=<span class="hljs-string">'sgd'</span>,<font></font>
              metrics=[<span class="hljs-string">'accuracy'</span>])
</code></pre><br>
<pre><code class="python hljs">model.fit(x_train, y_train,<font></font>
          validation_data=(x_test, y_test),<font></font>
          batch_size=batch_size,<font></font>
          epochs=<span class="hljs-number">5</span>)</code></pre><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Construya un nuevo modelo sin n√∫cleo CuDNN</font></font></h3><br>
<pre><code class="python hljs">slow_model = build_model(allow_cudnn_kernel=<span class="hljs-literal">False</span>)<font></font>
slow_model.set_weights(model.get_weights())<font></font>
slow_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="hljs-literal">True</span>), <font></font>
                   optimizer=<span class="hljs-string">'sgd'</span>, <font></font>
                   metrics=[<span class="hljs-string">'accuracy'</span>])<font></font>
slow_model.fit(x_train, y_train, <font></font>
               validation_data=(x_test, y_test), <font></font>
               batch_size=batch_size,<font></font>
               epochs=<span class="hljs-number">1</span>)  <span class="hljs-comment">#         .</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Como puede ver, el modelo construido con CuDNN es mucho m√°s r√°pido para el entrenamiento que el modelo que usa el n√∫cleo TensorFlow habitual. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El mismo modelo con soporte CuDNN se puede utilizar para la salida en un entorno de procesador √∫nico. </font><font style="vertical-align: inherit;">La anotaci√≥n </font></font><code>tf.device</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">simplemente indica el dispositivo utilizado. </font><font style="vertical-align: inherit;">El modelo se ejecutar√° de forma predeterminada en la CPU si la GPU no est√° disponible. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Simplemente no necesita preocuparse por el hardware en el que est√° trabajando. </font><font style="vertical-align: inherit;">¬øNo es genial?</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">with</span> tf.device(<span class="hljs-string">'CPU:0'</span>):<font></font>
  cpu_model = build_model(allow_cudnn_kernel=<span class="hljs-literal">True</span>)<font></font>
  cpu_model.set_weights(model.get_weights())<font></font>
  result = tf.argmax(cpu_model.predict_on_batch(tf.expand_dims(sample, <span class="hljs-number">0</span>)), axis=<span class="hljs-number">1</span>)<font></font>
  print(<span class="hljs-string">'Predicted result is: %s, target result is: %s'</span> % (result.numpy(), sample_label))<font></font>
  plt.imshow(sample, cmap=plt.get_cmap(<span class="hljs-string">'gray'</span>))
</code></pre><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">RNN con entrada de lista / diccionario o entrada anidada</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Las estructuras anidadas le permiten incluir m√°s informaci√≥n en un solo paso. </font><font style="vertical-align: inherit;">Por ejemplo, un cuadro de video puede contener entrada de audio y video simult√°neamente. </font><font style="vertical-align: inherit;">La dimensi√≥n de los datos en este caso puede ser:</font></font><br>
<br>
<pre><code class="plaintext hljs">[batch, timestep, {\"video\": [height, width, channel], \"audio\": [frequency]}]</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En otro ejemplo, los datos escritos a mano pueden tener coordenadas xey para la posici√≥n actual del l√°piz, as√≠ como informaci√≥n de presi√≥n. </font><font style="vertical-align: inherit;">Entonces los datos se pueden representar de la siguiente manera:</font></font><br>
<br>
<pre><code class="plaintext hljs">[batch, timestep, {\"location\": [x, y], \"pressure\": [force]}]</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El siguiente c√≥digo crea un ejemplo de una celda RNN personalizada que funciona con dicha entrada estructurada.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Definir una celda de usuario que admita entrada / salida anidada</font></font></h3><br>
<pre><code class="python hljs">NestedInput = collections.namedtuple(<span class="hljs-string">'NestedInput'</span>, [<span class="hljs-string">'feature1'</span>, <span class="hljs-string">'feature2'</span>])<font></font>
NestedState = collections.namedtuple(<span class="hljs-string">'NestedState'</span>, [<span class="hljs-string">'state1'</span>, <span class="hljs-string">'state2'</span>])<font></font>
<font></font>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">NestedCell</span>(<span class="hljs-params">tf.keras.layers.Layer</span>):</span><font></font>
<font></font>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, unit_1, unit_2, unit_3, **kwargs</span>):</span><font></font>
    self.unit_1 = unit_1<font></font>
    self.unit_2 = unit_2<font></font>
    self.unit_3 = unit_3<font></font>
    self.state_size = NestedState(state1=unit_1, <font></font>
                                  state2=tf.TensorShape([unit_2, unit_3]))<font></font>
    self.output_size = (unit_1, tf.TensorShape([unit_2, unit_3]))<font></font>
    super(NestedCell, self).__init__(**kwargs)<font></font>
<font></font>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">build</span>(<span class="hljs-params">self, input_shapes</span>):</span>
    <span class="hljs-comment"># #  input_shape  2 , [(batch, i1), (batch, i2, i3)]</span>
    input_1 = input_shapes.feature1[<span class="hljs-number">1</span>]<font></font>
    input_2, input_3 = input_shapes.feature2[<span class="hljs-number">1</span>:]<font></font>
<font></font>
    self.kernel_1 = self.add_weight(<font></font>
        shape=(input_1, self.unit_1), initializer=<span class="hljs-string">'uniform'</span>, name=<span class="hljs-string">'kernel_1'</span>)<font></font>
    self.kernel_2_3 = self.add_weight(<font></font>
        shape=(input_2, input_3, self.unit_2, self.unit_3),<font></font>
        initializer=<span class="hljs-string">'uniform'</span>,<font></font>
        name=<span class="hljs-string">'kernel_2_3'</span>)<font></font>
<font></font>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">call</span>(<span class="hljs-params">self, inputs, states</span>):</span>
    <span class="hljs-comment">#     [(batch, input_1), (batch, input_2, input_3)]</span>
    <span class="hljs-comment">#     [(batch, unit_1), (batch, unit_2, unit_3)]</span><font></font>
    input_1, input_2 = tf.nest.flatten(inputs)<font></font>
    s1, s2 = states<font></font>
<font></font>
    output_1 = tf.matmul(input_1, self.kernel_1)<font></font>
    output_2_3 = tf.einsum(<span class="hljs-string">'bij,ijkl-&gt;bkl'</span>, input_2, self.kernel_2_3)<font></font>
    state_1 = s1 + output_1<font></font>
    state_2_3 = s2 + output_2_3<font></font>
<font></font>
    output = [output_1, output_2_3]<font></font>
    new_states = NestedState(state1=state_1, state2=state_2_3)<font></font>
<font></font>
    <span class="hljs-keyword">return</span> output, new_states
</code></pre><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Construya un modelo RNN con entrada / salida anidada</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Construyamos un modelo de Keras que use una capa </font></font><code>tf.keras.layers.RNN</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">y una celda personalizada que acabamos de definir.</font></font><br>
<br>
<pre><code class="python hljs">unit_1 = <span class="hljs-number">10</span>
unit_2 = <span class="hljs-number">20</span>
unit_3 = <span class="hljs-number">30</span><font></font>
<font></font>
input_1 = <span class="hljs-number">32</span>
input_2 = <span class="hljs-number">64</span>
input_3 = <span class="hljs-number">32</span>
batch_size = <span class="hljs-number">64</span>
num_batch = <span class="hljs-number">100</span>
timestep = <span class="hljs-number">50</span><font></font>
<font></font>
cell = NestedCell(unit_1, unit_2, unit_3)<font></font>
rnn = tf.keras.layers.RNN(cell)<font></font>
<font></font>
inp_1 = tf.keras.Input((<span class="hljs-literal">None</span>, input_1))<font></font>
inp_2 = tf.keras.Input((<span class="hljs-literal">None</span>, input_2, input_3))<font></font>
<font></font>
outputs = rnn(NestedInput(feature1=inp_1, feature2=inp_2))<font></font>
<font></font>
model = tf.keras.models.Model([inp_1, inp_2], outputs)<font></font>
<font></font>
model.compile(optimizer=<span class="hljs-string">'adam'</span>, loss=<span class="hljs-string">'mse'</span>, metrics=[<span class="hljs-string">'accuracy'</span>])unit_1 = <span class="hljs-number">10</span>
unit_2 = <span class="hljs-number">20</span>
unit_3 = <span class="hljs-number">30</span><font></font>
<font></font>
input_1 = <span class="hljs-number">32</span>
input_2 = <span class="hljs-number">64</span>
input_3 = <span class="hljs-number">32</span>
batch_size = <span class="hljs-number">64</span>
num_batch = <span class="hljs-number">100</span>
timestep = <span class="hljs-number">50</span><font></font>
<font></font>
cell = NestedCell(unit_1, unit_2, unit_3)<font></font>
rnn = tf.keras.layers.RNN(cell)<font></font>
<font></font>
inp_1 = tf.keras.Input((<span class="hljs-literal">None</span>, input_1))<font></font>
inp_2 = tf.keras.Input((<span class="hljs-literal">None</span>, input_2, input_3))<font></font>
<font></font>
outputs = rnn(NestedInput(feature1=inp_1, feature2=inp_2))<font></font>
<font></font>
model = tf.keras.models.Model([inp_1, inp_2], outputs)<font></font>
<font></font>
model.compile(optimizer=<span class="hljs-string">'adam'</span>, loss=<span class="hljs-string">'mse'</span>, metrics=[<span class="hljs-string">'accuracy'</span>])
</code></pre><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Entrenar el modelo en datos generados aleatoriamente</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dado que no tenemos un buen conjunto de datos para este modelo, utilizamos datos aleatorios generados por la biblioteca Numpy para la demostraci√≥n.</font></font><br>
<br>
<pre><code class="python hljs">input_1_data = np.random.random((batch_size * num_batch, timestep, input_1))<font></font>
input_2_data = np.random.random((batch_size * num_batch, timestep, input_2, input_3))<font></font>
target_1_data = np.random.random((batch_size * num_batch, unit_1))<font></font>
target_2_data = np.random.random((batch_size * num_batch, unit_2, unit_3))<font></font>
input_data = [input_1_data, input_2_data]<font></font>
target_data = [target_1_data, target_2_data]<font></font>
<font></font>
model.fit(input_data, target_data, batch_size=batch_size)<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Con una capa, </font></font><code>tf.keras.layers.RNN</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">solo necesita determinar la l√≥gica matem√°tica de un solo paso dentro de la secuencia, y la capa </font></font><code>tf.keras.layers.RNN</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">manejar√° la iteraci√≥n de la secuencia por usted. </font><font style="vertical-align: inherit;">Esta es una forma incre√≠blemente poderosa de crear r√°pidamente prototipos de nuevos tipos de RNN (por ejemplo, la variante LSTM). </font></font><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Despu√©s de la verificaci√≥n, la traducci√≥n tambi√©n aparecer√° en Tensorflow.org. </font><font style="vertical-align: inherit;">Si desea participar en la traducci√≥n de la documentaci√≥n del sitio web de Tensorflow.org al ruso, comun√≠quese personalmente o env√≠e sus comentarios. </font><font style="vertical-align: inherit;">Cualquier correcci√≥n y comentario son apreciados.</font></font></i></div>
      
    </div><p class="reference-to-source js-reference-to-source">Source: https://habr.com/ru/post/undefined/</p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es487798/index.html">C√≥mo migramos de Oracle JDK y Java Web Start a AdoptOpenJDK y OpenWebStart</a></li>
<li><a href="../es487800/index.html">¬øPor qu√© es importante decirle al solicitante qu√© sali√≥ mal durante la entrevista (y c√≥mo hacerlo correctamente)?</a></li>
<li><a href="../es487802/index.html">UPS ininterrumpible APC Smart y c√≥mo cocinarlos</a></li>
<li><a href="../es487804/index.html">Meetup de equipos de crecimiento en Raiffeisenbank</a></li>
<li><a href="../es487806/index.html">Crear una peque√±a API de Deno</a></li>
<li><a href="../es487812/index.html">Prueba de espectro LED polaco Led E27</a></li>
<li><a href="../es487814/index.html">La velocidad y la fiabilidad son m√°s altas, y el precio es m√°s bajo. Nuevas unidades de estado s√≥lido Kingston KC2000</a></li>
<li><a href="../es487822/index.html">AvitoTech On Tour: encuentro de Android en Nizhny Novgorod</a></li>
<li><a href="../es487824/index.html">Descripci√≥n general de las l√°mparas LED Spectrum Led GU10 de Europa</a></li>
<li><a href="../es487826/index.html">Descripci√≥n general de las l√°mparas LED de Polonia Spectrum Led E14</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>