<!doctype html>
<html class="no-js" lang="zh-CN">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🐘 👨🏾‍🍳 👳 多标签分类 🚵 🌻 👨🏽‍⚕️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="您好，habrozhiteli！我们决定举从书由安德烈Burkov的摘录，机器学习，无需额外字，献给分类。
 
 为了描述图中的图像，可以同时使用几个标签：“针叶林”，“山峰”，“道路”。如果标签的可能值数量很大，但是它们都具有与标签相同的性质，则每个标签样本可以转换为多个标签数据，每个标签一个。所...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>多标签分类</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/488362/"><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><img src="https://habrastorage.org/webt/hm/vf/c_/hmvfc_yyxepplv1mj0crw1vu7pw.jpeg" align="left" alt="图片"></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">您好，habrozhiteli！</font><font style="vertical-align: inherit;">我们决定举从书由安德烈Burkov的摘录</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">，机器学习，无需额外字</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">，献给分类。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
为了描述图中的图像，可以同时使用几个标签：“针叶林”，“山峰”，“道路”。如果标签的可能值数量很大，但是它们都具有与标签相同的性质，则每个标签样本可以转换为多个标签数据，每个标签一个。所有这些新数据将具有相同的特征向量，并且只有一个标签。结果，任务变成了多类分类问题。可以使用“一劳永逸”的策略来解决。与通常的多类分类问题的唯一区别是出现了新的超参数：阈值。如果标签的相似性得分高于阈值，则将该标签分配给输入特征向量。在这种情况下，可以将多个标签分配给一个特征向量。使用控制集选择阈值。</font></font><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
为了解决带有多个标签的分类问题，可以类似地应用自然转换为多类的算法（决策树，逻辑回归，神经网络等）。它们为每个类返回一个估计值，因此我们可以定义一个阈值，然后为一个接近度得分超过此阈值的特征向量分配几个标签。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
使用二进制交叉熵作为成本函数，自然可以在多标签分类中训练神经网络。在这种情况下，神经网络的输出层每个标签有一个节点。输出层中的每个节点都有S型激活功能。因此，每个标签l是二进制的</font></font><img src="https://habrastorage.org/webt/2n/_n/mg/2n_nmgvuciuadryomnq3urrg1xw.jpeg" alt="图片"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">其中，l = 1，...，L且i = 1，...，N的概率的二进制交叉熵</font></font><img src="https://habrastorage.org/webt/lb/le/cf/lblecfgyuy23k8zvlasuajncsds.jpeg" alt="图片"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">样品xi是标记的1-，定义为</font></font><img src="https://habrastorage.org/webt/qd/1e/fx/qd1efxpmi6mmuhv_fhmsqpjgpi4.jpeg" alt="图片"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
最小化的标准-所有训练样本中的简单平均值二进制交叉熵的所有成员的及其所有标签。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
在可能的标签值数量很少的情况下，您可以尝试将具有多个标签的分类问题转换为多类分类问题。</font><font style="vertical-align: inherit;">想象以下问题。</font><font style="vertical-align: inherit;">您需要为图像分配两种类型的标签。</font><font style="vertical-align: inherit;">第一类标签可以具有两种可能的含义：{ </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">照片，绘画</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> }; </font><font style="vertical-align: inherit;">第二种标记可以具有三种可能的含义：{ </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">纵向，横向，其他</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">}。</font><font style="vertical-align: inherit;">对于两个源类的每种组合，您可以创建一个新的哑类，例如：</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/mz/px/zn/mzpxzn0rlrumwoql7gkk3no7ihk.jpeg" alt="图片"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
现在我们拥有相同的标记数据，但是我们用一个虚拟标签替换了一组真实标签，这些虚拟标签的值介于1到6之间。实际上，如果没有太多可能的类组合，则此方法会产生良好的结果。否则，需要使用更多的训练数据来补偿班级数量的增加。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
后一种方法的主要优点是，标签保持关联，这与上述方法不同，后者相互独立地预测每个标签。在许多任务中，标签之间的相关性可能是一个重要因素。例如，假设您要将电子邮件分为</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">垃圾邮件</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">和</font><i><font style="vertical-align: inherit;">非</font></i><i><font style="vertical-align: inherit;">垃圾邮件</font></i></font><i><font style="vertical-align: inherit;"></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">，同时又与普通且重要。</font><font style="vertical-align: inherit;">您可能希望排除诸如[ </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">垃圾邮件，重要</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ]之类的</font><font style="vertical-align: inherit;">预测</font><font style="vertical-align: inherit;">。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">7.5。</font><font style="vertical-align: inherit;">合奏训练</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
我们在第3章中介绍的基本算法有其局限性。由于其简单性，有时他们无法创建对您的任务足够有效的模型。在这种情况下，您可以尝试使用深度神经网络。但是，在实践中，深度神经网络需要大量的标记数据，而您可能没有。提高简单学习算法有效性的另一种方法是使用</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">集成训练</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
集成训练是一种训练范例，它不仅基于训练一个超级正确模型，而且基于大量精度不高的模型并结合这些</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">弱</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">模型</font><font style="vertical-align: inherit;">给出的预测</font><font style="vertical-align: inherit;">来获得更正确的</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">元模型</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
精度低的模型通常由</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">弱学习算法</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">训练</font><font style="vertical-align: inherit;">，这些</font><b><font style="vertical-align: inherit;">算法</font></b><font style="vertical-align: inherit;">不能训练复杂的模型，因此在训练和预测阶段显示出很高的速度。大多数情况下，决策树学习算法被用作弱算法，该算法通常会在几次迭代后停止破坏训练集。结果是很小的树并且不是很规则的树，但是，正如训练整体的想法所说的那样，如果树不相同并且每棵树至少比随机猜测好一点，我们可以通过组合大量这样的树来获得高精度。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
获取条目的最终预测</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">，使用某种加权投票的方法组合所有弱模型的预测。</font><font style="vertical-align: inherit;">对选票进行加权的具体形式取决于算法，但本质并不取决于算法：如果总体上弱模型预测电子邮件为垃圾邮件，</font><font style="vertical-align: inherit;">则将</font><i><font style="vertical-align: inherit;">垃圾邮件</font></i><font style="vertical-align: inherit;">标签</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">分配给样本</font><font style="vertical-align: inherit;">。</font><font style="vertical-align: inherit;">
训练合奏的两种主要方法是</font><b><font style="vertical-align: inherit;">加强</font></b><font style="vertical-align: inherit;">和</font><b><font style="vertical-align: inherit;">套袋</font></b><font style="vertical-align: inherit;">（聚合）。</font><font style="vertical-align: inherit;">促进和装袋这两个术语的翻译不准确且不习惯。</font></font><i><font style="vertical-align: inherit;"></font></i><font style="vertical-align: inherit;"></font><br>
<br><font style="vertical-align: inherit;"></font><b><font style="vertical-align: inherit;"></font></b><font style="vertical-align: inherit;"></font><b><font style="vertical-align: inherit;"></font></b><font style="vertical-align: inherit;"></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">7.5.1。</font><font style="vertical-align: inherit;">提振和装袋</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
增强方法是使用初始训练数据，并使用弱算法迭代创建几个模型。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
每个新模型都与以前的模型不同，在构造它时，一个弱算法试图“修复”先前模型所产生的错误。</font><font style="vertical-align: inherit;">最终的集成模型是这些许多弱迭代构建模型的组合。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
套袋的本质是创建大量训练数据的“副本”（每个副本与其他副本略有不同），然后对每个副本应用一个弱算法，以获得多个弱模型，然后将它们组合在一起。</font><font style="vertical-align: inherit;">一种基于装袋思想的广泛使用且高效的机器学习算法是</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">随机森林</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">7.5.2。</font><font style="vertical-align: inherit;">随机森林</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
“经典”装袋算法的工作原理如下。</font><font style="vertical-align: inherit;">从现有训练集中创建B个随机样本</font></font><img src="https://habrastorage.org/webt/-d/6r/gn/-d6rgnsn5wk-yzl3dbk0i7x2ole.jpeg" alt="图片"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">（每个b = 1，...，B），并在每个样本的基础上</font></font><img src="https://habrastorage.org/webt/-d/6r/gn/-d6rgnsn5wk-yzl3dbk0i7x2ole.jpeg" alt="图片"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">构建</font></font><img src="https://habrastorage.org/webt/nd/ew/4z/ndew4zvx7r0jpeilfknakojmrbs.jpeg" alt="图片"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">决策树</font><font style="vertical-align: inherit;">模型</font><font style="vertical-align: inherit;">。</font><font style="vertical-align: inherit;">为了得到</font></font><img src="https://habrastorage.org/webt/-d/6r/gn/-d6rgnsn5wk-yzl3dbk0i7x2ole.jpeg" alt="图片"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">某个b的</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">样本，需要制作一个带有替换</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">的</font><b><font style="vertical-align: inherit;">样本</font></b><font style="vertical-align: inherit;">。</font><font style="vertical-align: inherit;">也就是说，首先创建一个空样本，然后从训练集中选择一个随机样本，并将其精确副本放置在中</font></font><img src="https://habrastorage.org/webt/-d/6r/gn/-d6rgnsn5wk-yzl3dbk0i7x2ole.jpeg" alt="图片"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">，而样本本身仍保留在原始训练集中。</font><font style="vertical-align: inherit;">继续选择数据直到满足条件为止，</font></font><img src="https://habrastorage.org/webt/se/au/-5/seau-5gwous1c1cshmrx8rwubig.jpeg" alt="图片"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
通过训练，获得了</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">B个</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">决策树。</font><font style="vertical-align: inherit;">在回归的情况下，</font><font style="vertical-align: inherit;">将新样本</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">的预测</font><font style="vertical-align: inherit;">确定为</font><b><font style="vertical-align: inherit;">B</font></b><font style="vertical-align: inherit;">的平均值</font></font><b><font style="vertical-align: inherit;"></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 预报</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/re/zp/mj/rezpmjqa9lo7w4dxvqd6njwwqcm.jpeg" alt="图片"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
或在分类的情况下以多数票通过。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
随机森林与经典套袋只有一个不同。它使用一种改进的树学习算法，该算法在学习过程中进行每次拆分，都会检查特征的随机子集。这样做是为了消除树之间的相关性：如果一个或多个特征具有较大的预测能力，则许多树会选择它们来拆分数据。这将导致大量“相关树木”出现在“森林”中。具有高预测能力的符号相关阻止了预测精度的提高。好的模型最有可能与同一预测相吻合，而坏模型则不太可能与之吻合并且给出不同的预测，这说明了模型集成的高效率。相关性将使较差的模型更有可能达成共识，这会扭曲投票方式或影响平均值。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
进行调整最重要的超参数是树B的数量和每次分割必须考虑的特征随机子集的大小。</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
随机森林是最广泛使用的集成学习算法之一。是什么决定了它的有效性？原因是通过使用原始数据集中的几个样本，我们减少</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">了</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">最终模型</font><font style="vertical-align: inherit;">的</font><b><font style="vertical-align: inherit;">方差</font></b><font style="vertical-align: inherit;">。请记住，低方差意味着</font><b><font style="vertical-align: inherit;">再培训的</font></b><font style="vertical-align: inherit;">易感性</font></font><b><font style="vertical-align: inherit;"></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font><font style="vertical-align: inherit;">当模型试图解释数据集中的微小变化时，就会发生重新训练，因为数据集只是我们尝试模拟的现象的所有可能示例的一小部分。</font><font style="vertical-align: inherit;">如果无法成功地形成训练集，则可能会出现一些不良（但不可避免）的伪像：噪声，异常，代表性数据过多或不足。</font><font style="vertical-align: inherit;">通过创建几个随机样本并替换训练集，我们减少了这些伪影的影响。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">7.5.3。</font><font style="vertical-align: inherit;">梯度提升</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
基于提升思想的另一种有效的整体训练算法是梯度提升。</font><font style="vertical-align: inherit;">首先，考虑在回归中使用梯度增强。</font><font style="vertical-align: inherit;">我们将开始使用常量模型来构建有效的回归模型</font></font><img src="https://habrastorage.org/webt/7u/7x/jd/7u7xjdufljpsjwwu45j9_gkc3r0.jpeg" alt="图片"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">（就像在ID3中所做的那样）：</font></font><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/em/vj/bv/emvjbvtmptxl_d3bihzev4wbh7o.jpeg" alt="图片"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
然后更改训练集中所有样本i = 1，...，N的标签：</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/va/wc/ah/vawcahk0zsdpgnumh_bfjuozw_e.jpeg" alt="图片"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
在这里，它</font></font><img src="https://habrastorage.org/webt/rf/1x/pk/rf1xpkgfxmcivy-1tqpwv2vgroi.jpeg" alt="图片"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">被称为</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">余数，</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">并且是样本的新标签。</font></font><img src="https://habrastorage.org/webt/dk/ey/2r/dkey2rj3yf-zkei2029wfa2ujso.jpeg" alt="图片"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
现在，我们使用带有残差的修改后的训练集而不是原始标签来构建决策树的新模型。</font></font><img src="https://habrastorage.org/webt/mx/fw/um/mxfwumzpc5tq1wjpdate2rxra48.jpeg" alt="图片"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">现在将增强模型定义为</font></font><img src="https://habrastorage.org/webt/cy/c2/vz/cyc2vz0tmrrihcm6kta_7rnwmui.jpeg" alt="图片"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">其中α是学习速度（超参数）。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
然后，我们使用公式7.2重新计算残差，再次替换训练数据中的标签，教导决策树的新模型，在</font><font style="vertical-align: inherit;">重复过程时</font></font><img src="https://habrastorage.org/webt/p4/wk/nl/p4wknlhlvwiqtr7zz7lx_y5oq3s.jpeg" alt="图片"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">重新定义提升模型</font></font><img src="https://habrastorage.org/webt/n-/mn/ht/n-mnhtck0rar7pz4anzlbdc-bmo.jpeg" alt="图片"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">，直到合并预定的最大</font><font style="vertical-align: inherit;">树</font><font style="vertical-align: inherit;">数</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">M。</font></font></b><font style="vertical-align: inherit;"></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
让我们直观地了解这里发生了什么。通过计算残差，我们确定当前模型f预测每个训练样本的目标的好坏程度。然后，我们训练另一棵树来纠正当前模型的错误（这就是为什么我们使用剩余部分而不是实际标签的原因），并以一定权重α将新树添加到现有模型中。结果，添加到模型中的每个新树都部分纠正了先前树所犯的错误。该过程一直持续到树的最大数量M（另一个超参数）组合在一起为止。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
现在，让我们尝试回答为什么此算法称为梯度增强的问题。在梯度增强中，我们不像第4章中那样解决线性回归问题，不计算梯度。要查看梯度提升和梯度下降之间的相似之处，请记住为什么我们在线性回归中计算梯度：找出参数值的方向以最小化MSE成本函数。渐变显示了方向，但没有显示该方向的距离，因此在每次迭代中，我们都走了很小的一步，然后再次确定了方向。在梯度增强中也会发生同样的事情，但是我们不是直接计算梯度，而是以残差的形式使用其估计值：它们显示了应如何调整模型以减少误差（残差）。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
在梯度增强中，可以使用三个主要的超参数进行调整：树的数量，学习速度和树的深度。所有这三个因素都会影响模型的准确性。树木的深度也会影响学习和预测的速度：深度越小，速度越快。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
可以证明，通过残差学习可以优化标准误差标准的整体模型f。在这里，您可以看到与套袋的区别：提振可以减少偏见（或缺乏教育），而不是变化。结果，加强训练受到重新训练。但是，通过调整树木的深度和数量，可以大大避免重新训练。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
梯度提升与分级任务相似，但步骤略有不同。考虑二进制分类的情况。假设有M个回归决策树。与逻辑回归类似，决策树集合的预测使用S型函数建模：</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/wy/pd/gw/wypdgwjjgpzuojrelehnadtdggc.jpeg" alt="图片"></div><br><font style="vertical-align: inherit;"></font><img src="https://habrastorage.org/webt/w6/3d/kb/w63dkbj3f9j-ik9hrhqbexyxtem.jpeg" alt="图片"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">回归树</font><font style="vertical-align: inherit;">
在哪里</font><font style="vertical-align: inherit;">。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
再次，如逻辑回归一样，当试图找到最大化的模型时</font></font><img src="https://habrastorage.org/webt/c4/lv/tt/c4lvttexfzr8disbsv1ph_drzka.jpeg" alt="图片"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">，将应用最大似然原理。同样，为了避免数值溢出，我们将似然对数的总和最大化，而不是似然的乘积。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
该算法与初始常数模型开始</font></font><img src="https://habrastorage.org/webt/hn/7j/ep/hn7jepdxhudnxvjjgbulvfwtpyw.jpeg" alt="图片"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">，其中</font></font><img src="https://habrastorage.org/webt/gj/6s/f6/gj6sf6i874m3gq3_fbbxbkrudae.jpeg" alt="图片"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">（可以示出，这样的初始化为S形函数是最佳的。）然后，在每次迭代米，一个新的树FM被添加到模型中。寻找最佳树</font></font><img src="https://habrastorage.org/webt/b0/5w/rv/b05wrvcuk5hzvnrmkpebdnk_kxu.jpeg" alt="图片"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">为了寻找最佳树</font></font><img src="https://habrastorage.org/webt/xm/u4/dp/xmu4dpm1hi3-podtuiayxggydti.jpeg" alt="图片"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">，首先</font></font><img src="https://habrastorage.org/webt/ep/hx/gu/ephxgusen4ou8pc8atm4ksqathq.jpeg" alt="图片"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">为每个i = 1，...，N </font><font style="vertical-align: inherit;">计算</font><font style="vertical-align: inherit;">当前模型</font><font style="vertical-align: inherit;">的偏导数</font><font style="vertical-align: inherit;">：</font></font><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/wg/v_/oo/wgv_oogvmupu4q5j6g3rq7dphvk.jpeg" alt="图片"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
其中f是建立在先前迭代m-1上的集成分类器的模型。要计算</font></font><img src="https://habrastorage.org/webt/ep/hx/gu/ephxgusen4ou8pc8atm4ksqathq.jpeg" alt="图片"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">，我们需要找到</font></font><img src="https://habrastorage.org/webt/pv/pe/lb/pvpelb2jplsmq_jmkbdfyl-pzom.jpeg" alt="图片"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">所有i相对于f </font><font style="vertical-align: inherit;">的导数</font><font style="vertical-align: inherit;">。</font><font style="vertical-align: inherit;">注意，</font></font><img src="https://habrastorage.org/webt/nk/zn/0l/nkzn0lh0l0brnzl_vh0xxlmttrq.jpeg" alt="图片"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">上式中右项相对于f </font><font style="vertical-align: inherit;">的</font><font style="vertical-align: inherit;">导数为</font></font><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/xx/en/iu/xxeniu2qr35ln17asbfk2fqb3sc.jpeg" alt="图片"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
然后，通过替换</font></font><img src="https://habrastorage.org/webt/a1/fv/cu/a1fvcukqqvsu5wpv3x8i1zj5smc.jpeg" alt="图片"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">相应偏导数</font><font style="vertical-align: inherit;">的原始标签对训练集进行变换</font></font><img src="https://habrastorage.org/webt/ep/hx/gu/ephxgusen4ou8pc8atm4ksqathq.jpeg" alt="图片"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">，并在转换后的训练集的基础上构建新树，</font></font><img src="https://habrastorage.org/webt/w1/8f/41/w18f41gto37doyvyakgs4np_fyy.jpeg" alt="图片"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">然后，确定最佳更新步骤</font></font><img src="https://habrastorage.org/webt/1n/cf/sj/1ncfsjxfe-tao3ep_csuw-s-arw.jpeg" alt="图片"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">为：</font></font><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/co/qc/hw/coqchwpctxgy2ukbdwaybkouxx0.jpeg" alt="图片"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
在迭代m的末尾，我们通过</font></font><img src="https://habrastorage.org/webt/r7/ox/vy/r7oxvyc5mesbifpwfjtkcrtdr2q.jpeg" alt="图片"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">添加新树来</font><font style="vertical-align: inherit;">更新集成模型</font></font><img src="https://habrastorage.org/webt/s2/vp/u0/s2vpu0-7pmzuzv0n55f1z1fgktu.jpeg" alt="图片"><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/vg/ar/qn/vgarqnddik0vjhxxfsesr1t5qs8.jpeg" alt="图片"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
迭代一直进行到满足m = M的条件为止，此后训练停止并获得集成模型f。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
梯度提升是最强大的机器学习算法之一。</font><font style="vertical-align: inherit;">不仅因为它创建了非常准确的模型，还因为它能够处理具有数百万个数据和功能的海量数据集。</font><font style="vertical-align: inherit;">通常，它的准确性优于随机森林，但由于其一致性，它的学习速度会慢得多。</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN488346/index.html">在Linux上的Python 3.7虚拟环境中使用SCIP和GLPK安装或工具</a></li>
<li><a href="../zh-CN488348/index.html">网络研讨会“十大敏捷挑战和在一小时内克服挑战的方式”，2月17日，莫斯科时间</a></li>
<li><a href="../zh-CN488352/index.html">VDI成本比较：内部部署与公共云</a></li>
<li><a href="../zh-CN488356/index.html">在圣彼得堡国立海洋技术大学接受DassaultSystèmes产品培训</a></li>
<li><a href="../zh-CN488360/index.html">大数据神话与数字文化</a></li>
<li><a href="../zh-CN488366/index.html">再次关于“俄罗斯时区的时区信息不正确” [.Net bug，ID：693286]</a></li>
<li><a href="../zh-CN488368/index.html">我在进行第一个大型项目时学到的东西</a></li>
<li><a href="../zh-CN488370/index.html">TDD用于微控制器。第2部分：间谍如何摆脱成瘾</a></li>
<li><a href="../zh-CN488374/index.html">电报+ 1C + Webhooks + Apache +自签名证书</a></li>
<li><a href="../zh-CN488376/index.html">当原则“让所有事情都下地狱，那就去做！” 不起作用：拖拉笔记</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>