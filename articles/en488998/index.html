<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🤗 🚅 🗼 Why AI requirements can only make matters worse 🙅 💌 👆🏼</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="By creating more transparent neural networks, we can begin to overly trust them. It may be worth changing the methods by which they explain their work...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Why AI requirements can only make matters worse</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/488998/"><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">By creating more transparent neural networks, we can begin to overly trust them. </font><font style="vertical-align: inherit;">It may be worth changing the methods by which they explain their work.</font></font></h3><br>
<img src="https://cdn.technologyreview.com/i/images/sa-gettyimages-97233294-web.jpg?sw=1272&amp;cx=0&amp;cy=0&amp;cw=3000&amp;ch=1688"><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Apol Esan</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> once on trial took a ride in a robotic car from Uber. Instead of worrying about the empty driver's seat, passengers were asked to watch a “calming” screen, which showed how the car saw the road: dangers were drawn in orange and red, safe areas in dark blue. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
For Esan, who studies the interaction of people with AI at the Georgia Institute of Technology in Atlanta, the message that they tried to convey to him was understandable: “Don’t worry, these are the reasons why the machine behaves this way.” However, something in the alien image of the street did not reassure, but rather emphasized the strangeness of what was happening. Esan wondered: what if the robomobile could really explain?</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The success of deep learning is based on uncertain picking in the code: the best neural networks adjust and adapt to improve them further, and practical results overtake their theoretical understanding. In summary, the details of how the trained model works are usually unknown. We are already used to thinking of AI as a black box.</font></font><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
And most of the time it suits us - when it comes to tasks such as playing go, translating text, or picking up the next series with Netflix. But if AI is used to help make decisions in areas such as law enforcement, medical diagnostics, and robotic vehicles, then we need to understand how it makes its decisions and know when they turn out to be wrong. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
People need the opportunity to disagree with an automatic solution or reject it, says </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Iris Hawley</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , an IT specialist at Williams College in Williamsstown, Massachusetts. And without it, people will resist this technology. “Already now you can observe how this happens, in the form of people's reactions to facial recognition systems,” she says.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Esan is part of a small but growing group of researchers trying to improve AI's ability to explain and help us peer into the black box. The purpose of creating the so-called interpreted or explained by AI (III) is to help people understand on what signs of data the neural network is really learning - and decide whether the resulting model turned out to be accurate and unbiased. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
One solution is to create machine learning (MO) systems that demonstrate the insides of their work - the so-called aquarium AI, instead of the AI ​​in the black box. Aquarium models are usually radically simplified versions of the NS, in which it is easier to track how individual pieces of data affect the model. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
“There are people in this community who urge the use of aquarium models in any high stakes situation,” says</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Jennifer Worthman Vaughn</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , IT Specialist at Microsoft Research. “And overall, I agree.” Simple aquarium models can work just as well as more complex NSs, on certain types of structured data, such as tables or statistics. And in some cases this is enough. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
However, it all depends on the area of ​​work. If we want to learn from such fuzzy data as images or text, we have no choice but to use deep - and therefore opaque - neural networks. The ability of such NSs to find a meaningful connection between a huge number of disparate features is associated with their complexity.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
And even here, aquarium MO can help. One solution is to go through the data twice, training the imperfect aquarium model as a debugging step to catch potential errors that you would like to fix. After cleaning the data, you can also train a more accurate model of AI in a black box. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
However, such a balance is difficult to maintain. Too much transparency can cause information overload. In a </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">study</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> from 2018, which examined the interaction of untrained users with MO tools, Vaughn found that transparent models can actually complicate the search and correction of model errors.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Another approach is to include a visualization that shows several key properties of the model and the underlying data. The idea is to identify serious problems by eye. For example, a model may rely too heavily on certain attributes, which may be a signal for bias. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
These visualization tools have become extremely popular in a short time. But is there any use for them? In the first </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">study of</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> this kind, Vaughn and the team tried to answer this question, and eventually found several serious problems.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The team took two popular interpretive tools that provide an overview of the model with the help of graphs and charts, where it is noted what data the model mainly paid attention to during training. Eleven AI professionals with various backgrounds, backgrounds, and backgrounds have been hired from Microsoft. They took part in a simulation of interaction with the MO model, trained on national income data from the 1994 United States Census. The experiment was specifically designed to simulate how data scientists use interpreting tools to perform their daily tasks.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The team found something amazing. Yes, sometimes tools helped people find missing values ​​in the data. However, all this usefulness has faded in comparison with the tendency to excessive trust in visualizations, as well as errors in their understanding. Sometimes users could not even describe what exactly the visualizations demonstrate. This led to incorrect assumptions regarding the data set, models, and the interpreting tools themselves. It also inspired false confidence in the tools and aroused enthusiasm for putting these models into practice, although sometimes it seemed to the participants that something was going wrong. Which is unpleasant, it worked even when the output was specially tweaked so that the explanations of the work made no sense.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
To confirm the findings, the researchers conducted an online survey among 200 professionals in the field of Moscow, attracted through mailing lists and social networks. They found similar confusion and unfounded confidence. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
To make matters worse, many survey participants were willing to use visualizations to make decisions about model implementation, despite recognizing that they did not understand the underlying mathematics. “It was especially surprising to see people justify the oddities in the data by coming up with explanations for this,” says </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Harmanpreet Kaur</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> of the University of Michigan, co-author of the study. “The distortion of automation is a very important factor that we have not considered.”</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Oh, this is a distortion of automation. In other words, people tend to trust computers. And this is not a new phenomenon. From airplane autopilots to spellchecking systems, everywhere, according to research, people often tend to trust system solutions, even when they are obviously wrong. But when this happens with tools specifically designed to correct just this phenomenon, we have an even bigger problem. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
What can be done about this? Some believe that part of the problems of the first wave of III is connected with the fact that researchers of the Ministry of Defense dominate in it, most of which are experts using AI systems. Tim Miller from the University of Melbourne, studying the use of AI systems by people: “This is a mental hospital under the control of psychos.”</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
This is what Esan realized in the back seat of a Uber car without a driver. It will be easier to understand what the automated system does - and see where it is wrong - if it explains its actions in the way a person would. Esan and his colleague, </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mark Riddle</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , are developing an MO system that </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">automatically generates</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> similar explanations in natural language. In an early prototype, the couple took a neural network, trained to play a classic game from the 1980s, Frogger, and trained it to give explanations before each move. </font></font><br>
<br>
<img src="https://cdn.technologyreview.com/i/images/explainable-ai--frogger-0-25-screenshot.png?sw=2500&amp;cx=0&amp;cy=0&amp;cw=492&amp;ch=326"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">among the cars ... I can’t get through ... I'll wait for the gap ...</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
To do this, they showed the system many examples of how people play this game, commenting on actions out loud. Then they took a neural network that translated from one language to another, and adapted it to translate game actions into explanations in a natural language. And now, when the National Assembly sees an action in the game, it “translates” it into an explanation. The result is an AI playing Frogger that says things like “move left to be behind the blue truck with every move.”</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The work of Esan and Riddle is only the beginning. Firstly, it is not clear whether the MO system will always be able to explain its actions in natural language. Take AlphaZero from DeepMind playing the go board game. One of the most amazing features of this program is that it can make a winning move that human players could not even think about at that particular moment in the game. If AlphaZero could explain its moves, would that be meaningful?</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Reasons can help, whether we understand them or not, Esan says: “The goal of an III with a focus on people is not just to make the user accept what the AI ​​says - but also cause some thought.” Riddle recalls watching a broadcast of the match between DeepMind AI and Korean champion Lee Sedol. Commentators discussed what AlphaZero sees and thinks. “But AlphaZero doesn't work that way,” says Riddle. “However, it seemed to me that the comments were necessary to understand what was happening.”</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
And although this new wave of III researchers, I agree that if more people use AI systems, then these people should take part in the design from the very beginning - and different people need different explanations. This is confirmed by a new study by Howley and her colleagues, in which they showed that people's ability to understand interactive or static visualization depends on their level of education. Imagine AI diagnosing cancer, Esan says. I would like the explanation he gives to the oncologist to be different from the explanation for the patient.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ultimately, we want AI to be able to explain not only to scientists working with data and doctors, but also to policemen using an image recognition system, teachers using analytic programs at school, students trying to understand the work of tapes on social networks - and before any person in the back seat of a robomobile. </font><font style="vertical-align: inherit;">“We always knew that people tend to overly trust technology, and this is especially true for AI systems,” says Riddle. </font><font style="vertical-align: inherit;">“The more often you call the system smart, the more people are confident that it is smarter than people.” </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Explanations that everyone could understand could destroy this illusion.</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../en488982/index.html">S905X processor (secured boot) software security overview</a></li>
<li><a href="../en488984/index.html">“If sites work better, it will be perfect”: interview with Playwright developer Andrey Lushnikov</a></li>
<li><a href="../en488986/index.html">Who is a mentor and what kind of flexible skills should he have</a></li>
<li><a href="../en488990/index.html">Overview of the full-face mask of UNIX 5100, filters to it, comparison with UNIX 5000, 6100 models and PPM-88 mask</a></li>
<li><a href="../en488994/index.html">DDR5? Yes, we barely met DDR4</a></li>
<li><a href="../en489000/index.html">How to assemble a cool mitap: 16 tips from three “serial mitapers”. Leader-IT events # 1</a></li>
<li><a href="../en489002/index.html">Distributed wheelset registry: experience with Hyperledger Fabric</a></li>
<li><a href="../en489004/index.html">Cell Phone with Disc Dialer</a></li>
<li><a href="../en489008/index.html">Routing in complex chatbots with the Hobot framework</a></li>
<li><a href="../en489010/index.html">We share the largest in Russia layer of data on online training with projects in linguistics, personalization, peddesign, ML</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>