<!doctype html>
<html class="no-js" lang="de">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üïü üõåüèª ‚ùÑÔ∏è Unity Machine Learning: Unterrichten von MO-Agenten, √ºber W√§nde zu springen üìü üêâ üë©üèø‚Äçüé®</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In den letzten Jahren gab es gro√üe Durchbr√ºche beim Reinforcement Learning (RL): vom ersten erfolgreichen Einsatz im Rohpixel-Training bis zum Trainin...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Unity Machine Learning: Unterrichten von MO-Agenten, √ºber W√§nde zu springen</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/pixonic/blog/492548/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In den letzten Jahren gab es gro√üe Durchbr√ºche beim Reinforcement Learning (RL): vom ersten erfolgreichen Einsatz im Rohpixel-Training bis zum Training von Open AI Roborists, und f√ºr weitere Fortschritte sind immer ausgefeiltere Umgebungen erforderlich, die helfen Einheit kommt. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das Unity ML-Agents-Tool ist ein neues Plugin in der Unity-Spiel-Engine, mit dem Sie Unity als Umgebungskonstruktor f√ºr die Schulung von MO-Agenten verwenden k√∂nnen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das Unity ML-Agents Toolkit bietet eine Vielzahl von Bedingungen f√ºr Trainingsagenten, vom Fu√üballspielen √ºber das Gehen, Springen von W√§nden bis hin zum Trainieren von KI-Hunden zum Spielen von St√∂cken. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In diesem Artikel werden wir uns die Funktionsweise von Unity MO-Agenten ansehen und dann einem dieser Agenten beibringen, √ºber W√§nde zu springen.</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/9c6/5ad/bdb/9c65adbdbb38b939262c10e5ac728dde.gif" alt="Bild"><br>
<br>
<a name="habracut"></a><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Was sind Unity ML-Agenten?</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Unity ML-Agents ist ein neues Plugin f√ºr die Unity-Game-Engine, mit dem Sie vorgefertigte Umgebungen f√ºr die Schulung unserer Agenten erstellen oder verwenden k√∂nnen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das Plugin besteht aus drei Komponenten: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/zo/mx/m_/zomxm_3g9svsej0iarjpw1vbnta.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die erste - eine </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Lernumgebung</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ( </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">die Lernumgebung</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ), die Szenen der Einheit und Umgebungselemente enth√§lt. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die zweite ist die </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Python-API</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , die die RL-Algorithmen enth√§lt (z. B. PPO - Proximal Policy Optimization und SAC - Soft Actor-Critic). </font><font style="vertical-align: inherit;">Wir verwenden diese API, um Schulungen, Tests usw. zu starten. Sie ist √ºber die dritte Komponente - </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">einen externen Kommunikator</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - mit der Lernumgebung verbunden </font><font style="vertical-align: inherit;">.</font></font><br>
<br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Woraus die Lernumgebung besteht</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die Trainingskomponente besteht aus verschiedenen Elementen: </font></font><br>
 <br>
<img src="https://habrastorage.org/webt/hy/1u/pt/hy1upt9en2esnt-amk-iipklmn4.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Der erste Agent ist der B√ºhnenschauspieler. </font><font style="vertical-align: inherit;">Er ist es, den wir trainieren werden, indem wir eine Komponente namens ‚ÄûGehirn‚Äú optimieren, in der aufgezeichnet wird, welche Aktionen in jedem der m√∂glichen Zust√§nde ausgef√ºhrt werden m√ºssen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das dritte Element, die Akademie, verwaltet Agenten und ihre Entscheidungsprozesse und verarbeitet Anforderungen von der Python-API. </font><font style="vertical-align: inherit;">Um seine Rolle besser zu verstehen, erinnern wir uns an den RL-Prozess. </font><font style="vertical-align: inherit;">Es kann als ein Zyklus dargestellt werden, der wie folgt funktioniert: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/5y/ge/ef/5ygeefzasafou6bkwggdnaq7ows.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Angenommen, ein Agent muss lernen, wie man einen Plattformer spielt. </font><font style="vertical-align: inherit;">Der RL-Prozess sieht in diesem Fall folgenderma√üen aus:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Der Agent erh√§lt den Status </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">S </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0</font></font></sub></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> von der Umgebung - dies ist der erste Frame unseres Spiels.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Basierend auf dem Zustand </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">S </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0 f√ºhrt der</font></font></sub></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Agent die Aktion </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0 aus</font></font></sub></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> und verschiebt sich nach rechts.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Umgebung geht in einen neuen Zustand </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">S </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1 √ºber</font></font></sub></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Der Agent erh√§lt eine Belohnung von </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">R </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1,</font></font></sub></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> wenn er nicht tot ist ( </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">positive Belohnung</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> +1).</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dieser RL-Zyklus bildet eine Folge von Zustand, Aktion und Belohnung. </font><font style="vertical-align: inherit;">Das Ziel des Agenten ist es, die erwartete Gesamtbelohnung zu maximieren. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/qm/mr/ml/qmmrmlkz7urj8cywqpsg1ejajve.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Daher sendet die Akademie Anweisungen an die Agenten und sorgt f√ºr die Synchronisierung ihrer Ausf√ºhrung, n√§mlich:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sammlung von Beobachtungen;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Wahl der Ma√ünahme gem√§√ü den festgelegten Anweisungen;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aktionsausf√ºhrung;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zur√ºcksetzen, wenn die Anzahl der Schritte erreicht wurde oder das Ziel erreicht wurde.</font></font></li>
</ul><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wir bringen dem Agenten bei, durch die W√§nde zu springen</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Jetzt, da wir wissen, wie Unity-Agenten arbeiten, werden wir einen trainieren, durch W√§nde zu springen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bereits trainierte Modelle k√∂nnen auch auf </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GitHub</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> heruntergeladen werden </font><font style="vertical-align: inherit;">.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wall Jumping Lernumgebung</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das Ziel dieser Umgebung ist es, dem Agenten beizubringen, auf die gr√ºne Kachel zu gelangen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Betrachten Sie drei F√§lle: </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
1. Es gibt keine W√§nde, und unser Agent muss nur zur Kachel gelangen. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/17a/8fd/da8/17a8fdda8bbe767f64a34d8d05fd9be9.png" alt="Bild"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
2. Der Agent muss lernen, wie man springt, um das gr√ºne Pl√§ttchen zu erreichen. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/d2c/149/0d0/d2c1490d0b90c4fdefb8eb4922f446b2.png" alt="Bild"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
3. Der schwierigste Fall: Die Wand ist zu hoch, als dass der Agent dar√ºber springen k√∂nnte, also muss er zuerst auf den wei√üen Block springen. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/458/175/9b6/4581759b653a175597054fcf60a0050e.png" alt="Bild"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wir werden dem Agenten zwei Verhaltensszenarien in Abh√§ngigkeit von der H√∂he der Wand beibringen:</font></font><br>
<br>
<ul>
<li><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SmallWallJump</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> in F√§llen ohne W√§nde oder in niedrigen Wandh√∂hen;</font></font></li>
<li><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">BigWallJump</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> bei hohen Mauern.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
So wird das Belohnungssystem aussehen: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/rt/lo/hm/rtlohmnturcucyikrnhfsadztgi.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In unseren Beobachtungen verwenden wir keinen regul√§ren Rahmen, sondern 14 Reykast, von denen jeder 4 m√∂gliche Objekte erkennen kann. </font><font style="vertical-align: inherit;">In diesem Fall kann Reykast als Laserstrahl wahrgenommen werden, der bestimmen kann, ob sie ein Objekt passieren. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wir werden auch die globale Agentenposition in unserem Programm verwenden. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/a06/0fc/fa2/a060fcfa26e80154ef988d76dc9b8f05.png" alt="Bild"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In unserem Raum sind vier Optionen m√∂glich: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ni/g6/zs/nig6zskfzterisy_hskg3k-rsnk.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das Ziel ist es, ein gr√ºnes Pl√§ttchen </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">mit einer durchschnittlichen Belohnung von 0,8 zu erreichen</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Also lasst uns anfangen!</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
√ñffnen Sie zun√§chst das </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">UnitySDK-</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Projekt </font><font style="vertical-align: inherit;">. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Unter den Beispielen m√ºssen Sie die </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">WallJump-</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Szene finden und √∂ffnen </font><font style="vertical-align: inherit;">. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wie Sie sehen k√∂nnen, befinden sich viele Agenten auf der B√ºhne, von denen jeder aus demselben Fertighaus stammt und alle dasselbe ‚ÄûGehirn‚Äú haben. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/bad/195/820/bad195820da3e5b2c74309e9f44e9935.png" alt="Bild"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wie im Fall des klassischen Deep Reinforcement Learning kopieren wir nach dem Start mehrerer Instanzen des Spiels (z. B. 128 parallele Umgebungen) die Agenten nur noch und f√ºgen sie ein, um mehr unterschiedliche Zust√§nde zu erhalten. Und da wir unseren Agenten von Grund auf neu trainieren wollen, m√ºssen wir zuerst das ‚ÄûGehirn‚Äú vom Agenten entfernen. Gehen Sie dazu in den Prefabs-Ordner und √∂ffnen Sie Prefab.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Als N√§chstes m√ºssen Sie in der Prefab-Hierarchie den Agenten ausw√§hlen und zu den Einstellungen wechseln. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In den Verhaltensparametern m√ºssen Sie das Modell l√∂schen. Wenn wir √ºber mehrere GPUs verf√ºgen, k√∂nnen Sie das Inferenzger√§t von der CPU als GPU verwenden. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/d0d/bdb/6ec/d0dbdb6ec05655b6fecdf675d629d126.png" alt="Bild"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In der Komponente Wall Jump Agent m√ºssen Sie das Gehirn f√ºr einen Fall ohne W√§nde sowie f√ºr niedrige und hohe W√§nde entfernen. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/618/e81/c8c/618e81c8c3c0cb15ae5fbca791c33c8c.png" alt="Bild"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Danach k√∂nnen Sie Ihren Agenten von Grund auf neu trainieren. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bei unserem ersten Training √§ndern wir einfach die Gesamtzahl der Trainingsschritte f√ºr zwei Verhaltensszenarien: SmallWallJump und BigWallJump. So k√∂nnen wir das Ziel in nur 300.000 Schritten erreichen. </font><font style="vertical-align: inherit;">√Ñndern Sie dazu </font><font style="vertical-align: inherit;">in </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">config / Trainer config.yaml</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> max_steps f√ºr die F√§lle SmallWallJump und BigWallJump in 3e5.</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/b80/bbb/1b2/b80bbb1b21f0f9e445fa8d2583c39d52.png" alt="Bild"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Um unseren Agenten zu schulen, verwenden wir </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PPO</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (Proximal Policy Optimization). Der Algorithmus umfasst die Anh√§ufung von Erfahrungen bei der Interaktion mit der Umgebung und deren Verwendung zur Aktualisierung von Entscheidungsrichtlinien. Nach der Aktualisierung werden die vorherigen Ereignisse verworfen, und die nachfolgende Datenerfassung wird bereits gem√§√ü den Bestimmungen der aktualisierten Richtlinie durchgef√ºhrt. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Daher m√ºssen wir zun√§chst mithilfe der Python-API einen externen Kommunikator aufrufen, damit dieser die Akademie anweist, Agenten zu starten. √ñffnen Sie dazu das Terminal, in dem sich ml-agent-master befindet, und geben Sie Folgendes ein: Mit </font></font><br>
<br>
<code>mlagents-learn config/trainer_config.yaml ‚Äî run-id=‚ÄùWallJump_FirstTrain‚Äù ‚Äî train</code><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
diesem Befehl werden Sie aufgefordert, die Unity-Szene zu starten. Dr√ºcken Sie dazu ‚ñ∫ oben im Editor. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/212/874/4fb/2128744fb143fec6308d95938e0383f7.png" alt="Bild"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Mit dem folgenden Befehl k√∂nnen Sie die Schulung Ihrer Agenten in Tensorboard verfolgen:</font></font><br>
<br>
<code>tensorboard ‚Äî logdir=summaries</code><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wenn das Training beendet ist, m√ºssen Sie die gespeicherten Modelldateien in </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ml-agent-master / models nach UnitySDK / Assets / ML-Agents / examples / WallJump / TFModels verschieben</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">√ñffnen Sie dann den Unity-Editor erneut und w√§hlen Sie die </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">WallJump-</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Szene aus </font><font style="vertical-align: inherit;">, in der das fertige </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">WallJumpArea-</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Objekt </font><i><font style="vertical-align: inherit;">ge√∂ffnet</font></i><font style="vertical-align: inherit;"> wird </font><font style="vertical-align: inherit;">. </font><i><font style="vertical-align: inherit;">W√§hlen</font></i></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Sie danach den Agenten aus und ziehen Sie in seinen Verhaltensparametern die Datei </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SmallWallJump.nn</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> in den Modellplatzhalter. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/d0d/bdb/6ec/d0dbdb6ec05655b6fecdf675d629d126.png" alt="Bild"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bewegen Sie sich auch:</font></font><br>
<br>
<ol>
<li><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SmallWallJump.nn</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> bei No Wall Brain Placeholder.</font></font></li>
<li><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SmallWallJump.nn</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> bei Small Wall Brain Placeholder.</font></font></li>
<li><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">BigWallJump.nn</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> bei No Wall Brain Placeholder.</font></font></li>
</ol><br>
<img src="https://habrastorage.org/getpro/habr/post_images/618/e81/c8c/618e81c8c3c0cb15ae5fbca791c33c8c.png" alt="Bild"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dr√ºcken Sie danach die Taste ‚ñ∫ oben im Editor und fertig! </font><font style="vertical-align: inherit;">Der Konfigurationsalgorithmus f√ºr das Agententraining ist jetzt abgeschlossen.</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/9c6/5ad/bdb/9c65adbdbb38b939262c10e5ac728dde.gif" alt="Bild"><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Experimentierzeit</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Der beste Weg zu lernen ist, st√§ndig zu versuchen, etwas Neues zu bringen. </font><font style="vertical-align: inherit;">Nachdem wir bereits gute Ergebnisse erzielt haben, werden wir versuchen, einige Hypothesen aufzustellen und zu testen.</font></font><br>
<br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Reduzierung des Abzinsungskoeffizienten auf 0,95</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Also wissen wir das:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Je gr√∂√üer das Gamma, desto geringer der Rabatt. </font><font style="vertical-align: inherit;">Das hei√üt, der Agent ist mehr besorgt √ºber langfristige Belohnungen.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Andererseits ist der Rabatt umso gr√∂√üer, je kleiner das Gamma ist. </font><font style="vertical-align: inherit;">In diesem Fall ist die Priorit√§t des Agenten die kurzfristige Verg√ºtung.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die Idee dieses Experiments ist, dass, wenn wir den Rabatt durch Verringern des Farbumfangs von 0,99 auf 0,95 erh√∂hen, die kurzfristige Belohnung f√ºr den Agenten eine Priorit√§t darstellt - was ihm helfen kann, sich schnell der optimalen Verhaltensrichtlinie zu n√§hern. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/u_/cm/hv/u_cmhvpc0dk-npolzz49loxofiq.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Interessanterweise strebt der Agent bei einem Sprung durch eine niedrige Wand das gleiche Ergebnis an. </font><font style="vertical-align: inherit;">Dies kann durch die Tatsache erkl√§rt werden, dass dieser Fall recht einfach ist: Der Agent muss nur zur gr√ºnen Kachel gehen und gegebenenfalls springen, wenn sich eine Wand vor ihm befindet. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/yc/ic/tt/ycicttb0mkwxmwn5c1ghgm2iiiu.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Auf der anderen Seite funktioniert dies im Fall von Big Wall Jump schlechter, da sich unser Agent mehr um die kurzfristige Belohnung k√ºmmert und daher nicht versteht, dass er auf den wei√üen Block klettern muss, um √ºber die Mauer zu springen.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Erh√∂hte Komplexit√§t des neuronalen Netzwerks</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Schlie√ülich stellen wir die Hypothese auf, ob unser Agent intelligenter wird, wenn wir die Komplexit√§t des neuronalen Netzwerks erh√∂hen. </font><font style="vertical-align: inherit;">Erh√∂hen Sie dazu die Gr√∂√üe der verborgenen Ebene von 256 auf 512. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In diesem Fall funktioniert der neue Agent schlechter als unser erster Agent. </font><font style="vertical-align: inherit;">Dies bedeutet, dass es f√ºr uns keinen Sinn macht, die Komplexit√§t unseres Netzwerks zu erh√∂hen, da sich sonst auch die Lernzeit erh√∂ht. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/zo/g7/qm/zog7qm1uemu7l7taqc5uuwlqauw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Also haben wir den Agenten geschult, √ºber die Mauern zu springen, und das ist alles f√ºr heute. </font><font style="vertical-align: inherit;">Denken Sie daran, dass zum Vergleich der Ergebnisse trainierte Modelle hier heruntergeladen </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">werden k√∂nnen</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/af6/f00/ce3/af6f00ce3784c60469522296276d2651.gif" alt="Bild"></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de492530/index.html">Kombinieren Sie MVVMs in UIKit- und SwiftUI-Anwendungen f√ºr UIKit-Entwickler</a></li>
<li><a href="../de492534/index.html">Gibt es also echte Hurrikane in Moskau oder nicht? Wir analysieren den Fall vom 13. M√§rz 2020 in Verfolgung</a></li>
<li><a href="../de492538/index.html">Wrike TechClub: Bereitstellungsinfrastruktur - Prozesse und Tools (DevOps + QAA). Beitr√§ge in englischer Sprache</a></li>
<li><a href="../de492540/index.html">Das Spiel "Warte einen Moment!" auf Arduino</a></li>
<li><a href="../de492546/index.html">√úberpr√ºfen der Sicherheitsanf√§lligkeit einer Website mit Nikto</a></li>
<li><a href="../de492552/index.html">Wie man in Barcelona in Quarant√§ne lebt und arbeitet</a></li>
<li><a href="../de492558/index.html">Hallo, das ist COVID19: Lebt das Coronavirus auf der Oberfl√§che eines Smartphones?</a></li>
<li><a href="../de492560/index.html">Einfache Hash-Tabelle f√ºr GPU</a></li>
<li><a href="../de492562/index.html">Drei n√ºtzliche Apache Ignite-Webinare in Ihrem Quarant√§neprogramm</a></li>
<li><a href="../de492566/index.html">Analyse der Kombination eines Suchalgorithmus f√ºr gierige Klicks mit teilweiser Aufz√§hlung von Diagrammscheitelpunkten</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>