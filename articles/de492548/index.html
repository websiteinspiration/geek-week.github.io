<!doctype html>
<html class="no-js" lang="de">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🕟 🛌🏻 ❄️ Unity Machine Learning: Unterrichten von MO-Agenten, über Wände zu springen 📟 🐉 👩🏿‍🎨</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In den letzten Jahren gab es große Durchbrüche beim Reinforcement Learning (RL): vom ersten erfolgreichen Einsatz im Rohpixel-Training bis zum Trainin...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Unity Machine Learning: Unterrichten von MO-Agenten, über Wände zu springen</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/pixonic/blog/492548/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In den letzten Jahren gab es große Durchbrüche beim Reinforcement Learning (RL): vom ersten erfolgreichen Einsatz im Rohpixel-Training bis zum Training von Open AI Roborists, und für weitere Fortschritte sind immer ausgefeiltere Umgebungen erforderlich, die helfen Einheit kommt. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das Unity ML-Agents-Tool ist ein neues Plugin in der Unity-Spiel-Engine, mit dem Sie Unity als Umgebungskonstruktor für die Schulung von MO-Agenten verwenden können. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das Unity ML-Agents Toolkit bietet eine Vielzahl von Bedingungen für Trainingsagenten, vom Fußballspielen über das Gehen, Springen von Wänden bis hin zum Trainieren von KI-Hunden zum Spielen von Stöcken. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In diesem Artikel werden wir uns die Funktionsweise von Unity MO-Agenten ansehen und dann einem dieser Agenten beibringen, über Wände zu springen.</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/9c6/5ad/bdb/9c65adbdbb38b939262c10e5ac728dde.gif" alt="Bild"><br>
<br>
<a name="habracut"></a><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Was sind Unity ML-Agenten?</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Unity ML-Agents ist ein neues Plugin für die Unity-Game-Engine, mit dem Sie vorgefertigte Umgebungen für die Schulung unserer Agenten erstellen oder verwenden können. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das Plugin besteht aus drei Komponenten: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/zo/mx/m_/zomxm_3g9svsej0iarjpw1vbnta.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die erste - eine </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Lernumgebung</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ( </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">die Lernumgebung</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ), die Szenen der Einheit und Umgebungselemente enthält. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die zweite ist die </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Python-API</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , die die RL-Algorithmen enthält (z. B. PPO - Proximal Policy Optimization und SAC - Soft Actor-Critic). </font><font style="vertical-align: inherit;">Wir verwenden diese API, um Schulungen, Tests usw. zu starten. Sie ist über die dritte Komponente - </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">einen externen Kommunikator</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - mit der Lernumgebung verbunden </font><font style="vertical-align: inherit;">.</font></font><br>
<br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Woraus die Lernumgebung besteht</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die Trainingskomponente besteht aus verschiedenen Elementen: </font></font><br>
 <br>
<img src="https://habrastorage.org/webt/hy/1u/pt/hy1upt9en2esnt-amk-iipklmn4.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Der erste Agent ist der Bühnenschauspieler. </font><font style="vertical-align: inherit;">Er ist es, den wir trainieren werden, indem wir eine Komponente namens „Gehirn“ optimieren, in der aufgezeichnet wird, welche Aktionen in jedem der möglichen Zustände ausgeführt werden müssen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das dritte Element, die Akademie, verwaltet Agenten und ihre Entscheidungsprozesse und verarbeitet Anforderungen von der Python-API. </font><font style="vertical-align: inherit;">Um seine Rolle besser zu verstehen, erinnern wir uns an den RL-Prozess. </font><font style="vertical-align: inherit;">Es kann als ein Zyklus dargestellt werden, der wie folgt funktioniert: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/5y/ge/ef/5ygeefzasafou6bkwggdnaq7ows.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Angenommen, ein Agent muss lernen, wie man einen Plattformer spielt. </font><font style="vertical-align: inherit;">Der RL-Prozess sieht in diesem Fall folgendermaßen aus:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Der Agent erhält den Status </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">S </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0</font></font></sub></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> von der Umgebung - dies ist der erste Frame unseres Spiels.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Basierend auf dem Zustand </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">S </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0 führt der</font></font></sub></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Agent die Aktion </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0 aus</font></font></sub></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> und verschiebt sich nach rechts.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Umgebung geht in einen neuen Zustand </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">S </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1 über</font></font></sub></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Der Agent erhält eine Belohnung von </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">R </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1,</font></font></sub></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> wenn er nicht tot ist ( </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">positive Belohnung</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> +1).</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dieser RL-Zyklus bildet eine Folge von Zustand, Aktion und Belohnung. </font><font style="vertical-align: inherit;">Das Ziel des Agenten ist es, die erwartete Gesamtbelohnung zu maximieren. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/qm/mr/ml/qmmrmlkz7urj8cywqpsg1ejajve.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Daher sendet die Akademie Anweisungen an die Agenten und sorgt für die Synchronisierung ihrer Ausführung, nämlich:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sammlung von Beobachtungen;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Wahl der Maßnahme gemäß den festgelegten Anweisungen;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aktionsausführung;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zurücksetzen, wenn die Anzahl der Schritte erreicht wurde oder das Ziel erreicht wurde.</font></font></li>
</ul><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wir bringen dem Agenten bei, durch die Wände zu springen</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Jetzt, da wir wissen, wie Unity-Agenten arbeiten, werden wir einen trainieren, durch Wände zu springen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bereits trainierte Modelle können auch auf </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GitHub</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> heruntergeladen werden </font><font style="vertical-align: inherit;">.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wall Jumping Lernumgebung</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das Ziel dieser Umgebung ist es, dem Agenten beizubringen, auf die grüne Kachel zu gelangen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Betrachten Sie drei Fälle: </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
1. Es gibt keine Wände, und unser Agent muss nur zur Kachel gelangen. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/17a/8fd/da8/17a8fdda8bbe767f64a34d8d05fd9be9.png" alt="Bild"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
2. Der Agent muss lernen, wie man springt, um das grüne Plättchen zu erreichen. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/d2c/149/0d0/d2c1490d0b90c4fdefb8eb4922f446b2.png" alt="Bild"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
3. Der schwierigste Fall: Die Wand ist zu hoch, als dass der Agent darüber springen könnte, also muss er zuerst auf den weißen Block springen. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/458/175/9b6/4581759b653a175597054fcf60a0050e.png" alt="Bild"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wir werden dem Agenten zwei Verhaltensszenarien in Abhängigkeit von der Höhe der Wand beibringen:</font></font><br>
<br>
<ul>
<li><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SmallWallJump</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> in Fällen ohne Wände oder in niedrigen Wandhöhen;</font></font></li>
<li><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">BigWallJump</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> bei hohen Mauern.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
So wird das Belohnungssystem aussehen: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/rt/lo/hm/rtlohmnturcucyikrnhfsadztgi.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In unseren Beobachtungen verwenden wir keinen regulären Rahmen, sondern 14 Reykast, von denen jeder 4 mögliche Objekte erkennen kann. </font><font style="vertical-align: inherit;">In diesem Fall kann Reykast als Laserstrahl wahrgenommen werden, der bestimmen kann, ob sie ein Objekt passieren. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wir werden auch die globale Agentenposition in unserem Programm verwenden. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/a06/0fc/fa2/a060fcfa26e80154ef988d76dc9b8f05.png" alt="Bild"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In unserem Raum sind vier Optionen möglich: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ni/g6/zs/nig6zskfzterisy_hskg3k-rsnk.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das Ziel ist es, ein grünes Plättchen </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">mit einer durchschnittlichen Belohnung von 0,8 zu erreichen</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Also lasst uns anfangen!</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Öffnen Sie zunächst das </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">UnitySDK-</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Projekt </font><font style="vertical-align: inherit;">. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Unter den Beispielen müssen Sie die </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">WallJump-</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Szene finden und öffnen </font><font style="vertical-align: inherit;">. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wie Sie sehen können, befinden sich viele Agenten auf der Bühne, von denen jeder aus demselben Fertighaus stammt und alle dasselbe „Gehirn“ haben. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/bad/195/820/bad195820da3e5b2c74309e9f44e9935.png" alt="Bild"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wie im Fall des klassischen Deep Reinforcement Learning kopieren wir nach dem Start mehrerer Instanzen des Spiels (z. B. 128 parallele Umgebungen) die Agenten nur noch und fügen sie ein, um mehr unterschiedliche Zustände zu erhalten. Und da wir unseren Agenten von Grund auf neu trainieren wollen, müssen wir zuerst das „Gehirn“ vom Agenten entfernen. Gehen Sie dazu in den Prefabs-Ordner und öffnen Sie Prefab.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Als Nächstes müssen Sie in der Prefab-Hierarchie den Agenten auswählen und zu den Einstellungen wechseln. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In den Verhaltensparametern müssen Sie das Modell löschen. Wenn wir über mehrere GPUs verfügen, können Sie das Inferenzgerät von der CPU als GPU verwenden. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/d0d/bdb/6ec/d0dbdb6ec05655b6fecdf675d629d126.png" alt="Bild"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In der Komponente Wall Jump Agent müssen Sie das Gehirn für einen Fall ohne Wände sowie für niedrige und hohe Wände entfernen. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/618/e81/c8c/618e81c8c3c0cb15ae5fbca791c33c8c.png" alt="Bild"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Danach können Sie Ihren Agenten von Grund auf neu trainieren. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bei unserem ersten Training ändern wir einfach die Gesamtzahl der Trainingsschritte für zwei Verhaltensszenarien: SmallWallJump und BigWallJump. So können wir das Ziel in nur 300.000 Schritten erreichen. </font><font style="vertical-align: inherit;">Ändern Sie dazu </font><font style="vertical-align: inherit;">in </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">config / Trainer config.yaml</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> max_steps für die Fälle SmallWallJump und BigWallJump in 3e5.</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/b80/bbb/1b2/b80bbb1b21f0f9e445fa8d2583c39d52.png" alt="Bild"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Um unseren Agenten zu schulen, verwenden wir </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PPO</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (Proximal Policy Optimization). Der Algorithmus umfasst die Anhäufung von Erfahrungen bei der Interaktion mit der Umgebung und deren Verwendung zur Aktualisierung von Entscheidungsrichtlinien. Nach der Aktualisierung werden die vorherigen Ereignisse verworfen, und die nachfolgende Datenerfassung wird bereits gemäß den Bestimmungen der aktualisierten Richtlinie durchgeführt. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Daher müssen wir zunächst mithilfe der Python-API einen externen Kommunikator aufrufen, damit dieser die Akademie anweist, Agenten zu starten. Öffnen Sie dazu das Terminal, in dem sich ml-agent-master befindet, und geben Sie Folgendes ein: Mit </font></font><br>
<br>
<code>mlagents-learn config/trainer_config.yaml — run-id=”WallJump_FirstTrain” — train</code><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
diesem Befehl werden Sie aufgefordert, die Unity-Szene zu starten. Drücken Sie dazu ► oben im Editor. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/212/874/4fb/2128744fb143fec6308d95938e0383f7.png" alt="Bild"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Mit dem folgenden Befehl können Sie die Schulung Ihrer Agenten in Tensorboard verfolgen:</font></font><br>
<br>
<code>tensorboard — logdir=summaries</code><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wenn das Training beendet ist, müssen Sie die gespeicherten Modelldateien in </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ml-agent-master / models nach UnitySDK / Assets / ML-Agents / examples / WallJump / TFModels verschieben</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Öffnen Sie dann den Unity-Editor erneut und wählen Sie die </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">WallJump-</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Szene aus </font><font style="vertical-align: inherit;">, in der das fertige </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">WallJumpArea-</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Objekt </font><i><font style="vertical-align: inherit;">geöffnet</font></i><font style="vertical-align: inherit;"> wird </font><font style="vertical-align: inherit;">. </font><i><font style="vertical-align: inherit;">Wählen</font></i></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Sie danach den Agenten aus und ziehen Sie in seinen Verhaltensparametern die Datei </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SmallWallJump.nn</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> in den Modellplatzhalter. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/d0d/bdb/6ec/d0dbdb6ec05655b6fecdf675d629d126.png" alt="Bild"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bewegen Sie sich auch:</font></font><br>
<br>
<ol>
<li><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SmallWallJump.nn</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> bei No Wall Brain Placeholder.</font></font></li>
<li><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SmallWallJump.nn</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> bei Small Wall Brain Placeholder.</font></font></li>
<li><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">BigWallJump.nn</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> bei No Wall Brain Placeholder.</font></font></li>
</ol><br>
<img src="https://habrastorage.org/getpro/habr/post_images/618/e81/c8c/618e81c8c3c0cb15ae5fbca791c33c8c.png" alt="Bild"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Drücken Sie danach die Taste ► oben im Editor und fertig! </font><font style="vertical-align: inherit;">Der Konfigurationsalgorithmus für das Agententraining ist jetzt abgeschlossen.</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/9c6/5ad/bdb/9c65adbdbb38b939262c10e5ac728dde.gif" alt="Bild"><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Experimentierzeit</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Der beste Weg zu lernen ist, ständig zu versuchen, etwas Neues zu bringen. </font><font style="vertical-align: inherit;">Nachdem wir bereits gute Ergebnisse erzielt haben, werden wir versuchen, einige Hypothesen aufzustellen und zu testen.</font></font><br>
<br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Reduzierung des Abzinsungskoeffizienten auf 0,95</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Also wissen wir das:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Je größer das Gamma, desto geringer der Rabatt. </font><font style="vertical-align: inherit;">Das heißt, der Agent ist mehr besorgt über langfristige Belohnungen.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Andererseits ist der Rabatt umso größer, je kleiner das Gamma ist. </font><font style="vertical-align: inherit;">In diesem Fall ist die Priorität des Agenten die kurzfristige Vergütung.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die Idee dieses Experiments ist, dass, wenn wir den Rabatt durch Verringern des Farbumfangs von 0,99 auf 0,95 erhöhen, die kurzfristige Belohnung für den Agenten eine Priorität darstellt - was ihm helfen kann, sich schnell der optimalen Verhaltensrichtlinie zu nähern. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/u_/cm/hv/u_cmhvpc0dk-npolzz49loxofiq.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Interessanterweise strebt der Agent bei einem Sprung durch eine niedrige Wand das gleiche Ergebnis an. </font><font style="vertical-align: inherit;">Dies kann durch die Tatsache erklärt werden, dass dieser Fall recht einfach ist: Der Agent muss nur zur grünen Kachel gehen und gegebenenfalls springen, wenn sich eine Wand vor ihm befindet. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/yc/ic/tt/ycicttb0mkwxmwn5c1ghgm2iiiu.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Auf der anderen Seite funktioniert dies im Fall von Big Wall Jump schlechter, da sich unser Agent mehr um die kurzfristige Belohnung kümmert und daher nicht versteht, dass er auf den weißen Block klettern muss, um über die Mauer zu springen.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Erhöhte Komplexität des neuronalen Netzwerks</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Schließlich stellen wir die Hypothese auf, ob unser Agent intelligenter wird, wenn wir die Komplexität des neuronalen Netzwerks erhöhen. </font><font style="vertical-align: inherit;">Erhöhen Sie dazu die Größe der verborgenen Ebene von 256 auf 512. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In diesem Fall funktioniert der neue Agent schlechter als unser erster Agent. </font><font style="vertical-align: inherit;">Dies bedeutet, dass es für uns keinen Sinn macht, die Komplexität unseres Netzwerks zu erhöhen, da sich sonst auch die Lernzeit erhöht. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/zo/g7/qm/zog7qm1uemu7l7taqc5uuwlqauw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Also haben wir den Agenten geschult, über die Mauern zu springen, und das ist alles für heute. </font><font style="vertical-align: inherit;">Denken Sie daran, dass zum Vergleich der Ergebnisse trainierte Modelle hier heruntergeladen </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">werden können</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/af6/f00/ce3/af6f00ce3784c60469522296276d2651.gif" alt="Bild"></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de492530/index.html">Kombinieren Sie MVVMs in UIKit- und SwiftUI-Anwendungen für UIKit-Entwickler</a></li>
<li><a href="../de492534/index.html">Gibt es also echte Hurrikane in Moskau oder nicht? Wir analysieren den Fall vom 13. März 2020 in Verfolgung</a></li>
<li><a href="../de492538/index.html">Wrike TechClub: Bereitstellungsinfrastruktur - Prozesse und Tools (DevOps + QAA). Beiträge in englischer Sprache</a></li>
<li><a href="../de492540/index.html">Das Spiel "Warte einen Moment!" auf Arduino</a></li>
<li><a href="../de492546/index.html">Überprüfen der Sicherheitsanfälligkeit einer Website mit Nikto</a></li>
<li><a href="../de492552/index.html">Wie man in Barcelona in Quarantäne lebt und arbeitet</a></li>
<li><a href="../de492558/index.html">Hallo, das ist COVID19: Lebt das Coronavirus auf der Oberfläche eines Smartphones?</a></li>
<li><a href="../de492560/index.html">Einfache Hash-Tabelle für GPU</a></li>
<li><a href="../de492562/index.html">Drei nützliche Apache Ignite-Webinare in Ihrem Quarantäneprogramm</a></li>
<li><a href="../de492566/index.html">Analyse der Kombination eines Suchalgorithmus für gierige Klicks mit teilweiser Aufzählung von Diagrammscheitelpunkten</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>