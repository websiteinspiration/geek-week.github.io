<!doctype html>
<html class="no-js" lang="pt">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🛩️ 😿 🤘🏿 Treinar rivais de jogos inteligentes no Unity usando o método "brinque com você" usando ML-Agents 🧜🏽 👟 ☝🏿</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Olá Habr! 
 
 Como nossos leitores regulares sabem, publicamos livros longos e com sucesso sobre o Unity . Como parte do estudo do tópico, estávamos i...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Treinar rivais de jogos inteligentes no Unity usando o método "brinque com você" usando ML-Agents</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/507212/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Olá Habr! </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Como nossos leitores regulares sabem, </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">publicamos livros</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> longos e com sucesso </font><font style="vertical-align: inherit;">sobre o </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Unity</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Como parte do estudo do tópico, estávamos interessados, em particular, no </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ML-Agents Toolkit</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Hoje, chamamos a atenção para a tradução de um artigo do blog Unity sobre como treinar efetivamente agentes de jogos usando o método “consigo mesmo”; em particular, o artigo ajuda a entender por que esse método é mais eficaz do que o aprendizado tradicional reforçado. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/vb/sz/5m/vbsz5musziyq5yighwu_iq0eggu.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Gostar de ler!</font></font><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Este artigo fornece uma visão geral da tecnologia de reprodução automática (jogando consigo mesmo) e demonstra como ajuda a fornecer treinamento estável e eficaz no ambiente de demonstração de futebol do </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ML-Agents Toolkit</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nos ambientes de demonstração de tênis e futebol do Unity ML-Agents Toolkit, os agentes se enfrentam como rivais. Às vezes, treinar agentes em um cenário tão competitivo é uma tarefa não trivial. De fato, em versões anteriores do ML-Agents Toolkit, para que o agente aprendesse com confiança, era necessário um estudo sério do prêmio. Na </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">versão 0.14</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">foi adicionada uma oportunidade que permite ao usuário treinar agentes usando o RL (Aprendizagem por Reforço) com base no auto-jogo, um mecanismo crucial para alcançar alguns dos resultados mais </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">avançados de</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> aprendizado por reforço, como o </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;">OpenAI Five</font></a><font style="vertical-align: inherit;"> e </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">o AlphaStar da DeepMind</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . A brincadeira no trabalho coloca um ao outro as hipóstases atuais e passadas do agente. Assim, temos um adversário para o nosso agente, que pode melhorar gradualmente usando os algoritmos tradicionais de aprendizado por reforço. </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Um agente totalmente treinado</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> pode competir com sucesso com jogadores humanos avançados.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A brincadeira pessoal fornece um ambiente de aprendizado que é construído com os mesmos princípios da competição da perspectiva humana. Por exemplo, uma pessoa que aprende a jogar tênis escolherá poupar os oponentes aproximadamente no mesmo nível que ele, pois um oponente muito forte ou muito fraco não é tão conveniente para dominar o jogo. Do ponto de vista do desenvolvimento de suas próprias habilidades, pode ser muito mais valioso para um tenista iniciante vencer os mesmos iniciantes do que, digamos, uma criança em idade pré-escolar ou Novak Djokovic. O primeiro nem será capaz de acertar a bola e o segundo não lhe dará um saque que você possa vencer. Quando um iniciante desenvolve força suficiente, ele pode passar para o próximo nível ou solicitar um torneio mais sério para jogar contra oponentes mais habilidosos.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Neste artigo, consideraremos algumas sutilezas técnicas associadas à dinâmica do jogo, além de exemplos de trabalho em ambientes virtuais de tênis e futebol, refatorados de forma a ilustrar o jogo.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A história de um jogo com você mesmo em jogos</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O fenômeno de brincar consigo mesmo tem uma longa história, refletida na prática de desenvolver agentes artificiais de jogos projetados para competir com as pessoas nos jogos. </font><font style="vertical-align: inherit;">Um dos primeiros a usar esse sistema foi Arthur Samuel, que desenvolveu um simulador de xadrez na década de 1950 e publicou </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">esse trabalho</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> em 1959. </font><font style="vertical-align: inherit;">Esse sistema se tornou o precursor de um resultado histórico no aprendizado por reforço alcançado por Gerald Tesauro no TD-Gammon; </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">totais publicados</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">em 1995. TD-Gammon usou o algoritmo de diferença de tempo TD (λ) com a função de jogar sozinho para treinar o agente para jogar gamão para que ele pudesse competir com um profissional. Em alguns casos, foi observado que o TD-Gammon tem uma visão mais confiante de posições do que jogadores de classe mundial. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Brincar consigo mesmo se reflete em muitas das conquistas icônicas associadas ao RL. É importante notar que jogar consigo mesmo ajudou no desenvolvimento de agentes para jogar </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">xadrez e</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> combina habilidades sobre-humanas, agentes de elite do </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">DOTA 2</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , além de estratégias e contra-estratégias complexas em jogos como </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">luta livre</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> e </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;">esconde</font></a><font style="vertical-align: inherit;"> - </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">esconde.</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Nos resultados alcançados ao jogar consigo mesmo, observa-se frequentemente que os agentes do jogo escolhem estratégias que surpreendem pessoas especializadas. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Tocar com você mesmo dá aos agentes uma certa criatividade, independente da criatividade dos programadores. </font><font style="vertical-align: inherit;">O agente recebe apenas as regras do jogo e, em seguida - informações sobre se ele ganhou ou perdeu. </font><font style="vertical-align: inherit;">Além disso, com base nesses princípios básicos, o agente deve desenvolver um comportamento competente. </font><font style="vertical-align: inherit;">De acordo com o criador do TD-Gammon, essa abordagem ao aprendizado libera "... no sentido de que o programa não é restringido por inclinações e preconceitos humanos, que podem vir a ser errôneos e não confiáveis". </font><font style="vertical-align: inherit;">Graças a essa liberdade, os agentes descobrem estratégias brilhantes de jogos que mudam completamente a maneira como os engenheiros pensam sobre determinados jogos.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Treinamento de reforço competitivo</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dentro da estrutura da tarefa tradicional de aprendizado reforçado, o agente está tentando desenvolver uma linha de comportamento que maximize a recompensa total. O sinal de recompensa codifica a tarefa do agente - tal tarefa pode, por exemplo, traçar um curso ou coletar itens. O comportamento do agente está sujeito a restrições ambientais. Tais, por exemplo, gravidade, obstáculos, bem como a influência relativa de ações tomadas pelo próprio agente - por exemplo, a aplicação de força para o próprio movimento. Esses fatores limitam o comportamento do agente e são forças externas que ele precisa aprender a lidar para receber uma alta recompensa. Assim, o agente compete com a dinâmica do ambiente e deve passar de estado para estado precisamente para que a recompensa máxima seja alcançada.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/if/jf/xi/ifjfxil_iqm9pznr61qbpxujs38.png"><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O cenário típico de treinamento de reforço é mostrado à esquerda: o agente atua no ambiente, transfere para o próximo estado e recebe uma recompensa. O cenário de treinamento é mostrado à direita, onde o agente compete com um rival que, do ponto de vista do agente, é realmente um elemento do ambiente.</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
No caso de jogos competitivos, o agente compete não apenas com a dinâmica do ambiente, mas também com outro agente (possivelmente intelectual). Podemos assumir que o oponente está embutido no ambiente, e suas ações afetam diretamente o próximo estado que o agente “vê”, bem como a recompensa que ele receberá. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/zl/kv/xo/zlkvxoa9_bt0cdsthmddwll-fvc.png"><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Exemplo de tênis do ML-Agents Toolkit</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Considere a demonstração do ML-Agents Tennis. A raquete azul (esquerda) é o agente de aprendizado e o roxo (direita) é seu oponente. Para arremessar a bola pela rede, o agente deve levar em consideração a trajetória da bola que voa do oponente e fazer um ajuste no ângulo e velocidade da bola que leva em consideração as condições ambientais (gravidade). No entanto, em uma competição com um oponente, jogar a bola por cima da rede é apenas metade da batalha. Um oponente forte pode responder com um golpe irresistível e, como resultado, o agente perde. Um oponente fraco pode acertar a bola na rede. Um oponente igual pode retornar o saque e, portanto, o jogo continuará. De qualquer forma, o estado seguinte e a recompensa correspondente dependem das condições ambientais e do oponente. No entanto, em todas essas situações, o agente faz o mesmo tom. Portanto, como treinamento em jogos competitivos,bombear comportamentos rivais por um agente é um problema complexo.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Considerações para um oponente adequado não são triviais. Como está claro acima, a força relativa do oponente afeta significativamente o resultado de um jogo em particular. Se o oponente é muito forte, o agente pode achar difícil aprender a jogar do zero. Por outro lado, se o oponente é muito fraco, o agente pode aprender a vencer, mas essas habilidades podem ser inúteis na competição com um oponente mais forte ou simplesmente diferente. Portanto, precisamos de um oponente que seja aproximadamente igual em força ao agente (inflexível, mas não intransponível). Além disso, como as habilidades de nosso agente melhoram a cada jogo concluído, precisamos aumentar a força de seu oponente na mesma medida. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/fn/vk/pt/fnvkptgsipqopi2iiyflsa5ec_a.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ao jogar com você mesmo, um instantâneo do passado ou um agente em seu estado atual é o oponente incorporado ao ambiente.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
É aqui que o jogo conosco é útil! O próprio agente atende a ambos os requisitos para o oponente desejado. Ele é definitivamente aproximadamente igual em força a si mesmo, e suas habilidades melhoram com o tempo. Nesse caso, a política do próprio agente é incorporada ao ambiente (veja a figura). Aqueles que estão familiarizados com o </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">aumento gradual da educação em complexidade</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (aprendizagem no currículo), mostram que podemos assumir que o sistema está desenvolvendo naturalmente o currículo, após o qual o agente aprende a lutar contra oponentes cada vez mais poderosos. Assim, jogar consigo mesmo permite que você use o próprio ambiente para treinar agentes competitivos para jogos competitivos!</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nas próximas duas seções, detalhes mais técnicos do treinamento de agentes competitivos serão discutidos, em particular, a implementação e o uso do jogo com você mesmo no ML-Agents Toolkit.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Considerações práticas</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Alguns problemas práticos surgem em relação à estrutura para brincar consigo mesmo. </font><font style="vertical-align: inherit;">Em particular, é possível treinar novamente, em que o agente aprende a vencer apenas com um certo estilo de jogo, bem como a instabilidade inerente ao processo de aprendizagem, que pode surgir devido à instabilidade da função de transição (ou seja, devido à constante mudança de oponentes). </font><font style="vertical-align: inherit;">O primeiro problema surge porque queremos que nossos agentes tenham uma compreensão geral e capacidade de combater oponentes de tipos diferentes.</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O segundo problema pode ser ilustrado no ambiente do tênis: diferentes oponentes baterão na bola em velocidades e ângulos diferentes. Do ponto de vista do agente de aprendizagem, isso significa que, à medida que você aprende, as mesmas decisões levarão a diferentes resultados e, consequentemente, o agente estará em diferentes situações subsequentes. No aprendizado tradicional por reforço, as funções estacionárias de transição estão implícitas. Infelizmente, tendo preparado uma seleção de vários oponentes para o agente, a fim de resolver o primeiro problema, nós, sendo descuidados, podemos agravar o segundo.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para lidar com isso, manteremos um buffer com as políticas anteriores de agentes, das quais escolheremos rivais em potencial para nosso "aluno" a longo prazo. </font><font style="vertical-align: inherit;">Escolhendo um agente de políticas anteriores, obtemos para ele uma seleção de diversos oponentes. </font><font style="vertical-align: inherit;">Além disso, permitindo que o agente treine com um oponente fixo por um longo tempo, estabilizamos a função de transição e criamos um ambiente de aprendizado mais consistente. </font><font style="vertical-align: inherit;">Finalmente, esses aspectos algorítmicos podem ser controlados usando hiperparâmetros, que serão discutidos na próxima seção.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Detalhes de implementação e uso</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Escolhendo hiperparâmetros para brincar conosco, temos, antes de tudo, em mente um compromisso entre o nível do oponente, a universalidade da política final e a estabilidade do treinamento. Treinar em competição com um grupo de oponentes que mudam lentamente ou não muda e, portanto, produzem uma dispersão menor de resultados, é um processo mais estável do que treinar em competição com muitos oponentes diversos que mudam rapidamente. Os hiperparâmetros disponíveis permitem controlar com que frequência a política atual do agente será salva para uso posterior como um dos oponentes da amostra, com que frequência o novo oponente será salvo, posteriormente selecionado para sparring, com que frequência o novo oponente será selecionado, o número de oponentes salvos e a probabilidadeneste caso, o aluno terá que jogar contra seu próprio alter ego, e não contra um oponente selecionado da piscina.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Em jogos competitivos, o prêmio "cumulativo" concedido pelo ambiente talvez não seja a métrica mais informativa para acompanhar o progresso do aprendizado. O fato é que o prêmio acumulado depende inteiramente do nível do oponente. Um agente com uma certa habilidade no jogo receberá uma recompensa maior ou menor, dependendo de um oponente menos habilidoso ou mais habilidoso, respectivamente. Propomos a implementação </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">do sistema de classificação ELO</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , que permite calcular a habilidade relativa de jogo de dois jogadores de uma determinada população ao jogar com um valor zero. Durante uma única execução de treinamento, esse valor deve aumentar constantemente. Você pode acompanhá-lo, juntamente com outras métricas de aprendizado, por exemplo, o prêmio geral, usando o </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">TensorBoard</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Jogando com você no futebol</font></font></h3> <br>
<img src="https://habrastorage.org/webt/gl/m_/rh/glm_rhop1whkyve0i-wpubxfpqw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As versões mais recentes do ML-Agent Toolkit não incluem políticas de agente para o ambiente de aprendizado do Soccer, pois o processo de treinamento confiável não foi construído nele. </font><font style="vertical-align: inherit;">No entanto, usando o jogo conosco e com alguma refatoração, podemos treinar o agente em comportamentos não triviais. </font><font style="vertical-align: inherit;">A mudança mais significativa é a remoção de "posições de jogo" das características do agente. </font><font style="vertical-align: inherit;">No início do futebol, o “goleiro” e o “atacante” claramente se destacavam, então toda a jogabilidade parecia mais lógica. </font><font style="vertical-align: inherit;">Em </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">este vídeo</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">é apresentado um novo ambiente no qual é possível ver como o comportamento do papel é formado espontaneamente, no qual alguns agentes começam a atuar como atacantes e outros como goleiros. Agora, os próprios agentes estão aprendendo a desempenhar essas posições! A função de recompensa para todos os quatro agentes é definida como +1,0 para um gol marcado e -1,0 para um gol sofrido, com uma penalidade adicional de -0.0003 por etapa - essa penalidade é fornecida para estimular os agentes a atacar.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Aqui, enfatizamos mais uma vez que os agentes do ambiente de aprendizado do Soccer aprendem o comportamento cooperativo e, para isso, nenhum algoritmo explícito é usado relacionado ao comportamento de vários agentes ou à atribuição de funções. </font><font style="vertical-align: inherit;">Este resultado demonstra que um agente pode ser treinado em comportamentos complexos usando algoritmos relativamente simples - desde que a tarefa seja bem formulada. </font><font style="vertical-align: inherit;">A condição mais importante para isso é que os agentes possam observar seus colegas de equipe, ou seja, eles recebem informações sobre a posição relativa do companheiro de equipe. </font><font style="vertical-align: inherit;">Forçando uma luta agressiva pela bola, o agente indiretamente diz ao companheiro de equipe que ele deve se mover em defesa. </font><font style="vertical-align: inherit;">Pelo contrário, afastando-se na defesa, o agente provoca um companheiro de equipe para atacar.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Qual é o próximo</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Se você já usou algum dos novos recursos desta versão - conte-nos sobre eles. </font><font style="vertical-align: inherit;">Chamamos sua atenção para a página de </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">problemas</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> do </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;">ML-Agents GitHub</font></a><font style="vertical-align: inherit;"> , onde você pode falar sobre os erros encontrados, bem como a página de </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">fóruns</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> do </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;">Unity ML-Agents</font></a><font style="vertical-align: inherit;"> , onde são discutidas questões e problemas gerais.</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt507200/index.html">7 maneiras que os cientistas de dados enganam você</a></li>
<li><a href="../pt507202/index.html">Encontro do Avito Analytics</a></li>
<li><a href="../pt507204/index.html">Cozinha interior de design industrial: do esboço ao produto na caixa</a></li>
<li><a href="../pt507206/index.html">Arquitetura Y messenger</a></li>
<li><a href="../pt507210/index.html">O desempenho do Java moderno ao trabalhar com grandes quantidades de dados, parte 2</a></li>
<li><a href="../pt507214/index.html">Como criar e modificar formulários PDF interativos ou a nova habilidade ABBYY FineReader PDF</a></li>
<li><a href="../pt507218/index.html">Leia-me ou por que o texto não é lido até o fim</a></li>
<li><a href="../pt507222/index.html">Por que todos deveriam usar máscaras</a></li>
<li><a href="../pt507224/index.html">Como eliminar pontos cegos com testes visuais</a></li>
<li><a href="../pt507226/index.html">OCR para PDF no .NET - Como extrair texto de documentos PDF inacessíveis</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>