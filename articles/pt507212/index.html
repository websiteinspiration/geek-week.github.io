<!doctype html>
<html class="no-js" lang="pt">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üõ©Ô∏è üòø ü§òüèø Treinar rivais de jogos inteligentes no Unity usando o m√©todo "brinque com voc√™" usando ML-Agents üßúüèΩ üëü ‚òùüèø</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ol√° Habr! 
 
 Como nossos leitores regulares sabem, publicamos livros longos e com sucesso sobre o Unity . Como parte do estudo do t√≥pico, est√°vamos i...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Treinar rivais de jogos inteligentes no Unity usando o m√©todo "brinque com voc√™" usando ML-Agents</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/507212/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ol√° Habr! </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Como nossos leitores regulares sabem, </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">publicamos livros</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> longos e com sucesso </font><font style="vertical-align: inherit;">sobre o </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Unity</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Como parte do estudo do t√≥pico, est√°vamos interessados, em particular, no </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ML-Agents Toolkit</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Hoje, chamamos a aten√ß√£o para a tradu√ß√£o de um artigo do blog Unity sobre como treinar efetivamente agentes de jogos usando o m√©todo ‚Äúconsigo mesmo‚Äù; em particular, o artigo ajuda a entender por que esse m√©todo √© mais eficaz do que o aprendizado tradicional refor√ßado. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/vb/sz/5m/vbsz5musziyq5yighwu_iq0eggu.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Gostar de ler!</font></font><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Este artigo fornece uma vis√£o geral da tecnologia de reprodu√ß√£o autom√°tica (jogando consigo mesmo) e demonstra como ajuda a fornecer treinamento est√°vel e eficaz no ambiente de demonstra√ß√£o de futebol do </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ML-Agents Toolkit</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nos ambientes de demonstra√ß√£o de t√™nis e futebol do Unity ML-Agents Toolkit, os agentes se enfrentam como rivais. √Äs vezes, treinar agentes em um cen√°rio t√£o competitivo √© uma tarefa n√£o trivial. De fato, em vers√µes anteriores do ML-Agents Toolkit, para que o agente aprendesse com confian√ßa, era necess√°rio um estudo s√©rio do pr√™mio. Na </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">vers√£o 0.14</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">foi adicionada uma oportunidade que permite ao usu√°rio treinar agentes usando o RL (Aprendizagem por Refor√ßo) com base no auto-jogo, um mecanismo crucial para alcan√ßar alguns dos resultados mais </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">avan√ßados de</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> aprendizado por refor√ßo, como o </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;">OpenAI Five</font></a><font style="vertical-align: inherit;"> e </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">o AlphaStar da DeepMind</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . A brincadeira no trabalho coloca um ao outro as hip√≥stases atuais e passadas do agente. Assim, temos um advers√°rio para o nosso agente, que pode melhorar gradualmente usando os algoritmos tradicionais de aprendizado por refor√ßo. </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Um agente totalmente treinado</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> pode competir com sucesso com jogadores humanos avan√ßados.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A brincadeira pessoal fornece um ambiente de aprendizado que √© constru√≠do com os mesmos princ√≠pios da competi√ß√£o da perspectiva humana. Por exemplo, uma pessoa que aprende a jogar t√™nis escolher√° poupar os oponentes aproximadamente no mesmo n√≠vel que ele, pois um oponente muito forte ou muito fraco n√£o √© t√£o conveniente para dominar o jogo. Do ponto de vista do desenvolvimento de suas pr√≥prias habilidades, pode ser muito mais valioso para um tenista iniciante vencer os mesmos iniciantes do que, digamos, uma crian√ßa em idade pr√©-escolar ou Novak Djokovic. O primeiro nem ser√° capaz de acertar a bola e o segundo n√£o lhe dar√° um saque que voc√™ possa vencer. Quando um iniciante desenvolve for√ßa suficiente, ele pode passar para o pr√≥ximo n√≠vel ou solicitar um torneio mais s√©rio para jogar contra oponentes mais habilidosos.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Neste artigo, consideraremos algumas sutilezas t√©cnicas associadas √† din√¢mica do jogo, al√©m de exemplos de trabalho em ambientes virtuais de t√™nis e futebol, refatorados de forma a ilustrar o jogo.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A hist√≥ria de um jogo com voc√™ mesmo em jogos</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O fen√¥meno de brincar consigo mesmo tem uma longa hist√≥ria, refletida na pr√°tica de desenvolver agentes artificiais de jogos projetados para competir com as pessoas nos jogos. </font><font style="vertical-align: inherit;">Um dos primeiros a usar esse sistema foi Arthur Samuel, que desenvolveu um simulador de xadrez na d√©cada de 1950 e publicou </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">esse trabalho</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> em 1959. </font><font style="vertical-align: inherit;">Esse sistema se tornou o precursor de um resultado hist√≥rico no aprendizado por refor√ßo alcan√ßado por Gerald Tesauro no TD-Gammon; </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">totais publicados</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">em 1995. TD-Gammon usou o algoritmo de diferen√ßa de tempo TD (Œª) com a fun√ß√£o de jogar sozinho para treinar o agente para jogar gam√£o para que ele pudesse competir com um profissional. Em alguns casos, foi observado que o TD-Gammon tem uma vis√£o mais confiante de posi√ß√µes do que jogadores de classe mundial. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Brincar consigo mesmo se reflete em muitas das conquistas ic√¥nicas associadas ao RL. √â importante notar que jogar consigo mesmo ajudou no desenvolvimento de agentes para jogar </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">xadrez e</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> combina habilidades sobre-humanas, agentes de elite do </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">DOTA 2</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , al√©m de estrat√©gias e contra-estrat√©gias complexas em jogos como </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">luta livre</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> e </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;">esconde</font></a><font style="vertical-align: inherit;"> - </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">esconde.</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Nos resultados alcan√ßados ao jogar consigo mesmo, observa-se frequentemente que os agentes do jogo escolhem estrat√©gias que surpreendem pessoas especializadas. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Tocar com voc√™ mesmo d√° aos agentes uma certa criatividade, independente da criatividade dos programadores. </font><font style="vertical-align: inherit;">O agente recebe apenas as regras do jogo e, em seguida - informa√ß√µes sobre se ele ganhou ou perdeu. </font><font style="vertical-align: inherit;">Al√©m disso, com base nesses princ√≠pios b√°sicos, o agente deve desenvolver um comportamento competente. </font><font style="vertical-align: inherit;">De acordo com o criador do TD-Gammon, essa abordagem ao aprendizado libera "... no sentido de que o programa n√£o √© restringido por inclina√ß√µes e preconceitos humanos, que podem vir a ser err√¥neos e n√£o confi√°veis". </font><font style="vertical-align: inherit;">Gra√ßas a essa liberdade, os agentes descobrem estrat√©gias brilhantes de jogos que mudam completamente a maneira como os engenheiros pensam sobre determinados jogos.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Treinamento de refor√ßo competitivo</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dentro da estrutura da tarefa tradicional de aprendizado refor√ßado, o agente est√° tentando desenvolver uma linha de comportamento que maximize a recompensa total. O sinal de recompensa codifica a tarefa do agente - tal tarefa pode, por exemplo, tra√ßar um curso ou coletar itens. O comportamento do agente est√° sujeito a restri√ß√µes ambientais. Tais, por exemplo, gravidade, obst√°culos, bem como a influ√™ncia relativa de a√ß√µes tomadas pelo pr√≥prio agente - por exemplo, a aplica√ß√£o de for√ßa para o pr√≥prio movimento. Esses fatores limitam o comportamento do agente e s√£o for√ßas externas que ele precisa aprender a lidar para receber uma alta recompensa. Assim, o agente compete com a din√¢mica do ambiente e deve passar de estado para estado precisamente para que a recompensa m√°xima seja alcan√ßada.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/if/jf/xi/ifjfxil_iqm9pznr61qbpxujs38.png"><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O cen√°rio t√≠pico de treinamento de refor√ßo √© mostrado √† esquerda: o agente atua no ambiente, transfere para o pr√≥ximo estado e recebe uma recompensa. O cen√°rio de treinamento √© mostrado √† direita, onde o agente compete com um rival que, do ponto de vista do agente, √© realmente um elemento do ambiente.</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
No caso de jogos competitivos, o agente compete n√£o apenas com a din√¢mica do ambiente, mas tamb√©m com outro agente (possivelmente intelectual). Podemos assumir que o oponente est√° embutido no ambiente, e suas a√ß√µes afetam diretamente o pr√≥ximo estado que o agente ‚Äúv√™‚Äù, bem como a recompensa que ele receber√°. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/zl/kv/xo/zlkvxoa9_bt0cdsthmddwll-fvc.png"><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Exemplo de t√™nis do ML-Agents Toolkit</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Considere a demonstra√ß√£o do ML-Agents Tennis. A raquete azul (esquerda) √© o agente de aprendizado e o roxo (direita) √© seu oponente. Para arremessar a bola pela rede, o agente deve levar em considera√ß√£o a trajet√≥ria da bola que voa do oponente e fazer um ajuste no √¢ngulo e velocidade da bola que leva em considera√ß√£o as condi√ß√µes ambientais (gravidade). No entanto, em uma competi√ß√£o com um oponente, jogar a bola por cima da rede √© apenas metade da batalha. Um oponente forte pode responder com um golpe irresist√≠vel e, como resultado, o agente perde. Um oponente fraco pode acertar a bola na rede. Um oponente igual pode retornar o saque e, portanto, o jogo continuar√°. De qualquer forma, o estado seguinte e a recompensa correspondente dependem das condi√ß√µes ambientais e do oponente. No entanto, em todas essas situa√ß√µes, o agente faz o mesmo tom. Portanto, como treinamento em jogos competitivos,bombear comportamentos rivais por um agente √© um problema complexo.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Considera√ß√µes para um oponente adequado n√£o s√£o triviais. Como est√° claro acima, a for√ßa relativa do oponente afeta significativamente o resultado de um jogo em particular. Se o oponente √© muito forte, o agente pode achar dif√≠cil aprender a jogar do zero. Por outro lado, se o oponente √© muito fraco, o agente pode aprender a vencer, mas essas habilidades podem ser in√∫teis na competi√ß√£o com um oponente mais forte ou simplesmente diferente. Portanto, precisamos de um oponente que seja aproximadamente igual em for√ßa ao agente (inflex√≠vel, mas n√£o intranspon√≠vel). Al√©m disso, como as habilidades de nosso agente melhoram a cada jogo conclu√≠do, precisamos aumentar a for√ßa de seu oponente na mesma medida. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/fn/vk/pt/fnvkptgsipqopi2iiyflsa5ec_a.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ao jogar com voc√™ mesmo, um instant√¢neo do passado ou um agente em seu estado atual √© o oponente incorporado ao ambiente.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
√â aqui que o jogo conosco √© √∫til! O pr√≥prio agente atende a ambos os requisitos para o oponente desejado. Ele √© definitivamente aproximadamente igual em for√ßa a si mesmo, e suas habilidades melhoram com o tempo. Nesse caso, a pol√≠tica do pr√≥prio agente √© incorporada ao ambiente (veja a figura). Aqueles que est√£o familiarizados com o </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">aumento gradual da educa√ß√£o em complexidade</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (aprendizagem no curr√≠culo), mostram que podemos assumir que o sistema est√° desenvolvendo naturalmente o curr√≠culo, ap√≥s o qual o agente aprende a lutar contra oponentes cada vez mais poderosos. Assim, jogar consigo mesmo permite que voc√™ use o pr√≥prio ambiente para treinar agentes competitivos para jogos competitivos!</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nas pr√≥ximas duas se√ß√µes, detalhes mais t√©cnicos do treinamento de agentes competitivos ser√£o discutidos, em particular, a implementa√ß√£o e o uso do jogo com voc√™ mesmo no ML-Agents Toolkit.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Considera√ß√µes pr√°ticas</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Alguns problemas pr√°ticos surgem em rela√ß√£o √† estrutura para brincar consigo mesmo. </font><font style="vertical-align: inherit;">Em particular, √© poss√≠vel treinar novamente, em que o agente aprende a vencer apenas com um certo estilo de jogo, bem como a instabilidade inerente ao processo de aprendizagem, que pode surgir devido √† instabilidade da fun√ß√£o de transi√ß√£o (ou seja, devido √† constante mudan√ßa de oponentes). </font><font style="vertical-align: inherit;">O primeiro problema surge porque queremos que nossos agentes tenham uma compreens√£o geral e capacidade de combater oponentes de tipos diferentes.</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O segundo problema pode ser ilustrado no ambiente do t√™nis: diferentes oponentes bater√£o na bola em velocidades e √¢ngulos diferentes. Do ponto de vista do agente de aprendizagem, isso significa que, √† medida que voc√™ aprende, as mesmas decis√µes levar√£o a diferentes resultados e, consequentemente, o agente estar√° em diferentes situa√ß√µes subsequentes. No aprendizado tradicional por refor√ßo, as fun√ß√µes estacion√°rias de transi√ß√£o est√£o impl√≠citas. Infelizmente, tendo preparado uma sele√ß√£o de v√°rios oponentes para o agente, a fim de resolver o primeiro problema, n√≥s, sendo descuidados, podemos agravar o segundo.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para lidar com isso, manteremos um buffer com as pol√≠ticas anteriores de agentes, das quais escolheremos rivais em potencial para nosso "aluno" a longo prazo. </font><font style="vertical-align: inherit;">Escolhendo um agente de pol√≠ticas anteriores, obtemos para ele uma sele√ß√£o de diversos oponentes. </font><font style="vertical-align: inherit;">Al√©m disso, permitindo que o agente treine com um oponente fixo por um longo tempo, estabilizamos a fun√ß√£o de transi√ß√£o e criamos um ambiente de aprendizado mais consistente. </font><font style="vertical-align: inherit;">Finalmente, esses aspectos algor√≠tmicos podem ser controlados usando hiperpar√¢metros, que ser√£o discutidos na pr√≥xima se√ß√£o.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Detalhes de implementa√ß√£o e uso</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Escolhendo hiperpar√¢metros para brincar conosco, temos, antes de tudo, em mente um compromisso entre o n√≠vel do oponente, a universalidade da pol√≠tica final e a estabilidade do treinamento. Treinar em competi√ß√£o com um grupo de oponentes que mudam lentamente ou n√£o muda e, portanto, produzem uma dispers√£o menor de resultados, √© um processo mais est√°vel do que treinar em competi√ß√£o com muitos oponentes diversos que mudam rapidamente. Os hiperpar√¢metros dispon√≠veis permitem controlar com que frequ√™ncia a pol√≠tica atual do agente ser√° salva para uso posterior como um dos oponentes da amostra, com que frequ√™ncia o novo oponente ser√° salvo, posteriormente selecionado para sparring, com que frequ√™ncia o novo oponente ser√° selecionado, o n√∫mero de oponentes salvos e a probabilidadeneste caso, o aluno ter√° que jogar contra seu pr√≥prio alter ego, e n√£o contra um oponente selecionado da piscina.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Em jogos competitivos, o pr√™mio "cumulativo" concedido pelo ambiente talvez n√£o seja a m√©trica mais informativa para acompanhar o progresso do aprendizado. O fato √© que o pr√™mio acumulado depende inteiramente do n√≠vel do oponente. Um agente com uma certa habilidade no jogo receber√° uma recompensa maior ou menor, dependendo de um oponente menos habilidoso ou mais habilidoso, respectivamente. Propomos a implementa√ß√£o </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">do sistema de classifica√ß√£o ELO</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , que permite calcular a habilidade relativa de jogo de dois jogadores de uma determinada popula√ß√£o ao jogar com um valor zero. Durante uma √∫nica execu√ß√£o de treinamento, esse valor deve aumentar constantemente. Voc√™ pode acompanh√°-lo, juntamente com outras m√©tricas de aprendizado, por exemplo, o pr√™mio geral, usando o </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">TensorBoard</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Jogando com voc√™ no futebol</font></font></h3> <br>
<img src="https://habrastorage.org/webt/gl/m_/rh/glm_rhop1whkyve0i-wpubxfpqw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As vers√µes mais recentes do ML-Agent Toolkit n√£o incluem pol√≠ticas de agente para o ambiente de aprendizado do Soccer, pois o processo de treinamento confi√°vel n√£o foi constru√≠do nele. </font><font style="vertical-align: inherit;">No entanto, usando o jogo conosco e com alguma refatora√ß√£o, podemos treinar o agente em comportamentos n√£o triviais. </font><font style="vertical-align: inherit;">A mudan√ßa mais significativa √© a remo√ß√£o de "posi√ß√µes de jogo" das caracter√≠sticas do agente. </font><font style="vertical-align: inherit;">No in√≠cio do futebol, o ‚Äúgoleiro‚Äù e o ‚Äúatacante‚Äù claramente se destacavam, ent√£o toda a jogabilidade parecia mais l√≥gica. </font><font style="vertical-align: inherit;">Em </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">este v√≠deo</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">√© apresentado um novo ambiente no qual √© poss√≠vel ver como o comportamento do papel √© formado espontaneamente, no qual alguns agentes come√ßam a atuar como atacantes e outros como goleiros. Agora, os pr√≥prios agentes est√£o aprendendo a desempenhar essas posi√ß√µes! A fun√ß√£o de recompensa para todos os quatro agentes √© definida como +1,0 para um gol marcado e -1,0 para um gol sofrido, com uma penalidade adicional de -0.0003 por etapa - essa penalidade √© fornecida para estimular os agentes a atacar.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Aqui, enfatizamos mais uma vez que os agentes do ambiente de aprendizado do Soccer aprendem o comportamento cooperativo e, para isso, nenhum algoritmo expl√≠cito √© usado relacionado ao comportamento de v√°rios agentes ou √† atribui√ß√£o de fun√ß√µes. </font><font style="vertical-align: inherit;">Este resultado demonstra que um agente pode ser treinado em comportamentos complexos usando algoritmos relativamente simples - desde que a tarefa seja bem formulada. </font><font style="vertical-align: inherit;">A condi√ß√£o mais importante para isso √© que os agentes possam observar seus colegas de equipe, ou seja, eles recebem informa√ß√µes sobre a posi√ß√£o relativa do companheiro de equipe. </font><font style="vertical-align: inherit;">For√ßando uma luta agressiva pela bola, o agente indiretamente diz ao companheiro de equipe que ele deve se mover em defesa. </font><font style="vertical-align: inherit;">Pelo contr√°rio, afastando-se na defesa, o agente provoca um companheiro de equipe para atacar.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Qual √© o pr√≥ximo</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Se voc√™ j√° usou algum dos novos recursos desta vers√£o - conte-nos sobre eles. </font><font style="vertical-align: inherit;">Chamamos sua aten√ß√£o para a p√°gina de </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">problemas</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> do </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;">ML-Agents GitHub</font></a><font style="vertical-align: inherit;"> , onde voc√™ pode falar sobre os erros encontrados, bem como a p√°gina de </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">f√≥runs</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> do </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;">Unity ML-Agents</font></a><font style="vertical-align: inherit;"> , onde s√£o discutidas quest√µes e problemas gerais.</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt507200/index.html">7 maneiras que os cientistas de dados enganam voc√™</a></li>
<li><a href="../pt507202/index.html">Encontro do Avito Analytics</a></li>
<li><a href="../pt507204/index.html">Cozinha interior de design industrial: do esbo√ßo ao produto na caixa</a></li>
<li><a href="../pt507206/index.html">Arquitetura Y messenger</a></li>
<li><a href="../pt507210/index.html">O desempenho do Java moderno ao trabalhar com grandes quantidades de dados, parte 2</a></li>
<li><a href="../pt507214/index.html">Como criar e modificar formul√°rios PDF interativos ou a nova habilidade ABBYY FineReader PDF</a></li>
<li><a href="../pt507218/index.html">Leia-me ou por que o texto n√£o √© lido at√© o fim</a></li>
<li><a href="../pt507222/index.html">Por que todos deveriam usar m√°scaras</a></li>
<li><a href="../pt507224/index.html">Como eliminar pontos cegos com testes visuais</a></li>
<li><a href="../pt507226/index.html">OCR para PDF no .NET - Como extrair texto de documentos PDF inacess√≠veis</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>