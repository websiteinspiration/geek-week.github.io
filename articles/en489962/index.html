<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🧑🏾 👷🏻 🏏 Data Distribution in Apache Ignite 👨🏽‍🏭 🤵🏾 👨🏿‍🤝‍👨🏾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello! This post is a slightly abridged version of my eponymous lecture at the Apache Ignite community meeting . The full video version along with que...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Data Distribution in Apache Ignite</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/gridgain/blog/489962/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hello! </font><font style="vertical-align: inherit;">This post is a slightly abridged version of my eponymous lecture at the </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Apache Ignite</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> community meeting </font><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">The full video version along with questions and answers can be </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">viewed here</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , and the slides can be </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">downloaded here</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">In the report, I tried to show by examples how data is distributed in Apache Ignite.</font></font><br>
<a name="habracut"></a><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Why do you need to distribute anything</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A fairly standard history of the development of any system requiring data storage and processing is the achievement of a certain ceiling. Either there is a lot of data and they are not physically placed on the storage device, or the load is growing at such a rate that one server is no longer able to process such a number of requests. There are frequent cases when both of them occur. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As a rule, they come to one of two solutions: either sharding the existing storage, or switching to a distributed database. Both solutions have a number of common features, the most obvious of which is the use of more than one node for working with data. Further, many nodes I will call topology.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The problem of data distribution among topology nodes can be formulated as a set of requirements, which our distribution must satisfy:</font></font><br>
<br>
<ol>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">An algorithm is needed that will allow all nodes of the topology and client applications to come to the same conclusion about which node or nodes the certain object (or key) is on.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Uniformity of distribution. </font><font style="vertical-align: inherit;">The more evenly the data is distributed between nodes, the more evenly the load on these nodes will be distributed. </font><font style="vertical-align: inherit;">Here I make the assumption that our nodes have approximately the same resources.</font></font></li>
<li>  .      ,       ,    .   ,       ,       ,     .<br>
</li>
</ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Achieving the first two requirements is fairly easy. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A familiar approach, often used when balancing load between functionally equivalent servers, dividing modulo N, where N is the number of nodes in the topology and we have a one-to-one correspondence between the node number and its identifier. Then all we need to do is to represent the key of the object as a numerical value using a hash function and take the remainder of division by N from the obtained value. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/zi/7x/ii/zi7xiiu8xmrjgaeghuc8xkekubk.png" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The diagram shows the distribution of 16 keys over 3 nodes. It can be seen that this distribution is uniform, and the algorithm for obtaining the node for the object is simple and guarantees that if all nodes of the topology use this algorithm, then the same result will be obtained for the same key and the same N.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
But what happens if we introduce the 4th node into the topology? </font></font><br>
<br>
<img src="https://habrastorage.org/webt/82/ro/-i/82ro-ipa6d3lw8stb_fx7e9_wos.png" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Our function has changed, now we take the remainder of the division by 4, not by 3. And if the function has changed, then the distribution has changed, and very much. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Here, the previous location of the objects for the previous version of the topology of three nodes is shown in red, and the position of the objects for the new version of the topology of four nodes is green, respectively. This is very similar to the usual diff files, but instead of files we have nodes. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
It is easy to see that the data has moved not only to the new node, but also there was an exchange of data between nodes that were already in the topology. Those. we observe spurious traffic between nodes and the requirement of a minimal change in distribution is not fulfilled.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Two popular ways to solve the problem of data distribution, taking into account the listed requirements, are as follows:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Consistent hashing</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Largest Random Weight Algorithm (HRW), also known as Rendezvous hashing.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Both of these algorithms are very simple. </font><font style="vertical-align: inherit;">Their descriptions on Wikipedia fit into several sentences. </font><font style="vertical-align: inherit;">Although it’s hard to call them obvious. </font><font style="vertical-align: inherit;">For those interested, I recommend reading the original articles </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> and </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A Name-BasedMapping Scheme for Rendezvous</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Most understandably, in my opinion, the idea of ​​a consistent hashing algorithm </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">is conveyed</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> in </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;">this Stanford course</font></a><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Let's look at these algorithms in more detail.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Consistent Hashing</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The trick underlying the consistent hashing algorithm is to map both nodes and stored objects to the same identifier space. </font><font style="vertical-align: inherit;">This makes our seemingly different entities, objects and nodes comparable. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
To obtain such a mapping, we simply apply the same hash function to the keys of the objects and to the identifiers of the nodes. </font><font style="vertical-align: inherit;">The result of the hash function for the node will be called a token, this will be useful to us later. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We represent our identifier space in the form of a circle, i.e. </font><font style="vertical-align: inherit;">we simply assume that the maximum identifier value immediately follows the minimum identifier value.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Now, in order to determine on which node the object lives, you need to get the value of the hash function from its key, and then simply move clockwise around the circle until we encounter the token of a node on the way. The direction of movement is unimportant, but it must be fixed. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The imaginary clockwise movement is functionally equivalent to a binary search in a sorted array of node tokens. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ca/dh/fy/cadhfyrlc9b_maoinnjw0kvp6fm.png" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In the diagram, each sector of a particular color reflects the identifier space for which a particular node is responsible. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
If we add a new node, then ... </font></font><br>
<br>
<img src="https://habrastorage.org/webt/v7/an/3r/v7an3r9bwxn9hnziy-w7umr5viq.png" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
... it will divide one of the sectors into two parts and completely take over the corresponding keys. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In this example, node 3 took over part of the keys of node 1.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As you can see, this approach gives a rather uneven distribution of objects across nodes, because it is highly dependent on the identifiers of the nodes themselves. How can this approach be improved? </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
You can assign more than one token to nodes (usually hundreds). This can be achieved, for example, by introducing many hash functions for the node (one per token) or repeatedly applying the same hash function to the token obtained in the previous step. But we must not forget about the collisions. There should not be two nodes with the same token. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/7i/m2/ay/7im2ay5cr_ydc5hh62vlwcaqhp0.png" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In this example, each node has 4 tokens.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
What else is important to mention: if we want to ensure the safety of data in the event of a node leaving the topology, then we need to store the keys on several nodes (so-called replicas or backups). </font><font style="vertical-align: inherit;">In the case of the consistent hashing algorithm, the replicas will be the following N-1 nodes on the circle, where N is the replication factor. </font><font style="vertical-align: inherit;">Of course, the order of the nodes should be determined by a specific token (for example, by the first), because </font><font style="vertical-align: inherit;">when using multiple tokens for each of them, the arrangement of nodes may differ. </font><font style="vertical-align: inherit;">Pay attention to the scheme: it does not have a clear pattern of repetition of nodes.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As for the requirement of a minimal change in the distribution when changing the topology, it is satisfied because the mutual order of the nodes on the circle is unchanged. </font><font style="vertical-align: inherit;">Those. </font><font style="vertical-align: inherit;">removing a node from the topology will not change the order relation between the remaining nodes.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Rendezvous hashing</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The Rendezvous hashing algorithm seems even simpler than consistent hashing. The algorithm is based on the same principle of invariance of order relations. But instead of making nodes and objects comparable, we make only nodes for a specific object comparable. Those. we determine the order relation between nodes for each object independently. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Again hashing helps us with this. But now, in order to determine the weight of the node N for a given object O, we mix the identifier of the object with the identifier of the node and take the hash from this mix. Having done this operation for each node, we get a set of weights by which we sort the nodes. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The node that turned out to be the first and will be responsible for storing the object.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Since all nodes of the topology use the same input data, the result for them will be identical. Which satisfies the first requirement. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ox/hs/bw/oxhsbwgq8xrlfrirs8xupmx3hww.png" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Consider an example. Here we have an order relation between three nodes for four different keys. Yellow indicates the node with the highest weight, i.e. the node that will ultimately be responsible for a particular key. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Add another node to the topology.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/hc/a9/4s/hca94sz_p5pz-lwflppuqlm7c54.png" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
I deliberately placed it on the diagonal to take into account all the possible options. Here, node 3, shown in green, entered the topology. Therefore, the weight distribution of the nodes for each of the keys has changed. Red indicates the nodes that have changed their location in the list for a particular key, because the weights of these nodes were less than the weight of the added node. However, this change affected only one of the keys, K3. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Let's treacherously derive a node from a topology. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ds/gr/d_/dsgrd_q9ai9yi9p5x08ubfgflem.png" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Once again, the changes affected only one key, this time K1. The remaining objects were not affected. The reason, as in the case of consistent hashing, is the invariance of the order relationship between any pair of nodes. Those. the requirement of a minimum change in distribution is met and there is no spurious traffic between nodes.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The distribution for rendezvous looks pretty good and does not require additional tricks compared to consistent hashing like tokens. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In case we want to support replication, then the next node in the list will be the first replica for the object, the next node will be the second replica, etc.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">How rendezvous hashing is used in Apache Ignite</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The so-called affinity function is responsible for the distribution of data in Apache Ignite (see the </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">AffinityFunction</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> interface </font><font style="vertical-align: inherit;">). The default implementation is rendezvous hashing (see the </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">RendezvousAffinityFunction</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> class </font><font style="vertical-align: inherit;">). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The first thing you need to pay attention to is that Apache Ignite does not map stored objects directly to topology nodes. Instead, an additional concept is introduced - partition. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A partition is a container for objects and a replication unit. In addition, the number of partitions for a particular cache (this is an analog of the table in the familiar databases) is set at the configuration stage and does not change during the cache life cycle.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Thus, we can display objects on partitions using effective modulo division, and use rendezvous hashing to display partitions on nodes. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/wd/wr/vt/wdwrvtuau9ywcgq4uqk0hhqscu8.png" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Because the number of partitions for the cache is a constant, then we can calculate the partition distribution by nodes once and cache the result until the topology is changed. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Each node calculates this distribution independently, but on all nodes with the same input data this distribution will be identical. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Partition can have several copies, we call them backups. The primary partition is called the primary partition.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
For the best distribution of keys between partitions and partitions by nodes, the following rule must be fulfilled: the number of partitions should be significantly greater than the number of nodes, in turn, the number of keys should be significantly greater than the number of partitions. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Caches in Ignite are partitioned and replicated. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In a partitioned cache, the number of backups is set at the cache creation stage. Partitions - primaries and backups - are evenly distributed between nodes. Such a cache is best suited for working with operational data, as provides the best write performance, which directly depends on the number of backups. In general, the more backups, the more nodes must confirm the key record.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/hh/fs/_u/hhfs_ujczi2m1p8kl4c5z3p2ra4.png" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In this example, the cache has one backup. </font><font style="vertical-align: inherit;">Those. </font><font style="vertical-align: inherit;">we can lose one node and not lose data, because </font><font style="vertical-align: inherit;">Partition backups are never stored on the same node as the primary partition or its other backup. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In the replicated cache, the number of backups is always equal to the number of topology nodes minus 1. That is, </font><font style="vertical-align: inherit;">each node always contains copies of all partitions. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/n-/fk/by/n-fkbyie-hipm40twvyg_erjjm0.png" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Such a cache is best suited for working with rarely changed data (for example, directories) and provides the greatest availability, as </font><font style="vertical-align: inherit;">we can lose N-1 nodes (in this case 3) without losing data. </font><font style="vertical-align: inherit;">Also in this option, we will get maximum read performance if we allow to read data from both the primary partitions and backups.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Data colocation in Apache Ignite</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
An important concept to keep in mind for best performance is collocation. Colocation is the placement of any objects in the same place. In our case, objects are entities stored in the cache, and a place is a node. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
If objects are distributed across partitions of the same affinity function, it is logical that objects with the same affinity key will fall into the same partition, and therefore, to the same node. In Ignite, this is called affinity colocation. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
By default, an affinity key is the primary key of an object. But in Ignite, you can use any other field of an object as an affinity key.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Collocation significantly reduces the amount of data sent between nodes to perform calculations or SQL queries, which naturally leads to a reduction in the time spent on these tasks. Consider this concept by example. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Let our data model consist of two entities: order (Order) and order position (OrderItem). One order can correspond to many items. The order and line item identifiers are independent, but the line item has a foreign key that refers to the corresponding order. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Suppose we need to perform some task, which for each order must perform calculations for the positions of this order. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
By default, an affinity key is a primary key. Therefore, orders and positions will be distributed between nodes in accordance with their primary keys, which, I recall, are independent.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/tk/q-/vq/tkq-vq1_sbghpcqdsz8ythpehim.png" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
On the diagram, orders are represented by squares, and positions in circles. Color indicates that the item belongs to the order. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
With this distribution of data, our hypothetical task will be sent to the node where the desired order is located, and then it will need to read the positions from all the other nodes, or send a subtask to these nodes and get the calculation result. This is an unnecessary network interaction that can and should be avoided. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
What if we tell Ignite that order items must be placed on the same nodes as the orders themselves, i.e. collect data? </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As the affinity key for the position, we take the foreign key OrderId and this field will be used when calculating the partition to which the record belongs. Moreover, inside the partition, we can always find our object by the primary key.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/lv/uq/kg/lvuqkgzf9gpoot8gjnr25uxb6ve.png" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Now, if both caches (Order and OrderItem) use the same affinity function with the same parameters, our data will be nearby and we will not need to go around the network for order items.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Affinity configuration in Apache Ignite</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In the current implementation, an affinity function object is a cache configuration parameter. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The affinity function itself takes the following arguments when creating:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Number of partitions;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The number of backups (in fact, this is also the configuration parameter of the cache);</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Backup filter;</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Flag excludeNeighbors.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
These settings cannot be changed. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
With the number of partitions and backups, everything seems to be clear. I’ll talk about the backup filter and the excludeNeighbors flag a bit later. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
At run time, the input affinity function receives the current cluster topology - essentially a list of cluster nodes - and calculates the distribution of partitions by nodes in accordance with the examples that I showed when I talked about the rendezvous hashing algorithm. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As for the backup filter, this is a predicate that allows you to prohibit affinity functions from assigning backup partitions to a node for which the predicate returned false. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As an example, suppose that our physical nodes - servers - are located in the data center in different racks. Typically, each rack has its own independent power ...</font></font><br>
<br>
<img src="https://habrastorage.org/webt/5e/ts/wy/5etswypgpotv2e9exzu3gsyl9f8.png" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
... and if we lose the rack, then we lose the data. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/nz/hy/5d/nzhy5dzl77t7pxhtxbtqubcbzek.png" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In this example, we lost half of the partitions. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
But if we set the correct backup filter, then the distribution will change in such a way ... </font></font><br>
<br>
<img src="https://habrastorage.org/webt/20/90/gy/2090gyzlg_0-80-dhkw8hvplftm.png" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
... that if the rack is lost, there will be no data loss and they will still be available. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/oo/eg/k5/ooegk5vas7xhk7qzrergws5ieli.png" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The excludeNeighbors flag performs a similar function, and in fact it is an abbreviation for one specific case. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Often multiple Ignite nodes run on the same physical host. This case is very similar to the example with racks in the data center, only now we are fighting data loss with the loss of the host, not the racks. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/tk/ze/wn/tkzewnmjc5iigjd_appzyvfoheo.png" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
The rest is the same. You can implement this behavior using a backup filter. This flag is a historical legacy and may be removed in the next major release of Ignite.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
It seems that I talked about the affinity function and data distribution everything that a developer using Apache Ignite needs to know. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In conclusion, let's look at an example of the distribution of 16 partitions according to the topology of 3 nodes. For simplicity and clarity, we believe that partitions do not have backups. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
I just took and wrote a little test that brought me the real distribution: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/yy/lm/np/yylmnpenoifogzoo0lig3kzw_5g.png" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As you can see, the uniformity of the distribution is not ideal. But the error will be noticeably lower with an increase in the number of nodes and partitions. The main rule that must be observed is that the number of partitions is significantly greater than the number of nodes. Now, in Ignite, the default number of partitions for a partitioned cache is 1024. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Now add a new node to the topology.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/xd/xf/sp/xdxfspt29ky3x0gfceqi_xiswv0.png" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Part of the parties moved to him. At the same time, the requirement of a minimum change in distribution was observed: the new node received part of the partitions, while the other nodes did not exchange partitions. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
We remove from the topology the node that was present in it at the initial stage: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ym/b5/il/ymb5ilednyy4dxf6ynojfva7gao.png" alt="image"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Now all partitions that were associated with the zero node were redistributed among other nodes of the topology, without violating our distribution requirements.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
As you can see, the solution to complex problems is often based on fairly trivial, although not entirely obvious, ideas. </font><font style="vertical-align: inherit;">The described solutions are used in most distributed databases and do a good job. </font><font style="vertical-align: inherit;">But these decisions are randomized and therefore the uniformity of distribution is far from ideal. </font><font style="vertical-align: inherit;">Can uniformity be improved without sacrificing performance and other distribution requirements? </font><font style="vertical-align: inherit;">The question remains open.</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../en489952/index.html">Live and motivating desktop wallpapers</a></li>
<li><a href="../en489954/index.html">Seagate SkyHawk AI - huge and vindictive</a></li>
<li><a href="../en489956/index.html">We animate RecyclerView easily without switching to ViewPager2</a></li>
<li><a href="../en489958/index.html">Explanation: why wc on Haskell turned out to be “faster” than the analogue on C</a></li>
<li><a href="../en489960/index.html">Automated build of a Delphi application</a></li>
<li><a href="../en489968/index.html">We catch the degradation of AA and AAA batteries at currents of 0.3 from the capacitance</a></li>
<li><a href="../en489970/index.html">Programmer, Pack and John Steinbeck</a></li>
<li><a href="../en489974/index.html">Kha vs HTML5: Compiling JavaScript in C ++</a></li>
<li><a href="../en489984/index.html">AMA about udalenka: ask - we answer</a></li>
<li><a href="../en489986/index.html">Power Stage Designer Utility - Power Electronics Developer Tool</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>