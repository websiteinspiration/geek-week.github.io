<!doctype html>
<html class="no-js" lang="es">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü•• üßîüèº ü§Ω Clasificaci√≥n con etiquetas m√∫ltiples üë©üèæ‚Äçü§ù‚Äçüë©üèº üî£ üëâüèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hola habrozhiteli! Decidimos citar un extracto del libro de Andrei Burkov , Aprendizaje autom√°tico sin palabras adicionales , dedicado a la clasificac...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Clasificaci√≥n con etiquetas m√∫ltiples</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/488362/"><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><img src="https://habrastorage.org/webt/hm/vf/c_/hmvfc_yyxepplv1mj0crw1vu7pw.jpeg" align="left" alt="imagen"></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hola habrozhiteli! </font><font style="vertical-align: inherit;">Decidimos citar un extracto del libro de Andrei Burkov </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, Aprendizaje autom√°tico sin palabras adicionales</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , dedicado a la clasificaci√≥n.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para describir la imagen en la figura, se pueden usar varias etiquetas simult√°neamente: ‚Äúbosque de con√≠feras‚Äù, ‚Äúmonta√±as‚Äù, ‚Äúcamino‚Äù. Si el n√∫mero de valores posibles para las etiquetas es grande, pero todos tienen la misma naturaleza que las etiquetas, cada muestra etiquetada se puede convertir a varios datos etiquetados, uno para cada etiqueta. Todos estos nuevos datos tendr√°n los mismos vectores de caracter√≠sticas y solo una etiqueta. Como resultado, la tarea se convierte en un problema de clasificaci√≥n multiclase. Se puede resolver utilizando la estrategia "uno contra todos". La √∫nica diferencia con el problema habitual de clasificaci√≥n multiclase es la aparici√≥n de un nuevo hiperpar√°metro: el umbral. Si el puntaje de similitud de una etiqueta est√° por encima de un valor umbral, esta etiqueta se asigna al vector de caracter√≠sticas de entrada. En este escenario, se pueden asignar m√∫ltiples etiquetas a un vector caracter√≠stico.El valor umbral se selecciona utilizando el conjunto de control.</font></font><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para resolver el problema de clasificaci√≥n con muchas etiquetas, se pueden aplicar algoritmos que se convierten naturalmente en multiclase (√°rboles de decisi√≥n, regresi√≥n log√≠stica, redes neuronales, etc.). Devuelven una estimaci√≥n para cada clase, por lo que podemos definir un umbral y luego asignar varias etiquetas a un vector de caracter√≠sticas para el cual la puntuaci√≥n de proximidad excede este umbral. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Las redes neuronales se pueden entrenar naturalmente en clasificaciones de etiquetas m√∫ltiples utilizando la entrop√≠a cruzada binaria como una funci√≥n de costo. La capa de salida de la red neuronal en este caso tiene un nodo por etiqueta. Cada nodo en la capa de salida tiene una funci√≥n de activaci√≥n sigmoidea. En consecuencia, cada etiqueta l es binaria</font></font><img src="https://habrastorage.org/webt/2n/_n/mg/2n_nmgvuciuadryomnq3urrg1xw.jpeg" alt="imagen"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">donde l = 1, ..., L e i = 1, ..., N. La entrop√≠a cruzada binaria determina la probabilidad de </font></font><img src="https://habrastorage.org/webt/lb/le/cf/lblecfgyuy23k8zvlasuajncsds.jpeg" alt="imagen"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">que la muestra xi tenga la etiqueta l, se define como el </font></font><img src="https://habrastorage.org/webt/qd/1e/fx/qd1efxpmi6mmuhv_fhmsqpjgpi4.jpeg" alt="imagen"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
criterio de Minimizaci√≥n: un promedio simple de todos los miembros de la entrop√≠a cruzada binaria en todas las muestras de entrenamiento y todas sus etiquetas. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En los casos en que el n√∫mero de valores de etiqueta posibles es peque√±o, puede intentar convertir el problema de clasificaci√≥n con muchas etiquetas en un problema de clasificaci√≥n multiclase. </font><font style="vertical-align: inherit;">Imagina el siguiente problema. </font><font style="vertical-align: inherit;">Debe asignar dos tipos de etiquetas a las im√°genes. </font><font style="vertical-align: inherit;">Las etiquetas del primer tipo pueden tener dos significados posibles: { </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">foto, pintura</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> }; </font><font style="vertical-align: inherit;">las marcas del segundo tipo pueden tener tres significados posibles: { </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">vertical, horizontal, otro</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">}. </font><font style="vertical-align: inherit;">Para cada combinaci√≥n de dos clases de origen, puede crear una nueva clase ficticia, por ejemplo:</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/mz/px/zn/mzpxzn0rlrumwoql7gkk3no7ihk.jpeg" alt="imagen"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ahora tenemos los mismos datos etiquetados, pero reemplazamos el conjunto de etiquetas verdaderas con una etiqueta ficticia con valores del 1 al 6. En la pr√°ctica, este enfoque da buenos resultados cuando no hay demasiadas combinaciones posibles de clases. De lo contrario, es necesario utilizar muchos m√°s datos de entrenamiento para compensar el aumento en el conjunto de clases. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La principal ventaja de este √∫ltimo enfoque es que las etiquetas permanecen correlacionadas, a diferencia de los m√©todos descritos anteriormente, que predicen cada etiqueta independientemente una de la otra. En muchas tareas, la correlaci√≥n entre etiquetas puede ser un factor significativo. Por ejemplo, imagine que desea clasificar el correo electr√≥nico como </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">spam</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y </font><i><font style="vertical-align: inherit;">no </font></i><i><font style="vertical-align: inherit;">spam</font></i></font><i><font style="vertical-align: inherit;"></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, y al mismo tiempo que ordinario e importante. </font><font style="vertical-align: inherit;">Probablemente quiera excluir pron√≥sticos como [ </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">spam, importante</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ].</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">7.5. </font><font style="vertical-align: inherit;">Conjunto de entrenamiento</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Los algoritmos fundamentales que cubrimos en el Cap√≠tulo 3 tienen sus limitaciones. Debido a su simplicidad, a veces no pueden crear un modelo que sea lo suficientemente efectivo para su tarea. En tales casos, puede intentar usar redes neuronales profundas. Sin embargo, en la pr√°ctica, las redes neuronales profundas requieren una cantidad significativa de datos etiquetados, que puede que no tenga. Otra forma de aumentar la efectividad de los algoritmos de aprendizaje simples es usar el </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">entrenamiento conjunto</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El entrenamiento en conjunto es un paradigma de entrenamiento que se basa en el entrenamiento no solo de un modelo s√∫per correcto, sino de un gran n√∫mero de modelos con baja precisi√≥n y combinando los pron√≥sticos dados por estos </font><font style="vertical-align: inherit;">modelos </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">d√©biles</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> para obtener un </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">metamodelo</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> m√°s correcto </font><font style="vertical-align: inherit;">.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Los modelos con baja precisi√≥n generalmente se entrenan con </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">algoritmos de aprendizaje d√©biles</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> que no pueden entrenar modelos complejos y, por lo tanto, muestran alta velocidad en las etapas de entrenamiento y pron√≥stico. Muy a menudo, el algoritmo de aprendizaje del √°rbol de decisi√≥n se usa como el algoritmo d√©bil, que generalmente deja de romper el conjunto de entrenamiento despu√©s de varias iteraciones. El resultado son √°rboles peque√±os y no muy regulares, pero, como dice la idea de entrenar al conjunto, si los √°rboles no son id√©nticos y cada √°rbol es al menos un poco mejor que adivinar al azar, podemos obtener una alta precisi√≥n combinando una gran cantidad de tales √°rboles. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para obtener el pron√≥stico final para la entrada </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, los pron√≥sticos de todos los modelos d√©biles se combinan utilizando alg√∫n m√©todo de votaci√≥n ponderada. </font><font style="vertical-align: inherit;">La forma espec√≠fica de ponderar los votos depende del algoritmo, pero la esencia no depende de √©l: si, colectivamente, los modelos d√©biles predicen que el correo electr√≥nico es spam, asignamos </font><font style="vertical-align: inherit;">la etiqueta de </font><i><font style="vertical-align: inherit;">spam </font></i></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> a la muestra </font><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">
Los dos m√©todos principales para entrenar conjuntos son el </font><b><font style="vertical-align: inherit;">refuerzo</font></b><font style="vertical-align: inherit;"> y el </font><b><font style="vertical-align: inherit;">embolsado</font></b><font style="vertical-align: inherit;"> (agregaci√≥n). </font><font style="vertical-align: inherit;">Las traducciones de los t√©rminos refuerzo y embolsado son inexactas y no est√°n acostumbradas.</font></font><i><font style="vertical-align: inherit;"></font></i><font style="vertical-align: inherit;"></font><br>
<br><font style="vertical-align: inherit;"></font><b><font style="vertical-align: inherit;"></font></b><font style="vertical-align: inherit;"></font><b><font style="vertical-align: inherit;"></font></b><font style="vertical-align: inherit;"></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">7.5.1. </font><font style="vertical-align: inherit;">Impulso y embolsado</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El m√©todo de refuerzo es utilizar los datos de entrenamiento iniciales y crear iterativamente varios modelos usando un algoritmo d√©bil. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Cada nuevo modelo difiere de los anteriores en que, al construirlo, un algoritmo d√©bil intenta "arreglar" los errores cometidos por los modelos anteriores. </font><font style="vertical-align: inherit;">El modelo de conjunto final es una combinaci√≥n de estos muchos modelos d√©biles construidos iterativamente. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La esencia del embolsado es crear muchas "copias" de los datos de entrenamiento (cada copia es ligeramente diferente de las otras) y luego aplicar un algoritmo d√©bil a cada copia para obtener varios modelos d√©biles, y luego combinarlos. </font><font style="vertical-align: inherit;">Un algoritmo de aprendizaje autom√°tico eficiente y ampliamente utilizado basado en la idea de ensacado es un </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">bosque aleatorio</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">7.5.2. </font><font style="vertical-align: inherit;">Bosque al azar</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El algoritmo de embolsado "cl√°sico" funciona de la siguiente manera. </font><font style="vertical-align: inherit;">Las muestras aleatorias B se crean a partir del conjunto de entrenamiento existente </font></font><img src="https://habrastorage.org/webt/-d/6r/gn/-d6rgnsn5wk-yzl3dbk0i7x2ole.jpeg" alt="imagen"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(para cada b = 1, ..., B) y </font></font><img src="https://habrastorage.org/webt/-d/6r/gn/-d6rgnsn5wk-yzl3dbk0i7x2ole.jpeg" alt="imagen"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">se construye </font><font style="vertical-align: inherit;">un </font><font style="vertical-align: inherit;">modelo de </font></font><img src="https://habrastorage.org/webt/nd/ew/4z/ndew4zvx7r0jpeilfknakojmrbs.jpeg" alt="imagen"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">√°rbol de decisi√≥n </font><font style="vertical-align: inherit;">sobre la base de cada muestra </font><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Para obtener una muestra </font></font><img src="https://habrastorage.org/webt/-d/6r/gn/-d6rgnsn5wk-yzl3dbk0i7x2ole.jpeg" alt="imagen"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">para alguna b, se hace una </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">muestra con reemplazo</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Es decir, primero se crea una muestra vac√≠a, y luego se selecciona una muestra aleatoria del conjunto de entrenamiento, y se coloca su copia exacta </font></font><img src="https://habrastorage.org/webt/-d/6r/gn/-d6rgnsn5wk-yzl3dbk0i7x2ole.jpeg" alt="imagen"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, mientras que la muestra misma permanece en el conjunto de entrenamiento original. </font><font style="vertical-align: inherit;">La selecci√≥n de datos contin√∫a hasta que se cumpla la condici√≥n. </font></font><img src="https://habrastorage.org/webt/se/au/-5/seau-5gwous1c1cshmrx8rwubig.jpeg" alt="imagen"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Como resultado del entrenamiento, </font><font style="vertical-align: inherit;">se </font><font style="vertical-align: inherit;">obtienen </font><font style="vertical-align: inherit;">√°rboles de decisi√≥n </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">B. </font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El pron√≥stico para la nueva muestra </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , en el caso de regresi√≥n, se determina como el promedio de </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">B</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> pron√≥sticos</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/re/zp/mj/rezpmjqa9lo7w4dxvqd6njwwqcm.jpeg" alt="imagen"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
o por mayor√≠a de votos en caso de clasificaci√≥n.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El bosque aleatorio tiene solo una diferencia del embolsado cl√°sico. Utiliza un algoritmo de aprendizaje de √°rbol modificado que, con cada divisi√≥n en el proceso de aprendizaje, verifica un subconjunto aleatorio de caracter√≠sticas. Esto se hace para eliminar la correlaci√≥n entre √°rboles: si una o m√°s caracter√≠sticas tienen una gran capacidad predictiva, muchos √°rboles las elegir√°n para dividir los datos. Esto conducir√° a la aparici√≥n en el "bosque" de una gran cantidad de √°rboles correlacionados. La correlaci√≥n de signos con alta capacidad predictiva evita que la precisi√≥n de la predicci√≥n aumente. La alta eficiencia del conjunto de modelos se explica por el hecho de que los buenos modelos tienen m√°s probabilidades de estar de acuerdo con el mismo pron√≥stico, y los malos modelos probablemente no est√©n de acuerdo y dar√°n diferentes pron√≥sticos. La correlaci√≥n har√° que los modelos pobres tengan m√°s probabilidades de estar de acuerdo,lo que distorsionar√° el patr√≥n de votaci√≥n o afectar√° el promedio.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Los hiperpar√°metros m√°s importantes para el ajuste son el n√∫mero de √°rboles B y el tama√±o de un subconjunto aleatorio de caracter√≠sticas que deben considerarse para cada divisi√≥n. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El bosque aleatorio es uno de los algoritmos de aprendizaje por conjuntos m√°s utilizados. ¬øQu√© determina su efectividad? La raz√≥n es que al usar varias muestras del conjunto de datos original, reducimos la </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">varianza del</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> modelo final. Recuerde que una baja varianza significa una d√©bil predisposici√≥n a la </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">reentrenamiento.</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">La reentrenamiento ocurre cuando el modelo intenta explicar peque√±as variaciones en el conjunto de datos porque el conjunto de datos es solo una peque√±a muestra de todos los ejemplos posibles del fen√≥meno que estamos tratando de simular. </font><font style="vertical-align: inherit;">En el caso de un enfoque fallido para la formaci√≥n del conjunto de entrenamiento, pueden caer en √©l algunos artefactos indeseables (pero inevitables): ruido, datos anormales y excesivamente representativos o insuficientemente representativos. </font><font style="vertical-align: inherit;">Al crear varias muestras aleatorias con el reemplazo del conjunto de entrenamiento, reducimos la influencia de estos artefactos.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">7.5.3. </font><font style="vertical-align: inherit;">Aumento de gradiente</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Otro algoritmo efectivo de entrenamiento en conjunto basado en la idea de aumentar es el aumento de gradiente. </font><font style="vertical-align: inherit;">Primero, considere el uso del aumento de gradiente en la regresi√≥n. </font><font style="vertical-align: inherit;">Comenzaremos a construir un modelo de regresi√≥n efectivo con un modelo constante </font></font><img src="https://habrastorage.org/webt/7u/7x/jd/7u7xjdufljpsjwwu45j9_gkc3r0.jpeg" alt="imagen"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(como lo hicimos en ID3):</font></font><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/em/vj/bv/emvjbvtmptxl_d3bihzev4wbh7o.jpeg" alt="imagen"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Luego cambie las etiquetas en todas las muestras i = 1, ..., N en el conjunto de entrenamiento:</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/va/wc/ah/vawcahk0zsdpgnumh_bfjuozw_e.jpeg" alt="imagen"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
donde </font></font><img src="https://habrastorage.org/webt/rf/1x/pk/rf1xpkgfxmcivy-1tqpwv2vgroi.jpeg" alt="imagen"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">se llama el </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">resto</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y es la nueva etiqueta de la muestra </font></font><img src="https://habrastorage.org/webt/dk/ey/2r/dkey2rj3yf-zkei2029wfa2ujso.jpeg" alt="imagen"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ahora usamos el conjunto de entrenamiento modificado con los residuos en lugar de las etiquetas originales para construir un nuevo modelo del √°rbol de decisi√≥n. </font></font><img src="https://habrastorage.org/webt/mx/fw/um/mxfwumzpc5tq1wjpdate2rxra48.jpeg" alt="imagen"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El modelo de refuerzo ahora se define como </font></font><img src="https://habrastorage.org/webt/cy/c2/vz/cyc2vz0tmrrihcm6kta_7rnwmui.jpeg" alt="imagen"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">donde Œ± es la velocidad de aprendizaje (hiperpar√°metro). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Luego, recalculamos los residuos usando la Ecuaci√≥n 7.2, reemplazamos las etiquetas en los datos de entrenamiento nuevamente, ense√±amos un nuevo modelo del √°rbol de decisi√≥n, </font></font><img src="https://habrastorage.org/webt/p4/wk/nl/p4wknlhlvwiqtr7zz7lx_y5oq3s.jpeg" alt="imagen"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">redefinimos el modelo de refuerzo a medida </font></font><img src="https://habrastorage.org/webt/n-/mn/ht/n-mnhtck0rar7pz4anzlbdc-bmo.jpeg" alt="imagen"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">que repetimos el proceso, hasta que combinamos el n√∫mero m√°ximo predeterminado </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">M de</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> √°rboles.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Vamos a entender intuitivamente lo que est√° sucediendo aqu√≠. Al calcular los residuos, determinamos qu√© tan bien (o mal) el modelo actual f predice el objetivo de cada muestra de entrenamiento. Luego entrenamos otro √°rbol para corregir los errores del modelo actual (por eso usamos las sobras en lugar de las etiquetas reales) y agregamos un nuevo √°rbol al modelo existente con algo de peso Œ±. Como resultado, cada nuevo √°rbol agregado al modelo corrige parcialmente los errores cometidos por los √°rboles anteriores. El proceso contin√∫a hasta que se combina el n√∫mero m√°ximo M (otro hiperpar√°metro) de los √°rboles.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ahora intentemos responder a la pregunta de por qu√© este algoritmo se denomina aumento de gradiente. En el aumento de gradiente, no calculamos el gradiente, a diferencia de lo que hicimos en el cap√≠tulo 4, resolviendo el problema de regresi√≥n lineal. Para ver las similitudes entre el aumento de gradiente y el descenso de gradiente, recuerde por qu√© calculamos el gradiente en regresi√≥n lineal: para encontrar la direcci√≥n de los valores de los par√°metros para minimizar la funci√≥n de costo MSE. El gradiente muestra la direcci√≥n, pero no muestra qu√© tan lejos ir en esta direcci√≥n, por lo que en cada iteraci√≥n dimos un peque√±o paso y luego determinamos nuevamente la direcci√≥n. Lo mismo sucede con el aumento de gradiente, pero en lugar de calcular directamente el gradiente, usamos su estimaci√≥n en forma de residuos: muestran c√≥mo se debe ajustar el modelo para reducir el error (residual).</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En el aumento de gradiente, hay tres hiperpar√°metros principales disponibles para el ajuste: el n√∫mero de √°rboles, la velocidad de aprendizaje y la profundidad de los √°rboles. Los tres afectan la precisi√≥n del modelo. La profundidad de los √°rboles tambi√©n afecta la velocidad de aprendizaje y pron√≥stico: cuanto menor es la profundidad, m√°s r√°pido. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Se puede demostrar que el aprendizaje por residuos optimiza el modelo general f para el est√°ndar de error est√°ndar. Aqu√≠ puede ver la diferencia del embolsado: aumentar aumenta el sesgo (o la falta de educaci√≥n) en lugar de la varianza. Como resultado, el refuerzo est√° sujeto a reciclaje. Sin embargo, al ajustar la profundidad y el n√∫mero de √°rboles, se puede evitar en gran medida el reciclaje.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El aumento de gradiente es similar para las tareas de calificaci√≥n, pero los pasos son ligeramente diferentes. Considere el caso de la clasificaci√≥n binaria. Supongamos que hay M √°rboles de decisi√≥n de regresi√≥n. Por analog√≠a con la regresi√≥n log√≠stica, el pron√≥stico del conjunto de √°rboles de decisi√≥n se modela utilizando la funci√≥n sigmoidea:</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/wy/pd/gw/wypdgwjjgpzuojrelehnadtdggc.jpeg" alt="imagen"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
¬øD√≥nde </font></font><img src="https://habrastorage.org/webt/w6/3d/kb/w63dkbj3f9j-ik9hrhqbexyxtem.jpeg" alt="imagen"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">est√° el √°rbol de regresi√≥n? </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Y nuevamente, como en la regresi√≥n log√≠stica, cuando se trata de encontrar un modelo de maximizaci√≥n </font></font><img src="https://habrastorage.org/webt/c4/lv/tt/c4lvttexfzr8disbsv1ph_drzka.jpeg" alt="imagen"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, se aplica el principio de m√°xima verosimilitud. Del mismo modo, para evitar el desbordamiento num√©rico, maximizamos la suma de los logaritmos de probabilidad, en lugar del producto de la probabilidad. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El algoritmo comienza con el modelo constante inicial </font></font><img src="https://habrastorage.org/webt/hn/7j/ep/hn7jepdxhudnxvjjgbulvfwtpyw.jpeg" alt="imagen"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">donde </font></font><img src="https://habrastorage.org/webt/gj/6s/f6/gj6sf6i874m3gq3_fbbxbkrudae.jpeg" alt="imagen"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(se puede demostrar que dicha inicializaci√≥n es √≥ptima para la funci√≥n sigmoidea). Luego, en cada iteraci√≥n m, se agrega un nuevo √°rbol fm al modelo. Para encontrar el mejor √°rbol </font></font><img src="https://habrastorage.org/webt/b0/5w/rv/b05wrvcuk5hzvnrmkpebdnk_kxu.jpeg" alt="imagen"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Para encontrar el mejor √°rbol </font></font><img src="https://habrastorage.org/webt/xm/u4/dp/xmu4dpm1hi3-podtuiayxggydti.jpeg" alt="imagen"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, la derivada parcial del </font></font><img src="https://habrastorage.org/webt/ep/hx/gu/ephxgusen4ou8pc8atm4ksqathq.jpeg" alt="imagen"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">modelo actual se </font><font style="vertical-align: inherit;">calcula primero </font><font style="vertical-align: inherit;">para cada i = 1, ..., N:</font></font><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/wg/v_/oo/wgv_oogvmupu4q5j6g3rq7dphvk.jpeg" alt="imagen"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
donde f es el modelo del clasificador de conjunto construido en la iteraci√≥n anterior m - 1. Para calcular </font></font><img src="https://habrastorage.org/webt/ep/hx/gu/ephxgusen4ou8pc8atm4ksqathq.jpeg" alt="imagen"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, necesitamos encontrar las derivadas de con </font></font><img src="https://habrastorage.org/webt/pv/pe/lb/pvpelb2jplsmq_jmkbdfyl-pzom.jpeg" alt="imagen"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">respecto a f para todo i. </font><font style="vertical-align: inherit;">Tenga en cuenta que la </font></font><img src="https://habrastorage.org/webt/nk/zn/0l/nkzn0lh0l0brnzl_vh0xxlmttrq.jpeg" alt="imagen"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">derivada con respecto a f del t√©rmino correcto en la ecuaci√≥n anterior es</font></font><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/xx/en/iu/xxeniu2qr35ln17asbfk2fqb3sc.jpeg" alt="imagen"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Luego, el conjunto de entrenamiento se transforma al reemplazar la etiqueta original de la </font></font><img src="https://habrastorage.org/webt/a1/fv/cu/a1fvcukqqvsu5wpv3x8i1zj5smc.jpeg" alt="imagen"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">derivada parcial correspondiente </font></font><img src="https://habrastorage.org/webt/ep/hx/gu/ephxgusen4ou8pc8atm4ksqathq.jpeg" alt="imagen"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, y se construye un nuevo √°rbol sobre la base del conjunto de entrenamiento convertido. </font></font><img src="https://habrastorage.org/webt/w1/8f/41/w18f41gto37doyvyakgs4np_fyy.jpeg" alt="imagen"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A continuaci√≥n, el paso √≥ptimo de actualizaci√≥n se determina </font></font><img src="https://habrastorage.org/webt/1n/cf/sj/1ncfsjxfe-tao3ep_csuw-s-arw.jpeg" alt="imagen"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">como:</font></font><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/co/qc/hw/coqchwpctxgy2ukbdwaybkouxx0.jpeg" alt="imagen"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Al final de la iteraci√≥n m, actualizamos el modelo de conjunto </font></font><img src="https://habrastorage.org/webt/r7/ox/vy/r7oxvyc5mesbifpwfjtkcrtdr2q.jpeg" alt="imagen"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">agregando un nuevo √°rbol</font></font><img src="https://habrastorage.org/webt/s2/vp/u0/s2vpu0-7pmzuzv0n55f1z1fgktu.jpeg" alt="imagen"><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/vg/ar/qn/vgarqnddik0vjhxxfsesr1t5qs8.jpeg" alt="imagen"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Las iteraciones contin√∫an hasta que se cumple la condici√≥n m = M, despu√©s de lo cual se termina el entrenamiento y se obtiene el modelo de conjunto f. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El aumento de gradiente es uno de los algoritmos de aprendizaje autom√°tico m√°s potentes. </font><font style="vertical-align: inherit;">No solo porque crea modelos muy precisos, sino tambi√©n porque es capaz de procesar grandes conjuntos de datos con millones de datos y caracter√≠sticas. </font><font style="vertical-align: inherit;">Como regla, supera en precisi√≥n el bosque aleatorio, pero debido a la naturaleza consistente puede aprender mucho m√°s lentamente.</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es488346/index.html">Instalaci√≥n de herramientas o con SCIP y GLPK en un entorno virtual Python 3.7 en Linux</a></li>
<li><a href="../es488348/index.html">Seminario web "Diez desaf√≠os √°giles principales y formas de superarlos en una hora" 17 de febrero a las 20:00 hora de Mosc√∫</a></li>
<li><a href="../es488352/index.html">Comparaci√≥n de costos de VDI: en las instalaciones versus la nube p√∫blica</a></li>
<li><a href="../es488356/index.html">Capacitaci√≥n en la Universidad T√©cnica Mar√≠tima Estatal de San Petersburgo para productos Dassault Syst√®mes</a></li>
<li><a href="../es488360/index.html">Big Data Mitos y Cultura Digital</a></li>
<li><a href="../es488366/index.html">Y nuevamente sobre "Informaci√≥n de zona horaria incorrecta para zonas horarias rusas" [.Net bug, ID: 693286]</a></li>
<li><a href="../es488368/index.html">Lo que aprend√≠ mientras trabajaba en mi primer proyecto a gran escala</a></li>
<li><a href="../es488370/index.html">TDD para microcontroladores. Parte 2: c√≥mo los esp√≠as eliminan las adicciones</a></li>
<li><a href="../es488374/index.html">Telegram + 1C + Webhooks + Apache + Certificado autofirmado</a></li>
<li><a href="../es488376/index.html">Cuando el principio "al diablo con todo, t√≥malo y hazlo!" no funciona: notas de procrastinator</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>