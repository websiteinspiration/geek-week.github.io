<!doctype html>
<html class="no-js" lang="pt">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👩‍👦‍👦 🌾 👂🏻 Entropia: como as Árvores de Decisão tomam decisões 😇 ➡️ 👨🏿‍🌾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Uma tradução do artigo foi preparada antes do início do curso Machine Learning .
 
 
 
 Você é um especialista em ciência de dados que está atualmente...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Entropia: como as Árvores de Decisão tomam decisões</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/502200/"><i><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Uma tradução do artigo foi preparada antes do início do curso </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Machine Learning</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font></b></i><br>
<br>
<img src="https://habrastorage.org/webt/az/2h/3e/az2h3eq1jejcxtd0g4wi4gmamki.png"><br>
<hr><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Você é um especialista em ciência de dados que está atualmente seguindo um caminho de aprendizado. E você percorreu um longo caminho desde que escreveu sua primeira linha de código em Python ou R. Você conhece o Scikit-Learn como a palma da sua mão. Agora você está mais sentado no Kaggle do que no Facebook. Você não é novo na criação de florestas aleatórias impressionantes e outros modelos de conjunto de árvores de decisão que fazem um excelente trabalho. No entanto, você sabe que não conseguirá nada se não se desenvolver de forma abrangente. Você deseja aprofundar e entender os meandros e os conceitos subjacentes aos modelos populares de aprendizado de máquina. Bem, eu também.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Hoje vou falar sobre o conceito de entropia - um dos tópicos mais importantes da estatística, e mais tarde falaremos sobre o conceito de ganho de informação (ganho de informação) e descobriremos por que esses conceitos fundamentais formam a base de como as árvores de decisão são construídas a partir dos dados obtidos.</font></font><a name="habracut"></a><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Boa. Agora vamos transgredir. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O que é entropia? Em termos simples, a entropia não passa de uma medida de desordem. (Também pode ser considerada uma medida de pureza, e logo você verá o porquê. Mas eu gosto mais da bagunça porque parece mais legal.) A </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
fórmula matemática da entropia é a seguinte: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ey/wa/u-/eywau-ntm5stedcuyrelbhhoipu.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Entropia. Às vezes, é escrito como H.</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Aqui p </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">i</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> é a probabilidade de frequência de um elemento / classe i de nossos dados. Por simplicidade, suponha que tenhamos apenas duas classes: positiva e negativa. Então pegarei o valor de "+" ou "-". Se tivéssemos um total de 100 pontos em nosso conjunto de dados, 30 dos quais pertenciam à classe positiva e 70 à negativa, então p </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> seria 3/10 ep</font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">-</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> será 7/10. Tudo é simples aqui. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Se eu calcular a entropia das classes neste exemplo, é isso que recebo usando a fórmula acima: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/5_/hh/20/5_hh20bihmp119n_5vzmlq_vuyw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Entropia é de cerca de 0,88. Esse valor é considerado bastante alto, ou seja, temos um alto nível de entropia ou desordem (ou seja, um baixo valor de pureza). A entropia é medida no intervalo de 0 a 1. Dependendo do número de classes no seu conjunto de dados, o valor da entropia pode ser maior que 1, mas significa o mesmo que o nível de desordem é extremamente alto. Para simplificar a explicação, no artigo de hoje, teremos entropia variando de 0 a 1. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dê uma olhada no gráfico abaixo.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/jt/sx/zs/jtsxzsfwwbstp10fqo-rd0ndddw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
No eixo X, o número de pontos da classe positiva em cada círculo é refletido e, no eixo Y, as entropias correspondentes. Você pode notar imediatamente a forma de U invertida do gráfico. A entropia será a menor em extremos quando não houver elementos positivos no círculo, em princípio, ou quando houver apenas elementos positivos. Ou seja, quando os elementos são idênticos em um círculo, o distúrbio será 0. A entropia será mais alta no meio do gráfico, onde os elementos positivos e negativos serão distribuídos igualmente dentro do círculo. Aqui a maior entropia ou desordem será alcançada, uma vez que não haverá elementos predominantes.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Existe alguma razão para a entropia ser medida usando o logaritmo de base 2, ou por que a entropia é medida entre 0 e 1, e não em um intervalo diferente? Não, não há razão. Esta é apenas uma métrica. Não é tão importante entender por que isso está acontecendo. É importante saber como o que chegamos acima é calculado e como funciona. A entropia é uma medida de confusão ou incerteza, e o objetivo dos modelos de aprendizado de máquina e dos especialistas em ciência de dados em geral é reduzir essa incerteza. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Agora sabemos como a bagunça é medida. Em seguida, precisamos de um valor para medir a redução desse distúrbio nas informações adicionais (atributos / variáveis ​​independentes) da variável / classe alvo. É aqui que o ganho de informação ou o ganho de informação entra em jogo. Do ponto de vista da matemática, pode ser escrito da seguinte maneira:</font></font><br>
<br>
<img src="https://habrastorage.org/webt/bn/el/t4/bnelt40yxay8hkbanig088mpk6a.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 Simplesmente subtraímos a entropia Y de X da entropia Y para calcular a diminuição na incerteza sobre Y, desde que X sobre Y esteja disponível. Quanto mais forte a incerteza diminuir, mais informações poderão ser obtidas de Y sobre X. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Vejamos um exemplo simples da tabela de contingência para que Aproxime-se da questão de como as árvores de decisão usam a entropia e o ganho de informações para decidir em que base os nós do processo de aprendizado de dados são quebrados. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Exemplo: tabela de conjugação</font></font></b><br>
<br>
<img src="https://habrastorage.org/webt/s4/ea/e5/s4eae57mpuehp3mk_mjpmikuan0.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
 Aqui, nossa variável de destino será </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Responsabilidade</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , que pode assumir apenas dois valores: </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Normal"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> e </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Alto"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Também temos apenas um sinal, chamado Rating de Crédito, que distribui os valores em três categorias: </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Excelente"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Bom"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> e </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Ruim"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Foram feitas 14 observações. 7 deles pertencem à classe de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">responsabilidade normal</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> e 7 outros à classe de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">alta responsabilidade</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Esta é uma divisão em si. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Se observarmos a soma total dos valores na primeira linha, veremos que temos 4 observações com Valor </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">excelente</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> com </font><font style="vertical-align: inherit;">base no </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Rating de crédito</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Além disso, posso até dizer que minha variável de destino é quebrada pelo </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Rating de crédito “Excelente”</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Entre as observações com o valor </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">“Excelente”</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> por atributo</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Classificação de crédito</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , existem 3 que pertencem à classe de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">responsabilidade normal</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> e 1 que pertence à </font><font style="vertical-align: inherit;">classe de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">alta responsabilidade</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Da mesma forma, posso calcular resultados semelhantes para outros valores de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Rating de crédito</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> da tabela de contingência. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Por exemplo, eu uso a tabela de contingência acima para calcular independentemente a entropia de nossa variável de destino e depois calcular sua entropia, levando em consideração informações adicionais do atributo </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Rating de Crédito</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Assim, posso calcular a quantidade de informações adicionais que o </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Rating de crédito</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> fornecerá </font><font style="vertical-align: inherit;">para a variável de meta de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">responsabilidade</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 Então vamos começar.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/2v/2y/lg/2v2ylghvtk-f-e0eom6aeocsb1q.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 A entropia da nossa variável alvo é 1, o que significa desorganização máxima devido à distribuição uniforme de elementos entre </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Normal"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> e </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Alto"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . O próximo passo é calcular a entropia da variável de destino do </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Passivo</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , levando em consideração informações adicionais do </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Rating de Crédito</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Para fazer isso, calculamos a entropia de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">responsabilidade</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> para cada valor do </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Rating de crédito</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> e os adicionamos usando a taxa de observação ponderada média para cada valor. O motivo pelo qual usamos a média ponderada ficará mais claro quando falamos sobre árvores de decisão. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/rt/_5/fh/rt_5fhldx4dfjcioh7d_uiori6e.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 Obtivemos a entropia da nossa variável alvo com o atributo </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Classificação de crédito</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Agora podemos calcular o ganho de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">responsabilidade</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> informacional </font><font style="vertical-align: inherit;">do </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Rating de crédito</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> para entender o quão informativo esse recurso é. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/9b/d8/xv/9bd8xviw3mxbiqj28zmdqbiaazs.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 O conhecimento </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">do rating de crédito</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> nos ajudou a reduzir a incerteza de nossa variável de meta de </font><i><font style="vertical-align: inherit;">responsabilidade</font></i><font style="vertical-align: inherit;"> .</font></font><i><font style="vertical-align: inherit;"></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Não é um bom sinal de que deve funcionar? Dê-nos informações sobre a variável de destino? Bem, por essa mesma razão, as árvores de decisão usam entropia e ganho informacional. Eles determinam por qual critério dividir os nós em ramificações, para abordar a variável de destino com cada partição subseqüente e também para entender quando a construção da árvore precisa ser concluída! (além de hiperparâmetros, como profundidade máxima, é claro). Vamos ver como tudo isso funciona no exemplo a seguir, usando árvores de decisão. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Exemplo: árvore de decisão</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Vejamos um exemplo de construção de uma árvore de decisão, com o objetivo de prever se o crédito de uma pessoa será baixado ou não. A população será de 30 cópias. 16 pertencerão à classe </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">de</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> baixa </font><font style="vertical-align: inherit;">e os outros 14</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Não amortizado"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Teremos dois sinais, a saber, </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Equilíbrio"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , que pode assumir dois valores: "&lt;50K" ou "&gt; 50K" e </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Residência"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , que aceita três valores: </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"PRÓPRIO"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"ALUGAR"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ou </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"OUTROS"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Demonstrarei como o algoritmo da árvore de decisão decidirá qual atributo será quebrado primeiro e qual será o mais informativo, ou seja, elimina melhor a incerteza da variável de destino usando o conceito de entropia e ganho de informação. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sintoma 1: Equilíbrio</font></font></b><br>
<br>
<img src="https://habrastorage.org/webt/9b/d8/xv/9bd8xviw3mxbiqj28zmdqbiaazs.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
 Aqui, os círculos pertencem à </font><font style="vertical-align: inherit;">classe </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"write-off"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> e as estrelas correspondem à classe </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"non-write-off"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Particionando uma raiz pai por atributo</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O saldo</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> nos dará 2 nós de herdeiro. No nó esquerdo, haverá 13 observações, sendo 12/13 (probabilidade 0,92) de observações da classe </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"write-off"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> e apenas 1/13 (probabilidade 0,08) de observações da classe </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"non-write-off"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . No nó direito, haverá 17 das 30 observações, sendo 13/17 (probabilidade 0,76) das observações da classe </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"write-off"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> e 4/17 (probabilidade 0,24) das observações da classe </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"non-write-off"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Vamos calcular a entropia da raiz e ver o quanto a árvore pode reduzir a incerteza usando uma partição baseada em </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Balance</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br>
<img src="https://habrastorage.org/webt/yq/ke/do/yqkedojc2s80__h-vqqcptzewai.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 Uma divisão com base no </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Saldo</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> fornecerá um ganho informacional de 0,37. Vamos contar o mesmo para o sinal de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">residência</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">e compare os resultados. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sintoma 2: Residência Ao</font></font></b><br>
<br>
<img src="https://habrastorage.org/webt/mx/tm/sd/mxtmsdt2hm0mamxkdxzqnb9v7mg.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
 dividir uma árvore com base no </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Residence</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , você terá três nós de herdeiro. O nó descendente esquerdo receberá 8 observações, onde 7/8 (probabilidade 0,88) de observações da classe </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">de</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> baixa </font><font style="vertical-align: inherit;">e apenas 1/8 (probabilidade 0,12) de observações da classe de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">não</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> baixa </font><font style="vertical-align: inherit;">. O nó médio sucessor receberá 10 observações, onde 4/10 (probabilidade 0,4) de observações da classe </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">de</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> baixa </font><font style="vertical-align: inherit;">e 6/10 (probabilidade 0,6) de observações da classe de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">não</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> baixa </font><font style="vertical-align: inherit;">. O herdeiro certo receberá 12 observações, sendo 5/12 (probabilidade 0,42) de observações da classe </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">de</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> baixa </font><font style="vertical-align: inherit;">e 7/12 (probabilidade 0,58) de observações da classe de </font><i><font style="vertical-align: inherit;">não</font></i><font style="vertical-align: inherit;"> baixa</font></font><i><font style="vertical-align: inherit;"></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Já conhecemos a entropia do nó pai, portanto, simplesmente calculamos a entropia após a partição para entender o ganho informacional do atributo </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Residence</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br>
<img src="https://habrastorage.org/webt/cb/zt/zf/cbztzffw12-wkj6cjfayt_jzlcq.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 O ganho informativo do atributo </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Balance</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> é </font><font style="vertical-align: inherit;">quase três vezes maior que o da </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Residence</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ! Se você olhar os gráficos novamente, verá que a partição de acordo com o </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Balance</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> fornecerá nós descendentes mais limpos do que de acordo com a </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Residence</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . No entanto, o nó mais à esquerda no </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Residence</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> também </font><i><font style="vertical-align: inherit;">é</font></i><font style="vertical-align: inherit;"> bastante limpo, mas é aqui que a média ponderada entra em jogo. Apesar de o nó estar limpo, possui o menor número de observações e seu resultado é perdido no recálculo geral e no cálculo da entropia total de acordo com </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Residence</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Isso é importante porque procuramos o conteúdo informativo geral do atributo e não queremos que o resultado final seja distorcido pelo valor raro do atributo. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O próprio atributo </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Balance</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> fornece mais informações sobre a variável de destino do que </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Residence</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Assim, a entropia da nossa variável alvo é reduzida. O algoritmo da árvore de decisão usa esse resultado para fazer a primeira divisão de acordo com </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Balance</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">para decidir posteriormente em que base quebrar os seguintes nós. No mundo real, quando existem mais de dois recursos, o primeiro detalhamento ocorre de acordo com o recurso mais informativo e, a cada detalhamento subsequente, o ganho de informações será recontado para cada recurso adicional, pois não será o mesmo que o ganho de informações de cada recurso individualmente. Entropia e ganho informacional devem ser calculados após uma ou várias partições, o que afetará o resultado final. A árvore de decisão repetirá esse processo à medida que cresce em profundidade, até atingir uma certa profundidade ou algum tipo de divisão levar a um ganho informacional mais alto além de um determinado limite, que também pode ser especificado como um hiperparâmetro!</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Isso é tudo! </font><font style="vertical-align: inherit;">Agora você sabe qual entropia, ganho de informação e como eles são calculados. </font><font style="vertical-align: inherit;">Agora você entende como a árvore de decisão, sozinha ou como parte de um conjunto, toma decisões sobre a melhor ordem de particionamento por atributos e decide quando parar ao aprender os dados disponíveis. </font><font style="vertical-align: inherit;">Bem, se você precisar explicar a alguém como as árvores de decisão funcionam, espero que você lide adequadamente com essa tarefa. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Espero que você tenha aprendido algo útil para si mesmo neste artigo. </font><font style="vertical-align: inherit;">Se eu perdi alguma coisa ou me expressei incorretamente, escreva-me sobre isso. </font><font style="vertical-align: inherit;">Ficarei muito grato a você! </font><font style="vertical-align: inherit;">Obrigado.</font></font><br>
<br>
<hr><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Saiba mais sobre o curso.</font></font></a><br>
<br>
<hr></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt502178/index.html">oVirt em 2 horas. Parte 3. Configurações avançadas</a></li>
<li><a href="../pt502180/index.html">O dia em que o perímetro desapareceu. Soluções de segurança da Microsoft e parceiros</a></li>
<li><a href="../pt502182/index.html">Novamente sobre o MikroTik ou o tão esperado SOCKS5</a></li>
<li><a href="../pt502186/index.html">Webinar. Segurança da informação: SOC em quarentena</a></li>
<li><a href="../pt502196/index.html">В подходе к математике столетней давности найдены новые ключи к разгадке природы времени</a></li>
<li><a href="../pt502202/index.html">O desenvolvimento de tecnologia não tripulada no transporte ferroviário</a></li>
<li><a href="../pt502204/index.html">Gravando testes @SpringBootTest ao usar o Spring Shell em um aplicativo</a></li>
<li><a href="../pt502206/index.html">Yandex registrou os sons dos retrocomputadores</a></li>
<li><a href="../pt502208/index.html">Extensão do Chrome para ocultar recomendações perturbadoras no YouTube</a></li>
<li><a href="../pt502234/index.html">Инсайды от сотрудника Facebook: как попасть на стажировку, получить оффер и все о работе в компании</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>