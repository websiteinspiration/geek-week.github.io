<!doctype html>
<html class="no-js" lang="es">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👩🏾‍🔬 🛀🏾 🐀 Aprendizaje reforzado a través de redes neuronales competitivas 🧜🏿 🦄 🏹</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="En el clásico juego "tic-tac-toe" existe la oportunidad de presentar todos los movimientos probables, y nunca perder. Aproveché esta oportunidad como ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Aprendizaje reforzado a través de redes neuronales competitivas</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/505574/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En el clásico juego "tic-tac-toe" existe la oportunidad de presentar todos los movimientos probables, y nunca perder. Aproveché esta oportunidad como medida de mi entrenamiento en la red neuronal del juego. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La capacitación reforzada será útil para tareas con una decisión ambigua, complicada por las muchas opciones para elegir una acción con diferentes resultados para cada una. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Por supuesto, el tic-tac-toe no parece un juego difícil para entrenarlos con refuerzos. Sin embargo, es adecuado para dominar la metodología de capacitación a través de redes competitivas, lo que mejorará la calidad y reducirá el tiempo dedicado a la capacitación de la red. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A continuación, describiré el algoritmo de aprendizaje general con refuerzo a través de redes competitivas en el contexto de un juego de tres en raya con una demostración de una red entrenada para hacer movimientos "significativos", es decir, para jugar.</font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Grabar un juego de una red entrenada. </font></font></a><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Entrenar la red desde cero. </font></font></a><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fuentes.</font></font></a><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
También puede ingresar un modelo previamente </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow"><font style="vertical-align: inherit;">entrenado</font></a><font style="vertical-align: inherit;"> desde GitHub haciendo clic en el botón correspondiente para comenzar inmediatamente a probar una red neuronal.</font></font><br>
<a name="habracut"></a><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Los primeros pasos en el entrenamiento de redes neuronales</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Además del hecho de que una neurona tiene una función de activación, que modifica la solución resultante de una red neuronal, también podemos decir que las neuronas son memoria de red. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Cada capa aumenta el tiempo de entrenamiento debido a la propagación del error inverso a través de las capas "hacia arriba", y la señal se desvanece gradualmente antes de llegar a las capas "superiores", que comienzan el camino de la toma de decisiones. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Después de varias opciones para la configuración de la red, llegué a la conclusión de que para un juego simple con un campo 3x3 sería suficiente usar una red de una sola capa con 128 neuronas. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La red no debe tener demasiada memoria, esto puede conducir a un nuevo entrenamiento: la memorización completa de todas las opciones para el resultado del juego. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La fuerza de las redes neuronales en la expresividad de aproximar una solución basada en datos de entrada en condiciones de memoria limitada.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Reglas generales para la promoción de agentes.</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para el pronóstico relativo de una red neuronal, cada celda tiene una recompensa dinámica dependiendo de su importancia para el agente en este momento. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En la salida, la red neuronal predice el índice de la célula donde irá el agente. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Las recompensas de las celdas tienen la siguiente distribución: </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
cuantos menos movimientos se realicen, mayor </font><font style="vertical-align: inherit;">
será </font><font style="vertical-align: inherit;">la recompensa de 0.1 a 1.0 Una </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
celda ocupada tiene una recompensa de -1.0 Una </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">celda perdedora recibirá una recompensa de -0.4 Una </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
celda libre tiene una recompensa de 0.1 Un </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
movimiento a una celda libre aumenta su recompensa a 0.2 La </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
celda ganadora de un oponente tiene una recompensa de 0.5 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El tragamonedas ganador traerá una recompensa de 1.0</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Entrenamiento competitivo de redes neuronales</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En la competencia, los agentes serán entrenados en un entorno competitivo, lo que conducirá a nuevos resultados del juego y mejorará la calidad del entrenamiento para nuevas situaciones. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Cada agente tiene su propio campo para entrenar para ir a una celda libre y construir combinaciones ganadoras de movimientos. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Los agentes juegan 9 juegos en casa, luego pasan al campo competitivo por 1 juego, donde se juega hasta que el ganador tiene un límite de 9 movimientos, luego todo se repite nuevamente. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Al final de cada juego, ambas redes se entrenan en una nueva experiencia de rivalidad en un campo de juego común.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Prevención del oponente</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La red necesita ser entrenada para competir por la victoria en el campo, es decir. </font><font style="vertical-align: inherit;">recompensa por la prevención exitosa de ganar un oponente al aumentar la recompensa celular. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Otra métrica para entrenar redes neuronales son los indicadores de victoria en las competiciones. </font><font style="vertical-align: inherit;">Si el margen de victoria para un jugador es demasiado grande, entonces la red probablemente esté aprendiendo incorrectamente, y la razón de esto son las recompensas incorrectas por las acciones de los agentes, o cualquier otra acción y sus recompensas no se han tenido en cuenta. </font><font style="vertical-align: inherit;">El mejor resultado del entrenamiento puede considerarse una situación en la que las redes serán casi iguales, ganando y perdiendo aproximadamente la misma cantidad de veces.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aprendizaje competitivo con un hombre</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La implementación del entrenamiento de una red neuronal para jugar con humanos no es muy diferente de la competencia entre agentes. </font><font style="vertical-align: inherit;">La única diferencia seria es que la persona inicialmente juega razonablemente. </font><font style="vertical-align: inherit;">Una parte con tal oponente crea situaciones adicionales para el agente, lo que afectará favorablemente su experiencia de juego y, en consecuencia, su entrenamiento.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Terminación</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La red neuronal aprendió a jugar al tic-tac-toe solo después de la introducción de un algoritmo competitivo, que le permitió aprender cómo hacer movimientos en respuesta a los movimientos del oponente, aunque no perfectamente, como se planeó originalmente. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En general, creo que el proyecto se ha completado con éxito: se ha alcanzado el objetivo.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">¡Gracias por la atención!</font></font></h2><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ps Entrena redes competitivas, esto te permite mirar juegos simples desde un ángulo diferente.</font></font></i></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es505554/index.html">En el concurso "Jóvenes técnicos e inventores" no se necesitan verdaderos inventores jóvenes</a></li>
<li><a href="../es505556/index.html">Apple rastrea iPhones saqueados y le da a la policía saqueadores</a></li>
<li><a href="../es505558/index.html">Cámara de aprendizaje profundo Amazon DeepLens. Desempaquetar, conectar e implementar un proyecto</a></li>
<li><a href="../es505560/index.html">El segundo set para un programa de gestión de productos en el centro CS: lo que dicen los estudiantes</a></li>
<li><a href="../es505568/index.html">Transferencia de archivos usando tuberías y otras pequeñas cosas en Delphi</a></li>
<li><a href="../fr486176/index.html">Mémo de correspondance par e-mail d'entreprise</a></li>
<li><a href="../fr486178/index.html">FOSS News No. 1 - revue des nouvelles gratuites et open source du 27 janvier au 2 février 2020</a></li>
<li><a href="../fr486180/index.html">Conseils et sources pour créer des applications sans serveur</a></li>
<li><a href="../fr486184/index.html">Comment utiliser efficacement la recherche</a></li>
<li><a href="../fr486186/index.html">Nuage catastrophique: comment cela fonctionne</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>