<!doctype html>
<html class="no-js" lang="es">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèæ‚Äçüî¨ üõÄüèæ üêÄ Aprendizaje reforzado a trav√©s de redes neuronales competitivas üßúüèø ü¶Ñ üèπ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="En el cl√°sico juego "tic-tac-toe" existe la oportunidad de presentar todos los movimientos probables, y nunca perder. Aprovech√© esta oportunidad como ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Aprendizaje reforzado a trav√©s de redes neuronales competitivas</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/505574/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En el cl√°sico juego "tic-tac-toe" existe la oportunidad de presentar todos los movimientos probables, y nunca perder. Aprovech√© esta oportunidad como medida de mi entrenamiento en la red neuronal del juego. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La capacitaci√≥n reforzada ser√° √∫til para tareas con una decisi√≥n ambigua, complicada por las muchas opciones para elegir una acci√≥n con diferentes resultados para cada una. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Por supuesto, el tic-tac-toe no parece un juego dif√≠cil para entrenarlos con refuerzos. Sin embargo, es adecuado para dominar la metodolog√≠a de capacitaci√≥n a trav√©s de redes competitivas, lo que mejorar√° la calidad y reducir√° el tiempo dedicado a la capacitaci√≥n de la red. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A continuaci√≥n, describir√© el algoritmo de aprendizaje general con refuerzo a trav√©s de redes competitivas en el contexto de un juego de tres en raya con una demostraci√≥n de una red entrenada para hacer movimientos "significativos", es decir, para jugar.</font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Grabar un juego de una red entrenada. </font></font></a><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Entrenar la red desde cero. </font></font></a><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fuentes.</font></font></a><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Tambi√©n puede ingresar un modelo previamente </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow"><font style="vertical-align: inherit;">entrenado</font></a><font style="vertical-align: inherit;"> desde GitHub haciendo clic en el bot√≥n correspondiente para comenzar inmediatamente a probar una red neuronal.</font></font><br>
<a name="habracut"></a><br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Los primeros pasos en el entrenamiento de redes neuronales</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Adem√°s del hecho de que una neurona tiene una funci√≥n de activaci√≥n, que modifica la soluci√≥n resultante de una red neuronal, tambi√©n podemos decir que las neuronas son memoria de red. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Cada capa aumenta el tiempo de entrenamiento debido a la propagaci√≥n del error inverso a trav√©s de las capas "hacia arriba", y la se√±al se desvanece gradualmente antes de llegar a las capas "superiores", que comienzan el camino de la toma de decisiones. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Despu√©s de varias opciones para la configuraci√≥n de la red, llegu√© a la conclusi√≥n de que para un juego simple con un campo 3x3 ser√≠a suficiente usar una red de una sola capa con 128 neuronas. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La red no debe tener demasiada memoria, esto puede conducir a un nuevo entrenamiento: la memorizaci√≥n completa de todas las opciones para el resultado del juego. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La fuerza de las redes neuronales en la expresividad de aproximar una soluci√≥n basada en datos de entrada en condiciones de memoria limitada.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Reglas generales para la promoci√≥n de agentes.</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para el pron√≥stico relativo de una red neuronal, cada celda tiene una recompensa din√°mica dependiendo de su importancia para el agente en este momento. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En la salida, la red neuronal predice el √≠ndice de la c√©lula donde ir√° el agente. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Las recompensas de las celdas tienen la siguiente distribuci√≥n: </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
cuantos menos movimientos se realicen, mayor </font><font style="vertical-align: inherit;">
ser√° </font><font style="vertical-align: inherit;">la recompensa de 0.1 a 1.0 Una </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
celda ocupada tiene una recompensa de -1.0 Una </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">celda perdedora recibir√° una recompensa de -0.4 Una </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
celda libre tiene una recompensa de 0.1 Un </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
movimiento a una celda libre aumenta su recompensa a 0.2 La </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
celda ganadora de un oponente tiene una recompensa de 0.5 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El tragamonedas ganador traer√° una recompensa de 1.0</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Entrenamiento competitivo de redes neuronales</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En la competencia, los agentes ser√°n entrenados en un entorno competitivo, lo que conducir√° a nuevos resultados del juego y mejorar√° la calidad del entrenamiento para nuevas situaciones. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Cada agente tiene su propio campo para entrenar para ir a una celda libre y construir combinaciones ganadoras de movimientos. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Los agentes juegan 9 juegos en casa, luego pasan al campo competitivo por 1 juego, donde se juega hasta que el ganador tiene un l√≠mite de 9 movimientos, luego todo se repite nuevamente. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Al final de cada juego, ambas redes se entrenan en una nueva experiencia de rivalidad en un campo de juego com√∫n.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Prevenci√≥n del oponente</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La red necesita ser entrenada para competir por la victoria en el campo, es decir. </font><font style="vertical-align: inherit;">recompensa por la prevenci√≥n exitosa de ganar un oponente al aumentar la recompensa celular. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Otra m√©trica para entrenar redes neuronales son los indicadores de victoria en las competiciones. </font><font style="vertical-align: inherit;">Si el margen de victoria para un jugador es demasiado grande, entonces la red probablemente est√© aprendiendo incorrectamente, y la raz√≥n de esto son las recompensas incorrectas por las acciones de los agentes, o cualquier otra acci√≥n y sus recompensas no se han tenido en cuenta. </font><font style="vertical-align: inherit;">El mejor resultado del entrenamiento puede considerarse una situaci√≥n en la que las redes ser√°n casi iguales, ganando y perdiendo aproximadamente la misma cantidad de veces.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aprendizaje competitivo con un hombre</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La implementaci√≥n del entrenamiento de una red neuronal para jugar con humanos no es muy diferente de la competencia entre agentes. </font><font style="vertical-align: inherit;">La √∫nica diferencia seria es que la persona inicialmente juega razonablemente. </font><font style="vertical-align: inherit;">Una parte con tal oponente crea situaciones adicionales para el agente, lo que afectar√° favorablemente su experiencia de juego y, en consecuencia, su entrenamiento.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Terminaci√≥n</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La red neuronal aprendi√≥ a jugar al tic-tac-toe solo despu√©s de la introducci√≥n de un algoritmo competitivo, que le permiti√≥ aprender c√≥mo hacer movimientos en respuesta a los movimientos del oponente, aunque no perfectamente, como se plane√≥ originalmente. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En general, creo que el proyecto se ha completado con √©xito: se ha alcanzado el objetivo.</font></font><br>
<br>
<h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">¬°Gracias por la atenci√≥n!</font></font></h2><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ps Entrena redes competitivas, esto te permite mirar juegos simples desde un √°ngulo diferente.</font></font></i></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es505554/index.html">En el concurso "J√≥venes t√©cnicos e inventores" no se necesitan verdaderos inventores j√≥venes</a></li>
<li><a href="../es505556/index.html">Apple rastrea iPhones saqueados y le da a la polic√≠a saqueadores</a></li>
<li><a href="../es505558/index.html">C√°mara de aprendizaje profundo Amazon DeepLens. Desempaquetar, conectar e implementar un proyecto</a></li>
<li><a href="../es505560/index.html">El segundo set para un programa de gesti√≥n de productos en el centro CS: lo que dicen los estudiantes</a></li>
<li><a href="../es505568/index.html">Transferencia de archivos usando tuber√≠as y otras peque√±as cosas en Delphi</a></li>
<li><a href="../fr486176/index.html">M√©mo de correspondance par e-mail d'entreprise</a></li>
<li><a href="../fr486178/index.html">FOSS News No. 1 - revue des nouvelles gratuites et open source du 27 janvier au 2 f√©vrier 2020</a></li>
<li><a href="../fr486180/index.html">Conseils et sources pour cr√©er des applications sans serveur</a></li>
<li><a href="../fr486184/index.html">Comment utiliser efficacement la recherche</a></li>
<li><a href="../fr486186/index.html">Nuage catastrophique: comment cela fonctionne</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>