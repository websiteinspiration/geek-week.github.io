<!doctype html>
<html class="no-js" lang="es">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>游뎵 游둤 游 Entrop칤a: c칩mo los 치rboles de decisi칩n toman decisiones 游游낗 游땧 游댅</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Se prepar칩 una traducci칩n del art칤culo antes del inicio del curso de Machine Learning .
 
 
 
 Usted es un especialista en ciencia de datos que actual...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Entrop칤a: c칩mo los 치rboles de decisi칩n toman decisiones</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/502200/"><i><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Se prepar칩 una traducci칩n del art칤culo antes del inicio del curso de </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Machine Learning</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font></b></i><br>
<br>
<img src="https://habrastorage.org/webt/az/2h/3e/az2h3eq1jejcxtd0g4wi4gmamki.png"><br>
<hr><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Usted es un especialista en ciencia de datos que actualmente sigue un camino de aprendizaje. Y has recorrido un largo camino desde que escribiste tu primera l칤nea de c칩digo en Python o R. Conoces Scikit-Learn como el dorso de tu mano. Ahora est치s m치s sentado en Kaggle que en Facebook. No es nuevo en la creaci칩n de impresionantes bosques aleatorios y otros modelos de 치rboles de decisi칩n que hacen un excelente trabajo. Sin embargo, sabe que no lograr치 nada si no se desarrolla de manera integral. Desea profundizar y comprender las complejidades y conceptos que subyacen a los populares modelos de aprendizaje autom치tico. Bueno, yo tambi칠n.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Hoy hablar칠 sobre el concepto de entrop칤a, uno de los temas m치s importantes en estad칤stica, y luego hablaremos sobre el concepto de Ganancia de informaci칩n (ganancia de informaci칩n) y descubriremos por qu칠 estos conceptos fundamentales forman la base de c칩mo se construyen los 치rboles de decisi칩n a partir de los datos obtenidos.</font></font><a name="habracut"></a><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bueno. Ahora transgiremos. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
쯈u칠 es la entrop칤a? En t칠rminos simples, la entrop칤a no es m치s que una medida de desorden. (Tambi칠n se puede considerar una medida de pureza, y pronto ver치 por qu칠. Pero me gusta m치s el desorden porque suena m치s fresco.) La </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
f칩rmula matem치tica de la entrop칤a es la siguiente: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ey/wa/u-/eywau-ntm5stedcuyrelbhhoipu.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Entrop칤a. A veces se escribe como H.</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Aqu칤 p </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">i</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> es la probabilidad de frecuencia de un elemento / clase i de nuestros datos. Por simplicidad, supongamos que tenemos solo dos clases: positiva y negativa. Entonces tomar칠 el valor de "+" o "-". Si tuvi칠ramos un total de 100 puntos en nuestro conjunto de datos, 30 de los cuales pertenecer칤an a la clase positiva y 70 a la negativa, entonces p </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ser칤a 3/10, y p</font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">-</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ser치 el 7/10. Todo es simple aqu칤. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Si calculo la entrop칤a de las clases a partir de este ejemplo, esto es lo que obtengo usando la f칩rmula anterior: la </font></font><br>
<br>
<img src="https://habrastorage.org/webt/5_/hh/20/5_hh20bihmp119n_5vzmlq_vuyw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
entrop칤a es aproximadamente 0,88. Este valor se considera bastante alto, es decir, tenemos un alto nivel de entrop칤a o trastorno (es decir, un bajo valor de pureza). La entrop칤a se mide en el rango de 0 a 1. Dependiendo del n칰mero de clases en su conjunto de datos, el valor de la entrop칤a puede ser mayor que 1, pero significar치 lo mismo ya que el nivel de trastorno es extremadamente alto. Para simplificar la explicaci칩n, en el art칤culo de hoy tendremos una entrop칤a que va de 0 a 1. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Eche un vistazo a la tabla a continuaci칩n.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/jt/sx/zs/jtsxzsfwwbstp10fqo-rd0ndddw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En el eje X, se refleja el n칰mero de puntos de la clase positiva en cada c칤rculo, y en el eje Y, las entrop칤as correspondientes. Puede notar de inmediato la forma de U invertida del gr치fico. La entrop칤a ser치 la m치s peque침a en los extremos cuando no haya elementos positivos en el c칤rculo en principio, o cuando solo haya elementos positivos en ellos. Es decir, cuando hay elementos id칠nticos en un c칤rculo, el trastorno ser치 0. La entrop칤a ser치 m치s alta en el medio del gr치fico, donde los elementos positivos y negativos se distribuir치n uniformemente dentro del c칤rculo. Aqu칤 se lograr치 la mayor entrop칤a o desorden, ya que no habr치 elementos predominantes.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
쮿ay alguna raz칩n por la que la entrop칤a se mide usando el logaritmo de base 2, o por qu칠 la entrop칤a se mide entre 0 y 1, y no en un rango diferente? No, no hay raz칩n. Esto es solo una m칠trica. No es tan importante entender por qu칠 sucede esto. Es importante saber c칩mo se calcula lo que obtuvimos arriba y c칩mo funciona. La entrop칤a es una medida de confusi칩n o incertidumbre, y el objetivo de los modelos de aprendizaje autom치tico y especialistas en ciencia de datos en general es reducir esta incertidumbre. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ahora sabemos c칩mo se mide el desorden. A continuaci칩n, necesitamos un valor para medir la reducci칩n de este trastorno en la informaci칩n adicional (atributos / variables independientes) de la variable / clase objetivo. Aqu칤 es donde entra en juego la ganancia de informaci칩n o la ganancia de informaci칩n. Desde el punto de vista de las matem치ticas, se puede escribir de la siguiente manera:</font></font><br>
<br>
<img src="https://habrastorage.org/webt/bn/el/t4/bnelt40yxay8hkbanig088mpk6a.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 Simplemente restamos la entrop칤a Y de X de la entrop칤a Y para calcular la disminuci칩n de la incertidumbre sobre Y, siempre que X est칠 disponible sobre Y. Cuanto m치s fuerte disminuya la incertidumbre, m치s informaci칩n se puede obtener de Y sobre X. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Veamos un ejemplo simple de tabla de contingencia para que Ac칠rquese a la cuesti칩n de c칩mo los 치rboles de decisi칩n usan la entrop칤a y la ganancia de informaci칩n para decidir sobre qu칠 base romper los nodos en el proceso de aprendizaje de los datos. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ejemplo: tabla de conjugaci칩n</font></font></b><br>
<br>
<img src="https://habrastorage.org/webt/s4/ea/e5/s4eae57mpuehp3mk_mjpmikuan0.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
 Aqu칤, nuestra variable objetivo ser치 </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Responsabilidad</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , que solo puede tomar dos valores: </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Normal"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Alto"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Tambi칠n tenemos un solo signo, que se llama Calificaci칩n crediticia, distribuye los valores en tres categor칤as: </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Excelente"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Bueno"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Malo"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Se hicieron un total de 14 observaciones. 7 de ellos pertenecen a la clase de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Responsabilidad Normal</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , y 7 m치s a la clase de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Alta Responsabilidad</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Esta es una divisi칩n en s칤 misma. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Si observamos la suma total de los valores en la primera fila, veremos que tenemos 4 observaciones con un valor </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">excelente</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> basado en la </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">calificaci칩n crediticia</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Adem치s, incluso puedo decir que mi variable objetivo est치 rota por la </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">calificaci칩n crediticia "Excelente"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Entre las observaciones con el valor </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Excelente"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> por atributo</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Calificaci칩n crediticia</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , hay 3 que pertenecen a la clase de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Responsabilidad Normal</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y 1 que pertenece a la </font><font style="vertical-align: inherit;">clase de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Alta Responsabilidad</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Del mismo modo, puedo calcular resultados similares para otros valores de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Calificaci칩n crediticia</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> de la tabla de contingencia. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Por ejemplo, utilizo la tabla de contingencia anterior para calcular de forma independiente la entrop칤a de nuestra variable objetivo y luego calcular su entrop칤a, teniendo en cuenta informaci칩n adicional del atributo de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">calificaci칩n crediticia</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Entonces puedo calcular cu치nta informaci칩n adicional me dar치 la </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">calificaci칩n crediticia</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> para la variable objetivo de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">responsabilidad</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 Entonces empecemos.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/2v/2y/lg/2v2ylghvtk-f-e0eom6aeocsb1q.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 La entrop칤a de nuestra variable objetivo es 1, lo que significa un desorden m치ximo debido a la distribuci칩n uniforme de los elementos entre </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Normal"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Alto"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . El siguiente paso es calcular la entrop칤a de la variable objetivo del </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">pasivo</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , teniendo en cuenta informaci칩n adicional de la </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">calificaci칩n crediticia</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Para hacer esto, calculamos la entrop칤a del </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">pasivo</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> para cada valor de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">calificaci칩n crediticia</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y los sumamos usando el 칤ndice de observaci칩n promedio ponderado para cada valor. Por qu칠 usamos el promedio ponderado ser치 m치s claro cuando hablamos de 치rboles de decisi칩n. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/rt/_5/fh/rt_5fhldx4dfjcioh7d_uiori6e.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 Obtuvimos la entrop칤a de nuestra variable objetivo con el atributo </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Calificaci칩n crediticia</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Ahora podemos calcular la ganancia de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">responsabilidad</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> informativa </font><font style="vertical-align: inherit;">de la </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">calificaci칩n crediticia</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> para comprender cu치n informativa es esta caracter칤stica. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/9b/d8/xv/9bd8xviw3mxbiqj28zmdqbiaazs.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 Conocer la </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">calificaci칩n crediticia</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> nos </font><i><font style="vertical-align: inherit;">ha</font></i><font style="vertical-align: inherit;"> ayudado a reducir la incertidumbre de nuestra variable objetivo de </font><i><font style="vertical-align: inherit;">responsabilidad</font></i><font style="vertical-align: inherit;"> .</font></font><i><font style="vertical-align: inherit;"></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. 쯅o es una buena se침al que deber칤a funcionar? Danos informaci칩n sobre la variable objetivo? Bueno, por esta misma raz칩n, los 치rboles de decisi칩n usan entrop칤a y ganancia informativa. 춰Determinan seg칰n qu칠 criterio dividir los nodos en ramas, para acercarse a la variable objetivo con cada partici칩n subsiguiente, y tambi칠n para comprender cu치ndo debe completarse la construcci칩n del 치rbol! (adem치s de hiperpar치metros como la profundidad m치xima, por supuesto). Veamos c칩mo funciona todo esto en el siguiente ejemplo usando 치rboles de decisi칩n. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ejemplo: 츼rbol de decisi칩n</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Veamos un ejemplo de c칩mo construir un 치rbol de decisi칩n para predecir si el cr칠dito de una persona se cancelar치 o no. La poblaci칩n ser치 de 30 copias. 16 pertenecer치n a la clase de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">amortizaci칩n</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , y los otros 14 ser치n</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"No amortizaci칩n"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Tendremos dos signos, a saber, </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Balance"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , que puede tomar dos valores: "&lt;50K" o "&gt; 50K", y </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Residencia"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , que toma tres valores: </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"PROPIO"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"ALQUILER"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> u </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"OTRO"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Demostrar칠 c칩mo el algoritmo del 치rbol de decisi칩n decidir치 qu칠 atributo romper primero y qu칠 atributo ser치 m치s informativo, es decir, elimina mejor la incertidumbre de la variable objetivo utilizando el concepto de entrop칤a y ganancia de informaci칩n. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">S칤ntoma 1: Equilibrio</font></font></b><br>
<br>
<img src="https://habrastorage.org/webt/9b/d8/xv/9bd8xviw3mxbiqj28zmdqbiaazs.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
 Aqu칤 los c칤rculos pertenecen a la </font><font style="vertical-align: inherit;">clase de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"cancelaci칩n"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , y las estrellas corresponden a la clase de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"no cancelaci칩n"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Particionar una ra칤z principal por atributo</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El equilibrio</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> nos dar치 2 nodos herederos. En el nodo izquierdo habr치 13 observaciones, donde 12/13 (probabilidad 0,92) de observaciones de la clase de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"cancelaci칩n"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y solo 1/13 (probabilidad de 0,08) de observaciones de la clase </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"no cancelaci칩n"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . En el nodo derecho habr치 17 de 30 observaciones, donde 13/17 (probabilidad 0.76) de observaciones de la clase de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"cancelaci칩n"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y 4/17 (probabilidad 0.24) de observaciones de la clase de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"no cancelaci칩n"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Calculemos la entrop칤a de la ra칤z y veamos cu치nto puede reducir la incertidumbre el 치rbol utilizando una partici칩n basada en </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Balance</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br>
<img src="https://habrastorage.org/webt/yq/ke/do/yqkedojc2s80__h-vqqcptzewai.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 Una divisi칩n basada en </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Balance</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> dar치 una ganancia informativa de 0.37. Vamos a contar lo mismo para el signo de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">residencia</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">y compara los resultados. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">S칤ntoma 2: Residencia La</font></font></b><br>
<br>
<img src="https://habrastorage.org/webt/mx/tm/sd/mxtmsdt2hm0mamxkdxzqnb9v7mg.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
 divisi칩n de un 치rbol basado en </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Residencia</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> le dar치 3 nodos herederos. El nodo descendente izquierdo recibir치 8 observaciones, donde 7/8 (probabilidad 0,88) de observaciones de la clase de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">cancelaci칩n</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y solo 1/8 (probabilidad 0,12) de observaciones de la clase de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">no cancelaci칩n</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . El nodo sucesor promedio recibir치 10 observaciones, donde 4/10 (probabilidad 0.4) de observaciones de la clase de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">cancelaci칩n</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y 6/10 (probabilidad 0.6) de observaciones de la clase de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">no cancelaci칩n</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . El heredero correcto recibir치 12 observaciones, donde 5/12 (probabilidad 0,42) de observaciones de la clase de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">cancelaci칩n</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y 7/12 (probabilidad 0,58) de observaciones de la clase de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">no cancelaci칩n</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Ya conocemos la entrop칤a del nodo padre, por lo que simplemente calculamos la entrop칤a despu칠s de la partici칩n para comprender la ganancia informativa del atributo </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Residencia</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br>
<img src="https://habrastorage.org/webt/cb/zt/zf/cbztzffw12-wkj6cjfayt_jzlcq.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 춰La ganancia informativa del atributo </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Balance</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> es </font><font style="vertical-align: inherit;">casi 3 veces m치s que la de la </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Residencia</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ! Si observa nuevamente los gr치ficos, ver치 que la partici칩n de acuerdo con </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Balance</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> dar치 nodos descendientes m치s limpios que de acuerdo con </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Residence</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Sin embargo, el nodo m치s a la izquierda en </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Residencia</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> tambi칠n </font><i><font style="vertical-align: inherit;">est치</font></i><font style="vertical-align: inherit;"> bastante limpio, pero es aqu칤 donde entra en juego el promedio ponderado. A pesar de que el nodo est치 limpio, tiene el menor n칰mero de observaciones, y su resultado se pierde en el rec치lculo general y el c치lculo de la entrop칤a total seg칰n la </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Residencia</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Esto es importante porque estamos buscando el contenido informativo general del atributo y no queremos que el resultado final sea distorsionado por el raro valor del atributo. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El atributo </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Balance en</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> s칤 mismo </font><font style="vertical-align: inherit;">proporciona m치s informaci칩n sobre la variable objetivo que </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Residencia</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Por lo tanto, la entrop칤a de nuestra variable objetivo se reduce. El algoritmo del 치rbol de decisi칩n utiliza este resultado para realizar la primera divisi칩n de acuerdo con </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Balance</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">para luego decidir sobre qu칠 base romper los siguientes nodos. En el mundo real, cuando hay m치s de dos caracter칤sticas, el primer desglose ocurre de acuerdo con la caracter칤stica m치s informativa, y luego, con cada ruptura posterior, la ganancia de informaci칩n se contar치 para cada caracter칤stica adicional, ya que no ser치 lo mismo que la ganancia de informaci칩n de cada caracter칤stica individualmente. La entrop칤a y la ganancia informativa deben calcularse despu칠s de que se hayan producido una o varias particiones, lo que afectar치 el resultado final. 춰El 치rbol de decisi칩n repetir치 este proceso a medida que crezca en profundidad, hasta que alcance una cierta profundidad o alg칰n tipo de divisi칩n conduzca a una mayor ganancia de informaci칩n sobre un cierto umbral, que tambi칠n se puede especificar como un hiperpar치metro!</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
춰Eso es todo! </font><font style="vertical-align: inherit;">Ahora ya sabe qu칠 entrop칤a, ganancia de informaci칩n y c칩mo se calculan. </font><font style="vertical-align: inherit;">Ahora comprende c칩mo el 치rbol de decisi칩n, por s칤 mismo o como parte de un conjunto, toma decisiones sobre el mejor orden de partici칩n por atributos y decide cu치ndo detenerse al aprender los datos disponibles. </font><font style="vertical-align: inherit;">Bueno, si tiene que explicarle a alguien c칩mo funcionan los 치rboles de decisi칩n, espero que pueda hacer frente adecuadamente a esta tarea. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Espero que hayas aprendido algo 칰til para ti de este art칤culo. </font><font style="vertical-align: inherit;">Si me perd칤 algo o me expres칠 de manera incorrecta, escr칤beme al respecto. </font><font style="vertical-align: inherit;">춰Te estar칠 muy agradecido! </font><font style="vertical-align: inherit;">Gracias.</font></font><br>
<br>
<hr><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aprende m치s sobre el curso.</font></font></a><br>
<br>
<hr></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es502178/index.html">oVirt en 2 horas. Parte 3. Configuraciones avanzadas</a></li>
<li><a href="../es502180/index.html">El d칤a que desapareci칩 el per칤metro. Soluciones de seguridad de Microsoft y socios</a></li>
<li><a href="../es502182/index.html">De nuevo sobre MikroTik o el tan esperado SOCKS5</a></li>
<li><a href="../es502186/index.html">Seminario web Seguridad de la informaci칩n: SOC en cuarentena</a></li>
<li><a href="../es502196/index.html"> 쮏얨쮏얧 햨 햪햟혝햣햪햟혝햦햨햣 혜혝쮏햣혝햫햣햧 햢햟쒫쫧쮐혝햦 햫햟햧햢햣햫혦 햫쮏쒬햣 햨햩혩혢햦 햨 햟향햡햟햢햨햣 햦쮏얨 쒬햣햪햣햫햦</a></li>
<li><a href="../es502202/index.html">El desarrollo de tecnolog칤a no tripulada en el transporte ferroviario.</a></li>
<li><a href="../es502204/index.html">Escribir pruebas @SpringBootTest cuando se usa Spring Shell en una aplicaci칩n</a></li>
<li><a href="../es502206/index.html">Yandex grab칩 los sonidos de las retrocomputadoras</a></li>
<li><a href="../es502208/index.html">Extensi칩n de Chrome para ocultar recomendaciones que distraen en YouTube</a></li>
<li><a href="../es502234/index.html">햊햫혜햟햧햢혦 쮐 혜쮐혞햢햫햦햨햟 Facebook: 햨햟햨 쮏햟혜혝혧 햫햟 혜혝햟햤햦쮏쒫쥄, 쮏혞혢햦혝혧 쮐혟햣 햦 쒬햣  햟햠쮐햣  햨쮏쨿햟햫햦햦</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>