<!doctype html>
<html class="no-js" lang="es">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🕎 🖖 📲 Entropía: cómo los árboles de decisión toman decisiones 🛀🏽 😝 🔆</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Se preparó una traducción del artículo antes del inicio del curso de Machine Learning .
 
 
 
 Usted es un especialista en ciencia de datos que actual...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Entropía: cómo los árboles de decisión toman decisiones</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/502200/"><i><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Se preparó una traducción del artículo antes del inicio del curso de </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Machine Learning</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font></b></i><br>
<br>
<img src="https://habrastorage.org/webt/az/2h/3e/az2h3eq1jejcxtd0g4wi4gmamki.png"><br>
<hr><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Usted es un especialista en ciencia de datos que actualmente sigue un camino de aprendizaje. Y has recorrido un largo camino desde que escribiste tu primera línea de código en Python o R. Conoces Scikit-Learn como el dorso de tu mano. Ahora estás más sentado en Kaggle que en Facebook. No es nuevo en la creación de impresionantes bosques aleatorios y otros modelos de árboles de decisión que hacen un excelente trabajo. Sin embargo, sabe que no logrará nada si no se desarrolla de manera integral. Desea profundizar y comprender las complejidades y conceptos que subyacen a los populares modelos de aprendizaje automático. Bueno, yo también.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Hoy hablaré sobre el concepto de entropía, uno de los temas más importantes en estadística, y luego hablaremos sobre el concepto de Ganancia de información (ganancia de información) y descubriremos por qué estos conceptos fundamentales forman la base de cómo se construyen los árboles de decisión a partir de los datos obtenidos.</font></font><a name="habracut"></a><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bueno. Ahora transgiremos. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
¿Qué es la entropía? En términos simples, la entropía no es más que una medida de desorden. (También se puede considerar una medida de pureza, y pronto verá por qué. Pero me gusta más el desorden porque suena más fresco.) La </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
fórmula matemática de la entropía es la siguiente: </font></font><br>
<br>
<img src="https://habrastorage.org/webt/ey/wa/u-/eywau-ntm5stedcuyrelbhhoipu.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Entropía. A veces se escribe como H.</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Aquí p </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">i</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> es la probabilidad de frecuencia de un elemento / clase i de nuestros datos. Por simplicidad, supongamos que tenemos solo dos clases: positiva y negativa. Entonces tomaré el valor de "+" o "-". Si tuviéramos un total de 100 puntos en nuestro conjunto de datos, 30 de los cuales pertenecerían a la clase positiva y 70 a la negativa, entonces p </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> sería 3/10, y p</font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">-</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> será el 7/10. Todo es simple aquí. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Si calculo la entropía de las clases a partir de este ejemplo, esto es lo que obtengo usando la fórmula anterior: la </font></font><br>
<br>
<img src="https://habrastorage.org/webt/5_/hh/20/5_hh20bihmp119n_5vzmlq_vuyw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
entropía es aproximadamente 0,88. Este valor se considera bastante alto, es decir, tenemos un alto nivel de entropía o trastorno (es decir, un bajo valor de pureza). La entropía se mide en el rango de 0 a 1. Dependiendo del número de clases en su conjunto de datos, el valor de la entropía puede ser mayor que 1, pero significará lo mismo ya que el nivel de trastorno es extremadamente alto. Para simplificar la explicación, en el artículo de hoy tendremos una entropía que va de 0 a 1. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Eche un vistazo a la tabla a continuación.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/jt/sx/zs/jtsxzsfwwbstp10fqo-rd0ndddw.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En el eje X, se refleja el número de puntos de la clase positiva en cada círculo, y en el eje Y, las entropías correspondientes. Puede notar de inmediato la forma de U invertida del gráfico. La entropía será la más pequeña en los extremos cuando no haya elementos positivos en el círculo en principio, o cuando solo haya elementos positivos en ellos. Es decir, cuando hay elementos idénticos en un círculo, el trastorno será 0. La entropía será más alta en el medio del gráfico, donde los elementos positivos y negativos se distribuirán uniformemente dentro del círculo. Aquí se logrará la mayor entropía o desorden, ya que no habrá elementos predominantes.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
¿Hay alguna razón por la que la entropía se mide usando el logaritmo de base 2, o por qué la entropía se mide entre 0 y 1, y no en un rango diferente? No, no hay razón. Esto es solo una métrica. No es tan importante entender por qué sucede esto. Es importante saber cómo se calcula lo que obtuvimos arriba y cómo funciona. La entropía es una medida de confusión o incertidumbre, y el objetivo de los modelos de aprendizaje automático y especialistas en ciencia de datos en general es reducir esta incertidumbre. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ahora sabemos cómo se mide el desorden. A continuación, necesitamos un valor para medir la reducción de este trastorno en la información adicional (atributos / variables independientes) de la variable / clase objetivo. Aquí es donde entra en juego la ganancia de información o la ganancia de información. Desde el punto de vista de las matemáticas, se puede escribir de la siguiente manera:</font></font><br>
<br>
<img src="https://habrastorage.org/webt/bn/el/t4/bnelt40yxay8hkbanig088mpk6a.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 Simplemente restamos la entropía Y de X de la entropía Y para calcular la disminución de la incertidumbre sobre Y, siempre que X esté disponible sobre Y. Cuanto más fuerte disminuya la incertidumbre, más información se puede obtener de Y sobre X. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Veamos un ejemplo simple de tabla de contingencia para que Acérquese a la cuestión de cómo los árboles de decisión usan la entropía y la ganancia de información para decidir sobre qué base romper los nodos en el proceso de aprendizaje de los datos. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ejemplo: tabla de conjugación</font></font></b><br>
<br>
<img src="https://habrastorage.org/webt/s4/ea/e5/s4eae57mpuehp3mk_mjpmikuan0.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
 Aquí, nuestra variable objetivo será </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Responsabilidad</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , que solo puede tomar dos valores: </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Normal"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Alto"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. También tenemos un solo signo, que se llama Calificación crediticia, distribuye los valores en tres categorías: </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Excelente"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Bueno"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Malo"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Se hicieron un total de 14 observaciones. 7 de ellos pertenecen a la clase de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Responsabilidad Normal</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , y 7 más a la clase de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Alta Responsabilidad</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Esta es una división en sí misma. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Si observamos la suma total de los valores en la primera fila, veremos que tenemos 4 observaciones con un valor </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">excelente</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> basado en la </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">calificación crediticia</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Además, incluso puedo decir que mi variable objetivo está rota por la </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">calificación crediticia "Excelente"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Entre las observaciones con el valor </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Excelente"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> por atributo</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Calificación crediticia</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , hay 3 que pertenecen a la clase de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Responsabilidad Normal</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y 1 que pertenece a la </font><font style="vertical-align: inherit;">clase de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Alta Responsabilidad</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Del mismo modo, puedo calcular resultados similares para otros valores de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Calificación crediticia</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> de la tabla de contingencia. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Por ejemplo, utilizo la tabla de contingencia anterior para calcular de forma independiente la entropía de nuestra variable objetivo y luego calcular su entropía, teniendo en cuenta información adicional del atributo de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">calificación crediticia</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Entonces puedo calcular cuánta información adicional me dará la </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">calificación crediticia</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> para la variable objetivo de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">responsabilidad</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 Entonces empecemos.</font></font><br>
<br>
<img src="https://habrastorage.org/webt/2v/2y/lg/2v2ylghvtk-f-e0eom6aeocsb1q.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 La entropía de nuestra variable objetivo es 1, lo que significa un desorden máximo debido a la distribución uniforme de los elementos entre </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Normal"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Alto"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . El siguiente paso es calcular la entropía de la variable objetivo del </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">pasivo</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , teniendo en cuenta información adicional de la </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">calificación crediticia</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Para hacer esto, calculamos la entropía del </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">pasivo</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> para cada valor de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">calificación crediticia</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y los sumamos usando el índice de observación promedio ponderado para cada valor. Por qué usamos el promedio ponderado será más claro cuando hablamos de árboles de decisión. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/rt/_5/fh/rt_5fhldx4dfjcioh7d_uiori6e.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 Obtuvimos la entropía de nuestra variable objetivo con el atributo </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Calificación crediticia</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Ahora podemos calcular la ganancia de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">responsabilidad</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> informativa </font><font style="vertical-align: inherit;">de la </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">calificación crediticia</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> para comprender cuán informativa es esta característica. </font></font><br>
<br>
<img src="https://habrastorage.org/webt/9b/d8/xv/9bd8xviw3mxbiqj28zmdqbiaazs.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 Conocer la </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">calificación crediticia</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> nos </font><i><font style="vertical-align: inherit;">ha</font></i><font style="vertical-align: inherit;"> ayudado a reducir la incertidumbre de nuestra variable objetivo de </font><i><font style="vertical-align: inherit;">responsabilidad</font></i><font style="vertical-align: inherit;"> .</font></font><i><font style="vertical-align: inherit;"></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. ¿No es una buena señal que debería funcionar? Danos información sobre la variable objetivo? Bueno, por esta misma razón, los árboles de decisión usan entropía y ganancia informativa. ¡Determinan según qué criterio dividir los nodos en ramas, para acercarse a la variable objetivo con cada partición subsiguiente, y también para comprender cuándo debe completarse la construcción del árbol! (además de hiperparámetros como la profundidad máxima, por supuesto). Veamos cómo funciona todo esto en el siguiente ejemplo usando árboles de decisión. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ejemplo: Árbol de decisión</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Veamos un ejemplo de cómo construir un árbol de decisión para predecir si el crédito de una persona se cancelará o no. La población será de 30 copias. 16 pertenecerán a la clase de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">amortización</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , y los otros 14 serán</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"No amortización"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Tendremos dos signos, a saber, </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Balance"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , que puede tomar dos valores: "&lt;50K" o "&gt; 50K", y </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Residencia"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , que toma tres valores: </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"PROPIO"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"ALQUILER"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> u </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"OTRO"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Demostraré cómo el algoritmo del árbol de decisión decidirá qué atributo romper primero y qué atributo será más informativo, es decir, elimina mejor la incertidumbre de la variable objetivo utilizando el concepto de entropía y ganancia de información. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Síntoma 1: Equilibrio</font></font></b><br>
<br>
<img src="https://habrastorage.org/webt/9b/d8/xv/9bd8xviw3mxbiqj28zmdqbiaazs.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
 Aquí los círculos pertenecen a la </font><font style="vertical-align: inherit;">clase de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"cancelación"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , y las estrellas corresponden a la clase de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"no cancelación"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Particionar una raíz principal por atributo</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El equilibrio</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> nos dará 2 nodos herederos. En el nodo izquierdo habrá 13 observaciones, donde 12/13 (probabilidad 0,92) de observaciones de la clase de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"cancelación"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y solo 1/13 (probabilidad de 0,08) de observaciones de la clase </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"no cancelación"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . En el nodo derecho habrá 17 de 30 observaciones, donde 13/17 (probabilidad 0.76) de observaciones de la clase de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"cancelación"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y 4/17 (probabilidad 0.24) de observaciones de la clase de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"no cancelación"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Calculemos la entropía de la raíz y veamos cuánto puede reducir la incertidumbre el árbol utilizando una partición basada en </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Balance</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br>
<img src="https://habrastorage.org/webt/yq/ke/do/yqkedojc2s80__h-vqqcptzewai.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 Una división basada en </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Balance</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> dará una ganancia informativa de 0.37. Vamos a contar lo mismo para el signo de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">residencia</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">y compara los resultados. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Síntoma 2: Residencia La</font></font></b><br>
<br>
<img src="https://habrastorage.org/webt/mx/tm/sd/mxtmsdt2hm0mamxkdxzqnb9v7mg.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
 división de un árbol basado en </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Residencia</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> le dará 3 nodos herederos. El nodo descendente izquierdo recibirá 8 observaciones, donde 7/8 (probabilidad 0,88) de observaciones de la clase de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">cancelación</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y solo 1/8 (probabilidad 0,12) de observaciones de la clase de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">no cancelación</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . El nodo sucesor promedio recibirá 10 observaciones, donde 4/10 (probabilidad 0.4) de observaciones de la clase de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">cancelación</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y 6/10 (probabilidad 0.6) de observaciones de la clase de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">no cancelación</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . El heredero correcto recibirá 12 observaciones, donde 5/12 (probabilidad 0,42) de observaciones de la clase de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">cancelación</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y 7/12 (probabilidad 0,58) de observaciones de la clase de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">no cancelación</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Ya conocemos la entropía del nodo padre, por lo que simplemente calculamos la entropía después de la partición para comprender la ganancia informativa del atributo </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Residencia</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br>
<img src="https://habrastorage.org/webt/cb/zt/zf/cbztzffw12-wkj6cjfayt_jzlcq.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 ¡La ganancia informativa del atributo </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Balance</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> es </font><font style="vertical-align: inherit;">casi 3 veces más que la de la </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Residencia</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ! Si observa nuevamente los gráficos, verá que la partición de acuerdo con </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Balance</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> dará nodos descendientes más limpios que de acuerdo con </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Residence</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Sin embargo, el nodo más a la izquierda en </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Residencia</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> también </font><i><font style="vertical-align: inherit;">está</font></i><font style="vertical-align: inherit;"> bastante limpio, pero es aquí donde entra en juego el promedio ponderado. A pesar de que el nodo está limpio, tiene el menor número de observaciones, y su resultado se pierde en el recálculo general y el cálculo de la entropía total según la </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Residencia</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Esto es importante porque estamos buscando el contenido informativo general del atributo y no queremos que el resultado final sea distorsionado por el raro valor del atributo. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El atributo </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Balance en</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> sí mismo </font><font style="vertical-align: inherit;">proporciona más información sobre la variable objetivo que </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Residencia</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Por lo tanto, la entropía de nuestra variable objetivo se reduce. El algoritmo del árbol de decisión utiliza este resultado para realizar la primera división de acuerdo con </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Balance</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">para luego decidir sobre qué base romper los siguientes nodos. En el mundo real, cuando hay más de dos características, el primer desglose ocurre de acuerdo con la característica más informativa, y luego, con cada ruptura posterior, la ganancia de información se contará para cada característica adicional, ya que no será lo mismo que la ganancia de información de cada característica individualmente. La entropía y la ganancia informativa deben calcularse después de que se hayan producido una o varias particiones, lo que afectará el resultado final. ¡El árbol de decisión repetirá este proceso a medida que crezca en profundidad, hasta que alcance una cierta profundidad o algún tipo de división conduzca a una mayor ganancia de información sobre un cierto umbral, que también se puede especificar como un hiperparámetro!</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
¡Eso es todo! </font><font style="vertical-align: inherit;">Ahora ya sabe qué entropía, ganancia de información y cómo se calculan. </font><font style="vertical-align: inherit;">Ahora comprende cómo el árbol de decisión, por sí mismo o como parte de un conjunto, toma decisiones sobre el mejor orden de partición por atributos y decide cuándo detenerse al aprender los datos disponibles. </font><font style="vertical-align: inherit;">Bueno, si tiene que explicarle a alguien cómo funcionan los árboles de decisión, espero que pueda hacer frente adecuadamente a esta tarea. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Espero que hayas aprendido algo útil para ti de este artículo. </font><font style="vertical-align: inherit;">Si me perdí algo o me expresé de manera incorrecta, escríbeme al respecto. </font><font style="vertical-align: inherit;">¡Te estaré muy agradecido! </font><font style="vertical-align: inherit;">Gracias.</font></font><br>
<br>
<hr><br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aprende más sobre el curso.</font></font></a><br>
<br>
<hr></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es502178/index.html">oVirt en 2 horas. Parte 3. Configuraciones avanzadas</a></li>
<li><a href="../es502180/index.html">El día que desapareció el perímetro. Soluciones de seguridad de Microsoft y socios</a></li>
<li><a href="../es502182/index.html">De nuevo sobre MikroTik o el tan esperado SOCKS5</a></li>
<li><a href="../es502186/index.html">Seminario web Seguridad de la información: SOC en cuarentena</a></li>
<li><a href="../es502196/index.html">В подходе к математике столетней давности найдены новые ключи к разгадке природы времени</a></li>
<li><a href="../es502202/index.html">El desarrollo de tecnología no tripulada en el transporte ferroviario.</a></li>
<li><a href="../es502204/index.html">Escribir pruebas @SpringBootTest cuando se usa Spring Shell en una aplicación</a></li>
<li><a href="../es502206/index.html">Yandex grabó los sonidos de las retrocomputadoras</a></li>
<li><a href="../es502208/index.html">Extensión de Chrome para ocultar recomendaciones que distraen en YouTube</a></li>
<li><a href="../es502234/index.html">Инсайды от сотрудника Facebook: как попасть на стажировку, получить оффер и все о работе в компании</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>