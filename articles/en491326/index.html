<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üåÅ ü§∞ üë®üèº‚Äçüé§ Abstract on data classification methods üë®üèΩ‚Äçüé® üë©‚Äç‚ù§Ô∏è‚Äçüíã‚Äçüë© üâê</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="When studying Data Science, I decided to compile for myself a summary of the basic techniques used in data analysis. It reflects the names of the meth...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Abstract on data classification methods</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/491326/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">When studying Data Science, I decided to compile for myself a summary of the basic techniques used in data analysis. </font><font style="vertical-align: inherit;">It reflects the names of the methods, briefly describes the essence and provides Python code for quick application. </font><font style="vertical-align: inherit;">I was preparing a compendium for myself, but I thought that it might also be useful to someone, for example, before an interview, in a competition, or when starting a new project. </font><font style="vertical-align: inherit;">Designed for an audience that is generally familiar with all these methods, but has the need to refresh them in memory. </font><font style="vertical-align: inherit;">Article under the cut.</font></font><br>
<a name="habracut"></a><br>
 <p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Naive Bayes classifier</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">The formula for calculating the probability of classifying an observation as one or another class:</font></font></p><br>
  <p><img src="https://habrastorage.org/webt/xm/ks/pe/xmkspevmmrgf6mn_os_pvslb3qc.png"></p><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 For example, you need to calculate the probability that a sports match will take place provided that the weather is sunny. </font><font style="vertical-align: inherit;">The source data and calculations are shown in the table below:</font></font><br>
<br>
 <p><img src="https://habrastorage.org/webt/yd/gc/ik/ydgcik0buynftc9j_xo0bamb3s8.png"></p><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 You can calculate by the formula (3/9) * (9/14) / (5/14) = 60%, or just from common sense 3 / (2 + 3) = 60%. </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Strengths</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - easy to interpret the result, suitable for large samples and multi-class classification. </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Weaknesses</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - the assumption that the characteristics are independent is not always fulfilled; characteristics should make up a complete group of events.</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-comment">#imports</span>
<span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_iris
<span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split
<span class="hljs-keyword">from</span> sklearn.naive_bayes <span class="hljs-keyword">import</span> GaussianNB<font></font>
<font></font>
<span class="hljs-comment">#model fit</span>
X, y = load_iris(return_X_y=<span class="hljs-literal">True</span>)<font></font>
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="hljs-number">0.5</span>, random_state=<span class="hljs-number">0</span>)<font></font>
gnb = GaussianNB()<font></font>
y_pred = gnb.fit(X_train, y_train).predict(X_test)<font></font>
<font></font>
<span class="hljs-comment">#result</span>
print(<span class="hljs-string">"Number of mislabeled points out of a total %d points : %d"</span>
            % (X_test.shape[<span class="hljs-number">0</span>], (y_test != y_pred).sum())) 
</code></pre> <br>
 <br>
 <p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Method of nearest neighbors</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Classifies each observation according to the degree of similarity to other observations. </font><font style="vertical-align: inherit;">The algorithm is nonparametric (there are no restrictions on the data, for example, the distribution function) and uses lazy training (pre-trained models are not used, all available data are used during classification).</font></font></p><br>
  <p><img src="https://habrastorage.org/webt/l7/vw/1n/l7vw1nxyzyori9nkeminukor90q.png"></p><br>
  <br>
 <strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Strengths</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - easy to interpret the result, well suited for tasks with a small number of explanatory variables. </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Weaknesses</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - low accuracy compared to other methods. </font><font style="vertical-align: inherit;">It requires significant computing power with a large number of explanatory variables and large samples.</font></font><br>
 <br>
  <br>
 <pre><code class="python hljs"><span class="hljs-comment">#imports</span>
<span class="hljs-keyword">from</span> sklearn.neighbors <span class="hljs-keyword">import</span> KNeighborsClassifier<font></font>
<font></font>
<span class="hljs-comment">#model fit</span>
X = [[<span class="hljs-number">0</span>], [<span class="hljs-number">1</span>], [<span class="hljs-number">2</span>], [<span class="hljs-number">3</span>]]<font></font>
y = [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]<font></font>
neigh = KNeighborsClassifier(n_neighbors=<span class="hljs-number">3</span>)<font></font>
neigh.fit(X, y)<font></font>
<font></font>
<span class="hljs-comment">#result</span>
print(neigh.predict([[<span class="hljs-number">1.1</span>]]))<font></font>
print(neigh.predict_proba([[<span class="hljs-number">0.9</span>]]))
 </code></pre> <br>
 <br>
 <p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Support Vector Method (SVM)</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Each data object is represented as a vector (point) in p-dimensional space. </font><font style="vertical-align: inherit;">The task is to separate the points with a hyperplane. </font><font style="vertical-align: inherit;">That is, is it possible to find such a hyperplane so that the distance from it to the nearest point is maximum. </font><font style="vertical-align: inherit;">There can be many sought-after hyperplanes; therefore, it is believed that maximizing the gap between classes contributes to a more confident classification.</font></font></p><br>
  <p><img src="https://habrastorage.org/getpro/habr/post_images/bcf/cdb/f99/bcfcdbf99544b1cd6ccb0f99ec519131.jpg"></p><br>
  <br>
 <p> <strong> </strong> ‚Äî     .   ,   ,   .      . <strong> </strong> ‚Äî  ,   ,   ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"> </a>,    .        .<br>
 </p> <br>
  <br>
 <pre><code class="python hljs"><span class="hljs-comment">#imports</span>
<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> svm<font></font>
<font></font>
<span class="hljs-comment">#model fit</span>
X = [[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>]]<font></font>
y = [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>]<font></font>
clf = svm.SVC()<font></font>
clf.fit(X, y)<font></font>
<font></font>
<span class="hljs-comment">#result</span>
clf.predict([[<span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>]])
 </code></pre> <br>
 <p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Decision trees</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Dividing data into subsamples according to a certain condition in the form of a tree structure. </font><font style="vertical-align: inherit;">Mathematically, the division into classes occurs until all the conditions that determine the class as precisely as possible are found, i.e., when there are no representatives of another class in each class. </font><font style="vertical-align: inherit;">In practice, a limited number of characteristics and layers are used, and there are always two branches.</font></font></p><br>
  <p><img src="https://habrastorage.org/getpro/habr/post_images/b3c/162/1f1/b3c1621f19b930a48abce372977cadbb.png"></p><br>
 <br>
 <strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Strengths</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - it is possible to simulate complex processes and easily interpret them. </font><font style="vertical-align: inherit;">Multiclass classification is possible. </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Weaknesses</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - it is easy to retrain the model if you make many layers. </font><font style="vertical-align: inherit;">Emissions can affect accuracy; the solution to these problems is to trim the lower levels.</font></font><br>
  <pre><code class="python hljs"><span class="hljs-comment">#imports</span>
<span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_iris
<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> tree<font></font>
<font></font>
<span class="hljs-comment">#model fit</span>
X, y = load_iris(return_X_y=<span class="hljs-literal">True</span>)<font></font>
clf = tree.DecisionTreeClassifier()<font></font>
clf = clf.fit(X, y)<font></font>
<font></font>
<span class="hljs-comment">#result</span>
tree.plot_tree(clf.fit(iris.data, iris.target)) </code></pre> <br>
 <p>-<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"> / </a>.        .  ‚Äî     .       (random patching)             .       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow">oob-</a>.</p><br>
 <p><strong> </strong>:   ,     ,  ,    ,   ,       .     ,    . <strong> </strong> ‚Äî    ,      .  ,     ( 100 000),     ‚Äî .</p> <br>
 <pre><code class="python hljs"><span class="hljs-comment">#imports</span>
<span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier
<span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_classification<font></font>
<font></font>
<span class="hljs-comment">#model fit</span>
X, y = make_classification(n_samples=<span class="hljs-number">1000</span>, n_features=<span class="hljs-number">4</span>,<font></font>
                           n_informative=<span class="hljs-number">2</span>, n_redundant=<span class="hljs-number">0</span>,<font></font>
                           random_state=<span class="hljs-number">0</span>, shuffle=<span class="hljs-literal">False</span>)<font></font>
                           <font></font>
clf = RandomForestClassifier(max_depth=<span class="hljs-number">2</span>, random_state=<span class="hljs-number">0</span>)<font></font>
clf.fit(X, y)<font></font>
<font></font>
<span class="hljs-comment">#result</span><font></font>
print(clf.feature_importances_)<font></font>
print(clf.predict([[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]]))</code></pre> <br>
 <p>-<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow">  </a>.      (  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow">hinge loss function</a>).      .<br>
  </p><p><img src="https://habrastorage.org/webt/d7/wk/si/d7wksizyut_vgl2idpts9-eythq.png"></p><br>
  <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 There is also a version of stochastic gradient descent, which is used for large samples. </font><font style="vertical-align: inherit;">Its essence is that it considers the derivative not for the entire sample, but for each observation (online learning) (or for the mini-batch observation group) and changes weights. </font><font style="vertical-align: inherit;">As a result, he comes to the same optimum as with a conventional HS. </font><font style="vertical-align: inherit;">There are methods of using HS for OLS, logit, tobit and other methods ( </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">evidence</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ).</font></font><br>
 <br>
 <p><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Strengths</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : high accuracy of classification and forecasting, suitable for multi-class classification. </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Weaknesses</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - sensitivity to model parameters.</font></font></p><br>
  <pre><code class="python hljs"><span class="hljs-comment">#imports</span>
<span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> SGDClassifier<font></font>
<font></font>
<span class="hljs-comment">#model fit</span>
X = [[<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>], [<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>]]<font></font>
y = [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>]<font></font>
clf = SGDClassifier(loss=<span class="hljs-string">"hinge"</span>, penalty=<span class="hljs-string">"l2"</span>, max_iter=<span class="hljs-number">5</span>)<font></font>
clf.fit(X, y)<font></font>
<font></font>
<span class="hljs-comment">#result</span>
clf.predict([[<span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>]])<font></font>
clf.coef_<font></font>
clf.intercept_</code></pre> <br>
 <br>
 <p>-<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"> </a>.   .         ,    . ,     ,    . </p><br>
 <br>
 <p><strong> </strong>:     ,    ,    ,    . <strong> </strong> ‚Äî    .</p><br>
  <br>
 <pre><code class="python hljs"><span class="hljs-comment">#imports</span>
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> ensemble
<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets
<span class="hljs-keyword">from</span> sklearn.utils <span class="hljs-keyword">import</span> shuffle
<span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> mean_squared_error<font></font>
<font></font>
<span class="hljs-comment">#model fit</span><font></font>
boston = datasets.load_boston()<font></font>
X, y = shuffle(boston.data, boston.target, random_state=<span class="hljs-number">13</span>)<font></font>
X = X.astype(np.float32)<font></font>
offset = int(X.shape[<span class="hljs-number">0</span>] * <span class="hljs-number">0.9</span>)<font></font>
X_train, y_train = X[:offset], y[:offset]<font></font>
X_test, y_test = X[offset:], y[offset:]<font></font>
params = {<span class="hljs-string">'n_estimators'</span>: <span class="hljs-number">500</span>, <span class="hljs-string">'max_depth'</span>: <span class="hljs-number">4</span>, <span class="hljs-string">'min_samples_split'</span>: <span class="hljs-number">2</span>,
          <span class="hljs-string">'learning_rate'</span>: <span class="hljs-number">0.01</span>, <span class="hljs-string">'loss'</span>: <span class="hljs-string">'ls'</span>}<font></font>
clf = ensemble.GradientBoostingRegressor(**params)<font></font>
clf.fit(X_train, y_train)<font></font>
<font></font>
<span class="hljs-comment">#result</span><font></font>
mse = mean_squared_error(y_test, clf.predict(X_test))<font></font>
print(<span class="hljs-string">"MSE: %.4f"</span> % mse)</code></pre> <br>
 <br>
 <p>-<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow"> /logit</a>.     0  1,     (<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow">log likelihood</a>).  ‚Äî    Y       w.<br>
  </p><p><img src="https://habrastorage.org/getpro/habr/post_images/859/2c1/173/8592c1173d2ff17239fca69ec8b18cac.jpg"></p><br>
  <br>
  <p><img src="https://habrastorage.org/getpro/habr/post_images/cfe/2ca/c37/cfe2cac37843ae065a3ef157a02d389c.jpg"></p><br>
  <br>
 <br>
 <p><strong> </strong>:  ,      . <strong> </strong> ‚Äî    ,    .</p><br>
  <pre><code class="python hljs"><span class="hljs-comment">#imports</span>
<span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_iris
<span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression<font></font>
<font></font>
<span class="hljs-comment">#model fit</span>
X, y = load_iris(return_X_y=<span class="hljs-literal">True</span>)<font></font>
clf = LogisticRegression(random_state=<span class="hljs-number">0</span>).fit(X, y)<font></font>
clf.predict(X[:<span class="hljs-number">2</span>, :])<font></font>
<font></font>
<span class="hljs-comment">#result</span>
clf.predict_proba(X[:<span class="hljs-number">2</span>, :])<font></font>
clf.score(X, y) </code></pre> <br>
 <br>
 <p>-<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow">Probit</a>.     ,     ,   ,      .</p><br>
  <pre><code class="python hljs"><span class="hljs-comment">#imports</span>
<span class="hljs-keyword">import</span> statsmodels<font></font>
<font></font>
<span class="hljs-comment">#model fit</span><font></font>
result_3 = statsmodels.discrete.<font></font>
    discrete_model.Probit(labf_part, ind_var_probit )<font></font>
<font></font>
<span class="hljs-comment">#result</span>
print(result_3.summary()) </code></pre> <br>
 <p>-<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=" rel="nofollow">Tobit</a>. ,      .</p><br>
  <pre><code class="python hljs"><span class="hljs-comment">#imports</span>
<span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_regression
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">from</span> tobit <span class="hljs-keyword">import</span> *<font></font>
<font></font>
<span class="hljs-comment">#model fit</span><font></font>
tr = TobitModel()<font></font>
<font></font>
<span class="hljs-comment">#result</span>
tr = tr.fit(x, y, cens, verbose=<span class="hljs-literal">False</span>)<font></font>
tr.coef_</code></pre> <br>
 <p>  -  , ,     .       ,  ,    ,    .   .</p><br>
 <br>
<br>
 <br></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../en491308/index.html">ClickHouse - visually fast and intuitive data analysis in Tabix. Igor Strykhar</a></li>
<li><a href="../en491310/index.html">How to crack a password archive yourself</a></li>
<li><a href="../en491312/index.html">How we took a sieve from a man and helped the mill</a></li>
<li><a href="../en491318/index.html">A selection of books on cybersecurity: how to conduct a pentest and what to oppose social engineering</a></li>
<li><a href="../en491320/index.html">Please stop recommending git flow</a></li>
<li><a href="../en491332/index.html">Hourly planning and other scrum event optimization</a></li>
<li><a href="../en491336/index.html">Play like a game designer</a></li>
<li><a href="../en491338/index.html">Can 5G replace Wi-Fi - discuss opinions</a></li>
<li><a href="../en491342/index.html">How to lose all users of your telegram bot. Brief instruction</a></li>
<li><a href="../en491344/index.html">New entrant in quantum computing with unique technology</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>