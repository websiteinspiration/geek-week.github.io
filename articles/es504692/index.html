<!doctype html>
<html class="no-js" lang="es">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üö∂üèø üëî ü§πüèº Conceptos b√°sicos de ZFS: almacenamiento y rendimiento üèüÔ∏è üìç üîÄ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Esta primavera, ya hemos discutido algunos temas introductorios, como c√≥mo verificar la velocidad de sus discos y qu√© es RAID . En el segundo de ellos...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Conceptos b√°sicos de ZFS: almacenamiento y rendimiento</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/504692/"><img src="https://habrastorage.org/getpro/habr/post_images/abf/883/e96/abf883e96b01dbc78420e0dc1a158460.jpg"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Esta primavera, ya hemos discutido algunos temas introductorios, </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">como c√≥mo verificar la velocidad de sus discos</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">qu√© es RAID</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">En el segundo de ellos, incluso prometimos continuar estudiando el rendimiento de varias topolog√≠as de discos m√∫ltiples en ZFS. </font><font style="vertical-align: inherit;">Este es el sistema de archivos de pr√≥xima generaci√≥n que se est√° implementando en todas partes: desde </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Apple</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> hasta </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ubuntu</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Bueno, hoy es el mejor d√≠a para conocer ZFS, lectores curiosos. </font><font style="vertical-align: inherit;">Solo tenga en cuenta que, seg√∫n una evaluaci√≥n conservadora realizada por el desarrollador de OpenZFS, Matt Arens, "es realmente complicado". </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pero antes de llegar a los n√∫meros, y lo har√°n, lo prometo, para todas las variantes de configuraci√≥n de vosmidiskovoy ZFS, debe hablar sobre </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">c√≥mo</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> hacer que ZFS almacene datos en el disco.</font></font><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zpool, vdev y dispositivo</font></font></h1><br>
<img src="https://habrastorage.org/getpro/habr/post_images/674/1c6/ab3/6741c6ab310f4e0edf2adf7e2ca4c6bb.png"><br>
<i><font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Este diagrama de grupo completo incluye tres vdevs auxiliares, uno para cada clase y cuatro para RAIDz2. </font></font></font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/b9f/82c/887/b9f82c88748c44d1f86cc412a053bf94.png"><br>
<i><font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Por lo general, no hay raz√≥n para crear un grupo de tipos y tama√±os de </font></font></font></i><font style="vertical-align: inherit;"><i><font color="gray"><font style="vertical-align: inherit;">vdev </font></font></i><i><font color="gray"><font style="vertical-align: inherit;">inapropiados, pero si lo desea, nada le impide hacerlo.</font></font></i></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Comprender realmente el sistema de archivos ZFS , debe observar cuidadosamente su estructura real. </font><font style="vertical-align: inherit;">Primero, ZFS combina los niveles tradicionales de gesti√≥n de volumen y el sistema de archivos. </font><font style="vertical-align: inherit;">En segundo lugar, utiliza un mecanismo de copia transaccional al escribir. </font><font style="vertical-align: inherit;">Estas caracter√≠sticas significan que el sistema es estructuralmente muy diferente de los sistemas de archivos ordinarios y las matrices RAID. </font><font style="vertical-align: inherit;">El primer conjunto de componentes b√°sicos para comprender: un grupo de almacenamiento (zpool), un dispositivo virtual (vdev) y un dispositivo real (dispositivo).</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">zpool</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El grupo de almacenamiento de zpool es la estructura ZFS m√°s alta. Cada grupo contiene uno o m√°s dispositivos virtuales. A su vez, cada uno de ellos contiene uno o m√°s dispositivos reales (dispositivo). Las piscinas virtuales son bloques aut√≥nomos. Una computadora f√≠sica puede contener dos o m√°s grupos separados, pero cada uno es completamente independiente de los dem√°s. Los grupos no pueden compartir dispositivos virtuales. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La redundancia de ZFS est√° a nivel de dispositivos virtuales, pero no a nivel de agrupaciones. En el nivel de grupo, no hay absolutamente ninguna redundancia: si se pierde una unidad vdev o un vdev especial, se pierde todo el grupo junto con √©l.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Las agrupaciones de almacenamiento modernas pueden sobrevivir a la p√©rdida de un registro de cach√© o dispositivo virtual, aunque pueden perder una peque√±a cantidad de datos sucios si pierden el registro vdev durante un corte de energ√≠a o un bloqueo del sistema. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Existe una idea err√≥nea com√∫n de que las "bandas de datos" (tiras) de ZFS se registran en todo el grupo. Esto no es verdad. Zpool no es un RAID0 divertido en absoluto, es m√°s bien un </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">JBOD</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> divertido </font><font style="vertical-align: inherit;">con un complejo mecanismo de distribuci√≥n variable.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En su mayor parte, los registros se distribuyen entre los dispositivos virtuales disponibles de acuerdo con el espacio disponible, por lo que, en teor√≠a, todos se completar√°n al mismo tiempo. En versiones posteriores de ZFS, se tiene en cuenta el uso actual (utilizaci√≥n) de vdev: si un dispositivo virtual est√° significativamente m√°s cargado que el otro (por ejemplo, debido a la carga de lectura), se omitir√° temporalmente para la escritura, a pesar de la presencia del coeficiente de espacio libre m√°s alto. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Un mecanismo de detecci√≥n de reciclaje integrado en los m√©todos modernos de distribuci√≥n de registros ZFS puede reducir la latencia y aumentar el rendimiento durante los per√≠odos de carga inusualmente alta, pero esto no es </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">carta blanca.</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">mezcla involuntariamente HDD lentos y SSD r√°pidos en un grupo. </font><font style="vertical-align: inherit;">Tal grupo desigual seguir√° funcionando a la velocidad del dispositivo m√°s lento, es decir, como si estuviera completamente compuesto por dichos dispositivos.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">vdev</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Cada grupo de almacenamiento consta de uno o m√°s dispositivos virtuales (dispositivo virtual, vdev). </font><font style="vertical-align: inherit;">A su vez, cada vdev incluye uno o m√°s dispositivos reales. </font><font style="vertical-align: inherit;">La mayor√≠a de los dispositivos virtuales se utilizan para almacenar datos f√°cilmente, pero hay varias clases vdev auxiliares, que incluyen CACHE, LOG y SPECIAL. </font><font style="vertical-align: inherit;">Cada uno de estos tipos de vdev puede tener una de cinco topolog√≠as: dispositivo √∫nico, RAIDz1, RAIDz2, RAIDz3 o espejo.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
RAIDz1, RAIDz2 y RAIDz3 son variaciones especiales de lo que los antiguos llaman RAID doble paridad (diagonal). 1, 2 y 3 se refieren a cu√°ntos bloques de paridad se asignan para cada banda de datos. En lugar de discos separados para paridad, los dispositivos RAIDz virtuales distribuyen uniformemente esta paridad entre los discos. Una matriz RAIDz puede perder tantos discos como bloques de paridad; si pierde a otro, fallar√° y se llevar√° el grupo de almacenamiento con √©l.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En los dispositivos virtuales reflejados (espejo vdev), cada bloque se almacena en cada dispositivo en vdev. Aunque son los espejos de dos anchos m√°s comunes, puede haber cualquier n√∫mero arbitrario de dispositivos en el espejo: en instalaciones grandes, los triples se usan a menudo para aumentar el rendimiento de lectura y la tolerancia a fallas. El espejo vdev puede sobrevivir a cualquier falla mientras al menos un dispositivo en vdev contin√∫a funcionando. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Los vdevs individuales son inherentemente peligrosos. Tal dispositivo virtual no sobrevivir√° a una sola falla, y si se usa como almacenamiento o un vdev especial, entonces su falla conducir√° a la destrucci√≥n de todo el grupo. Ten mucho, mucho cuidado aqu√≠.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Los dispositivos virtuales CACHE, LOG y SPECIAL se pueden crear utilizando cualquiera de las topolog√≠as enumeradas anteriormente, pero recuerde que perder un dispositivo virtual SPECIAL significa perder un grupo, por lo que se recomienda encarecidamente una topolog√≠a redundante.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">dispositivo</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Este es probablemente el t√©rmino m√°s f√°cil de entender en ZFS: es literalmente un dispositivo de acceso aleatorio en bloque. Recuerde que los dispositivos virtuales est√°n formados por dispositivos individuales, y el grupo est√° formado por dispositivos virtuales. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Los discos, magn√©ticos o de estado s√≥lido, son los dispositivos de bloque m√°s comunes que se utilizan como bloques de construcci√≥n vdev. Sin embargo, cualquier dispositivo con un identificador en / dev es adecuado, por lo que puede usar matrices RAID de hardware completas como dispositivos separados. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Un archivo sin formato simple es uno de los dispositivos de bloque alternativos m√°s importantes desde los que se puede construir vdev. Probar grupos de </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">archivos dispersos</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">&nbsp;- Una forma muy conveniente de verificar los comandos del grupo y ver cu√°nto espacio hay disponible en el grupo o dispositivo virtual de esta topolog√≠a. </font></font><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/5cf/aa5/62c/5cfaa562cb208b654af113f7535b8f57.png"><br>
<i><font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Puede crear un grupo de prueba a partir de archivos dispersos en solo unos segundos, pero no olvide eliminar todo el grupo y sus componentes m√°s tarde.</font></font></font></i> <br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Suponga que desea colocar un servidor en ocho discos y planea usar discos de 10 TB (~ 9300 GiB), pero no est√° seguro de cu√°l La topolog√≠a se adapta mejor a sus necesidades. En el ejemplo anterior, en cuesti√≥n de segundos construimos un grupo de prueba a partir de archivos dispersos, y ahora sabemos que RAIDz2 vdev de ocho unidades de 10 TB proporciona 50 TiB de capacidad √∫til.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Otra clase especial de dispositivos es SPARE (repuesto). Los dispositivos intercambiables en caliente, a diferencia de los dispositivos convencionales, pertenecen al grupo completo, no solo a un dispositivo virtual. Si falla alg√∫n vdev en el grupo y hay un dispositivo de repuesto conectado al grupo y accesible, entonces se unir√° autom√°ticamente al vdev afectado. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Despu√©s de conectarse al vdev afectado, el dispositivo de reserva comienza a recibir copias o la reconstrucci√≥n de los datos que deber√≠an estar en el dispositivo perdido. En RAID tradicional, esto se llama reconstrucci√≥n, mientras que en ZFS se llama "recuperaci√≥n".</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Es importante tener en cuenta que los dispositivos de reemplazo no reemplazan permanentemente los dispositivos con fallas. </font><font style="vertical-align: inherit;">Esto es solo un reemplazo temporal para reducir el tiempo durante el cual se observa la degradaci√≥n de vdev. </font><font style="vertical-align: inherit;">Despu√©s de que el administrador reemplaz√≥ el dispositivo vdev fallido, la redundancia se restaura a este dispositivo permanente y SPARE se desconecta de vdev y vuelve a funcionar como repuesto para todo el grupo.</font></font><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Conjuntos de datos, bloques y sectores</font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El siguiente conjunto de componentes b√°sicos que debe comprender en nuestro viaje a trav√©s de ZFS no es tanto el hardware, sino c√≥mo se organizan y almacenan los datos. </font><font style="vertical-align: inherit;">Omitimos varios niveles aqu√≠, como el metaslab, para no acumular los detalles y mantener una comprensi√≥n de la estructura general.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Conjunto de datos</font></font></h3><br>
<img src="https://habrastorage.org/getpro/habr/post_images/bd3/c48/d9d/bd3c48d9dff6e0f493a5d90d1dca6d1d.png"><br>
<i><font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cuando creamos un conjunto de datos por primera vez, muestra todo el espacio de grupo disponible. Luego establecemos la cuota y cambiamos el punto de montaje. ¬°Magia! </font></font></font></i> <br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/a18/3de/210/a183de210cdc57cd1421652201cbf2c3.png"><br>
<i><font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zvol es en su mayor parte un conjunto de datos, desprovisto de su capa de sistema de archivos, que reemplazamos aqu√≠ con un</font></font></font></i> <br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
sistema de archivos</font><i><font color="gray"><font style="vertical-align: inherit;"> ext4 completamente normal</font></font></i><font style="vertical-align: inherit;"> . El conjunto de datos ZFS es m√°s o menos lo mismo que un sistema de archivos montado est√°ndar. Al igual que un sistema de archivos normal, a primera vista parece ser "simplemente otra carpeta". Pero tambi√©n, como los sistemas de archivos montados convencionales, cada conjunto de datos ZFS tiene su propio conjunto de propiedades b√°sicas.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En primer lugar, un conjunto de datos puede tener una cuota asignada. Si est√° instalado</font></font><code>zfs set quota=100G poolname/datasetname</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, no puede escribir en la carpeta montada</font></font><code>/poolname/datasetname</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">m√°s de 100 GiB.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
¬øNota la presencia - y ausencia - de barras al comienzo de cada l√≠nea? Cada conjunto de datos tiene su propio lugar tanto en la jerarqu√≠a de ZFS como en la jerarqu√≠a de montaje del sistema. No hay una barra diagonal en la jerarqu√≠a de ZFS: comienza con el nombre del grupo y luego la ruta de un conjunto de datos al siguiente. Por ejemplo, </font></font><code>pool/parent/child</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">para un conjunto de datos nombrado </font></font><code>child</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">bajo el conjunto </font></font><code>parent</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">de </font><font style="vertical-align: inherit;">datos primario </font><font style="vertical-align: inherit;">en un grupo con un nombre de creatividad </font></font><code>pool</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Por defecto, el punto de montaje del conjunto de datos ser√° equivalente a su nombre en la jerarqu√≠a ZFS, con una barra al principio - la piscina con el nombre se </font></font><code>pool</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">monta como </font></font><code>/pool</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, el conjunto de datos est√° </font></font><code>parent</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">montado en </font></font><code>/pool/parent</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, y el ni√±o </font><font style="vertical-align: inherit;">conjunto de datos </font><font style="vertical-align: inherit;">se monta </font></font><code>child</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">en </font></font><code>/pool/parent/child</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Sin embargo, el punto de montaje del sistema para el conjunto de datos se puede cambiar. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Si indicamos</font></font><code>zfs set mountpoint=/lol pool/parent/child</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, el conjunto de datos se </font></font><code>pool/parent/child</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">monta en el sistema como </font></font><code>/lol</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Adem√°s de los conjuntos de datos, debemos mencionar los vol√∫menes (zvols). </font><font style="vertical-align: inherit;">Un volumen es aproximadamente similar a un conjunto de datos, excepto que en realidad no tiene un sistema de archivos, es solo un dispositivo de bloque. </font><font style="vertical-align: inherit;">Puede, por ejemplo, crear </font></font><code>zvol</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">con un nombre </font></font><code>mypool/myzvol</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, luego formatearlo con el sistema de archivos ext4 y luego montar este sistema de archivos; ahora tiene el sistema de archivos ext4, ¬°pero con soporte para todas las caracter√≠sticas de seguridad de ZFS! </font><font style="vertical-align: inherit;">Esto puede parecer una tonter√≠a en una computadora, pero tiene mucho m√°s sentido como backend cuando se exporta un dispositivo iSCSI.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bloques</font></font></h3><br>
<img src="https://habrastorage.org/getpro/habr/post_images/74b/4dd/d00/74b4ddd009e67db1b1b6c4467bcf6fa3.png"><br>
<i><font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Un archivo est√° representado por uno o m√°s bloques. Cada bloque se almacena en un dispositivo virtual. El tama√±o del bloque suele ser igual al par√°metro de </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">tama√±o de registro</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , pero se puede reducir a </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2 ^</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> si hay metadatos o un archivo peque√±o. </font></font></font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/8d1/7fd/ad2/8d17fdad2eda641c801e5e6a302f6e38.png"><br>
<i><font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Realmente, </font><font style="vertical-align: inherit;">no </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">estamos</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> bromeando sobre el gran da√±o al rendimiento si instala un cambio de ceniza demasiado peque√±o.</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
En el grupo ZFS, todos los datos, incluidos los metadatos, se almacenan en bloques. El tama√±o m√°ximo de bloque para cada conjunto de datos se define en la propiedad</font></font><code>recordsize</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(tama√±o de registro). El tama√±o del registro puede variar, pero esto no cambiar√° el tama√±o o la ubicaci√≥n de los bloques que ya se han escrito en el conjunto de datos; es v√°lido solo para los nuevos bloques a medida que se escriben.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A menos que se especifique lo contrario, el tama√±o de grabaci√≥n actual es de 128 KiB por defecto. Este es un tipo de compromiso dif√≠cil en el que el rendimiento no ser√° ideal, pero no terrible en la mayor√≠a de los casos. </font></font><code>Recordsize</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">se puede configurar en cualquier valor de 4K a 1M (con configuraciones adicionales </font></font><code>recordsize</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">puede configurar a√∫n m√°s, pero rara vez es una buena idea). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Cualquier bloque se refiere a los datos de un solo archivo: no puede comprimir dos archivos diferentes en un bloque. Cada archivo consta de uno o m√°s bloques, seg√∫n el tama√±o. Si el tama√±o del archivo es menor que el tama√±o del registro, se guardar√° en un bloque m√°s peque√±o; por ejemplo, un bloque con un archivo de 2 KiB ocupar√° solo un sector de 4 KiB en el disco. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Si el archivo es lo suficientemente grande y requiere varios bloques, todos los registros con este archivo tendr√°n un tama√±o</font></font><code>recordsize</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">&nbsp;- incluido el √∫ltimo registro, cuya parte principal puede resultar ser </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">espacio no utilizado</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Los vol√∫menes Zvol no tienen una propiedad </font></font><code>recordsize</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">&nbsp;; en cambio, tienen una propiedad equivalente </font></font><code>volblocksize</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sectores</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El √∫ltimo bloque de construcci√≥n m√°s b√°sico es el sector. Esta es la unidad f√≠sica m√°s peque√±a que se puede escribir o leer desde la unidad base. Durante varias d√©cadas, la mayor√≠a de los discos utilizaron sectores de 512 bytes. Recientemente, la mayor√≠a de las unidades est√°n configuradas para 4 sectores KiB y, en algunos, especialmente SSD, 8 sectores KiB o incluso m√°s. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ZFS tiene una propiedad que le permite establecer manualmente el tama√±o del sector. Esta es una propiedad </font></font><code>ashift</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Es algo confuso que el cambio de ceniza sea una potencia de dos. Por ejemplo, </font></font><code>ashift=9</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">significa un tama√±o de sector de 2 ^ 9 o 512 bytes.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ZFS solicita al sistema operativo informaci√≥n detallada sobre cada dispositivo de bloque cuando se agrega al nuevo vdev, y te√≥ricamente establece autom√°ticamente el cambio de ceniza de manera adecuada en funci√≥n de esta informaci√≥n. Desafortunadamente, muchos discos mienten sobre el tama√±o de su sector para mantener la compatibilidad con Windows XP (que no pudo entender los discos con otros tama√±os de sector). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Esto significa que se recomienda encarecidamente al administrador de ZFS que conozca el tama√±o real del sector de sus dispositivos e instale manualmente</font></font><code>ashift</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Si se establece un cambio de ceniza demasiado peque√±o, entonces el n√∫mero de operaciones de lectura / escritura aumenta astron√≥micamente. Por lo tanto, escribir "sectores" de 512 bytes en el sector real de 4 KiB significa escribir el primer "sector", luego leer el sector de 4 KiB, cambiarlo con el segundo "sector" de 512 bytes, escribirlo nuevamente en el nuevo sector de 4 KiB, y as√≠ sucesivamente para cada entrada </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En el mundo real, tal penalizaci√≥n supera a las </font></font><code>ashift=13</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SSD </font><font style="vertical-align: inherit;">Samsung EVO, por lo que debe actuar </font><font style="vertical-align: inherit;">, pero estas SSD mienten sobre el tama√±o de su sector y, por lo tanto, se establece de forma predeterminada </font></font><code>ashift=9</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Si un administrador de sistema experimentado no cambia esta configuraci√≥n, entonces esta SSD es </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">m√°s lenta que una</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> HDD magn√©tica normal. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A modo de comparaci√≥n, para un tama√±o demasiado grande</font></font><code>ashift</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">pr√°cticamente no hay penalidad. </font><font style="vertical-align: inherit;">No hay una disminuci√≥n real de la productividad, y el aumento del espacio no utilizado es infinitamente peque√±o (o igual a cero con la compresi√≥n habilitada). </font><font style="vertical-align: inherit;">Por lo tanto, recomendamos encarecidamente que incluso las unidades que realmente utilizan sectores de 512 bytes se instalen </font></font><code>ashift=12</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">o incluso </font></font><code>ashift=13</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">que miren con confianza hacia el futuro. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La propiedad se </font></font><code>ashift</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">establece para cada dispositivo virtual vdev, y </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">no para el grupo</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , como muchos piensan err√≥neamente, y no cambia despu√©s de la instalaci√≥n. </font><font style="vertical-align: inherit;">Si accidentalmente derrib√≥ </font></font><code>ashift</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">al agregar un nuevo vdev al grupo, entonces contamin√≥ irrevocablemente este grupo con un dispositivo de bajo rendimiento y, por regla general, no hay otra forma que destruir el grupo y comenzar de nuevo. </font><font style="vertical-align: inherit;">Incluso eliminar vdev no te salvar√° de una configuraci√≥n rota</font></font><code>ashift</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">!</font></font><br>
<br>
<h3>   </h3><br>
<img src="https://habrastorage.org/getpro/habr/post_images/38b/a1e/4a8/38ba1e4a8fa0e255081ed8db259a302f.gif"><br>
<i><font color="gray">      &nbsp;‚Äî     ,   </font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/90d/4cb/a35/90d4cba35ffa5a3e44a7ca5f61d4491b.gif"><br>
<i><font color="gray">         ,     </font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/c8b/2af/ffb/c8b2afffbc46f63f6a7fe1167edf5dcb.gif"><br>
<i><font color="gray">  ,      ,   ¬´ ¬ª   ¬´ ¬ª,        </font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/4c1/e2b/818/4c1e2b818077cb07d651527e214363fe.gif"><br>
<i><font color="gray">     ,       ‚Äî      ,     ,       </font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Copy on Write (CoW) es la base fundamental de lo que hace que ZFS sea tan incre√≠ble. El concepto b√°sico es simple: si le pide al sistema de archivos tradicional que modifique el archivo, har√° exactamente lo que solicit√≥. Si le pide al sistema de archivos que copie durante la grabaci√≥n que haga lo mismo, dir√° "bueno", pero le mentir√°. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En cambio, el sistema de archivos de copia-escritura escribe la nueva versi√≥n del bloque modificado, y luego actualiza los metadatos del archivo para romper la conexi√≥n con el bloque anterior y asociar el nuevo bloque que acaba de escribir.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La desconexi√≥n de la unidad anterior y la conexi√≥n de la nueva se realiza en una sola operaci√≥n, por lo que no se puede interrumpir: si restablece la alimentaci√≥n despu√©s de que esto suceda, tiene una nueva versi√≥n del archivo, y si restablece la alimentaci√≥n antes, entonces tiene la versi√≥n anterior. </font><font style="vertical-align: inherit;">En cualquier caso, no habr√° conflicto en el sistema de archivos. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La copia al escribir en ZFS se lleva a cabo no solo en el nivel del sistema de archivos, sino tambi√©n en el nivel de administraci√≥n del disco. </font><font style="vertical-align: inherit;">Esto significa que ZFS no est√° sujeto a un espacio en el registro (un </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">agujero en el RAID</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ), un fen√≥meno cuando la tira solo logr√≥ grabar parcialmente antes de que el sistema se bloqueara, con la matriz da√±ada despu√©s de un reinicio. </font><font style="vertical-align: inherit;">Aqu√≠ la tira es at√≥mica, vdev siempre es consistente y </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bob es tu t√≠o</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ZIL: Registro de intenciones de ZFS</font></font></h3><br>
<img src="https://habrastorage.org/getpro/habr/post_images/567/71c/73f/56771c73f9a28ebaed161e02313deadb.png"><br>
<i><font color="gray"> ZFS     &nbsp;‚Äî  ,      ZIL,            </font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/cec/7c5/437/cec7c5437087f6816f9cdea5f6829820.png"><br>
<i><font color="gray"> ,   ZIL,    .      </font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/075/65a/d6b/07565ad6b2f431db3e6bc20cd24a653b.png"><br>
<i><font color="gray">SLOG,   LOG-, ‚Äî   &nbsp;‚Äî , ,  &nbsp;‚Äî&nbsp;vdev,  ZIL      </font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/927/0f7/539/9270f7539b759aa37896d41e04c4ec47.png"><br>
<i><font color="gray">      ZIL &nbsp;‚Äî    ZIL   SLOG,      </font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Hay dos categor√≠as principales de operaciones de escritura: s√≠ncrono (sincronizaci√≥n) y as√≠ncrono (as√≠ncrono). Para la mayor√≠a de las cargas de trabajo, la gran mayor√≠a de las operaciones de escritura son as√≠ncronas: el sistema de archivos le permite agregarlas y entregarlas en lotes, reduciendo la fragmentaci√≥n y aumentando significativamente el rendimiento. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Las grabaciones sincr√≥nicas son un asunto completamente diferente. Cuando una aplicaci√≥n solicita una escritura s√≠ncrona, le dice al sistema de archivos: "Debe confirmar esto en la memoria no vol√°til en </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">este momento</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , y hasta entonces no puedo hacer nada m√°s". Por lo tanto, las grabaciones s√≠ncronas deben enviarse inmediatamente al disco, y si eso aumenta la fragmentaci√≥n o reduce el ancho de banda, entonces que as√≠ sea.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ZFS trata los registros s√≠ncronos de manera diferente a los sistemas de archivos normales; en lugar de cargarlos inmediatamente en el almacenamiento normal, ZFS los registra en un √°rea de almacenamiento especial llamada registro de intenci√≥n ZFS: el registro de intenci√≥n ZFS o ZIL. El truco es que estos registros </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">tambi√©n</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> permanecen en la memoria, que se agregan junto con las solicitudes de escritura asincr√≥nicas regulares, para luego ser volcados al almacenamiento como TXG perfectamente normales (Grupos de transacciones, Grupos de transacciones). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En funcionamiento normal, ZIL se graba y nunca se vuelve a leer. Cuando, despu√©s de unos momentos, las grabaciones de ZIL se arreglan en el almacenamiento principal en TXG ordinario de RAM, se desconectan de ZIL. Lo √∫nico cuando se lee algo de ZIL es al importar el grupo.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Si ZFS falla (el sistema operativo falla o se corta la corriente) cuando hay datos en ZIL, estos datos se leer√°n durante la pr√≥xima importaci√≥n del grupo (por ejemplo, cuando se reinicia el sistema de emergencia). Todo lo que est√° en el ZIL se leer√°, se combinar√° en grupos TXG, se comprometer√° con el almacenamiento principal y luego se desconectar√° del ZIL durante el proceso de importaci√≥n. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Una de las clases de ayuda vdev se llama LOG o SLOG, el dispositivo LOG secundario. Tiene una tarea: proporcionar al grupo un dispositivo vdev separado y, preferiblemente, mucho m√°s r√°pido, con una resistencia de escritura muy alta para almacenar ZIL, en lugar de almacenar ZIL en el almacenamiento vdev principal. ZIL se comporta igual independientemente de la ubicaci√≥n de almacenamiento, pero si vdev con LOG tiene un rendimiento de escritura muy alto, las escrituras sincr√≥nicas ser√°n m√°s r√°pidas.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Agregar vdev con LOG al grupo </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">no puede</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> mejorar el rendimiento de escritura as√≠ncrona, incluso si fuerza todas las escrituras a ZIL usando </font></font><code>zfs set sync=always</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, todav√≠a estar√°n vinculadas al repositorio principal en TXG de la misma manera y al mismo ritmo que sin un registro. </font><font style="vertical-align: inherit;">La √∫nica mejora directa del rendimiento es el retraso en la grabaci√≥n s√≠ncrona (ya que una mayor velocidad de registro acelera las operaciones </font></font><code>sync</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Sin embargo, en un entorno que ya requiere una gran cantidad de escrituras sincr√≥nicas, vdev LOG puede acelerar indirectamente las escrituras asincr√≥nicas y las lecturas no almacenadas en cach√©. </font><font style="vertical-align: inherit;">Cargar registros ZIL en un registro vdev separado significa menos competencia por IOPS en el almacenamiento primario, lo que en cierta medida mejora el rendimiento de todas las operaciones de lectura y escritura.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Instant√°neas</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El mecanismo de copia de escritura tambi√©n es una base esencial para las instant√°neas at√≥micas de ZFS y la replicaci√≥n as√≠ncrona incremental. </font><font style="vertical-align: inherit;">El sistema de archivos activo tiene un √°rbol de punteros que marca todos los registros con datos actuales; cuando toma una instant√°nea, simplemente hace una copia de este √°rbol de punteros. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Cuando se sobrescribe un registro en el sistema de archivos activo, ZFS primero escribe la nueva versi√≥n del bloque en el espacio no utilizado. </font><font style="vertical-align: inherit;">Luego, separa la versi√≥n anterior del bloque del sistema de archivos actual. </font><font style="vertical-align: inherit;">Pero si alguna instant√°nea se refiere al bloque anterior, a√∫n permanece sin cambios. </font><font style="vertical-align: inherit;">¬°El viejo bloque no se restaurar√° en realidad como espacio libre hasta que se destruyan todas las instant√°neas que enlazan con este bloque!</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Replicaci√≥n</font></font></h3><br>
<img src="https://habrastorage.org/getpro/habr/post_images/e69/167/01d/e6916701d4aa3ff27bb42efc43be60da.png"><br>
<i><font color="gray">  Steam  2015   158&nbsp;   126&nbsp;927 .        rsync&nbsp;‚Äî  ZFS    ¬´ ¬ª  750% .</font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/25f/376/0ab/25f3760ab64d6647571b9c02804b39f0.png"><br>
<i><font color="gray">      40-     Windows 7&nbsp;‚Äî   .  ZFS   289  ,  rsync&nbsp;‚Äî  ¬´¬ª  161  ,    ,   rsync   --inplace.</font></i><br>
<br>
<img src="https://habrastorage.org/getpro/habr/post_images/776/46b/a3a/77646ba3ac20eeb0933d7dc7d644296c.png"><br>
<i><font color="gray">    ,  rsync    .  1,9         &nbsp;‚Äî    ,   ZFS   1148  ,  rsync,    rsync --inplace</font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Una vez que comprenda c√≥mo funcionan las instant√°neas, es f√°cil comprender la esencia de la replicaci√≥n. Dado que una instant√°nea es solo un √°rbol de punteros a los registros, se deduce que si hacemos una </font></font><code>zfs send</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">instant√°nea, enviamos este √°rbol y todos los registros asociados con √©l. Cuando se pasa esta </font></font><code>zfs send</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">en </font></font><code>zfs receive</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">que el objeto de destino, escribe tanto en el contenido real del bloque y el √°rbol de punteros que hacen referencia a los bloques para el conjunto de datos de destino. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Todo se vuelve a√∫n m√°s interesante en el segundo </font></font><code>zfs send</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Ahora tenemos dos sistemas, cada uno de los cuales contiene </font></font><code>poolname/datasetname@1</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, y usted toma una nueva instant√°nea </font></font><code>poolname/datasetname@2</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Por lo tanto, en el grupo de origen que tiene </font></font><code>datasetname@1</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">y </font></font><code>datasetname@2</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, y en el grupo de destino hasta ahora, solo la primera instant√°nea </font></font><code>datasetname@1</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dado que tenemos una instant√°nea com√∫n entre el origen y el destino</font></font><code>datasetname@1</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, podemos hacer </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">incrementalmente</font></font></i> <code>zfs send</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> encima. Cuando le decimos al sistema </font></font><code>zfs send -i poolname/datasetname@1 poolname/datasetname@2</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, compara dos √°rboles de punteros. Los punteros que existen solo en </font></font><code>@2</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, obviamente, se refieren a nuevos bloques, por lo que necesitamos el contenido de estos bloques. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En un sistema remoto, el procesamiento incremental es </font></font><code>send</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">igual de simple. Primero, registramos todas las nuevas entradas incluidas en la secuencia </font></font><code>send</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">y luego agregamos punteros a estos bloques. Voila, en nuestro </font></font><code>@2</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">nuevo sistema! </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La replicaci√≥n incremental asincr√≥nica de ZFS es una gran mejora con respecto a los m√©todos anteriores que no son instant√°neas como rsync. En ambos casos, solo se transmiten los datos modificados, pero rsync debe </font><i><font style="vertical-align: inherit;">leer</font></i><font style="vertical-align: inherit;"> primero</font></font><i><font style="vertical-align: inherit;"></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">desde el disco todos los datos en ambos lados para verificar la cantidad y compararla. </font><font style="vertical-align: inherit;">Por el contrario, la replicaci√≥n de ZFS no lee m√°s que √°rboles de punteros y cualquier bloque que no est√© representado en la instant√°nea general.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Compresi√≥n en l√≠nea</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El mecanismo de copia en escritura tambi√©n simplifica el sistema de compresi√≥n incorporado. En un sistema de archivos tradicional, la compresi√≥n es problem√°tica: tanto la versi√≥n anterior como la nueva versi√≥n de los datos modificados est√°n en el mismo espacio. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Si considera un dato en el medio de un archivo que comienza su vida como un megabyte de ceros desde 0x00000000 y as√≠ sucesivamente, es muy f√°cil comprimirlo en un sector del disco. Pero, ¬øqu√© sucede si reemplazamos este megabyte de ceros con un megabyte de datos incompresibles como JPEG o ruido pseudoaleatorio? De repente, este megabyte de datos requerir√° no uno, sino 256 sectores de 4 KiB, y en este lugar en el disco solo se reserva un sector.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ZFS no tiene ese problema, ya que los registros cambiados siempre se escriben en el espacio no utilizado: el bloque original ocupa solo un sector de 4 KiB, y un nuevo registro tomar√° 256, pero esto no es un problema: un fragmento recientemente cambiado desde el "medio" del archivo se escribir√≠a en el espacio no utilizado independientemente de si su tama√±o ha cambiado o no, por lo que para ZFS esta es una situaci√≥n normal. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La compresi√≥n ZFS incorporada est√° deshabilitada de manera predeterminada, y el sistema ofrece algoritmos de complemento, ahora entre ellos est√°n LZ4, gzip (1-9), LZJB y ZLE.</font></font><br>
<br>
<ul>
<li><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">LZ4</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> es un algoritmo de transmisi√≥n que ofrece compresi√≥n y descompresi√≥n extremadamente r√°pidas y ganancias de rendimiento para la mayor√≠a de los casos de uso, incluso en CPU bastante lentas.</font></font><br>
</li>
<li><b>GZIP</b> ‚Äî  ,       Unix-.        1-9,       CPU      9.       (   )  ,    &nbsp;   c CPU&nbsp;‚Äî    ,     .<br>
</li>
<li><b>LZJB</b> ‚Äî    ZFS.       , LZ4     .<br>
</li>
<li><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ZLE</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : codificaci√≥n de nivel cero, codificaci√≥n de nivel cero. </font><font style="vertical-align: inherit;">No toca los datos normales en absoluto, pero comprime grandes secuencias de ceros. </font><font style="vertical-align: inherit;">√ötil para conjuntos de datos completamente incompresibles (por ejemplo, JPEG, MP4 u otros formatos ya comprimidos), ya que ignora los datos incompresibles, pero comprime el espacio no utilizado en los registros resultantes.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Recomendamos la compresi√≥n LZ4 para casi todos los casos de uso; </font><font style="vertical-align: inherit;">La penalizaci√≥n de rendimiento por encontrar datos incompresibles es muy peque√±a, y la </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ganancia de</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> rendimiento para datos t√≠picos es significativa. </font><font style="vertical-align: inherit;">Copiar una imagen de m√°quina virtual para una nueva instalaci√≥n del sistema operativo Windows (sistema operativo reci√©n instalado, sin datos a√∫n) </font></font><code>compression=lz4</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">pasado </font><font style="vertical-align: inherit;">con un </font><font style="vertical-align: inherit;">27% m√°s r√°pido que con </font></font><code>compression=none</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, en </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">esta prueba de 2015</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ARC - cach√© de reemplazo adaptativo</font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
ZFS es el √∫nico sistema de archivos moderno conocido por nosotros que utiliza su propio mecanismo de almacenamiento en cach√© de lectura y no se basa en la memoria cach√© de la p√°gina del sistema operativo para almacenar copias de los bloques le√≠dos recientemente en la RAM. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Aunque su propio cach√© no est√° exento de problemas, ZFS no puede responder a las nuevas solicitudes de asignaci√≥n de memoria tan r√°pido como el n√∫cleo, por lo que una nueva llamada de </font></font><code>malloc()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">asignaci√≥n de memoria puede fallar si necesita RAM ocupada actualmente por ARC. </font><font style="vertical-align: inherit;">Pero hay buenas razones para usar su propio cach√©, al menos por ahora.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Todos los sistemas operativos modernos conocidos, incluidos MacOS, Windows, Linux y BSD, utilizan el algoritmo LRU (menos utilizado recientemente) para implementar el cach√© de la p√°gina. Este es un algoritmo primitivo que eleva el bloque en cach√© "hacia arriba en la cola" despu√©s de cada lectura y empuja hacia arriba los bloques en "cola hacia abajo" seg√∫n sea necesario para agregar nuevos errores de cach√© (bloques que deber√≠an haberse le√≠do desde el disco, no desde el cach√©) hacia arriba. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Por lo general, el algoritmo funciona bien, pero en sistemas con grandes conjuntos de datos de trabajo, LRU conduce f√°cilmente a la agitaci√≥n, desplazando los bloques que se necesitan con frecuencia para dejar espacio para los bloques que nunca se volver√°n a leer del cach√©. </font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ARCO</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">&nbsp;- un algoritmo mucho menos ingenuo, que puede considerarse como un cach√© "ponderado". Despu√©s de cada lectura del bloque en cach√©, se vuelve un poco "m√°s pesado" y se hace m√°s dif√≠cil desplazarse, e incluso despu√©s de desplazarse, el bloque se </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">rastrea</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> durante un cierto per√≠odo de tiempo. Un bloque que se ha exprimido pero que debe volver a leerse en la cach√© tambi√©n se volver√° "m√°s pesado".</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El resultado final de todo esto es una memoria cach√© con una proporci√≥n de aciertos mucho mayor: la relaci√≥n entre aciertos en la memoria cach√© (le√≠da de la memoria cach√©) y errores (lectura desde el disco). </font><font style="vertical-align: inherit;">Estas son estad√≠sticas extremadamente importantes: no solo los golpes de cach√© en s√≠ mismos sirven √≥rdenes de magnitud m√°s r√°pido, sino que los errores de cach√© tambi√©n se pueden servir m√°s r√°pido, porque cuantos m√°s golpes de cach√©, menos solicitudes de disco concurrentes y menos demora para los errores restantes que deber√≠an servirse conducir.</font></font><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Conclusi√≥n</font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Despu√©s de estudiar la sem√°ntica b√°sica de ZFS, c√≥mo funciona la copia al escribir, as√≠ como las relaciones entre grupos de almacenamiento, dispositivos virtuales, bloques, sectores y archivos, estamos listos para discutir el rendimiento real con n√∫meros reales. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En la siguiente parte, veremos el rendimiento real de los grupos con vdev reflejado y RAIDz, en comparaci√≥n entre s√≠, as√≠ como en comparaci√≥n con las topolog√≠as RAID de kernel de Linux tradicionales, que examinamos </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">anteriormente</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Al principio, quer√≠amos considerar solo lo b√°sico, las topolog√≠as ZFS en s√≠ mismas, pero despu√©s de </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">esto</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , estaremos listos para hablar sobre ajustes y ajustes ZFS m√°s avanzados, incluido el uso de tipos de vdev auxiliares como L2ARC, SLOG y Special Allocation.</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es504680/index.html">Descripci√≥n general de la biblioteca NLP de SpaL</a></li>
<li><a href="../es504682/index.html">Mensaje de Nostalgia: j2me, Gravity Defied, 64kb</a></li>
<li><a href="../es504686/index.html">Como dibujar un gato</a></li>
<li><a href="../es504688/index.html">Las m√°scaras son in√∫tiles: cr√≠tica cient√≠fica de la pol√≠tica social en KOVID-19</a></li>
<li><a href="../es504690/index.html">La historia de c√≥mo configur√© Azure AD B2C en React and React Native Part 3 (Tutorial)</a></li>
<li><a href="../es504694/index.html">C√≥mo compilar un decorador: C ++, Python y su propia implementaci√≥n. Parte 1</a></li>
<li><a href="../es504696/index.html">Noticias del mundo de OpenStreetMap No. 513 (12.05.2020-18.05.2020)</a></li>
<li><a href="../es504698/index.html">Incorporaci√≥n en un sitio remoto</a></li>
<li><a href="../es504700/index.html">Tableta gr√°fica sovi√©tica "bosquejo"</a></li>
<li><a href="../es504702/index.html">La gente no quiere saber ingl√©s</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>