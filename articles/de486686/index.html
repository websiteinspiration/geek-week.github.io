<!doctype html>
<html class="no-js" lang="de">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üõÖ ‚è© ü§∞üèº Informationen zum Implementieren einer Deep-Learning-Bibliothek in Python ‚òëÔ∏è ü¶ê üôÜüèæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Deep-Learning-Technologien haben in kurzer Zeit einen langen Weg zur√ºckgelegt - von einfachen neuronalen Netzen bis hin zu ziemlich komplexen Architek...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Informationen zum Implementieren einer Deep-Learning-Bibliothek in Python</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ruvds/blog/486686/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Deep-Learning-Technologien haben in kurzer Zeit einen langen Weg zur√ºckgelegt - von einfachen neuronalen Netzen bis hin zu ziemlich komplexen Architekturen. Um die rasche Verbreitung dieser Technologien zu unterst√ºtzen, wurden verschiedene Bibliotheken und Deep-Learning-Plattformen entwickelt. Eines der Hauptziele solcher Bibliotheken ist es, Entwicklern einfache Schnittstellen zum Erstellen und Trainieren neuronaler Netzwerkmodelle bereitzustellen. Mit solchen Bibliotheken k√∂nnen ihre Benutzer den zu l√∂senden Aufgaben mehr Aufmerksamkeit schenken und nicht den Feinheiten der Modellimplementierung. Dazu m√ºssen Sie m√∂glicherweise die Implementierung grundlegender Mechanismen hinter mehreren Abstraktionsebenen verbergen. Dies erschwert wiederum das Verst√§ndnis der Grundprinzipien, auf denen Deep-Learning-Bibliotheken basieren.</font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/webt/bp/yi/sl/bpyislfb1o7e-qh7exvklh1oxuw.jpeg"></a><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Der Artikel, dessen √úbersetzung wir ver√∂ffentlichen, zielt darauf ab, die Merkmale des Ger√§ts von Low-Level-Bausteinen von Deep-Learning-Bibliotheken zu analysieren. </font><font style="vertical-align: inherit;">Zun√§chst sprechen wir kurz √ºber die Essenz des tiefen Lernens. </font><font style="vertical-align: inherit;">Dadurch k√∂nnen wir die funktionalen Anforderungen f√ºr die jeweilige Software verstehen. </font><font style="vertical-align: inherit;">Anschlie√üend betrachten wir die Entwicklung einer einfachen, aber funktionierenden Deep-Learning-Bibliothek in Python mit NumPy. </font><font style="vertical-align: inherit;">Diese Bibliothek bietet End-to-End-Schulungen f√ºr einfache neuronale Netzwerkmodelle. </font><font style="vertical-align: inherit;">Auf dem Weg werden wir √ºber die verschiedenen Komponenten von Deep-Learning-Frameworks sprechen. </font><font style="vertical-align: inherit;">Die Bibliothek, die wir in Betracht ziehen werden, ist ziemlich klein, weniger als 100 Codezeilen. </font><font style="vertical-align: inherit;">Und das bedeutet, dass es ganz einfach sein wird, es herauszufinden. </font><font style="vertical-align: inherit;">Den vollst√§ndigen Projektcode, mit dem wir uns befassen werden, finden Sie </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">hier</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<a name="habracut"></a><br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Allgemeine Information</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In der Regel bestehen Deep-Learning-Bibliotheken (wie TensorFlow und PyTorch) aus den in der folgenden Abbildung gezeigten Komponenten.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c49/b9f/396/c49b9f39652c0260a9e30ee4e5dea146.jpg"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Komponenten des Deep-Learning-Frameworks</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Lassen Sie uns diese Komponenten analysieren.</font></font><br>
<br>
<h3><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ñç Bediener</font></font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die Konzepte ‚ÄûOperator‚Äú und ‚ÄûSchicht‚Äú (Schicht) werden normalerweise synonym verwendet. </font><font style="vertical-align: inherit;">Dies sind die Grundbausteine ‚Äã‚Äãeines jeden neuronalen Netzwerks. </font><font style="vertical-align: inherit;">Operatoren sind Vektorfunktionen, die Daten transformieren. </font><font style="vertical-align: inherit;">Unter den h√§ufig verwendeten Operatoren kann man beispielsweise lineare und Faltungsschichten, Unterabtastschichten (Pooling), halblineare (ReLU) und Sigmoid- (Sigmoid) Aktivierungsfunktionen unterscheiden.</font></font><br>
<br>
<h3><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ñçOptimizer (Optimierer)</font></font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Optimierer sind die Grundlage f√ºr Deep-Learning-Bibliotheken. </font><font style="vertical-align: inherit;">Sie beschreiben Methoden zur Anpassung von Modellparametern anhand bestimmter Kriterien und unter Ber√ºcksichtigung des Ziels der Optimierung. </font><font style="vertical-align: inherit;">Unter den bekannten Optimierern sind SGD, RMSProp und Adam zu nennen.</font></font><br>
<br>
<h3><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ñç Verlustfunktionen</font></font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Verlustfunktionen sind analytische und differenzierbare mathematische Ausdr√ºcke, die als Ersatz f√ºr das Ziel der Optimierung bei der L√∂sung eines Problems verwendet werden. </font><font style="vertical-align: inherit;">Beispielsweise werden die Kreuzentropiefunktion und die st√ºckweise lineare Funktion normalerweise bei Klassifizierungsproblemen verwendet.</font></font><br>
<br>
<h3><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ñç Initialisierer</font></font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Initialisierer liefern Anfangswerte f√ºr Modellparameter. </font><font style="vertical-align: inherit;">Es sind diese Werte, die die Parameter zu Beginn des Trainings haben. </font><font style="vertical-align: inherit;">Initialisierer spielen eine wichtige Rolle beim Training neuronaler Netze, da erfolglose Anfangsparameter bedeuten k√∂nnen, dass das Netzwerk langsam oder gar nicht lernt. </font><font style="vertical-align: inherit;">Es gibt viele M√∂glichkeiten, die Gewichte eines neuronalen Netzwerks zu initialisieren. </font><font style="vertical-align: inherit;">Zum Beispiel k√∂nnen Sie ihnen kleine Zufallswerte aus der Normalverteilung zuweisen. </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Auf dieser</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Seite erfahren Sie mehr √ºber die verschiedenen Arten von Initialisierern.</font></font><br>
<br>
<h3><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ñç Regularisierer</font></font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Regularizer sind Tools, die eine Umschulung des Netzwerks vermeiden und dem Netzwerk helfen, sich zu verallgemeinern. Sie k√∂nnen das Netzwerk explizit oder implizit neu trainieren. Explizite Methoden beinhalten strukturelle Einschr√§nkungen der Gewichte. Zum Beispiel durch Minimierung ihrer L1-Norm und L2-Norm, wodurch die Gewichtswerte dementsprechend besser verteilt und gleichm√§√üiger verteilt werden. Implizite Methoden werden von spezialisierten Operatoren dargestellt, die die Transformation von Zwischendarstellungen durchf√ºhren. Dies erfolgt entweder durch explizite Normalisierung, beispielsweise mithilfe der Paketnormalisierungstechnik (BatchNorm), oder durch √Ñndern der Netzwerkkonnektivit√§t mithilfe der DropOut- und DropConnect-Algorithmen.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die oben genannten Komponenten geh√∂ren normalerweise zum Schnittstellenteil der Bibliothek. </font><font style="vertical-align: inherit;">Mit "Schnittstellenteil" meine ich hier die Entit√§ten, mit denen der Benutzer interagieren kann. </font><font style="vertical-align: inherit;">Sie geben ihm praktische Werkzeuge zum effizienten Entwerfen einer neuronalen Netzwerkarchitektur. </font><font style="vertical-align: inherit;">Wenn wir √ºber die internen Mechanismen von Bibliotheken sprechen, k√∂nnen sie die automatische Berechnung von Gradienten der Verlustfunktion unter Ber√ºcksichtigung verschiedener Parameter des Modells unterst√ºtzen. </font><font style="vertical-align: inherit;">Diese Technik wird allgemein als automatische Differenzierung (AD) bezeichnet.</font></font><br>
<br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Automatische Differenzierung</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Jede Deep-Learning-Bibliothek bietet dem Benutzer einige automatische Differenzierungsfunktionen. Dies gibt ihm die M√∂glichkeit, sich auf die Beschreibung der Struktur des Modells (Berechnungsdiagramm) zu konzentrieren und die Aufgabe der Berechnung der Gradienten auf das AD-Modul zu √ºbertragen. Nehmen wir ein Beispiel, das uns wissen l√§sst, wie alles funktioniert. Angenommen, wir m√∂chten die partiellen Ableitungen der folgenden Funktion in Bezug auf ihre Eingangsvariablen X‚ÇÅ und X‚ÇÇ berechnen: </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Y = sin (x‚ÇÅ) + X‚ÇÅ * X‚ÇÇ </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die folgende Abbildung, die ich hier entlehnt </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">habe</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , zeigt das Diagramm der Berechnungen und die Berechnung der Ableitungen unter Verwendung einer Kettenregel.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/605/635/189/605635189c56a2f87927ec1a0c5b6318.png"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Berechnungsgraph und Berechnung von Ableitungen durch eine Kettenregel</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Was Sie hier sehen, ist so etwas wie ein ‚Äûumgekehrter Modus‚Äú der automatischen Differenzierung. </font><font style="vertical-align: inherit;">Der bekannte Fehlerr√ºckausbreitungsalgorithmus ist ein Sonderfall des obigen Algorithmus f√ºr den Fall, dass die oben befindliche Funktion eine Verlustfunktion ist. </font><font style="vertical-align: inherit;">AD nutzt die Tatsache aus, dass jede komplexe Funktion aus elementaren arithmetischen Operationen und elementaren Funktionen besteht. </font><font style="vertical-align: inherit;">Infolgedessen k√∂nnen Derivate berechnet werden, indem eine Kettenregel auf diese Operationen angewendet wird.</font></font><br>
<br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Implementierung</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Im vorherigen Abschnitt haben wir die Komponenten untersucht, die f√ºr die Erstellung einer Deep-Learning-Bibliothek erforderlich sind, die f√ºr die Erstellung und das End-to-End-Training neuronaler Netze konzipiert ist. Um das Beispiel nicht zu komplizieren, </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ahme</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ich hier das Entwurfsmuster der </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;">Caffe-</font></a><font style="vertical-align: inherit;"> Bibliothek nach </font><font style="vertical-align: inherit;">. Hier deklarieren wir zwei abstrakte Klassen - </font></font><code>Function</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">und </font></font><code>Optimizer</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Dar√ºber hinaus gibt es eine Klasse </font></font><code>Tensor</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, bei der es sich um eine einfache Struktur handelt, die zwei mehrdimensionale NumPy-Arrays enth√§lt. Einer von ihnen dient zum Speichern von Parameterwerten, der andere zum Speichern ihrer Farbverl√§ufe. Alle Parameter in verschiedenen Ebenen (Operatoren) sind vom Typ </font></font><code>Tensor</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Bevor wir weiter gehen, werfen Sie einen Blick auf die allgemeinen Umrisse der Bibliothek.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d05/98c/068/d0598c068139ecda1f2aacbd9ea5f068.jpg"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">UML-Diagramm</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
der</font><i><font color="#999999"><font style="vertical-align: inherit;"> Bibliothek</font></font></i><font style="vertical-align: inherit;"> Zum Zeitpunkt des Schreibens dieses Materials enth√§lt diese Bibliothek eine Implementierung der linearen Schicht, der ReLU-Aktivierungsfunktion, der SoftMaxLoss-Schicht und des SGD-Optimierers. Als Ergebnis stellt sich heraus, dass die Bibliothek zum Trainieren von Klassifizierungsmodellen verwendet werden kann, die aus vollst√§ndig verbundenen Schichten bestehen und eine nichtlineare Aktivierungsfunktion verwenden. Schauen wir uns nun einige Details zu den abstrakten Klassen an, die wir haben.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Eine abstrakte Klasse</font></font><code>Function</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">bietet eine Schnittstelle f√ºr Operatoren. Hier ist sein Code:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword">class</span>&nbsp; <span class="hljs-title">Function</span>(<span class="hljs-params">object</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self</span>):</span>&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">raise</span> NotImplementedError<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backward</span>(<span class="hljs-params">self</span>):</span>&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">raise</span> NotImplementedError<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getParams</span>(<span class="hljs-params">self</span>):</span>&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span> []</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Alle Operatoren werden durch die Vererbung einer abstrakten Klasse implementiert </font></font><code>Function</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Jeder Betreiber muss eine Implementierung der Methoden </font></font><code>forward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">und </font><font style="vertical-align: inherit;">bereitstellen </font></font><code>backward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Operatoren k√∂nnen eine Implementierung einer optionalen Methode enthalten </font></font><code>getParams()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, die ihre Parameter zur√ºckgibt (falls vorhanden). Die Methode </font></font><code>forward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">empf√§ngt Eingabedaten und gibt das Ergebnis ihrer Transformation durch den Operator zur√ºck. Dar√ºber hinaus l√∂st er die internen Probleme, die f√ºr die Berechnung von Gradienten erforderlich sind. Das Verfahren </font></font><code>backward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">akzeptiert die partiellen Ableitungen der Verlustfunktion in Bezug auf die Ausgaben des Operators und implementiert die Berechnung der partiellen Ableitungen der Verlustfunktion in Bezug auf die Eingabedaten des Operators und die Parameter (falls vorhanden). Beachten Sie, dass die Methode</font></font><code>backward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Im Wesentlichen bietet unsere Bibliothek die M√∂glichkeit, eine automatische Differenzierung durchzuf√ºhren. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Um dies alles anhand eines konkreten Beispiels zu behandeln, werfen wir einen Blick auf die Implementierung der Funktion </font></font><code>Linear</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Linear</span>(<span class="hljs-params">Function</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,in_nodes,out_nodes</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.weights = Tensor((in_nodes,out_nodes))<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.bias&nbsp; &nbsp; = Tensor((<span class="hljs-number">1</span>,out_nodes))<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.type = <span class="hljs-string">'linear'</span><font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self,x</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;output = np.dot(x,self.weights.data)+self.bias.data<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.input = x&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span> output<font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backward</span>(<span class="hljs-params">self,d_y</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.weights.grad += np.dot(self.input.T,d_y)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.bias.grad&nbsp; &nbsp; += np.sum(d_y,axis=<span class="hljs-number">0</span>,keepdims=<span class="hljs-literal">True</span>)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;grad_input &nbsp; &nbsp; &nbsp; &nbsp; = np.dot(d_y,self.weights.data.T)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span> grad_input<font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getParams</span>(<span class="hljs-params">self</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span> [self.weights,self.bias]</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die Methode </font></font><code>forward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">implementiert die Transformation der Ansicht </font></font><code>Y = X*W+b</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">und gibt das Ergebnis zur√ºck. Au√üerdem wird der Eingabewert gespeichert </font></font><code>X</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, da er zur Berechnung der partiellen Ableitung </font></font><code>dY</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">der Verlustfunktion in Bezug auf den Ausgabewert </font></font><code>Y</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">in der Methode ben√∂tigt wird </font></font><code>backward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Methode </font></font><code>backward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">empf√§ngt die partiellen Ableitungen, berechnet in Bezug auf den Eingabewert </font></font><code>X</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">und die Parameter </font></font><code>W</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">und </font></font><code>b</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Dar√ºber hinaus werden die in Bezug auf den Eingabewert berechneten partiellen Ableitungen zur√ºckgegeben </font></font><code>X</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, die auf die vorherige Schicht √ºbertragen werden. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Eine abstrakte Klasse </font></font><code>Optimizer</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">bietet eine Schnittstelle f√ºr Optimierer:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Optimizer</span>(<span class="hljs-params">object</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,parameters</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.parameters = parameters<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">step</span>(<span class="hljs-params">self</span>):</span>&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">raise</span> NotImplementedError<font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">zeroGrad</span>(<span class="hljs-params">self</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> self.parameters:<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;p.grad = <span class="hljs-number">0.</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Alle Optimierer werden durch Erben von der Basisklasse implementiert </font></font><code>Optimizer</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Eine Klasse, die eine bestimmte Optimierung beschreibt, sollte eine Implementierung der Methode bereitstellen </font></font><code>step()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Diese Methode aktualisiert die Modellparameter unter Verwendung ihrer partiellen Ableitungen, die in Bezug auf den optimierten Wert der Verlustfunktion berechnet wurden. </font><font style="vertical-align: inherit;">In der Funktion wird eine Verkn√ºpfung zu verschiedenen Modellparametern bereitgestellt </font></font><code>__init__()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Bitte beachten Sie, dass die universelle Funktionalit√§t zum Zur√ºcksetzen von Gradientenwerten in der Basisklasse selbst implementiert ist. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Um dies alles besser zu verstehen, betrachten wir ein spezielles Beispiel - die Implementierung des SGD-Algorithmus (Stochastic Gradient Descent) mit Unterst√ºtzung f√ºr die Anpassung des Impulses und die Reduzierung von Gewichten:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">SGD</span>(<span class="hljs-params">Optimizer</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,parameters,lr=<span class="hljs-number">.001</span>,weight_decay=<span class="hljs-number">0.0</span>,momentum = <span class="hljs-number">.9</span></span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;super().__init__(parameters)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.lr &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = lr<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.weight_decay = weight_decay<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.momentum &nbsp; &nbsp; = momentum<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.velocity &nbsp; &nbsp; = []<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> parameters:<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.velocity.append(np.zeros_like(p.grad))<font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">step</span>(<span class="hljs-params">self</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> p,v <span class="hljs-keyword">in</span> zip(self.parameters,self.velocity):<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v = self.momentum*v+p.grad+self.weight_decay*p.data<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;p.data=p.data-self.lr*v</code></pre><br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die L√∂sung f√ºr das eigentliche Problem</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Jetzt haben wir alles Notwendige, um das (tiefe) neuronale Netzwerkmodell mithilfe unserer Bibliothek zu trainieren. </font><font style="vertical-align: inherit;">Daf√ºr ben√∂tigen wir folgende Entit√§ten:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Modell: Berechnungsdiagramm.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Daten und Zielwert: Daten f√ºr das Netzwerktraining.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Verlustfunktion: Ersatz f√ºr das Optimierungsziel.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Optimierer: Ein Mechanismus zum Aktualisieren von Modellparametern.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Der folgende Pseudocode beschreibt einen typischen Testzyklus:</font></font><br>
<br>
<pre><code class="python hljs">model <span class="hljs-comment"># </span>
data,target <span class="hljs-comment"># </span>
loss_fn <span class="hljs-comment"># </span>
optim <span class="hljs-comment">#,         </span>
Repeat:<span class="hljs-comment">#   ,    ,     </span>
&nbsp;&nbsp;&nbsp;optim.zeroGrad() <span class="hljs-comment">#    </span>
&nbsp;&nbsp;&nbsp;output = model.forward(data) <span class="hljs-comment">#   </span>
&nbsp;&nbsp;&nbsp;loss &nbsp; = loss_fn(output,target) <span class="hljs-comment"># </span>
&nbsp;&nbsp;&nbsp;grad &nbsp; = loss.backward() <span class="hljs-comment">#      </span>
&nbsp;&nbsp;&nbsp;model.backward(grad) <span class="hljs-comment">#    </span>
&nbsp;&nbsp;&nbsp;optim.step() <span class="hljs-comment">#  </span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Obwohl dies in der Deep-Learning-Bibliothek nicht erforderlich ist, kann es n√ºtzlich sein, die oben genannten Funktionen in eine separate Klasse aufzunehmen. </font><font style="vertical-align: inherit;">Dies erm√∂glicht es uns, beim Erlernen neuer Modelle nicht dieselben Schritte zu wiederholen (diese Idee entspricht der Philosophie der Abstraktion von Frameworks wie </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Keras auf</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> hoher Ebene </font><font style="vertical-align: inherit;">). </font><font style="vertical-align: inherit;">Um dies zu erreichen, deklarieren Sie eine Klasse </font></font><code>Model</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Model</span>():</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.computation_graph = []<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.parameters&nbsp; &nbsp; &nbsp; &nbsp; = []<font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">add</span>(<span class="hljs-params">self,layer</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.computation_graph.append(layer)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.parameters+=layer.getParams()<font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__innitializeNetwork</span>(<span class="hljs-params">self</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> self.computation_graph:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">if</span> f.type==<span class="hljs-string">'linear'</span>:<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;weights,bias = f.getParams()<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;weights.data = <span class="hljs-number">.01</span>*np.random.randn(weights.data.shape[<span class="hljs-number">0</span>],weights.data.shape[<span class="hljs-number">1</span>])<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bias.data&nbsp; &nbsp; = <span class="hljs-number">0.</span><font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fit</span>(<span class="hljs-params">self,data,target,batch_size,num_epochs,optimizer,loss_fn</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loss_history = []<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.__innitializeNetwork()<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;data_gen = DataGenerator(data,target,batch_size)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;itr = <span class="hljs-number">0</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(num_epochs):
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> X,Y <span class="hljs-keyword">in</span> data_gen:<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;optimizer.zeroGrad()<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> self.computation_graph: X=f.forward(X)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loss = loss_fn.forward(X,Y)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;grad = loss_fn.backward()<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> self.computation_graph[::<span class="hljs-number">-1</span>]: grad = f.backward(grad)&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loss_history+=[loss]<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(<span class="hljs-string">"Loss at epoch = {} and iteration = {}: {}"</span>.format(epoch,itr,loss_history[<span class="hljs-number">-1</span>]))<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;itr+=<span class="hljs-number">1</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;optimizer.step()<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span> loss_history<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span>(<span class="hljs-params">self,data</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;X = data<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> self.computation_graph: X = f.forward(X)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span> X</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Diese Klasse enth√§lt die folgenden Funktionen:</font></font><br>
<br>
<ul>
<li>  :  <code>add()</code>   ,    .        <code>computation_graph</code>.</li>
<li> : ,   ,       ,    .</li>
<li> :    <code>fit()</code>       .       ,    .</li>
<li>  :  <code>predict()</code>   ,       ,   .</li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Da diese Klasse nicht der Grundbaustein von Deep-Learning-Systemen ist, habe ich sie in einem separaten Modul implementiert </font></font><code>utilities.py</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Beachten Sie, dass die Methode </font></font><code>fit()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">eine Klasse verwendet, </font></font><code>DataGenerator</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">deren Implementierung sich im selben Modul befindet. </font><font style="vertical-align: inherit;">Diese Klasse ist nur ein Wrapper f√ºr Trainingsdaten und generiert Minipakete f√ºr jede Iteration des Trainings.</font></font><br>
<br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Modelltraining</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Betrachten Sie nun den letzten Code, in dem das neuronale Netzwerkmodell unter Verwendung der oben beschriebenen Bibliothek trainiert wird. </font><font style="vertical-align: inherit;">Ich werde ein mehrschichtiges Netzwerk auf spiralf√∂rmig angeordneten Daten trainieren. </font><font style="vertical-align: inherit;">Ich wurde von </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">dieser</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Ver√∂ffentlichung </font><font style="vertical-align: inherit;">aufgefordert </font><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Code zum Generieren und Visualisieren dieser Daten finden Sie in der Datei </font></font><code>utilities.py</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/895/eb0/856/895eb085662368f9c1d171153a301fb0.png"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Daten mit drei spiralf√∂rmig angeordneten Klassen.&nbsp;</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Die vorherige Abbildung zeigt die Visualisierung der Daten, auf denen das Modell trainiert wird. </font><font style="vertical-align: inherit;">Diese Daten sind nichtlinear trennbar. </font><font style="vertical-align: inherit;">Wir k√∂nnen hoffen, dass ein Netzwerk mit einer verborgenen Schicht nichtlineare Entscheidungsgrenzen korrekt finden kann. </font><font style="vertical-align: inherit;">Wenn Sie alles zusammenstellen, wor√ºber wir gesprochen haben, erhalten Sie das folgende Codefragment, mit dem Sie das Modell trainieren k√∂nnen:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">import</span> dl_numpy <span class="hljs-keyword">as</span> DL
<span class="hljs-keyword">import</span> utilities<font></font>
<font></font>
batch_size&nbsp; &nbsp; &nbsp; &nbsp; = <span class="hljs-number">20</span>
num_epochs&nbsp; &nbsp; &nbsp; &nbsp; = <span class="hljs-number">200</span>
samples_per_class = <span class="hljs-number">100</span>
num_classes &nbsp; &nbsp; &nbsp; = <span class="hljs-number">3</span>
hidden_units&nbsp; &nbsp; &nbsp; = <span class="hljs-number">100</span><font></font>
data,target &nbsp; &nbsp; &nbsp; = utilities.genSpiralData(samples_per_class,num_classes)<font></font>
model &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = utilities.Model()<font></font>
model.add(DL.Linear(<span class="hljs-number">2</span>,hidden_units))<font></font>
model.add(DL.ReLU())<font></font>
model.add(DL.Linear(hidden_units,num_classes))<font></font>
optim &nbsp; = DL.SGD(model.parameters,lr=<span class="hljs-number">1.0</span>,weight_decay=<span class="hljs-number">0.001</span>,momentum=<span class="hljs-number">.9</span>)<font></font>
loss_fn = DL.SoftmaxWithLoss()<font></font>
model.fit(data,target,batch_size,num_epochs,optim,loss_fn)<font></font>
predicted_labels = np.argmax(model.predict(data),axis=<span class="hljs-number">1</span>)<font></font>
accuracy &nbsp; &nbsp; &nbsp; &nbsp; = np.sum(predicted_labels==target)/len(target)<font></font>
print(<span class="hljs-string">"Model Accuracy = {}"</span>.format(accuracy))<font></font>
utilities.plot2DDataWithDecisionBoundary(data,target,model)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das Bild unten zeigt die gleichen Daten und die entscheidenden Grenzen des trainierten Modells.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/502/b0b/84f/502b0b84fee2c960c918494e8d63e33a.png"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Daten- und Entscheidungsgrenzen des trainierten Modells</font></font></font></i><br>
 <br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zusammenfassung</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Angesichts der zunehmenden Komplexit√§t von Deep-Learning-Modellen besteht die Tendenz, die F√§higkeiten der jeweiligen Bibliotheken zu erh√∂hen und die Menge an Code zu erh√∂hen, die zur Implementierung dieser F√§higkeiten erforderlich ist. Die grundlegendste Funktionalit√§t solcher Bibliotheken kann jedoch immer noch in relativ kompakter Form implementiert werden. Obwohl die von uns erstellte Bibliothek f√ºr das End-to-End-Training einfacher Netzwerke verwendet werden kann, ist sie in vielerlei Hinsicht begrenzt. Wir sprechen √ºber Einschr√§nkungen im Bereich der F√§higkeiten, die es erm√∂glichen, Deep-Learning-Frameworks in Bereichen wie Bildverarbeitung, Sprach- und Texterkennung zu verwenden. Damit sind die M√∂glichkeiten solcher Frameworks nat√ºrlich nicht begrenzt. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ich glaube, dass jeder das </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;">Projekt</font></a><font style="vertical-align: inherit;"> teilen kann</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, den Code, den wir hier untersucht haben, und als √úbung darin einzuf√ºhren, was sie darin sehen m√∂chten. </font><font style="vertical-align: inherit;">Hier sind einige Mechanismen, die Sie versuchen k√∂nnen, selbst zu implementieren:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Operatoren: Faltung, Unterabtastung.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Optimierer: Adam, RMSProp.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Regulierungsbeh√∂rden: BatchNorm, DropOut.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ich hoffe, dieses Material hat es Ihnen erm√∂glicht, zumindest aus den Augenwinkeln zu sehen, was in den Eingeweiden von Bibliotheken f√ºr tiefes Lernen geschieht. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Liebe Leser! </font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Welche Deep-Learning-Bibliotheken verwenden Sie?</font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/files/1ba/550/d25/1ba550d25e8846ce8805de564da6aa63.png"></a></div>
      
    </div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de486686/">https://habr.com/ru/post/de486686/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de486676/index.html">Unvermeidlichkeit der FPGA-Penetration in Rechenzentren</a></li>
<li><a href="../de486678/index.html">Quarz in ASP.NET Core</a></li>
<li><a href="../de486680/index.html">ML, VR & Robots (und ein bisschen Cloud)</a></li>
<li><a href="../de486682/index.html">Docker Compose: Mit Makefile vereinfachen</a></li>
<li><a href="../de486684/index.html">Meine Antwort an diejenigen, die glauben, dass der Wert von TDD √ºbertrieben ist</a></li>
<li><a href="../de486688/index.html">Node.js, Tor, Puppenspieler und Cheerio: anonymes Web-Scraping</a></li>
<li><a href="../de486690/index.html">5 Tipps zum Schreiben von Qualit√§tspfeilfunktionen</a></li>
<li><a href="../de486692/index.html">Chrome-Konsolenfunktionen, die Sie m√∂glicherweise noch nie verwendet haben</a></li>
<li><a href="../de486694/index.html">Nachrichten aus der Welt von OpenStreetMap Nr. 496 (14.01.2020 20.01.2020)</a></li>
<li><a href="../de486702/index.html">Digitale Veranstaltungen in Moskau vom 3. bis 9. Februar</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>