<!doctype html>
<html class="no-js" lang="de">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🛅 ⏩ 🤰🏼 Informationen zum Implementieren einer Deep-Learning-Bibliothek in Python ☑️ 🦐 🙆🏾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Deep-Learning-Technologien haben in kurzer Zeit einen langen Weg zurückgelegt - von einfachen neuronalen Netzen bis hin zu ziemlich komplexen Architek...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>Informationen zum Implementieren einer Deep-Learning-Bibliothek in Python</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ruvds/blog/486686/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Deep-Learning-Technologien haben in kurzer Zeit einen langen Weg zurückgelegt - von einfachen neuronalen Netzen bis hin zu ziemlich komplexen Architekturen. Um die rasche Verbreitung dieser Technologien zu unterstützen, wurden verschiedene Bibliotheken und Deep-Learning-Plattformen entwickelt. Eines der Hauptziele solcher Bibliotheken ist es, Entwicklern einfache Schnittstellen zum Erstellen und Trainieren neuronaler Netzwerkmodelle bereitzustellen. Mit solchen Bibliotheken können ihre Benutzer den zu lösenden Aufgaben mehr Aufmerksamkeit schenken und nicht den Feinheiten der Modellimplementierung. Dazu müssen Sie möglicherweise die Implementierung grundlegender Mechanismen hinter mehreren Abstraktionsebenen verbergen. Dies erschwert wiederum das Verständnis der Grundprinzipien, auf denen Deep-Learning-Bibliotheken basieren.</font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/webt/bp/yi/sl/bpyislfb1o7e-qh7exvklh1oxuw.jpeg"></a><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Der Artikel, dessen Übersetzung wir veröffentlichen, zielt darauf ab, die Merkmale des Geräts von Low-Level-Bausteinen von Deep-Learning-Bibliotheken zu analysieren. </font><font style="vertical-align: inherit;">Zunächst sprechen wir kurz über die Essenz des tiefen Lernens. </font><font style="vertical-align: inherit;">Dadurch können wir die funktionalen Anforderungen für die jeweilige Software verstehen. </font><font style="vertical-align: inherit;">Anschließend betrachten wir die Entwicklung einer einfachen, aber funktionierenden Deep-Learning-Bibliothek in Python mit NumPy. </font><font style="vertical-align: inherit;">Diese Bibliothek bietet End-to-End-Schulungen für einfache neuronale Netzwerkmodelle. </font><font style="vertical-align: inherit;">Auf dem Weg werden wir über die verschiedenen Komponenten von Deep-Learning-Frameworks sprechen. </font><font style="vertical-align: inherit;">Die Bibliothek, die wir in Betracht ziehen werden, ist ziemlich klein, weniger als 100 Codezeilen. </font><font style="vertical-align: inherit;">Und das bedeutet, dass es ganz einfach sein wird, es herauszufinden. </font><font style="vertical-align: inherit;">Den vollständigen Projektcode, mit dem wir uns befassen werden, finden Sie </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">hier</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<a name="habracut"></a><br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Allgemeine Information</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In der Regel bestehen Deep-Learning-Bibliotheken (wie TensorFlow und PyTorch) aus den in der folgenden Abbildung gezeigten Komponenten.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c49/b9f/396/c49b9f39652c0260a9e30ee4e5dea146.jpg"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Komponenten des Deep-Learning-Frameworks</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Lassen Sie uns diese Komponenten analysieren.</font></font><br>
<br>
<h3><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">▍ Bediener</font></font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die Konzepte „Operator“ und „Schicht“ (Schicht) werden normalerweise synonym verwendet. </font><font style="vertical-align: inherit;">Dies sind die Grundbausteine ​​eines jeden neuronalen Netzwerks. </font><font style="vertical-align: inherit;">Operatoren sind Vektorfunktionen, die Daten transformieren. </font><font style="vertical-align: inherit;">Unter den häufig verwendeten Operatoren kann man beispielsweise lineare und Faltungsschichten, Unterabtastschichten (Pooling), halblineare (ReLU) und Sigmoid- (Sigmoid) Aktivierungsfunktionen unterscheiden.</font></font><br>
<br>
<h3><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">▍Optimizer (Optimierer)</font></font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Optimierer sind die Grundlage für Deep-Learning-Bibliotheken. </font><font style="vertical-align: inherit;">Sie beschreiben Methoden zur Anpassung von Modellparametern anhand bestimmter Kriterien und unter Berücksichtigung des Ziels der Optimierung. </font><font style="vertical-align: inherit;">Unter den bekannten Optimierern sind SGD, RMSProp und Adam zu nennen.</font></font><br>
<br>
<h3><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">▍ Verlustfunktionen</font></font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Verlustfunktionen sind analytische und differenzierbare mathematische Ausdrücke, die als Ersatz für das Ziel der Optimierung bei der Lösung eines Problems verwendet werden. </font><font style="vertical-align: inherit;">Beispielsweise werden die Kreuzentropiefunktion und die stückweise lineare Funktion normalerweise bei Klassifizierungsproblemen verwendet.</font></font><br>
<br>
<h3><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">▍ Initialisierer</font></font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Initialisierer liefern Anfangswerte für Modellparameter. </font><font style="vertical-align: inherit;">Es sind diese Werte, die die Parameter zu Beginn des Trainings haben. </font><font style="vertical-align: inherit;">Initialisierer spielen eine wichtige Rolle beim Training neuronaler Netze, da erfolglose Anfangsparameter bedeuten können, dass das Netzwerk langsam oder gar nicht lernt. </font><font style="vertical-align: inherit;">Es gibt viele Möglichkeiten, die Gewichte eines neuronalen Netzwerks zu initialisieren. </font><font style="vertical-align: inherit;">Zum Beispiel können Sie ihnen kleine Zufallswerte aus der Normalverteilung zuweisen. </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Auf dieser</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Seite erfahren Sie mehr über die verschiedenen Arten von Initialisierern.</font></font><br>
<br>
<h3><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">▍ Regularisierer</font></font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Regularizer sind Tools, die eine Umschulung des Netzwerks vermeiden und dem Netzwerk helfen, sich zu verallgemeinern. Sie können das Netzwerk explizit oder implizit neu trainieren. Explizite Methoden beinhalten strukturelle Einschränkungen der Gewichte. Zum Beispiel durch Minimierung ihrer L1-Norm und L2-Norm, wodurch die Gewichtswerte dementsprechend besser verteilt und gleichmäßiger verteilt werden. Implizite Methoden werden von spezialisierten Operatoren dargestellt, die die Transformation von Zwischendarstellungen durchführen. Dies erfolgt entweder durch explizite Normalisierung, beispielsweise mithilfe der Paketnormalisierungstechnik (BatchNorm), oder durch Ändern der Netzwerkkonnektivität mithilfe der DropOut- und DropConnect-Algorithmen.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die oben genannten Komponenten gehören normalerweise zum Schnittstellenteil der Bibliothek. </font><font style="vertical-align: inherit;">Mit "Schnittstellenteil" meine ich hier die Entitäten, mit denen der Benutzer interagieren kann. </font><font style="vertical-align: inherit;">Sie geben ihm praktische Werkzeuge zum effizienten Entwerfen einer neuronalen Netzwerkarchitektur. </font><font style="vertical-align: inherit;">Wenn wir über die internen Mechanismen von Bibliotheken sprechen, können sie die automatische Berechnung von Gradienten der Verlustfunktion unter Berücksichtigung verschiedener Parameter des Modells unterstützen. </font><font style="vertical-align: inherit;">Diese Technik wird allgemein als automatische Differenzierung (AD) bezeichnet.</font></font><br>
<br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Automatische Differenzierung</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Jede Deep-Learning-Bibliothek bietet dem Benutzer einige automatische Differenzierungsfunktionen. Dies gibt ihm die Möglichkeit, sich auf die Beschreibung der Struktur des Modells (Berechnungsdiagramm) zu konzentrieren und die Aufgabe der Berechnung der Gradienten auf das AD-Modul zu übertragen. Nehmen wir ein Beispiel, das uns wissen lässt, wie alles funktioniert. Angenommen, wir möchten die partiellen Ableitungen der folgenden Funktion in Bezug auf ihre Eingangsvariablen X₁ und X₂ berechnen: </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Y = sin (x₁) + X₁ * X₂ </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die folgende Abbildung, die ich hier entlehnt </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">habe</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , zeigt das Diagramm der Berechnungen und die Berechnung der Ableitungen unter Verwendung einer Kettenregel.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/605/635/189/605635189c56a2f87927ec1a0c5b6318.png"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Berechnungsgraph und Berechnung von Ableitungen durch eine Kettenregel</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Was Sie hier sehen, ist so etwas wie ein „umgekehrter Modus“ der automatischen Differenzierung. </font><font style="vertical-align: inherit;">Der bekannte Fehlerrückausbreitungsalgorithmus ist ein Sonderfall des obigen Algorithmus für den Fall, dass die oben befindliche Funktion eine Verlustfunktion ist. </font><font style="vertical-align: inherit;">AD nutzt die Tatsache aus, dass jede komplexe Funktion aus elementaren arithmetischen Operationen und elementaren Funktionen besteht. </font><font style="vertical-align: inherit;">Infolgedessen können Derivate berechnet werden, indem eine Kettenregel auf diese Operationen angewendet wird.</font></font><br>
<br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Implementierung</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Im vorherigen Abschnitt haben wir die Komponenten untersucht, die für die Erstellung einer Deep-Learning-Bibliothek erforderlich sind, die für die Erstellung und das End-to-End-Training neuronaler Netze konzipiert ist. Um das Beispiel nicht zu komplizieren, </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ahme</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ich hier das Entwurfsmuster der </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;">Caffe-</font></a><font style="vertical-align: inherit;"> Bibliothek nach </font><font style="vertical-align: inherit;">. Hier deklarieren wir zwei abstrakte Klassen - </font></font><code>Function</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">und </font></font><code>Optimizer</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Darüber hinaus gibt es eine Klasse </font></font><code>Tensor</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, bei der es sich um eine einfache Struktur handelt, die zwei mehrdimensionale NumPy-Arrays enthält. Einer von ihnen dient zum Speichern von Parameterwerten, der andere zum Speichern ihrer Farbverläufe. Alle Parameter in verschiedenen Ebenen (Operatoren) sind vom Typ </font></font><code>Tensor</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Bevor wir weiter gehen, werfen Sie einen Blick auf die allgemeinen Umrisse der Bibliothek.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d05/98c/068/d0598c068139ecda1f2aacbd9ea5f068.jpg"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">UML-Diagramm</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
der</font><i><font color="#999999"><font style="vertical-align: inherit;"> Bibliothek</font></font></i><font style="vertical-align: inherit;"> Zum Zeitpunkt des Schreibens dieses Materials enthält diese Bibliothek eine Implementierung der linearen Schicht, der ReLU-Aktivierungsfunktion, der SoftMaxLoss-Schicht und des SGD-Optimierers. Als Ergebnis stellt sich heraus, dass die Bibliothek zum Trainieren von Klassifizierungsmodellen verwendet werden kann, die aus vollständig verbundenen Schichten bestehen und eine nichtlineare Aktivierungsfunktion verwenden. Schauen wir uns nun einige Details zu den abstrakten Klassen an, die wir haben.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Eine abstrakte Klasse</font></font><code>Function</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">bietet eine Schnittstelle für Operatoren. Hier ist sein Code:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword">class</span>&nbsp; <span class="hljs-title">Function</span>(<span class="hljs-params">object</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self</span>):</span>&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">raise</span> NotImplementedError<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backward</span>(<span class="hljs-params">self</span>):</span>&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">raise</span> NotImplementedError<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getParams</span>(<span class="hljs-params">self</span>):</span>&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span> []</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Alle Operatoren werden durch die Vererbung einer abstrakten Klasse implementiert </font></font><code>Function</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Jeder Betreiber muss eine Implementierung der Methoden </font></font><code>forward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">und </font><font style="vertical-align: inherit;">bereitstellen </font></font><code>backward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Operatoren können eine Implementierung einer optionalen Methode enthalten </font></font><code>getParams()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, die ihre Parameter zurückgibt (falls vorhanden). Die Methode </font></font><code>forward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">empfängt Eingabedaten und gibt das Ergebnis ihrer Transformation durch den Operator zurück. Darüber hinaus löst er die internen Probleme, die für die Berechnung von Gradienten erforderlich sind. Das Verfahren </font></font><code>backward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">akzeptiert die partiellen Ableitungen der Verlustfunktion in Bezug auf die Ausgaben des Operators und implementiert die Berechnung der partiellen Ableitungen der Verlustfunktion in Bezug auf die Eingabedaten des Operators und die Parameter (falls vorhanden). Beachten Sie, dass die Methode</font></font><code>backward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Im Wesentlichen bietet unsere Bibliothek die Möglichkeit, eine automatische Differenzierung durchzuführen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Um dies alles anhand eines konkreten Beispiels zu behandeln, werfen wir einen Blick auf die Implementierung der Funktion </font></font><code>Linear</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Linear</span>(<span class="hljs-params">Function</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,in_nodes,out_nodes</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.weights = Tensor((in_nodes,out_nodes))<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.bias&nbsp; &nbsp; = Tensor((<span class="hljs-number">1</span>,out_nodes))<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.type = <span class="hljs-string">'linear'</span><font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self,x</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;output = np.dot(x,self.weights.data)+self.bias.data<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.input = x&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span> output<font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backward</span>(<span class="hljs-params">self,d_y</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.weights.grad += np.dot(self.input.T,d_y)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.bias.grad&nbsp; &nbsp; += np.sum(d_y,axis=<span class="hljs-number">0</span>,keepdims=<span class="hljs-literal">True</span>)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;grad_input &nbsp; &nbsp; &nbsp; &nbsp; = np.dot(d_y,self.weights.data.T)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span> grad_input<font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getParams</span>(<span class="hljs-params">self</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span> [self.weights,self.bias]</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die Methode </font></font><code>forward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">implementiert die Transformation der Ansicht </font></font><code>Y = X*W+b</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">und gibt das Ergebnis zurück. Außerdem wird der Eingabewert gespeichert </font></font><code>X</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, da er zur Berechnung der partiellen Ableitung </font></font><code>dY</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">der Verlustfunktion in Bezug auf den Ausgabewert </font></font><code>Y</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">in der Methode benötigt wird </font></font><code>backward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Methode </font></font><code>backward()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">empfängt die partiellen Ableitungen, berechnet in Bezug auf den Eingabewert </font></font><code>X</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">und die Parameter </font></font><code>W</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">und </font></font><code>b</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Darüber hinaus werden die in Bezug auf den Eingabewert berechneten partiellen Ableitungen zurückgegeben </font></font><code>X</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, die auf die vorherige Schicht übertragen werden. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Eine abstrakte Klasse </font></font><code>Optimizer</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">bietet eine Schnittstelle für Optimierer:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Optimizer</span>(<span class="hljs-params">object</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,parameters</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.parameters = parameters<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">step</span>(<span class="hljs-params">self</span>):</span>&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">raise</span> NotImplementedError<font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">zeroGrad</span>(<span class="hljs-params">self</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> self.parameters:<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;p.grad = <span class="hljs-number">0.</span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Alle Optimierer werden durch Erben von der Basisklasse implementiert </font></font><code>Optimizer</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Eine Klasse, die eine bestimmte Optimierung beschreibt, sollte eine Implementierung der Methode bereitstellen </font></font><code>step()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Diese Methode aktualisiert die Modellparameter unter Verwendung ihrer partiellen Ableitungen, die in Bezug auf den optimierten Wert der Verlustfunktion berechnet wurden. </font><font style="vertical-align: inherit;">In der Funktion wird eine Verknüpfung zu verschiedenen Modellparametern bereitgestellt </font></font><code>__init__()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Bitte beachten Sie, dass die universelle Funktionalität zum Zurücksetzen von Gradientenwerten in der Basisklasse selbst implementiert ist. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Um dies alles besser zu verstehen, betrachten wir ein spezielles Beispiel - die Implementierung des SGD-Algorithmus (Stochastic Gradient Descent) mit Unterstützung für die Anpassung des Impulses und die Reduzierung von Gewichten:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">SGD</span>(<span class="hljs-params">Optimizer</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,parameters,lr=<span class="hljs-number">.001</span>,weight_decay=<span class="hljs-number">0.0</span>,momentum = <span class="hljs-number">.9</span></span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;super().__init__(parameters)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.lr &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = lr<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.weight_decay = weight_decay<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.momentum &nbsp; &nbsp; = momentum<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.velocity &nbsp; &nbsp; = []<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> parameters:<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.velocity.append(np.zeros_like(p.grad))<font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">step</span>(<span class="hljs-params">self</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> p,v <span class="hljs-keyword">in</span> zip(self.parameters,self.velocity):<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v = self.momentum*v+p.grad+self.weight_decay*p.data<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;p.data=p.data-self.lr*v</code></pre><br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Lösung für das eigentliche Problem</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Jetzt haben wir alles Notwendige, um das (tiefe) neuronale Netzwerkmodell mithilfe unserer Bibliothek zu trainieren. </font><font style="vertical-align: inherit;">Dafür benötigen wir folgende Entitäten:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Modell: Berechnungsdiagramm.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Daten und Zielwert: Daten für das Netzwerktraining.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Verlustfunktion: Ersatz für das Optimierungsziel.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Optimierer: Ein Mechanismus zum Aktualisieren von Modellparametern.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Der folgende Pseudocode beschreibt einen typischen Testzyklus:</font></font><br>
<br>
<pre><code class="python hljs">model <span class="hljs-comment"># </span>
data,target <span class="hljs-comment"># </span>
loss_fn <span class="hljs-comment"># </span>
optim <span class="hljs-comment">#,         </span>
Repeat:<span class="hljs-comment">#   ,    ,     </span>
&nbsp;&nbsp;&nbsp;optim.zeroGrad() <span class="hljs-comment">#    </span>
&nbsp;&nbsp;&nbsp;output = model.forward(data) <span class="hljs-comment">#   </span>
&nbsp;&nbsp;&nbsp;loss &nbsp; = loss_fn(output,target) <span class="hljs-comment"># </span>
&nbsp;&nbsp;&nbsp;grad &nbsp; = loss.backward() <span class="hljs-comment">#      </span>
&nbsp;&nbsp;&nbsp;model.backward(grad) <span class="hljs-comment">#    </span>
&nbsp;&nbsp;&nbsp;optim.step() <span class="hljs-comment">#  </span></code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Obwohl dies in der Deep-Learning-Bibliothek nicht erforderlich ist, kann es nützlich sein, die oben genannten Funktionen in eine separate Klasse aufzunehmen. </font><font style="vertical-align: inherit;">Dies ermöglicht es uns, beim Erlernen neuer Modelle nicht dieselben Schritte zu wiederholen (diese Idee entspricht der Philosophie der Abstraktion von Frameworks wie </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Keras auf</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> hoher Ebene </font><font style="vertical-align: inherit;">). </font><font style="vertical-align: inherit;">Um dies zu erreichen, deklarieren Sie eine Klasse </font></font><code>Model</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Model</span>():</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.computation_graph = []<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.parameters&nbsp; &nbsp; &nbsp; &nbsp; = []<font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">add</span>(<span class="hljs-params">self,layer</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.computation_graph.append(layer)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.parameters+=layer.getParams()<font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__innitializeNetwork</span>(<span class="hljs-params">self</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> self.computation_graph:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">if</span> f.type==<span class="hljs-string">'linear'</span>:<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;weights,bias = f.getParams()<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;weights.data = <span class="hljs-number">.01</span>*np.random.randn(weights.data.shape[<span class="hljs-number">0</span>],weights.data.shape[<span class="hljs-number">1</span>])<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bias.data&nbsp; &nbsp; = <span class="hljs-number">0.</span><font></font>
<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fit</span>(<span class="hljs-params">self,data,target,batch_size,num_epochs,optimizer,loss_fn</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loss_history = []<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.__innitializeNetwork()<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;data_gen = DataGenerator(data,target,batch_size)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;itr = <span class="hljs-number">0</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(num_epochs):
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> X,Y <span class="hljs-keyword">in</span> data_gen:<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;optimizer.zeroGrad()<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> self.computation_graph: X=f.forward(X)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loss = loss_fn.forward(X,Y)<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;grad = loss_fn.backward()<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> self.computation_graph[::<span class="hljs-number">-1</span>]: grad = f.backward(grad)&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loss_history+=[loss]<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(<span class="hljs-string">"Loss at epoch = {} and iteration = {}: {}"</span>.format(epoch,itr,loss_history[<span class="hljs-number">-1</span>]))<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;itr+=<span class="hljs-number">1</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;optimizer.step()<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span> loss_history<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span>(<span class="hljs-params">self,data</span>):</span><font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;X = data<font></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> self.computation_graph: X = f.forward(X)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span> X</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Diese Klasse enthält die folgenden Funktionen:</font></font><br>
<br>
<ul>
<li>  :  <code>add()</code>   ,    .        <code>computation_graph</code>.</li>
<li> : ,   ,       ,    .</li>
<li> :    <code>fit()</code>       .       ,    .</li>
<li>  :  <code>predict()</code>   ,       ,   .</li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Da diese Klasse nicht der Grundbaustein von Deep-Learning-Systemen ist, habe ich sie in einem separaten Modul implementiert </font></font><code>utilities.py</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Beachten Sie, dass die Methode </font></font><code>fit()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">eine Klasse verwendet, </font></font><code>DataGenerator</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">deren Implementierung sich im selben Modul befindet. </font><font style="vertical-align: inherit;">Diese Klasse ist nur ein Wrapper für Trainingsdaten und generiert Minipakete für jede Iteration des Trainings.</font></font><br>
<br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Modelltraining</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Betrachten Sie nun den letzten Code, in dem das neuronale Netzwerkmodell unter Verwendung der oben beschriebenen Bibliothek trainiert wird. </font><font style="vertical-align: inherit;">Ich werde ein mehrschichtiges Netzwerk auf spiralförmig angeordneten Daten trainieren. </font><font style="vertical-align: inherit;">Ich wurde von </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">dieser</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Veröffentlichung </font><font style="vertical-align: inherit;">aufgefordert </font><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Code zum Generieren und Visualisieren dieser Daten finden Sie in der Datei </font></font><code>utilities.py</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/895/eb0/856/895eb085662368f9c1d171153a301fb0.png"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Daten mit drei spiralförmig angeordneten Klassen.&nbsp;</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Die vorherige Abbildung zeigt die Visualisierung der Daten, auf denen das Modell trainiert wird. </font><font style="vertical-align: inherit;">Diese Daten sind nichtlinear trennbar. </font><font style="vertical-align: inherit;">Wir können hoffen, dass ein Netzwerk mit einer verborgenen Schicht nichtlineare Entscheidungsgrenzen korrekt finden kann. </font><font style="vertical-align: inherit;">Wenn Sie alles zusammenstellen, worüber wir gesprochen haben, erhalten Sie das folgende Codefragment, mit dem Sie das Modell trainieren können:</font></font><br>
<br>
<pre><code class="python hljs"><span class="hljs-keyword">import</span> dl_numpy <span class="hljs-keyword">as</span> DL
<span class="hljs-keyword">import</span> utilities<font></font>
<font></font>
batch_size&nbsp; &nbsp; &nbsp; &nbsp; = <span class="hljs-number">20</span>
num_epochs&nbsp; &nbsp; &nbsp; &nbsp; = <span class="hljs-number">200</span>
samples_per_class = <span class="hljs-number">100</span>
num_classes &nbsp; &nbsp; &nbsp; = <span class="hljs-number">3</span>
hidden_units&nbsp; &nbsp; &nbsp; = <span class="hljs-number">100</span><font></font>
data,target &nbsp; &nbsp; &nbsp; = utilities.genSpiralData(samples_per_class,num_classes)<font></font>
model &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = utilities.Model()<font></font>
model.add(DL.Linear(<span class="hljs-number">2</span>,hidden_units))<font></font>
model.add(DL.ReLU())<font></font>
model.add(DL.Linear(hidden_units,num_classes))<font></font>
optim &nbsp; = DL.SGD(model.parameters,lr=<span class="hljs-number">1.0</span>,weight_decay=<span class="hljs-number">0.001</span>,momentum=<span class="hljs-number">.9</span>)<font></font>
loss_fn = DL.SoftmaxWithLoss()<font></font>
model.fit(data,target,batch_size,num_epochs,optim,loss_fn)<font></font>
predicted_labels = np.argmax(model.predict(data),axis=<span class="hljs-number">1</span>)<font></font>
accuracy &nbsp; &nbsp; &nbsp; &nbsp; = np.sum(predicted_labels==target)/len(target)<font></font>
print(<span class="hljs-string">"Model Accuracy = {}"</span>.format(accuracy))<font></font>
utilities.plot2DDataWithDecisionBoundary(data,target,model)</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das Bild unten zeigt die gleichen Daten und die entscheidenden Grenzen des trainierten Modells.</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/502/b0b/84f/502b0b84fee2c960c918494e8d63e33a.png"></div><br>
<i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Daten- und Entscheidungsgrenzen des trainierten Modells</font></font></font></i><br>
 <br>
<h2><font color="#3AC1EF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zusammenfassung</font></font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Angesichts der zunehmenden Komplexität von Deep-Learning-Modellen besteht die Tendenz, die Fähigkeiten der jeweiligen Bibliotheken zu erhöhen und die Menge an Code zu erhöhen, die zur Implementierung dieser Fähigkeiten erforderlich ist. Die grundlegendste Funktionalität solcher Bibliotheken kann jedoch immer noch in relativ kompakter Form implementiert werden. Obwohl die von uns erstellte Bibliothek für das End-to-End-Training einfacher Netzwerke verwendet werden kann, ist sie in vielerlei Hinsicht begrenzt. Wir sprechen über Einschränkungen im Bereich der Fähigkeiten, die es ermöglichen, Deep-Learning-Frameworks in Bereichen wie Bildverarbeitung, Sprach- und Texterkennung zu verwenden. Damit sind die Möglichkeiten solcher Frameworks natürlich nicht begrenzt. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ich glaube, dass jeder das </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;">Projekt</font></a><font style="vertical-align: inherit;"> teilen kann</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, den Code, den wir hier untersucht haben, und als Übung darin einzuführen, was sie darin sehen möchten. </font><font style="vertical-align: inherit;">Hier sind einige Mechanismen, die Sie versuchen können, selbst zu implementieren:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Operatoren: Faltung, Unterabtastung.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Optimierer: Adam, RMSProp.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Regulierungsbehörden: BatchNorm, DropOut.</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Ich hoffe, dieses Material hat es Ihnen ermöglicht, zumindest aus den Augenwinkeln zu sehen, was in den Eingeweiden von Bibliotheken für tiefes Lernen geschieht. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Liebe Leser! </font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Welche Deep-Learning-Bibliotheken verwenden Sie?</font></font><br>
<br>
<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/files/1ba/550/d25/1ba550d25e8846ce8805de564da6aa63.png"></a></div>
      
    </div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de486686/">https://habr.com/ru/post/de486686/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de486676/index.html">Unvermeidlichkeit der FPGA-Penetration in Rechenzentren</a></li>
<li><a href="../de486678/index.html">Quarz in ASP.NET Core</a></li>
<li><a href="../de486680/index.html">ML, VR & Robots (und ein bisschen Cloud)</a></li>
<li><a href="../de486682/index.html">Docker Compose: Mit Makefile vereinfachen</a></li>
<li><a href="../de486684/index.html">Meine Antwort an diejenigen, die glauben, dass der Wert von TDD übertrieben ist</a></li>
<li><a href="../de486688/index.html">Node.js, Tor, Puppenspieler und Cheerio: anonymes Web-Scraping</a></li>
<li><a href="../de486690/index.html">5 Tipps zum Schreiben von Qualitätspfeilfunktionen</a></li>
<li><a href="../de486692/index.html">Chrome-Konsolenfunktionen, die Sie möglicherweise noch nie verwendet haben</a></li>
<li><a href="../de486694/index.html">Nachrichten aus der Welt von OpenStreetMap Nr. 496 (14.01.2020 20.01.2020)</a></li>
<li><a href="../de486702/index.html">Digitale Veranstaltungen in Moskau vom 3. bis 9. Februar</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>