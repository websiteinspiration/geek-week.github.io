<!doctype html>
<html class="no-js" lang="ja">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-13');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🔛 🧒🏻 🤙🏻 PyTorchの紹介：自然言語処理のディープラーニング 👨🏿‍🎨 😒 ☔️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="こんにちは、habrozhiteli！自然言語処理（NLP）は、人工知能の分野で非常に重要なタスクです。実装が成功すると、AmazonのAmazonやGoogle Translateなどの製品が可能になります。この本は、データサイエンティストやNLPソフトウェア開発者向けの主要なツールの1つであるP...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://geek-week.github.io/index.html"></a>
    <div class="page-header-text">Get best of the week</div>
  </header>
  <section class="page js-page"><h1>PyTorchの紹介：自然言語処理のディープラーニング</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/475488/"><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><img src="https://habrastorage.org/webt/gj/ib/ak/gjibakead8idlzldl2vxqq3gmuc.jpeg" align="left" alt="画像"></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">こんにちは、habrozhiteli！自然言語処理（NLP）は、人工知能の分野で非常に重要なタスクです。実装が成功すると、AmazonのAmazonやGoogle Translateなどの製品が可能になります。この本は、データサイエンティストやNLPソフトウェア開発者向けの主要なツールの1つであるPython言語のディープラーニングライブラリであるPyTorchの学習に役立ちます。 Delip RaoとBrian McMahanは、NLPとディープラーニングアルゴリズムを習得します。また、PyTorchを使用して、テキスト分析を使用するアプリケーションを実装する方法を示します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
この本では•計算グラフと教師による学習のパラダイム。</font><font style="vertical-align: inherit;">•テンソルを操作するために最適化されたPyTorchライブラリの基本。</font><font style="vertical-align: inherit;">•従来のNLPの概念と方法の概要。</font><font style="vertical-align: inherit;">•プロアクティブニューラルネットワーク（多層パーセプトロンなど）。</font><font style="vertical-align: inherit;">•長期短期記憶（LSTM）および制御された反復ブロックによるRNNの改善•予測およびシーケンス変換モデル。</font><font style="vertical-align: inherit;">•本番環境で使用されるNLPシステムの設計パターン。</font></font><br>
<a name="habracut"></a><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">抜粋。</font><font style="vertical-align: inherit;">単語のネストとその他のタイプ</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
自然言語でテキストを処理する際の問題を解決する場合、さまざまなタイプの離散データ型を処理する必要があります。最も明白な例は言葉です。もちろんたくさんの単語（辞書）。他の例の中で、シンボル、品詞のラベル、名前付きエンティティ、エンティティの名前付きタイプ、解析に関連する機能、製品カタログ内の位置など。実際、有限（または無限カウント可能）セット。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
NLPでのディープラーニングのアプリケーションの成功の基礎は、密なベクトルの形式での離散データ型（たとえば、単語）の表現です。 「表現学習」および「埋め込み」という用語は、離散データ型からベクトル空間内の点まで表示/表現する学習を意味します。離散型が単語の場合、密なベクトル表現は単語の埋め込みと呼ばれます。発生回数に基づくネスト方法の例、たとえばTF-IDF（「用語頻度はドキュメントの逆頻度」）の例は、第2章ですでに見ています。この章では、トレーニングベースのネスト方法と予測ベースのネスト方法に焦点を当てます（ Baroniらによる記事[Baroni et al。、2014]）、特定の学習タスクの目的関数を最大化することにより、パフォーマンストレーニングが行われます。たとえば、文脈によって単語を予測します。トレーニングベースの投資方法は、その幅広い適用性と高い効率のために現在標準です。実際、NLPタスクへの単語の埋め込みは非常に広く行われているため、「NLPのシラチャ」と呼ばれます。これは、タスクでの使用によりソリューションの効率が向上することが期待できるためです。しかし、このニックネームは少し誤解を招くものです。なぜなら、シラシとは異なり、アタッチメントは通常、事後にモデルに追加されないが、その基本的なコンポーネントだからです。トレーニングベースの投資方法は、その幅広い適用性と高い効率のために現在標準です。実際、NLPタスクへの単語の埋め込みは非常に広く行われているため、「NLPのシラチャ」と呼ばれます。これは、タスクでの使用によりソリューションの効率が向上することが期待できるためです。しかし、このニックネームは少し誤解を招くものです。なぜなら、シラシとは異なり、アタッチメントは通常、事後にモデルに追加されないが、その基本的なコンポーネントだからです。トレーニングベースの投資方法は、その幅広い適用性と高い効率のために現在標準です。実際、NLPタスクへの単語の埋め込みは非常に広く行われているため、「NLPのシラチャ」と呼ばれます。これは、タスクでの使用によりソリューションの効率が向上することが期待できるためです。しかし、このニックネームは少し誤解を招くものです。なぜなら、シラシとは異なり、アタッチメントは通常、事後にモデルに追加されないが、その基本的なコンポーネントだからです。アタッチメントは通常、事後にモデルに追加されませんが、その基本的なコンポーネントを表します。アタッチメントは通常、事後にモデルに追加されませんが、その基本的なコンポーネントを表します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
この章では、単語の埋め込みに関連するベクトル表現について説明します。単語の埋め込み方法、教師の有無にかかわらずタスクを教えるための単語の埋め込み最適化方法、視覚的な埋め込みの視覚化方法、そして文章とドキュメントの単語埋め込みの組み合わせ方法。</font><font style="vertical-align: inherit;">ただし、ここで説明する方法は、任意の離散型に適用されることを忘れないでください。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">なぜ投資トレーニング</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
前の章では、単語のベクトル表現を作成する通常の方法を示しました。つまり、ユニタリ表現-特定の単語に対応する値1を含むものを除いて、すべての位置にゼロがある、辞書のサイズに一致する長さのベクトルの使用方法を学習しました。さらに、出現回数の表現、つまり、対応する位置にある文の単語の出現回数を含むモデルの一意の単語の数に等しい長さのベクトルに出会いました。そのような表現は、意味のある内容/意味がベクトルのいくつかの次元によって反映されるため、分布表現とも呼ばれます。分布表現の歴史は何十年も前にさかのぼります（ファースの記事[ファース、1935]を参照）多くの機械学習モデルやニューラルネットワークに最適です。これらの表現はヒューリスティックに構築され1、データに関するトレーニングは行われません。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
分散表現は、その中の単語がはるかに小さい次元の密なベクトル（たとえば、辞書全体のサイズではなく、d = 100）で表されるため、その名前が付けられました。単語</font></font><img src="https://habrastorage.org/webt/vr/yx/0r/vryx0rlwodoo2krhnfdldvleqaq.png" alt="画像"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">の意味とその他のプロパティは、この高密度のいくつかの次元に分布しています。ベクター。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
トレーニングの結果として得られる低次元の密な表現には、前の章で遭遇した出現回数を含むユニタリーベクトルよりもいくつかの利点があります。まず、次元削減は計算上効率的です。第2に、発生数に基づく表現は、異なる次元で同じ情報を過剰にコーディングした高次元ベクトルにつながり、それらの統計的検出力は大きすぎません。第3に、入力データの次元が大きすぎると、機械学習と最適化で問題が発生する可能性があります。これは、しばしば次元の呪い（</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">http://bit.ly/2CrhQXm）</font></a><font style="vertical-align: inherit;">と呼ばれる現象です。</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">）</font><font style="vertical-align: inherit;">この問題を次元で解決するには、特異値分解（SVD）や主成分分析（PCA）など、次元を削減するさまざまな方法が使用されますが、皮肉なことに、これらのアプローチは数百万の次元（ NLPの典型的なケース）。</font><font style="vertical-align: inherit;">第4に、問題に固有のデータから学習した（またはそれに基づいてフィットした）表現は、この特定のタスクに最適です。</font><font style="vertical-align: inherit;">TF-IDFなどのヒューリスティックアルゴリズムやSVDなどの次元削減方法の場合、ターゲットの最適化関数がこの埋め込み方法を使用する特定のタスクに適しているかどうかは不明です。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">投資効率</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
埋め込みがどのように機能するかを理解するために、図3に示すように、線形レイヤーの重み行列に乗算されるユニタリーベクトルの例を考えます。 5.1。 3章と4章では、ユニタリベクトルのサイズは辞書のサイズと一致しました。ベクトルは、特定の単語に対応する位置に1を含み、その存在を示すため、ユニタリーと呼ばれます。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/or/4q/ag/or4qagtvn_dh4sxqok-q0tcsfmi.png" alt="画像"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
図。 5.1。ユニタリベクトルと線形レイヤーの重みの行列の場合の行列乗算の例。ユニタリベクトルにはすべて0と1つのユニットしか含まれないため、このユニットの位置は、行列を乗算するときに選択演算子の役割を果たします。これは、ウェイトマトリックスのセルと結果のベクトルの暗色化として図に示されています。この検索方法は機能しますが、ユニタリベクトルに重み行列の各数値が乗算され、合計が行で計算されるため、計算リソースを大量に消費し、非効率的です。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
定義により、入力でユニタリベクトルを受け取る線形層の重み行列の行数は、このユニタリベクトルのサイズと等しくなければなりません。図に示すように、行列を掛けるとき5.1、結果のベクトルは、実際にはユニタリベクトルの非ゼロ要素に対応する文字列です。この観察に基づいて、乗算ステップをスキップし、整数値をインデックスとして使用して目的の行を抽出できます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
投資パフォーマンスに関する最後の注意：図の例にもかかわらず </font><font style="vertical-align: inherit;">5.1、重み行列の次元が入力ユニタリベクトルの次元と一致する場合、これは常に当てはまるわけではありません。</font><font style="vertical-align: inherit;">実際、添付ファイルは、ユニタリベクトルを使用する場合や出現回数を表す場合に必要となるよりも低い次元のスペースから単語を表すためによく使用されます。</font><font style="vertical-align: inherit;">科学論文への典型的な投資規模は25から500の測定であり、特定の値の選択は利用可能なGPUメモリの量に制限されます。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">愛着学習アプローチ</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
この章のタスクは、単語への投資の具体的なテクニックを教えることではなく、投資とは何か、それらをどのようにどこに適用できるか、モデルでどのように使用するのが最適か、そしてどのような制限があるかを理解するのに役立ちます。実際には、実際には、単語埋め込みのための新しい学習アルゴリズムを書く必要はほとんどありません。ただし、このサブセクションでは、そのようなトレーニングに対する最新のアプローチの概要を示します。単語をネストするすべての方法のトレーニングは、単語（つまり、マークされていないデータ）のみを使用して行われますが、教師が行います。これは、補助問題を解決するために最適化された表現がテキスト本文の多くの統計的および言語的特性を取り込む必要があるという考慮から、データが暗黙的にマークされる教師による補助教育タスクの作成により可能になります。少なくともいくらかの利益をもたらすために。このようなヘルパータスクの例をいくつか示します。</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">与えられた単語列の次の単語を予測します。</font><font style="vertical-align: inherit;">また、言語モデリングの問題の名前も付いています。</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">前後の単語から欠落している単語を予測します。</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">指定された単語について、位置に関係なく、特定のウィンドウ内の単語を予測します。</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
もちろん、このリストは完全ではなく、補助問題の選択は、アルゴリズム開発者の直感と計算コストに依存します。</font><font style="vertical-align: inherit;">例としては、GloVe、Continuous Bag-of-Words（CBOW）、スキップグラムなどがあります。詳細は、Goldbergの本（Goldberg、2017）の第10章に記載されていますが、ここではCBOWモデルについて簡単に説明します。</font><font style="vertical-align: inherit;">ただし、ほとんどの場合、事前トレーニング済みの単語の添付ファイルを使用して、既存のタスクに適合させるだけで十分です。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">事前訓練された単語の添付ファイルの実用的な使用</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
この章の大部分、および本の残りの部分では、事前にトレーニングされた単語の添付ファイルの使用について説明します。</font><font style="vertical-align: inherit;">上記の多くの方法の1つを使用して、大規模なボディ（Googleニュース、Wikipedia、Common Crawl1など）で事前トレーニングします。単語の添付ファイルを自由にダウンロードして使用できます。</font><font style="vertical-align: inherit;">さらにこの章では、これらの添付ファイルを正しく見つけて読み込む方法、単語の埋め込みのいくつかのプロパティを研究する方法、およびNLPタスクで事前トレーニング済みの単語の埋め込みを使用する例を示します。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">添付ファイルをダウンロード</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
単語の添付ファイルは非常に人気があり、広く普及しているため、元のWord2Vec2からStanford GloVe（</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">https://stanford.io/2PSIvPZ</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">）まで、FacebookのFastText3（</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">https://fasttext.cc</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">）など、</font><font style="vertical-align: inherit;">さまざまなオプションをダウンロードでき</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">ます。 /</font></a><font style="vertical-align: inherit;">）その他多数。通常、添付ファイルは次の形式で配信されます。各行は単語/タイプで始まり、その後に一連の数字（つまり、ベクトル表現）が続きます。このシーケンスの長さは、プレゼンテーションのサイズ（添付ファイルのサイズ）と同じです。投資の規模は通常、数百程度です。トークンのタイプの数は、ほとんどの場合、ディクショナリのサイズに等しく、約100万になります。たとえば、GloVeの犬と猫のベクトルの最初の7次元は次のとおりです。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/n5/g7/x0/n5g7x0mos74bxchmiuexmcl7yw8.png" alt="画像"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
添付ファイルの効率的な読み込みと処理のために、ヘルパークラスPreTrainedEmbeddingsを記述します（例5.1）。</font><font style="vertical-align: inherit;">これは、RAMに格納されているすべての単語ベクトルのインデックスを作成し、近似最近傍計算パッケージannoyを利用して最近傍の高速検索とクエリを簡素化します。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
例5.1。</font><font style="vertical-align: inherit;">事前トレーニング済みのWord添付ファイルの使用</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/nx/hh/hh/nxhhhhz9wsdjl1tzms5ur8e4bvk.png" alt="画像"></div><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/y1/8u/vb/y18uvbmsjpbvwno_wp5dsui8_38.png" alt="画像"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
これらの例では、GloVeという単語の埋め込みを使用しています。</font><font style="vertical-align: inherit;">例5.1の入力[1]に示すように、それらをダウンロードして、PreTrainedEmbeddingsクラスのインスタンスを作成する必要があります。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">単語の添付ファイル間の関係</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
単語の埋め込みの重要な特性は、構文的および意味的関係のエンコーディングであり、単語の使用パターンの形で表されます。たとえば、猫と犬は通常非常によく似て話されます（彼らはペット、摂食習慣などについて話します）。その結果、猫と犬という言葉の愛着は、アヒルや象など、他の動物の名前への愛着よりもはるかに近くなります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
単語の埋め込みにエンコードされた意味関係を研究する方法はたくさんあります。最も一般的な方法の1つは、類推タスク（SATなどの試験における論理的思考タスクの一般的なタイプの1つ）を使用することです</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
。Word1：Word2 :: Word3：______</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
このタスクでは、最初の2ワードと3ワードの間の接続を考慮して、4番目を決定する必要があります。単語をネストすることで、この問題を空間的にエンコードできます。まず、Word1からWord2を減算します。それらの間の差分ベクトルは、Word1とWord2の間の関係をエンコードします。次に、この差をSlovo3に追加すると、結果は4番目の欠落語に最も近いベクトルになります。類推の問題を解決するには、この取得したベクトルを使用して、インデックスによって最近傍をクエリするだけで十分です。例5.2に示した対応する関数は、上記の説明とまったく同じです。これは、ベクトル計算と最近傍の近似インデックスを使用して、アナロジーで欠落している要素を見つけます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
例5.2。単語の埋め込みを使用して類推の問題を解決する</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/q3/qv/sl/q3qvslovjualenm1jkjn6sftbtk.png" alt="画像"></div><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/1u/i_/hi/1ui_hizmvkfh5xqy1aqkegqqjfc.png" alt="画像"></div><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
興味深いことに、単純な言葉の類比を使用して、単語の埋め込みがさまざまな意味的および構文的関係をどのようにキャプチャできるかを示すことができます（例5.3）。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
例5.3 </font><font style="vertical-align: inherit;">SATのアナロジーでのタスクの例について、多くの言語的つながりの単語をネストすることでコーディングする</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/q1/rf/v0/q1rfv0qixjwl9mob64rfru-2vgw.png" alt="画像"></div><br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/hk/wt/j0/hkwtj0_ea88pxvm-2q3zelcjlfq.png" alt="画像"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
つながりが言語の機能を明確に反映しているように見えるかもしれませんが、すべてがそれほど単純なわけではありません。</font><font style="vertical-align: inherit;">例5.4が示すように、単語のベクトルはそれらの同時発生に基づいて決定されるため、接続は誤って定義される可能性があります。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
例5.4。</font><font style="vertical-align: inherit;">共起に基づいて単語の意味をコーディングすることの危険性を示す例-うまくいかない場合があります！</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/b1/xf/t3/b1xft37n3jlu6lwivpjpi_shnmm.png" alt="画像"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
例5.5は、性別の役割をエンコードする場合の最も一般的な組み合わせの1つを示しています。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
例5.5 </font><font style="vertical-align: inherit;">添付ファイルによってエンコードされた性別などの保護された属性に注意してください。</font><font style="vertical-align: inherit;">それらは、将来のモデルで望ましくないバイアスにつながる可能性があります。</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/ga/5y/cl/ga5yclk4wb32h5rpwujbl96yry0.png" alt="画像"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
言語のパターンと根強い文化的偏見を区別するのは非常に難しいことがわかりました。</font><font style="vertical-align: inherit;">たとえば、医師は必ずしも男性であるとは限らず、看護師は必ずしも女性であるとは限りませんが、そのような偏見は解決されるため、例5.6に示すように、言語に反映され、結果として単語ベクトルに反映されます。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
例5.6。</font><font style="vertical-align: inherit;">文化的偏見が言葉のベクトルに「縫い付けられる」</font></font><br>
<br>
<div style="text-align:center;"><img src="https://habrastorage.org/webt/oz/ap/xz/ozapxzj5jxnwwxdcq33sc_dig9q.png" alt="画像"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
NLPアプリケーションでの人気と普及率の増加を考慮に入れて、投資で起こり得る体系的なエラーを忘れてはなりません。</font><font style="vertical-align: inherit;">単語の埋め込みにおける系統的エラーの根絶は、科学研究の新しく非常に興味深い分野です（Bolukbashi et al。[Bolukbasi et al。、2016]の記事を参照）。</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">横断的</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">倫理とNLPに関する最新情報が記載さ</font><font style="vertical-align: inherit;">れている</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;">ethicsinnlp.orgをご覧</font></a><font style="vertical-align: inherit;">になる</font><font style="vertical-align: inherit;">ことをお勧めします</font><font style="vertical-align: inherit;">。</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">著者について</font></font></h3><br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Delip Rao</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">は、サンフランシスコを拠点とするコンサルティング会社Joostwareの創設者で、機械学習とNLP研究を専門としています。フェイクニュースチャレンジの共同創設者の1人-AIの分野のハッカーと研究者を、メディアでの事実確認のタスクについてまとめるように設計されたイニシアチブ。 Delipは以前、TwitterとAmazon（Alexa）でNLP関連の研究とソフトウェア製品に取り組んでいました。</font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ブライアンマクマハン</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">はウェルズファーゴのリサーチフェローで、主にNLPに焦点を当てています。以前はJoostwareで働いていました。</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
»本の詳細については、</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">出版社のWebサイトをご覧ください</font></font></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
» </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">目次</font></font></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
» </font><b><font style="vertical-align: inherit;">おいしい</font></b><font style="vertical-align: inherit;"> 
エージェントの</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">抜粋</font></font></a><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 25％</font><b><font style="vertical-align: inherit;">割引</font></b><font style="vertical-align: inherit;">クーポン</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">-PyTorch</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
本の紙のバージョンの支払い時に、電子本は電子メールで送られます。</font></font></div>
      
    </div>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../ja475474/index.html">ほとんどの場合、XMLは誤って適用されます</a></li>
<li><a href="../ja475476/index.html">データメッシュ：モノリスなしでデータを操作する方法</a></li>
<li><a href="../ja475478/index.html">Netflixエクスペリエンス：Netflix Inside</a></li>
<li><a href="../ja475480/index.html">あなたは何者ですか？なりすましと人間を区別する方法</a></li>
<li><a href="../ja475486/index.html">ARクリエーター：新しい職業の出現</a></li>
<li><a href="../ja475490/index.html">圧力下での作業</a></li>
<li><a href="../ja475494/index.html">「シニョーラの後に人生はありますか？」またはSECR-2019で話し合うこと</a></li>
<li><a href="../ja475496/index.html">展開前にスマートコントラクトのアドレスを決定する方法：暗号交換にCREATE2を使用する</a></li>
<li><a href="../ja475498/index.html">Windows Server CoreとGUIおよびソフトウェアの互換性</a></li>
<li><a href="../ja475506/index.html">ベルリンでの仕事と生活についてのミハイルチンコフへのインタビュー</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter63335242 = new Ya.Metrika({
                  id:63335242,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/63335242" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-13', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Geek Week | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=i62cJ2037o_BACd40gCrIso3niu0Sjx2sDFYJkeYdRk&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>